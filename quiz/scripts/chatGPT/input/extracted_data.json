[
  {
    "No": "1",
    "question": "A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain\ncloud.example.com for the resources stored within VPCs.\nThe company has the following DNS resolution requirements:\nOn-premises systems should be able to resolve and connect to cloud.example.com.\nAll VPCs should be able to resolve cloud.example.com.\nThere is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.\nWhich architecture should the company use to meet these requirements with the HIGHEST performance?",
    "choices": [
      {
        "key": "A",
        "text": "Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the"
      },
      {
        "key": "B",
        "text": "Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all"
      },
      {
        "key": "C",
        "text": "Associate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs"
      },
      {
        "key": "D",
        "text": "Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (74%) D (26%)",
    "page_images": []
  },
  {
    "No": "2",
    "question": "A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated\nwith different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of\nweather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the\nability to fail over to a different AWS Region.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda"
      },
      {
        "key": "B",
        "text": "Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both"
      },
      {
        "key": "C",
        "text": "Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target"
      },
      {
        "key": "D",
        "text": "Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "3",
    "question": "A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the\nProduction OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.\nThe company recently acquired a new business unit and invited the new unit's existing AWS account to the organization. Once onboarded, the\nadministrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company's policies.\nWhich option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term\nmaintenance?",
    "choices": [
      {
        "key": "A",
        "text": "Remove the organization's root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company's standard"
      },
      {
        "key": "B",
        "text": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the"
      },
      {
        "key": "C",
        "text": "Convert the organization's root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to"
      },
      {
        "key": "D",
        "text": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (86%) 14%",
    "page_images": []
  },
  {
    "No": "4",
    "question": "A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a\nstateful application. The application connects to a PostgreSQL database running on a separate server. The application's user base is expected to\ngrow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon\nEC2 Auto Scaling, and Elastic Load Balancing.\nWhich solution will provide a consistent user experience that will allow the application and database tiers to scale?",
    "choices": [
      {
        "key": "A",
        "text": "Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and"
      },
      {
        "key": "B",
        "text": "Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions"
      },
      {
        "key": "C",
        "text": "Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled."
      },
      {
        "key": "D",
        "text": "Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (92%) 8%",
    "page_images": []
  },
  {
    "No": "5",
    "question": "A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and\ninternet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are\npresent in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to\nolder devices, which the company identified by the User-Agent headers.\nThe company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company\nhas already migrated the applications into a set of AWS Lambda functions.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each"
      },
      {
        "key": "C",
        "text": "Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each"
      },
      {
        "key": "D",
        "text": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (44%) D (22%) B (19%) Other",
    "page_images": []
  },
  {
    "No": "6",
    "question": "A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3\nbucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to\naccess the files from its own AWS account (Account B).\nWhich combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A."
      },
      {
        "key": "B",
        "text": "In Account A, set the S3 bucket policy to the following: [image_4_0] [image_4_1] [image_4_2]"
      },
      {
        "key": "C",
        "text": "In Account A, set the S3 bucket policy to the following: [image_4_0] [image_4_1] [image_4_2]"
      },
      {
        "key": "D",
        "text": "In Account B, set the permissions of User_DataProcessor to the following: [image_4_0] [image_4_1] [image_4_2]"
      },
      {
        "key": "E",
        "text": "In Account B, set the permissions of User_DataProcessor to the following: [image_4_0] [image_4_1] [image_4_2]"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (73%) D (23%)",
    "page_images": [
      "image_4_0.png",
      "image_4_1.png",
      "image_4_2.png",
      "image_4_3.png"
    ]
  },
  {
    "No": "7",
    "question": "A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices\nthat run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is\nvariable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless\narchitecture that minimizes operational complexity.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle"
      },
      {
        "key": "B",
        "text": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container"
      },
      {
        "key": "C",
        "text": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes"
      },
      {
        "key": "D",
        "text": "Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (81%) Other",
    "page_images": []
  },
  {
    "No": "8",
    "question": "A company has a multi-tier web application that runs on a fieet of Amazon EC2 instances behind an Application Load Balancer (ALB). The\ninstances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the\nmaximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application's data. The DB instance\nhas a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.\nThe company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region.\nThe company does not have a large enough budget for an active-active strategy.\nWhat should a solutions architect recommend to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Reconfigure the application's Route 53 record with a latency-based routing policy that load balances trafic between the two ALBs. Create"
      },
      {
        "key": "B",
        "text": "Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure"
      },
      {
        "key": "C",
        "text": "Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region."
      },
      {
        "key": "D",
        "text": "Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "9",
    "question": "A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node\ncluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application\nto function, each piece of the infrastructure must be healthy and must be in an active state.\nA solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least\npossible downtime.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Use an Elastic Load Balancer to distribute trafic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling"
      },
      {
        "key": "B",
        "text": "Use an Elastic Load Balancer to distribute trafic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited"
      },
      {
        "key": "C",
        "text": "Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in"
      },
      {
        "key": "D",
        "text": "Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones."
      },
      {
        "key": "E",
        "text": "Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum"
      },
      {
        "key": "F",
        "text": "Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ADF (96%)",
    "page_images": []
  },
  {
    "No": "10",
    "question": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load\nBalancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that\npoints to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.\nAfter an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP\nheaders that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the\nerror occurs.\nWhile the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page\nto visitors.\nWhich combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3."
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is"
      },
      {
        "key": "C",
        "text": "Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS"
      },
      {
        "key": "D",
        "text": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than"
      },
      {
        "key": "0",
        "text": "Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server."
      },
      {
        "key": "E",
        "text": "Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "AE (96%)",
    "page_images": []
  },
  {
    "No": "11",
    "question": "A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the\ncompany can use to share a common network across multiple accounts.\nThe company's infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to\nmanage the network. Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to\ncreate AWS resources within subnets.\nWhich combination of actions should the solutions architect perform to meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Create a transit gateway in the infrastructure account."
      },
      {
        "key": "B",
        "text": "Enable resource sharing from the AWS Organizations management account."
      },
      {
        "key": "C",
        "text": "Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and"
      },
      {
        "key": "D",
        "text": "Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will"
      },
      {
        "key": "E",
        "text": "Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BD (94%) 6%",
    "page_images": []
  },
  {
    "No": "12",
    "question": "A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API\ncalls. The third-party SaaS application also runs on AWS inside a VPC.\nThe company will consume the third-party SaaS application from inside a VPC. The company has internal security policies that mandate the use of\nprivate connectivity that does not traverse the internet. No resources that run in the company VPC are allowed to be accessed from outside the\ncompany's VPC. All permissions must conform to the principles of least privilege.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application"
      },
      {
        "key": "B",
        "text": "Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. Configure network ACLs to limit"
      },
      {
        "key": "C",
        "text": "Create a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed"
      },
      {
        "key": "D",
        "text": "Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (97%)",
    "page_images": []
  },
  {
    "No": "13",
    "question": "A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to\nperform patching. Management requires a single report showing the patch status of all the servers and instances.\nWhich set of actions should a solutions architect take to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch"
      },
      {
        "key": "B",
        "text": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks"
      },
      {
        "key": "C",
        "text": "Use an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to"
      },
      {
        "key": "D",
        "text": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "14",
    "question": "A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on\nthe application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied\nto a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2\ninstances.\nWhich set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?",
    "choices": [
      {
        "key": "A",
        "text": "Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and"
      },
      {
        "key": "B",
        "text": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an"
      },
      {
        "key": "C",
        "text": "Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data."
      },
      {
        "key": "D",
        "text": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "15",
    "question": "A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The\ncompany's applications and databases are running in Account B.\nA solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the\nAmazon RDS endpoint was created in a private hosted zone for Amazon Route 53.\nDuring deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance.\nThe solutions architect confirmed that the record set was created correctly in Route 53.\nWhich combination of steps should the solutions architect take to resolve this issue? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance's private IP in the private hosted zone."
      },
      {
        "key": "B",
        "text": "Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file."
      },
      {
        "key": "C",
        "text": "Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B."
      },
      {
        "key": "D",
        "text": "Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts."
      },
      {
        "key": "E",
        "text": "Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CE (100%)",
    "page_images": []
  },
  {
    "No": "16",
    "question": "A company used Amazon EC2 instances to deploy a web fieet to host a blog site. The EC2 instances are behind an Application Load Balancer\n(ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.\nThe company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user trafic. At peak times of day,\nusers report buffering and timeout issues while attempting to reach the site or watch videos.\nWhich is the MOST cost-eficient and scalable deployment that will resolve the issues for users?",
    "choices": [
      {
        "key": "A",
        "text": "Reconfigure Amazon EFS to enable maximum I/O."
      },
      {
        "key": "B",
        "text": "Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at"
      },
      {
        "key": "C",
        "text": "Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3."
      },
      {
        "key": "D",
        "text": "Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (98%)",
    "page_images": []
  },
  {
    "No": "17",
    "question": "A company with global ofices has a single 1 Gbps AWS Direct Connect connection to a single AWS Region. The company's on-premises network\nuses the connection to communicate with the company's resources in the AWS Cloud. The connection has a single private virtual interface that\nconnects to a single VPC.\nA solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. The solution also must\nprovide connectivity to other Regions through the same pair of Direct Connect connections as the company expands into other Regions.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct"
      },
      {
        "key": "B",
        "text": "Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new private virtual interface on the new"
      },
      {
        "key": "C",
        "text": "Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new public virtual interface on the new"
      },
      {
        "key": "D",
        "text": "Provision a transit gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "18",
    "question": "A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by\ncustom recognition software for categorization.\nThe website contains static content that has variable trafic with peaks in certain months. The architecture consists of Amazon EC2 instances\nrunning in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue.\nThe company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove\ndependencies on third-party software.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace"
      },
      {
        "key": "B",
        "text": "Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue"
      },
      {
        "key": "C",
        "text": "Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS"
      },
      {
        "key": "D",
        "text": "Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (85%) D (15%)",
    "page_images": []
  },
  {
    "No": "19",
    "question": "A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current\ndeployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the\nnew function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to\ndecrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert\nwhen errors are identified.\nHow can this be accomplished?",
    "choices": [
      {
        "key": "A",
        "text": "Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API"
      },
      {
        "key": "B",
        "text": "Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift trafic to the new version, and use pre-trafic"
      },
      {
        "key": "C",
        "text": "Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests"
      },
      {
        "key": "D",
        "text": "Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "20",
    "question": "A company is planning to store a large number of archived documents and make the documents available to employees through the corporate\nintranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible\nto the public.\nThe documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low.\nAvailability and speed of retrieval are not concerns of the company.\nWhich solution will meet these requirements at the LOWEST cost?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default."
      },
      {
        "key": "B",
        "text": "Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the"
      },
      {
        "key": "C",
        "text": "Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived"
      },
      {
        "key": "D",
        "text": "Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (65%) D (34%)",
    "page_images": []
  },
  {
    "No": "21",
    "question": "A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to\nsign in to the company's AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-\npremises environment and all the company's AWS accounts.\nThe company's security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a\nsingle location.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0. Enable automatic provisioning"
      },
      {
        "key": "B",
        "text": "Configure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source. Enable automatic provisioning"
      },
      {
        "key": "C",
        "text": "In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provider."
      },
      {
        "key": "D",
        "text": "In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (82%) Other",
    "page_images": []
  },
  {
    "No": "22",
    "question": "A software company has deployed an application that consumes a REST API by using Amazon API Gateway, AWS Lambda functions, and an\nAmazon DynamoDB table. The application is showing an increase in the number of errors during PUT requests. Most of the PUT calls come from a\nsmall number of clients that are authenticated with specific API keys.\nA solutions architect has identified that a large number of the PUT requests originate from one client. The API is noncritical, and clients can\ntolerate retries of unsuccessful calls. However, the errors are displayed to customers and are causing damage to the API's reputation.\nWhat should the solutions architect recommend to improve the customer experience?",
    "choices": [
      {
        "key": "A",
        "text": "Implement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and"
      },
      {
        "key": "B",
        "text": "Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without"
      },
      {
        "key": "C",
        "text": "Turn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is"
      },
      {
        "key": "D",
        "text": "Implement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in trafic."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (68%) A (31%)",
    "page_images": []
  },
  {
    "No": "23",
    "question": "A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file\nsystem also runs on several EC2 instances that store 200 TB of data. The application reads and modifies the data on the shared file system and\ngenerates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete. The\ncompute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage\ninstances are all in the same AWS Region.\nA solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access\nto the needed data for the duration of the 72-hour run.\nWhich solution will provide the LARGEST overall cost reduction while meeting these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the"
      },
      {
        "key": "B",
        "text": "Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach"
      },
      {
        "key": "C",
        "text": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs"
      },
      {
        "key": "D",
        "text": "Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (92%) 8%",
    "page_images": []
  },
  {
    "No": "24",
    "question": "A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is\nhighly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible.\nThe service must use fixed address assignments so other companies can add the addresses to their allow lists.\nAssuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static"
      },
      {
        "key": "B",
        "text": "Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create"
      },
      {
        "key": "C",
        "text": "Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer"
      },
      {
        "key": "D",
        "text": "Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "25",
    "question": "A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the\ncompany's data center.\nThe system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes\nand 2 hours to finish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in\nless than 5 minutes and have no SLA. The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to\nmeet SLAs. However, user jobs can be delayed.\nA solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-\nterm commitments. The solution must maintain high availability and must not affect the SLAs.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Split the 12 instances across two Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand"
      },
      {
        "key": "B",
        "text": "Split the 12 instances across three Availability Zones in the chosen AWS Region. In one of the Availability Zones, run all four instances as"
      },
      {
        "key": "C",
        "text": "Split the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand"
      },
      {
        "key": "D",
        "text": "Split the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On-"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (90%) 10%",
    "page_images": []
  },
  {
    "No": "26",
    "question": "A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in\nAmazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve\nsecurity:\nThe database must use strong, randomly generated passwords stored in a secure AWS managed service.\nThe application resources must be deployed through AWS CloudFormation.\nThe application must rotate credentials for the database every 90 days.\nA solutions architect will generate a CloudFormation template to deploy the application.\nWhich resources specified in the CloudFormation template will meet the security engineer's requirements with the LEAST amount of operational\noverhead?",
    "choices": [
      {
        "key": "A",
        "text": "Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the"
      },
      {
        "key": "B",
        "text": "Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda"
      },
      {
        "key": "C",
        "text": "Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the"
      },
      {
        "key": "D",
        "text": "Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "27",
    "question": "A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data\naccessible publicly through a simple API over HTTPS. The solution must scale automatically in response to demand.\nWhich solutions meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway's AWS integration"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to Dynamo DB by using API Gateway's AWS"
      },
      {
        "key": "C",
        "text": "Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the"
      },
      {
        "key": "D",
        "text": "Create an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data"
      },
      {
        "key": "E",
        "text": "Create a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "AC (83%) Other",
    "page_images": []
  },
  {
    "No": "28",
    "question": "A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will\nredirect online visitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are\nmanaged by Amazon Route 53.\nA solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.\nWhich combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose\nthree.)",
    "choices": [
      {
        "key": "A",
        "text": "Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with"
      },
      {
        "key": "B",
        "text": "Create an Application Load Balancer that includes HTTP and HTTPS listeners."
      },
      {
        "key": "C",
        "text": "Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a"
      },
      {
        "key": "D",
        "text": "Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function."
      },
      {
        "key": "E",
        "text": "Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function."
      },
      {
        "key": "F",
        "text": "Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CEF (68%) BCF (22%) 5%",
    "page_images": []
  },
  {
    "No": "29",
    "question": "A company that has multiple AWS accounts is using AWS Organizations. The company's AWS accounts host VPCs, Amazon EC2 instances, and\ncontainers.\nThe company's compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2\ninstances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-\nrelated resources with a key of “costCenter” and a value or “compliance”.\nThe company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the\ncompliance team's AWS account. The cost calculation must be as accurate as possible.\nWhat should a solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports"
      },
      {
        "key": "B",
        "text": "In the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to"
      },
      {
        "key": "C",
        "text": "In the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly"
      },
      {
        "key": "D",
        "text": "Create a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (96%)",
    "page_images": []
  },
  {
    "No": "30",
    "question": "A company has 50 AWS accounts that are members of an organization in AWS Organizations. Each account contains multiple VPCs. The company\nwants to use AWS Transit Gateway to establish connectivity between the VPCs in each member account. Each time a new member account is\ncreated, the company wants to automate the process of creating a new VPC and a transit gateway attachment.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager."
      },
      {
        "key": "B",
        "text": "From the management account, share the transit gateway with member accounts by using an AWS Organizations SCP."
      },
      {
        "key": "C",
        "text": "Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway"
      },
      {
        "key": "D",
        "text": "Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a peering transit"
      },
      {
        "key": "E",
        "text": "From the management account, share the transit gateway with member accounts by using AWS Service Catalog."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (100%)",
    "page_images": []
  },
  {
    "No": "31",
    "question": "An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS\nOrganizations account structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by\nprocurement managers. The procurement team's policy indicates that developers should be able to obtain third-party software from an approved\nlist only and use Private Marketplace in AWS Marketplace to achieve this requirement. The procurement team wants administration of Private\nMarketplace to be restricted to a role named procurement-manager-role, which could be assumed by procurement managers. Other IAM users,\ngroups, roles, and account administrators in the company should be denied Private Marketplace administrative access.\nWhat is the MOST eficient way to design an architecture to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the PowerUserAccess managed policy to"
      },
      {
        "key": "B",
        "text": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the AdministratorAccess managed"
      },
      {
        "key": "C",
        "text": "Create an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the"
      },
      {
        "key": "D",
        "text": "Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developers. Add the"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (91%) 9%",
    "page_images": []
  },
  {
    "No": "32",
    "question": "A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon\nDynamoDB. The developers account resides in a dedicated organizational unit (OU). The solutions architect has implemented the following SCP\non the developers account:\nWhen this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy.\nWhat should the solutions architect do to eliminate the developers' ability to use services outside the scope of this policy?",
    "choices": [
      {
        "key": "A",
        "text": "Create an explicit deny statement for each AWS service that should be constrained."
      },
      {
        "key": "B",
        "text": "Remove the FullAWSAccess SCP from the developers account's OU."
      },
      {
        "key": "C",
        "text": "Modify the FullAWSAccess SCP to explicitly deny all services."
      },
      {
        "key": "D",
        "text": "Add an explicit deny statement using a wildcard to the end of the SCP. [image_20_0]"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (81%) Other",
    "page_images": [
      "image_20_0.png"
    ]
  },
  {
    "No": "33",
    "question": "A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC. Mobile clients\nconnect to the API by using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing\npolicy with the IP addresses of all the EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to trafic. The app\nhas not been able to keep up with the trafic.\nA solutions architect needs to implement a solution so that the app can handle the new and varying load.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Separate the API into individual AWS Lambda functions. Configure an Amazon API Gateway REST API with Lambda integration for the"
      },
      {
        "key": "B",
        "text": "Containerize the API logic. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Run the containers in the cluster by using"
      },
      {
        "key": "C",
        "text": "Create an Auto Scaling group. Place all the EC2 instances in the Auto Scaling group. Configure the Auto Scaling group to perform scaling"
      },
      {
        "key": "D",
        "text": "Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (56%) C (24%) D (20%)",
    "page_images": []
  },
  {
    "No": "34",
    "question": "A company has created an OU in AWS Organizations for each of its engineering teams. Each OU owns multiple AWS accounts. The organization\nhas hundreds of AWS accounts.\nA solutions architect must design a solution so that each OU can view a breakdown of usage costs across its AWS accounts.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access Manager. Allow each team to visualize the CUR"
      },
      {
        "key": "B",
        "text": "Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR"
      },
      {
        "key": "C",
        "text": "Create an AWS Cost and Usage Report (CUR) in each AWS Organizations member account. Allow each team to visualize the CUR through an"
      },
      {
        "key": "D",
        "text": "Create an AWS Cost and Usage Report (CUR) by using AWS Systems Manager. Allow each team to visualize the CUR through Systems"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "35",
    "question": "A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily.\nThe company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company\nalready has established an AWS Direct Connect connection between the on-premises network and AWS.\nWhich data migration strategy should the company use?",
    "choices": [
      {
        "key": "A",
        "text": "Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new"
      },
      {
        "key": "B",
        "text": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx."
      },
      {
        "key": "C",
        "text": "Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File"
      },
      {
        "key": "D",
        "text": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (63%) A (38%)",
    "page_images": []
  },
  {
    "No": "36",
    "question": "A company's solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3\nbucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a\nsecond Region.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Configure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set by using"
      },
      {
        "key": "B",
        "text": "Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda"
      },
      {
        "key": "C",
        "text": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront"
      },
      {
        "key": "D",
        "text": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (94%) 3%",
    "page_images": []
  },
  {
    "No": "37",
    "question": "A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in trafic that resulted in downtime and a\nsignificant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a\ndependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily\nusers.\nWhich steps should the solutions architect take to design an appropriate solution?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance."
      },
      {
        "key": "B",
        "text": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group"
      },
      {
        "key": "C",
        "text": "Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application"
      },
      {
        "key": "D",
        "text": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (94%) 4%",
    "page_images": []
  },
  {
    "No": "38",
    "question": "A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an\nAmazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations\nmember accounts.\nA solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of\nCloudFormation stacks. Trusted access has been enabled in Organizations.\nWhat should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
    "choices": [
      {
        "key": "A",
        "text": "Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an"
      },
      {
        "key": "B",
        "text": "Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization."
      },
      {
        "key": "C",
        "text": "Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the"
      },
      {
        "key": "D",
        "text": "Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "39",
    "question": "A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux and Windows. The company has a large on-\npremises infrastructure that consists of physical machines and VMs that host numerous applications.\nThe company must capture details about the system configuration, system performance, running processes, and network connections of its on-\npremises workloads. The company also must divide the on-premises applications into groups for AWS migrations. The company needs\nrecommendations for Amazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effective manner.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs."
      },
      {
        "key": "B",
        "text": "Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs."
      },
      {
        "key": "C",
        "text": "Group servers into applications for migration by using AWS Systems Manager Application Manager."
      },
      {
        "key": "D",
        "text": "Group servers into applications for migration by using AWS Migration Hub."
      },
      {
        "key": "E",
        "text": "Generate recommended instance types and associated costs by using AWS Migration Hub."
      },
      {
        "key": "F",
        "text": "Import data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ADE (91%) 9%",
    "page_images": []
  },
  {
    "No": "40",
    "question": "A company is hosting an image-processing service on AWS in a VPC. The VPC extends across two Availability Zones. Each Availability Zone\ncontains one public subnet and one private subnet.\nThe service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service.\nThe service needs to communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The\nEC2 instances retrieve approximately 1 ТВ of data from an S3 bucket each day.\nThe company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without\ncompromising the service's security posture or increasing the time spent on ongoing operations.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Replace the NAT gateways with NAT instances. In the VPC route table, create a route from the private subnets to the NAT instances."
      },
      {
        "key": "B",
        "text": "Move the EC2 instances to the public subnets. Remove the NAT gateways."
      },
      {
        "key": "C",
        "text": "Set up an S3 gateway VPC endpoint in the VPAttach an endpoint policy to the endpoint to allow the required actions on the S3 bucket."
      },
      {
        "key": "D",
        "text": "Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances. Host the images on the EFS volume."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "41",
    "question": "A company recently deployed an application on AWS. The application uses Amazon DynamoDB. The company measured the application load and\nconfigured the RCUs and WCUs on the DynamoDB table to match the expected peak load. The peak load occurs once a week for a 4-hour period\nand is double the average load. The application load is close to the average load for the rest of the week. The access pattern includes many more\nwrites to the table than reads of the table.\nA solutions architect needs to implement a solution to minimize the cost of the table.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average"
      },
      {
        "key": "B",
        "text": "Configure on-demand capacity mode for the table."
      },
      {
        "key": "C",
        "text": "Configure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table."
      },
      {
        "key": "D",
        "text": "Configure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (72%) B (17%) 11%",
    "page_images": []
  },
  {
    "No": "42",
    "question": "A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users\nupload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a\nmessage queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting\nprocessing is significantly higher during business hours, with the number of files rapidly declining after business hours.\nWhat is the MOST cost-effective migration recommendation?",
    "choices": [
      {
        "key": "A",
        "text": "Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue,"
      },
      {
        "key": "B",
        "text": "Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue,"
      },
      {
        "key": "C",
        "text": "Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue,"
      },
      {
        "key": "D",
        "text": "Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (94%) 3%",
    "page_images": []
  },
  {
    "No": "43",
    "question": "A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes\nfrom an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the\ncompany deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.\nThe company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Replace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier"
      },
      {
        "key": "B",
        "text": "Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to"
      },
      {
        "key": "C",
        "text": "Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to"
      },
      {
        "key": "D",
        "text": "Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": []
  },
  {
    "No": "44",
    "question": "A company has 10 accounts that are part of an organization in AWS Organizations. AWS Config is configured in each account. All accounts belong\nto either the Prod OU or the NonProd OU.\nThe company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic\nwhen an Amazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source. The company's security team is subscribed to the SNS\ntopic.\nFor all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0\nas the source.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic."
      },
      {
        "key": "B",
        "text": "Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU."
      },
      {
        "key": "C",
        "text": "Configure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0."
      },
      {
        "key": "D",
        "text": "Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (58%) A (39%)",
    "page_images": []
  },
  {
    "No": "45",
    "question": "A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud.\nThe company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an\nApplication Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a\nserverless architecture.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "For each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs."
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to"
      },
      {
        "key": "C",
        "text": "Deploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB"
      },
      {
        "key": "D",
        "text": "Containerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (89%) 11%",
    "page_images": []
  },
  {
    "No": "46",
    "question": "A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company's data center. As\npart of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and\nrunning processes. The company then wants to query and analyze the data.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in"
      },
      {
        "key": "B",
        "text": "Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update"
      },
      {
        "key": "C",
        "text": "Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-"
      },
      {
        "key": "D",
        "text": "Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (89%) 11%",
    "page_images": []
  },
  {
    "No": "47",
    "question": "A company is building a serverless application that runs on an AWS Lambda function that is attached to a VPC. The company needs to integrate\nthe application with a new service from an external provider. The external provider supports only requests that come from public IPv4 addresses\nthat are in an allow list.\nThe company must provide a single public IP address to the external provider before the application can start using the new service.\nWhich solution will give the application the ability to access the new service?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway."
      },
      {
        "key": "B",
        "text": "Deploy an egress-only internet gateway. Associate an Elastic IP address with the egress-only internet gateway. Configure the elastic"
      },
      {
        "key": "C",
        "text": "Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the Lambda function to use the internet"
      },
      {
        "key": "D",
        "text": "Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the default route in the public VPC route"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (94%) 4%",
    "page_images": []
  },
  {
    "No": "48",
    "question": "A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The\nconsumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an\nAmazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.\nDuring testing, the application does not meet performance requirements. Under high load, the application opens a large number of database\nconnections. The solutions architect must improve the application's performance.\nWhich actions should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Use the cluster endpoint of the Aurora database."
      },
      {
        "key": "B",
        "text": "Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database."
      },
      {
        "key": "C",
        "text": "Use the Lambda Provisioned Concurrency feature."
      },
      {
        "key": "D",
        "text": "Move the code for opening the database connection in the Lambda function outside of the event handler."
      },
      {
        "key": "E",
        "text": "Change the API Gateway endpoint to an edge-optimized endpoint."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BD (98%)",
    "page_images": []
  },
  {
    "No": "49",
    "question": "A company is planning to host a web application on AWS and wants to load balance the trafic across a group of Amazon EC2 instances. One of\nthe security requirements is to enable end-to-end encryption in transit between the client and the web server.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "key": "A",
        "text": "Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM),"
      },
      {
        "key": "B",
        "text": "Associate the EC2 instances with a target group. Provision an SSL certificate using AWS Certificate Manager (ACM). Create an Amazon"
      },
      {
        "key": "C",
        "text": "Place the EC2 instances behind an Application Load Balancer (ALB) Provision an SSL certificate using AWS Certificate Manager (ACM), and"
      },
      {
        "key": "D",
        "text": "Place the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (56%) D (34%) 9%",
    "page_images": []
  },
  {
    "No": "50",
    "question": "A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js\napplications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into\nreports. When the aggregation jobs run, some of the load jobs fail to run correctly.\nThe company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the\ncompany's customers.\nWhat should a solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora"
      },
      {
        "key": "B",
        "text": "Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from"
      },
      {
        "key": "C",
        "text": "Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from"
      },
      {
        "key": "D",
        "text": "Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (94%) 6%",
    "page_images": []
  },
  {
    "No": "51",
    "question": "A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption\nwith S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket\nmust be encrypted by keys that the company's security team manages. The S3 bucket does not have versioning enabled.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all"
      },
      {
        "key": "B",
        "text": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS)."
      },
      {
        "key": "C",
        "text": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS)."
      },
      {
        "key": "D",
        "text": "In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (58%) D (41%)",
    "page_images": []
  },
  {
    "No": "52",
    "question": "A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2\ninstances. The EC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).\nThe company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an\norigin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.\nA solutions architect must configure the application so that itis highly available and fault tolerant.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Provision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add"
      },
      {
        "key": "B",
        "text": "Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a"
      },
      {
        "key": "C",
        "text": "Provision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the"
      },
      {
        "key": "D",
        "text": "Provision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "53",
    "question": "A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a\ntransit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured\nbetween all of the company's global ofices and the transit account. The company has AWS Config enabled on all of its accounts.\nThe company's networking team needs to centrally manage a list of internal IP address ranges that belong to the global ofices. Developers will\nreference this list to gain access to their applications securely.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification"
      },
      {
        "key": "B",
        "text": "Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each"
      },
      {
        "key": "C",
        "text": "In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the"
      },
      {
        "key": "D",
        "text": "In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "54",
    "question": "A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and\nuses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API\nmethod.\nThe company wants to create a CSV report every 2 weeks to show each API Lambda function's recommended configured memory, recommended\ncost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.\nWhich solution will meet these requirements with the LEAST development time?",
    "choices": [
      {
        "key": "A",
        "text": "Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period."
      },
      {
        "key": "B",
        "text": "Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the"
      },
      {
        "key": "C",
        "text": "Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export"
      },
      {
        "key": "D",
        "text": "Purchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (97%)",
    "page_images": []
  },
  {
    "No": "55",
    "question": "A company's factory and automation applications are running in a single VPC. More than 20 applications run on a combination of Amazon EC2,\nAmazon Elastic Container Service (Amazon ECS), and Amazon RDS.\nThe company has software engineers spread across three teams. One of the three teams owns each application, and each time is responsible for\nthe cost and performance of all of its applications. Team resources have tags that represent their application and team. The teams use IAM\naccess for daily activities.\nThe company needs to determine which costs on the monthly AWS bill are attributable to each application or team. The company also must be\nable to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must\nrecommend an AWS Billing and Cost Management solution that provides these cost reports.\nWhich combination of actions will meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Activate the user-define cost allocation tags that represent the application and the team."
      },
      {
        "key": "B",
        "text": "Activate the AWS generated cost allocation tags that represent the application and the team."
      },
      {
        "key": "C",
        "text": "Create a cost category for each application in Billing and Cost Management."
      },
      {
        "key": "D",
        "text": "Activate IAM access to Billing and Cost Management."
      },
      {
        "key": "E",
        "text": "Create a cost budget."
      },
      {
        "key": "F",
        "text": "Enable Cost Explorer."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACF (58%) ADF (42%)",
    "page_images": []
  },
  {
    "No": "56",
    "question": "An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall.\nThe third party accepts only one public CIDR block in each client's allow list.\nThe customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind\nan Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT\ngateways provide internet access to the private subnets.\nHow should a solutions architect ensure that the web application can continue to call the third-party API after the migration?",
    "choices": [
      {
        "key": "A",
        "text": "Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC."
      },
      {
        "key": "B",
        "text": "Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and"
      },
      {
        "key": "C",
        "text": "Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB."
      },
      {
        "key": "D",
        "text": "Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "57",
    "question": "A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An administrator created the following\nSCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:\nDevelopers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the administrator address this\nproblem?",
    "choices": [
      {
        "key": "A",
        "text": "Add s3:CreateBucket with “Allow” effect to the SCP. [image_34_0]"
      },
      {
        "key": "B",
        "text": "Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111."
      },
      {
        "key": "C",
        "text": "Instruct the developers to add Amazon S3 permissions to their IAM entities."
      },
      {
        "key": "D",
        "text": "Remove the SCP from account 1111-1111-1111."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (84%) A (16%)",
    "page_images": [
      "image_34_0.png"
    ]
  },
  {
    "No": "58",
    "question": "A company has a monolithic application that is critical to the company's business. The company hosts the application on an Amazon EC2\ninstance that runs Amazon Linux 2. The company's application team receives a directive from the legal department to back up the data from the\ninstance's encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the\nadministrative SSH key pair for the instance. The application must continue to serve the users.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain [image_34_0]"
      },
      {
        "key": "B",
        "text": "Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new"
      },
      {
        "key": "C",
        "text": "Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3."
      },
      {
        "key": "D",
        "text": "Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (63%) C (36%)",
    "page_images": [
      "image_34_0.png"
    ]
  },
  {
    "No": "59",
    "question": "A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucket in a new AWS account. The solutions\narchitect must implement a solution that uses the AWS CLI.\nWhich combination of steps will successfully copy the data? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket."
      },
      {
        "key": "B",
        "text": "Create a bucket policy to allow a user in the destination account to list the source bucket's contents and read the source bucket's objects."
      },
      {
        "key": "C",
        "text": "Create an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the"
      },
      {
        "key": "D",
        "text": "Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get"
      },
      {
        "key": "E",
        "text": "Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data."
      },
      {
        "key": "F",
        "text": "Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDF (92%) 5%",
    "page_images": []
  },
  {
    "No": "60",
    "question": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web\napplication introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to\nsupport a canary release.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config"
      },
      {
        "key": "B",
        "text": "Deploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load."
      },
      {
        "key": "C",
        "text": "Create a version for every new deployed Lambda function. Use the AWS CLI update-function-configuration command with the routing-config"
      },
      {
        "key": "D",
        "text": "Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (97%)",
    "page_images": []
  },
  {
    "No": "61",
    "question": "A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties.\nThe company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the files are uploaded, they are moved to the\ndata lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route\n53.\nWhat should a solutions architect do to improve the reliability and scalability of the SFTP solution?",
    "choices": [
      {
        "key": "A",
        "text": "Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS"
      },
      {
        "key": "B",
        "text": "Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint"
      },
      {
        "key": "C",
        "text": "Migrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the file"
      },
      {
        "key": "D",
        "text": "Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "62",
    "question": "A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions\narchitect must preserve the software and configuration settings during the migration.\nWhat should the solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the"
      },
      {
        "key": "B",
        "text": "Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3"
      },
      {
        "key": "C",
        "text": "Configure AWS Storage Gateway for files service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared"
      },
      {
        "key": "D",
        "text": "Create a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "63",
    "question": "A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed\nimage in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and\nruns by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.\nThe application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing\nfrequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application's\narchitecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic"
      },
      {
        "key": "B",
        "text": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task"
      },
      {
        "key": "C",
        "text": "Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of"
      },
      {
        "key": "D",
        "text": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task"
      },
      {
        "key": "E",
        "text": "Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "AB (90%) 7%",
    "page_images": []
  },
  {
    "No": "64",
    "question": "A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization.\nThe company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB\ninstances that are not encrypted at rest in the company's production OU.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "key": "A",
        "text": "Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU."
      },
      {
        "key": "B",
        "text": "Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the"
      },
      {
        "key": "C",
        "text": "Use AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU."
      },
      {
        "key": "D",
        "text": "Create a custom SCP in AWS Control Tower. Apply the SCP to the production OU."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": []
  },
  {
    "No": "65",
    "question": "A startup company hosts a fieet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company's engineers rely\nheavily on SSH access to the instances for troubleshooting.\nThe company's existing architecture includes the following:\n• A VPC with private and public subnets, and a NAT gateway.\n• Site-to-Site VPN for connectivity with the on-premises environment.\n• EC2 security groups with direct SSH access from the on-premises environment.\nThe company needs to increase security controls around SSH access and provide auditing of commands run by the engineers.\nWhich strategy should a solutions architect use?",
    "choices": [
      {
        "key": "A",
        "text": "Install and configure EC2 Instance Connect on the fieet of EC2 instances. Remove all security group rules attached to EC2 instances that"
      },
      {
        "key": "B",
        "text": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Install the Amazon"
      },
      {
        "key": "C",
        "text": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Enable AWS Config for"
      },
      {
        "key": "D",
        "text": "Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (91%) 9%",
    "page_images": []
  },
  {
    "No": "66",
    "question": "A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed,\ndevelopers use their company email address to request an account. The company wants to ensure that developers are not launching costly\nservices or running services unnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts."
      },
      {
        "key": "B",
        "text": "Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process."
      },
      {
        "key": "C",
        "text": "Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts."
      },
      {
        "key": "D",
        "text": "Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts."
      },
      {
        "key": "E",
        "text": "Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all"
      },
      {
        "key": "F",
        "text": "Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BCF (77%) BDF (21%)",
    "page_images": []
  },
  {
    "No": "67",
    "question": "A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the\napplications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions\nby using a deployment package. The company has configured automated backups for Aurora.\nThe company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application\nprocesses critical data, so the company must minimize downtime.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda"
      },
      {
        "key": "B",
        "text": "Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda"
      },
      {
        "key": "C",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant"
      },
      {
        "key": "D",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (97%)",
    "page_images": []
  },
  {
    "No": "68",
    "question": "A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an\nAmazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess\na file that the script has already processed.\nThe company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the\nfile processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term\nmanagement overhead.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the"
      },
      {
        "key": "B",
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create"
      },
      {
        "key": "C",
        "text": "Migrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to"
      },
      {
        "key": "D",
        "text": "Migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (80%) D (20%)",
    "page_images": []
  },
  {
    "No": "69",
    "question": "A financial services company in North America plans to release a new online web application to its customers on AWS. The company will launch\nthe application in the us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet\nuser trafic. The company also wants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-\npassive failover.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB)"
      },
      {
        "key": "B",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across"
      },
      {
        "key": "C",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across"
      },
      {
        "key": "D",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB)"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "70",
    "question": "A company has an environment that has a single AWS account. A solutions architect is reviewing the environment to recommend what the\ncompany could improve specifically in terms of access to the AWS Management Console. The company's IT support workers currently access the\nconsole for administrative tasks, authenticating with named IAM users that have been mapped to their job role.\nThe IT support workers no longer want to maintain both their Active Directory and IAM user accounts. They want to be able to access the console\nby using their existing Active Directory credentials. The solutions architect is using AWS IAM Identity Center (AWS Single Sign-On) to implement\nthis functionality.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure a directory in"
      },
      {
        "key": "B",
        "text": "Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure an AD"
      },
      {
        "key": "C",
        "text": "Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure a directory in AWS Directory"
      },
      {
        "key": "D",
        "text": "Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure an AD Connector to connect to"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (80%) B (15%)5%",
    "page_images": []
  },
  {
    "No": "71",
    "question": "A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-\neast-1 Region. The files range in size from 1 GB to 10 GB.\nUsers who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload\nfor these users. A solutions architect must improve the app's performance for these uploads.\nWhich solutions will meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads."
      },
      {
        "key": "B",
        "text": "Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3"
      },
      {
        "key": "C",
        "text": "Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region."
      },
      {
        "key": "D",
        "text": "Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3."
      },
      {
        "key": "E",
        "text": "Modify the app to add random prefixes to the files before uploading."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (96%)",
    "page_images": []
  },
  {
    "No": "72",
    "question": "An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After a failover test, the application lost the\nconnections to the database and could not re-establish the connections. After a restart of the application, the application re-established the\nconnections.\nA solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Aurora MySQL Serverless v1 DB instance. Migrate the RDS DB instance to the Aurora Serverless v1 DB instance. Update"
      },
      {
        "key": "B",
        "text": "Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS"
      },
      {
        "key": "C",
        "text": "Create a two-node Amazon Aurora MySQL DB cluster. Migrate the RDS DB instance to the Aurora DB cluster. Create an RDS proxy. Configure"
      },
      {
        "key": "D",
        "text": "Create an Amazon S3 bucket. Export the database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Configure Amazon"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "73",
    "question": "A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution and send data. Each device needs to be able\nto send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Set up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to"
      },
      {
        "key": "B",
        "text": "Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in"
      },
      {
        "key": "C",
        "text": "Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT"
      },
      {
        "key": "D",
        "text": "Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (95%) 5%",
    "page_images": []
  },
  {
    "No": "74",
    "question": "A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved\nresources and that engineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to\nenforce the new restriction on the IAM role that the engineers use for access.\nWhat should the solutions architect do to create the solution?",
    "choices": [
      {
        "key": "A",
        "text": "Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket. Update the IAM policy for the engineers'"
      },
      {
        "key": "B",
        "text": "Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS"
      },
      {
        "key": "C",
        "text": "Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy"
      },
      {
        "key": "D",
        "text": "Provision resources in AWS CloudFormation stacks. Update the IAM policy for the engineers' IAM role to only allow access to their own"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (97%)",
    "page_images": []
  },
  {
    "No": "75",
    "question": "A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The\napplication is designed to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and\nneeds to be stored in a durable location where it can be retrieved with low latency. The data is ephemeral and the company is required to store the\ndata for 120 days only, after which the data can be deleted.\nThe solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.\nWhich storage strategy is the MOST cost-effective and meets the design requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Design the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieval. Configure a"
      },
      {
        "key": "B",
        "text": "Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the"
      },
      {
        "key": "C",
        "text": "Design the application to store each incoming record in a single table in an Amazon RDS MySQL database. Run a nightly cron job that runs"
      },
      {
        "key": "D",
        "text": "Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (75%) D (25%)",
    "page_images": []
  },
  {
    "No": "76",
    "question": "A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all\ntimes for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.\nWhich solution will provide the HIGHEST availability for the database?",
    "choices": [
      {
        "key": "A",
        "text": "Configure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance."
      },
      {
        "key": "B",
        "text": "Configure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to"
      },
      {
        "key": "C",
        "text": "Configure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from"
      },
      {
        "key": "D",
        "text": "Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (91%) 9%",
    "page_images": []
  },
  {
    "No": "77",
    "question": "Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to\nVPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A. Example Corp. just acquired AnyCompany, which\nhas a VPC named VPC B. There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B.\nExample Corp. wants to connect from its on-premise servers to VPC B. Example Corp. has properly set up the network ACL and security groups.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "choices": [
      {
        "key": "A",
        "text": "Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for"
      },
      {
        "key": "B",
        "text": "Create a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN"
      },
      {
        "key": "C",
        "text": "Update the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks."
      },
      {
        "key": "D",
        "text": "Modify the Site-to-Site VPN's virtual private gateway definition to include VPC A and VPC B. Split the two routers of the virtual private"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (87%) 9%",
    "page_images": []
  },
  {
    "No": "78",
    "question": "A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the\nmigrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends\noutbound email messages to the company's customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The\napplication can use SMTP only.\nThe company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has\ncreated and validated the SES domain. The company has lifted the SES limits.\nWhat should the company do to modify the application to send email messages from Amazon SES?",
    "choices": [
      {
        "key": "A",
        "text": "Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and"
      },
      {
        "key": "B",
        "text": "Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to"
      },
      {
        "key": "C",
        "text": "Configure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail"
      },
      {
        "key": "D",
        "text": "Configure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (85%) Other",
    "page_images": []
  },
  {
    "No": "79",
    "question": "A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method.\nThe acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found\nit dificult to generate a cost report that contains meaningful groups for all the teams.\nThe acquiring company's finance team needs a solution to report on costs for all the companies through a self-managed application.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena."
      },
      {
        "key": "B",
        "text": "Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in"
      },
      {
        "key": "C",
        "text": "Create an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the"
      },
      {
        "key": "D",
        "text": "Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "80",
    "question": "A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company's Node.js API servers on Amazon EC2\ninstances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General\nPurpose SSD volume.\nThe number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are\nconsistently overloaded and RDS metrics show high write latency.\nWhich of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this\nplatform cost-eficient? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS."
      },
      {
        "key": "B",
        "text": "Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas."
      },
      {
        "key": "C",
        "text": "Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data."
      },
      {
        "key": "D",
        "text": "Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load."
      },
      {
        "key": "E",
        "text": "Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "CE (63%) BC (18%) Other",
    "page_images": []
  },
  {
    "No": "81",
    "question": "A company is building an electronic document management system in which users upload their documents. The application stack is entirely\nserverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for\ndelivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs\ncall AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.\nThe company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of\nEurope.\nWhich combination of actions will meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs."
      },
      {
        "key": "B",
        "text": "Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution."
      },
      {
        "key": "C",
        "text": "Change the API Gateway Regional endpoints to edge-optimized endpoints."
      },
      {
        "key": "D",
        "text": "Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster."
      },
      {
        "key": "E",
        "text": "Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (45%) CD (40%) Other",
    "page_images": []
  },
  {
    "No": "82",
    "question": "An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and\nvideos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.\nThe company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are\naccessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions\narchitect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure S3 Intelligent-Tiering on the S3 bucket."
      },
      {
        "key": "B",
        "text": "Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days."
      },
      {
        "key": "C",
        "text": "Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances."
      },
      {
        "key": "D",
        "text": "Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (95%) 5%",
    "page_images": []
  },
  {
    "No": "83",
    "question": "A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during\nthe past year.\nA solutions architect needs to review data trends for the past 12 months and identity the appropriate storage class for the objects.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Download AWS Cost and Usage Reports for the last 12 months of S3 usage. Review AWS Trusted Advisor recommendations for cost"
      },
      {
        "key": "B",
        "text": "Use S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard to analyze storage trends."
      },
      {
        "key": "C",
        "text": "Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends."
      },
      {
        "key": "D",
        "text": "Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 months. Import the .csv file to an Amazon"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (78%) 12% 10%",
    "page_images": []
  },
  {
    "No": "84",
    "question": "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently\ndeployed in one AWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts.\nWhat should the solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions."
      },
      {
        "key": "B",
        "text": "Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage"
      },
      {
        "key": "C",
        "text": "Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary"
      },
      {
        "key": "D",
        "text": "Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "85",
    "question": "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently\ndeployed in one AWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts.\nWhat should the solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions."
      },
      {
        "key": "B",
        "text": "Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage"
      },
      {
        "key": "C",
        "text": "Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary"
      },
      {
        "key": "D",
        "text": "Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "86",
    "question": "A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be\nupgraded to support the modern design for the application with the following requirements:\n• It should allow changes to be released several times every hour.\n• It should be able to roll back the changes as quickly as possible.\nWhich design will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing"
      },
      {
        "key": "B",
        "text": "Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To"
      },
      {
        "key": "C",
        "text": "Use AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code"
      },
      {
        "key": "D",
        "text": "Roll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances. and"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "87",
    "question": "A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where\nthe application needs to access an Amazon Aurora DB Cluster. The EC2 instances are all associated with the same security group. The DB cluster\nis associated with its own security group.\nThe solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Add an inbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the source over the default Aurora port."
      },
      {
        "key": "B",
        "text": "Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora"
      },
      {
        "key": "C",
        "text": "Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port."
      },
      {
        "key": "D",
        "text": "Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the default Aurora"
      },
      {
        "key": "E",
        "text": "Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the ephemeral"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BC (76%) AC (24%)",
    "page_images": []
  },
  {
    "No": "88",
    "question": "A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports\nfor overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for\neach business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team\nwants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for\nany cloud spending that exceeds a set threshold.\nWhich solution is the MOST cost-effective way to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each"
      },
      {
        "key": "B",
        "text": "Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application,"
      },
      {
        "key": "C",
        "text": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each"
      },
      {
        "key": "D",
        "text": "Enable AWS Cost and Usage Reports in the organization's management account and configure reports grouped by application, environment."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "89",
    "question": "A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is\ndeleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.\nHow can the company prevent users from accidentally deleting data in this way?",
    "choices": [
      {
        "key": "A",
        "text": "Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources."
      },
      {
        "key": "B",
        "text": "Configure a stack policy that disallows the deletion of RDS and EBS resources."
      },
      {
        "key": "C",
        "text": "Modify IAM policies lo deny deleting RDS and EBS resources that are tagged with an \"aws:cloudformation:stack-name\" tag."
      },
      {
        "key": "D",
        "text": "Use AWS Config rules to prevent deleting RDS and EBS resources."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (85%) B (15%)",
    "page_images": []
  },
  {
    "No": "90",
    "question": "A company has VPC fiow logs enabled for Its NAT gateway. The company is seeing Action = ACCEPT for inbound trafic that comes from public IP\naddress 198.51.100.2 destined for a private Amazon EC2 instance.\nA solutions architect must determine whether the trafic represents unsolicited inbound connections from the internet. The first two octets of the\nVPC CIDR block are 203.0.\nWhich set of steps should the solutions architect take to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's"
      },
      {
        "key": "B",
        "text": "Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private"
      },
      {
        "key": "C",
        "text": "Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's"
      },
      {
        "key": "D",
        "text": "Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (66%) D (34%)",
    "page_images": []
  },
  {
    "No": "91",
    "question": "A company consists or two separate business units. Each business unit has its own AWS account within a single organization in AWS\nOrganizations. The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3\nbucket in each account and configured low-way replication between the S3 buckets. The S3 buckets have millions of objects.\nRecently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be\nstored with encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).\nWhat is the MOST operationally eficient solution that meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location."
      },
      {
        "key": "B",
        "text": "Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on"
      },
      {
        "key": "C",
        "text": "Turn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI."
      },
      {
        "key": "D",
        "text": "Create an AWS Key Management Service, (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (85%) Other",
    "page_images": []
  },
  {
    "No": "92",
    "question": "A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3\nbucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes\nevery day.\nThe company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must\nretain all the data indefinitely for compliance reasons.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive."
      },
      {
        "key": "B",
        "text": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old 10 S3 Glacier"
      },
      {
        "key": "C",
        "text": "Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1"
      },
      {
        "key": "D",
        "text": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (87%) 10%",
    "page_images": []
  },
  {
    "No": "93",
    "question": "A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of\nfiles in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises\nfor ML experiments and wants to use AWS.\nThe company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be\nencrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps. and multiple departments share the\nconnection.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a"
      },
      {
        "key": "B",
        "text": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN"
      },
      {
        "key": "C",
        "text": "Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN"
      },
      {
        "key": "D",
        "text": "Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "94",
    "question": "A company has migrated Its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files\nthrough a web application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on\nAmazon EC2 instances and an Amazon RDS for PostgreSQL database.\nWhen forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team\nmember then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before entering\nthe information into another system that uses an API.\nA solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction. minimize time\nto market, and minimize tong-term operational overhead.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Develop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes"
      },
      {
        "key": "B",
        "text": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence"
      },
      {
        "key": "C",
        "text": "Host a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine teaming (AI/ML)"
      },
      {
        "key": "D",
        "text": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": []
  },
  {
    "No": "95",
    "question": "A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a\nfieet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the\norders. The company does not want to make any major changes to the application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up"
      },
      {
        "key": "B",
        "text": "Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end"
      },
      {
        "key": "C",
        "text": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up"
      },
      {
        "key": "D",
        "text": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (87%) 13%",
    "page_images": []
  },
  {
    "No": "96",
    "question": "A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The\nsolutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.\nThe solutions architect created the following IAM policy and attached it to an IAM role:\nDuring tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object\nresulted in an error message. The error message stated that the action was forbidden.\nWhich action must the solutions architect add to the IAM policy to meet all the requirements?",
    "choices": [
      {
        "key": "A",
        "text": "kms:GenerateDataKey"
      },
      {
        "key": "B",
        "text": "kms:GetKeyPolicy"
      },
      {
        "key": "C",
        "text": "kms:GetPublicKey"
      },
      {
        "key": "D",
        "text": "kms:Sign"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [
      "image_55_0.png"
    ]
  },
  {
    "No": "97",
    "question": "A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application\nLoad Balancer. The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not\nadversely affect legitimate trafic to the application.\nHow should a solutions architect configure the web ACLs to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid"
      },
      {
        "key": "B",
        "text": "Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the"
      },
      {
        "key": "C",
        "text": "Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using"
      },
      {
        "key": "D",
        "text": "Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "98",
    "question": "A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company\nmanages common security group rules for the AWS accounts in the organization.\nThe company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company's on-premises\nnetwork. Developers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own\nAWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.\nThe solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS account. Deploy an AWS Lambda function in"
      },
      {
        "key": "B",
        "text": "Create new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all"
      },
      {
        "key": "C",
        "text": "Create a new customer-managed prefix list in the security team's AWS account. Populate the customer-managed prefix list with all internal"
      },
      {
        "key": "D",
        "text": "Create an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (84%) D (16%)",
    "page_images": []
  },
  {
    "No": "99",
    "question": "A company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a VPN. The company is\nhosting internal applications with VPCs in multiple AWS accounts. Currently, the applications are accessible from the company's on-premises\nofice network through an AWS Site-to-Site VPN connection. The VPC in the company's main AWS account has peering connections established\nwith VPCs in other AWS accounts.\nA solutions architect must design a scalable AWS Client VPN solution for employees to use while they work from home.\nWhat is the MOST cost-effective solution that meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create a Client VPN endpoint in each AWS account. Configure required routing that allows access to internal applications."
      },
      {
        "key": "B",
        "text": "Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications."
      },
      {
        "key": "C",
        "text": "Create a Client VPN endpoint in the main AWS account. Provision a transit gateway that is connected to each AWS account. Configure"
      },
      {
        "key": "D",
        "text": "Create a Client VPN endpoint in the main AWS account. Establish connectivity between the Client VPN endpoint and the AWS Site-to-Site"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (66%) C (34%)",
    "page_images": []
  },
  {
    "No": "100",
    "question": "A company is running an application in the AWS Cloud. Recent application metrics show inconsistent response times and a significant increase in\nerror rates. Calls to third-party services are causing the delays. Currently, the application calls third-party services synchronously by directly\ninvoking an AWS Lambda function.\nA solutions architect needs to decouple the third-party service calls and ensure that all the calls are eventually completed.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function."
      },
      {
        "key": "B",
        "text": "Use an AWS Step Functions state machine to pass events to the Lambda function."
      },
      {
        "key": "C",
        "text": "Use an Amazon EventBridge rule to pass events to the Lambda function."
      },
      {
        "key": "D",
        "text": "Use an Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "101",
    "question": "A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS\naccounts in AWS Organizations.\nThe sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The\nmarketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key\nManagement Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in\nthe marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3"
      },
      {
        "key": "B",
        "text": "Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the"
      },
      {
        "key": "C",
        "text": "Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that"
      },
      {
        "key": "D",
        "text": "Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (74%) C (15%) 8%",
    "page_images": []
  },
  {
    "No": "102",
    "question": "A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises\ninstallation of a Microsoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service. A solutions\narchitect must design a heterogeneous database migration on AWS.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities."
      },
      {
        "key": "B",
        "text": "Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration"
      },
      {
        "key": "C",
        "text": "Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration"
      },
      {
        "key": "D",
        "text": "Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "103",
    "question": "A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the\nicons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account\nthat members of the design team can access.\nAfter the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the\nproduction account. A solutions architect must provide the design team with access to the production account without exposing other parts of the\nweb application to the risk of unwanted changes.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "In the production account, create a new IAM policy that allows read and write access to the S3 bucket."
      },
      {
        "key": "B",
        "text": "In the development account, create a new IAM policy that allows read and write access to the S3 bucket."
      },
      {
        "key": "C",
        "text": "In the production account, create a role Attach the new policy to the role. Define the development account as a trusted entity."
      },
      {
        "key": "D",
        "text": "In the development account, create a role. Attach the new policy to the role Define the production account as a trusted entity."
      },
      {
        "key": "E",
        "text": "In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to"
      },
      {
        "key": "F",
        "text": "In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACE (92%) 8%",
    "page_images": []
  },
  {
    "No": "104",
    "question": "A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's\ndevelopment team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU\nthan expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.\nA solutions architect must mitigate the performance issues before the company launches the application to production.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that"
      },
      {
        "key": "B",
        "text": "Create a second Elastic Beanstalk environment. Apply the trafic-splitting deployment policy. Specify a percentage of incoming trafic to"
      },
      {
        "key": "C",
        "text": "Modify the existing environment's capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a"
      },
      {
        "key": "D",
        "text": "Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": []
  },
  {
    "No": "105",
    "question": "A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed\nMySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of trafic during the month.\nHowever, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load\nBalancers and Auto Scaling within its infrastructure to meet the increased demand.\nWhich of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?",
    "choices": [
      {
        "key": "A",
        "text": "Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes."
      },
      {
        "key": "B",
        "text": "Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load"
      },
      {
        "key": "C",
        "text": "Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific"
      },
      {
        "key": "D",
        "text": "Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (95%) 5%",
    "page_images": []
  },
  {
    "No": "106",
    "question": "A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable. but\nthe company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative\noverhead to maintain the servers.\nWhich solution will meet these requirements with the LEAST code changes?",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container"
      },
      {
        "key": "B",
        "text": "Migrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration."
      },
      {
        "key": "C",
        "text": "Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container."
      },
      {
        "key": "D",
        "text": "Migrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (90%) 10%",
    "page_images": []
  },
  {
    "No": "107",
    "question": "A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes\nthe Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign\nthe application to support failover to another AWS Region.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an API Gateway endpoint in the us-west-2 Region to direct trafic to the Lambda function in us-east-1. Configure Amazon Route 53 to"
      },
      {
        "key": "B",
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct trafic to the SQS queue instead of to the"
      },
      {
        "key": "C",
        "text": "Deploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 10 direct trafic to the Lambda function"
      },
      {
        "key": "D",
        "text": "Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (92%) 8%",
    "page_images": []
  },
  {
    "No": "108",
    "question": "A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated\nbilling and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has\nmultiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and\nproduction.\nThe HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved\nInstances (RIs) in its production AWS account. The HR department will install the new application on this account. The HR department wants to\nmake sure that other departments cannot share the RI discounts.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "In the AWS Billing and Cost Management console for the HR department's production account turn off RI sharing."
      },
      {
        "key": "B",
        "text": "Remove the HR department's production AWS account from the organization. Add the account 10 the consolidating billing configuration"
      },
      {
        "key": "C",
        "text": "In the AWS Billing and Cost Management console. use the organization's management account 10 turn off RI Sharing for the HR"
      },
      {
        "key": "D",
        "text": "Create an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (82%) D (18%)",
    "page_images": []
  },
  {
    "No": "109",
    "question": "A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a\nprivate subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager\nSession Manager is configured, and AWS Systems Manager Agent is running on all the EC2 instances.\nThe company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being\nterminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon\nCloudWatch logs that are collected from the application, but the logs are inconclusive.\nHow should the solutions architect gain access to an EC2 instance to troubleshoot the issue?",
    "choices": [
      {
        "key": "A",
        "text": "Suspend the Auto Scaling group's HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy."
      },
      {
        "key": "B",
        "text": "Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy."
      },
      {
        "key": "C",
        "text": "Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an"
      },
      {
        "key": "D",
        "text": "Suspend the Auto Scaling group's Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (89%) 11%",
    "page_images": []
  },
  {
    "No": "110",
    "question": "A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under\ndifferent OUs in AWS Organizations.\nAdministrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the\nability to automatically update and remediate noncompliant AWS WAF rules in all accounts.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store"
      },
      {
        "key": "B",
        "text": "Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy"
      },
      {
        "key": "C",
        "text": "Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers"
      },
      {
        "key": "D",
        "text": "Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "111",
    "question": "A solutions architect is auditing the security setup or an AWS Lambda function for a company. The Lambda function retrieves, the latest changes\nfrom an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the\ndatabase credentials to the Lambda function.\nThe Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with\nAWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised,\nthe company needs a solution that minimizes the impact of the compromise.\nWhat should the solutions architect recommend to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access"
      },
      {
        "key": "B",
        "text": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access"
      },
      {
        "key": "C",
        "text": "Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store."
      },
      {
        "key": "D",
        "text": "Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (84%) D (16%)",
    "page_images": []
  },
  {
    "No": "112",
    "question": "A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is\nreviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected\nFramework.\nWhile reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several\nlarge instance types account for a high proportion of the costs. The solutions architect finds out that the company's developers are launching new\nAmazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types.\nThe solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to"
      },
      {
        "key": "B",
        "text": "In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the"
      },
      {
        "key": "C",
        "text": "Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for"
      },
      {
        "key": "D",
        "text": "Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "113",
    "question": "A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the\nsame organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team\nresponsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.\nWhich actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Config rule in each account to find resources with missing tags."
      },
      {
        "key": "B",
        "text": "Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing."
      },
      {
        "key": "C",
        "text": "Use Amazon Inspector in the organization to find resources with missing tags."
      },
      {
        "key": "D",
        "text": "Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing."
      },
      {
        "key": "E",
        "text": "Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag."
      },
      {
        "key": "F",
        "text": "Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ABE (82%) Other",
    "page_images": []
  },
  {
    "No": "114",
    "question": "A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due\nto heavy ingestion and it frequently runs out of storage.\nThe company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should\ninclude the following attributes:\n• Managed AWS services to minimize operational complexity.\n• A buffer that automatically scales to match the throughput of data and requires no ongoing administration.\n• A visualization tool to create dashboards to observe events in near-real time.\n• Support for semi-structured JSON data and dynamic schemas.\nWhich combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events."
      },
      {
        "key": "B",
        "text": "Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events."
      },
      {
        "key": "C",
        "text": "Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-"
      },
      {
        "key": "D",
        "text": "Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-"
      },
      {
        "key": "E",
        "text": "Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (100%)",
    "page_images": []
  },
  {
    "No": "115",
    "question": "A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private\nsubnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company's applications read from and write to\nAmazon Kinesis Data Streams. Most of the workloads run in private subnets.\nA solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications.\nThe solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that\nNatGateway-Bytes charges are increasing the cost in the EC2-Other category.\nWhat should the solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for trafic that can be removed. Ensure that security groups are blocking"
      },
      {
        "key": "B",
        "text": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the"
      },
      {
        "key": "C",
        "text": "Enable VPC Flow Logs and Amazon Detective. Review Detective findings for trafic that is not related to Kinesis Data Streams. Configure"
      },
      {
        "key": "D",
        "text": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows trafic from the"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (87%) 13%",
    "page_images": []
  },
  {
    "No": "116",
    "question": "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and\nus-east-1 Regions. The company wants to be able to route network trafic from its on-premises infrastructure into VPCs in either of those Regions.\nThe company also needs to support trafic that is routed directly between VPCs in those Regions. No single points of failure can exist on the\nnetwork.\nThe company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a\nseparate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a\nsingle AWS Transit Gateway that is configured to route all inter-VPC trafic within that Region.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same"
      },
      {
        "key": "B",
        "text": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct"
      },
      {
        "key": "C",
        "text": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same"
      },
      {
        "key": "D",
        "text": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (93%) 3%",
    "page_images": []
  },
  {
    "No": "117",
    "question": "A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new\nIAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user.\nThe company has a multi-Region AWS CloudTrail trail in the AWS account.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via"
      },
      {
        "key": "B",
        "text": "Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic."
      },
      {
        "key": "C",
        "text": "Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access."
      },
      {
        "key": "D",
        "text": "Invoke an AWS Step Functions state machine to remove access."
      },
      {
        "key": "E",
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to notify the security team."
      },
      {
        "key": "F",
        "text": "Use Amazon Pinpoint to notify the security team."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ADE (88%) 6%",
    "page_images": []
  },
  {
    "No": "118",
    "question": "A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and\napplications. The company also wants to keep the trafic on a private network. Multi-factor authentication (MFA) is required at login, and specific\nroles are assigned to user groups.\nThe company must create separate accounts for development. staging, production, and shared network. The production account and the shared\nnetwork account must have connectivity to all accounts. The development account and the staging account must have access only to each other.\nWhich combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization"
      },
      {
        "key": "B",
        "text": "Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login."
      },
      {
        "key": "C",
        "text": "Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables."
      },
      {
        "key": "D",
        "text": "Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing"
      },
      {
        "key": "E",
        "text": "Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA"
      },
      {
        "key": "F",
        "text": "Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognoto user pools and Identity pools to manage access to"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ACD (100%)",
    "page_images": []
  },
  {
    "No": "119",
    "question": "A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production.\nAll the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases.\nThe databases are between 500 GB and 800 GB in size.\nThe development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7\ndays a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or\nproduction as the key.\nWhat should a solutions architect do to reduce costs with the LEAST operational effort?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or slops"
      },
      {
        "key": "B",
        "text": "Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that"
      },
      {
        "key": "C",
        "text": "Create an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that"
      },
      {
        "key": "D",
        "text": "Create an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "120",
    "question": "A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed an Amazon API Gateway REST API with AWS\nLambda integration in multiple AWS Regions and in the same production account.\nThe company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of API calls per second. The\npremium tier offers up to 3,000 calls per second, and customers are identified by a unique API key. Several premium tier customers in various\nRegions report that they receive error responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logs indicate\nthat the Lambda function is never invoked.\nWhat could be the cause of the error messages for these customers?",
    "choices": [
      {
        "key": "A",
        "text": "The Lambda function reached its concurrency limit."
      },
      {
        "key": "B",
        "text": "The Lambda function its Region limit for concurrency."
      },
      {
        "key": "C",
        "text": "The company reached its API Gateway account limit for calls per second."
      },
      {
        "key": "D",
        "text": "The company reached its API Gateway default per-method limit for calls per second."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "121",
    "question": "A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor\nthe inbound trafic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available\nfrom its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology.\nThe company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a\ndedicated VPC. The company needs to use the security tool to inspect all packets that come in and out of the VPC. This inspection must occur in\nreal time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available\nwithin an AWS Region.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC"
      },
      {
        "key": "B",
        "text": "Deploy the web application behind a Network Load Balancer"
      },
      {
        "key": "C",
        "text": "Deploy an Application Load Balancer in front of the security tool instances"
      },
      {
        "key": "D",
        "text": "Provision a Gateway Load Balancer for each Availability Zone to redirect the trafic to the security tool"
      },
      {
        "key": "E",
        "text": "Provision a transit gateway to facilitate communication between VPCs."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (54%) DE (40%) 4%",
    "page_images": []
  },
  {
    "No": "122",
    "question": "A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the\nvendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique\nformat. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis.\nThe company needs to design a new data analysis solution that can deliver faster and optimize costs.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to"
      },
      {
        "key": "B",
        "text": "Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a"
      },
      {
        "key": "C",
        "text": "Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use"
      },
      {
        "key": "D",
        "text": "Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (91%) 9%",
    "page_images": []
  },
  {
    "No": "123",
    "question": "A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes\nnetworking and security strategies. The company has set up an AWS Direct Connect connection in a central network account.\nThe company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the\nresources on AWS seamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to\nthe internet through its on-premises data center.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create a Direct Connect gateway in the central account. In each of the accounts, create an association proposal by using the Direct"
      },
      {
        "key": "B",
        "text": "Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect"
      },
      {
        "key": "C",
        "text": "Provision an internet gateway. Attach the internet gateway to subnets. Allow internet trafic through the gateway."
      },
      {
        "key": "D",
        "text": "Share the transit gateway with other accounts. Attach VPCs to the transit gateway."
      },
      {
        "key": "E",
        "text": "Provision VPC peering as necessary."
      },
      {
        "key": "F",
        "text": "Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet trafic"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BDF (100%)",
    "page_images": []
  },
  {
    "No": "124",
    "question": "A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved\nInstances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved\nInstances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances\nin their own respective AWS accounts autonomously.\nA solutions architect needs to enforce the new process in the most secure way possible.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled."
      },
      {
        "key": "B",
        "text": "Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and"
      },
      {
        "key": "C",
        "text": "In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the"
      },
      {
        "key": "D",
        "text": "Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the"
      },
      {
        "key": "E",
        "text": "Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (100%)",
    "page_images": []
  },
  {
    "No": "125",
    "question": "A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-\nAZ mode.\nA recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the\noutage time to less than 20 seconds.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon ElastiCache for Memcached in front of the database"
      },
      {
        "key": "B",
        "text": "Use Amazon ElastiCache for Redis in front of the database"
      },
      {
        "key": "C",
        "text": "Use RDS Proxy in front of the database."
      },
      {
        "key": "D",
        "text": "Migrate the database to Amazon Aurora MySQL."
      },
      {
        "key": "E",
        "text": "Create an Amazon Aurora Replica."
      },
      {
        "key": "F",
        "text": "Create an RDS for MySQL read replica"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CDE (89%) 11%",
    "page_images": []
  },
  {
    "No": "126",
    "question": "An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company\nto have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least\nprivilege security access using an API or command line tool to the customer account.\nWhat is the MOST secure way to allow org1 to access resources in org2?",
    "choices": [
      {
        "key": "A",
        "text": "The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks."
      },
      {
        "key": "B",
        "text": "The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the"
      },
      {
        "key": "C",
        "text": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM"
      },
      {
        "key": "D",
        "text": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": []
  },
  {
    "No": "127",
    "question": "A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a\npublic registry. The image can run in as many containers as required to generate the route map.\nThe company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the\nhubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom\nconfiguration that processes orders only in the section's area.\nThe company needs the ability to allocate resources cost-effectively based on the number of running containers.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning"
      },
      {
        "key": "B",
        "text": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning"
      },
      {
        "key": "C",
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch"
      },
      {
        "key": "D",
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (79%) B (21%)",
    "page_images": []
  },
  {
    "No": "128",
    "question": "A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of\nAmazon EC2 instances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account,\na shared services VPC is located in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS\nCloudFormation to attempt to peer the application VPC with the shared services VPC, an error message indicates a peering failure.\nWhich factors could cause this error? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "The IPv4 CIDR ranges of the two VPCs overlap"
      },
      {
        "key": "B",
        "text": "The VPCs are not in the same Region"
      },
      {
        "key": "C",
        "text": "One or both accounts do not have access to an Internet gateway"
      },
      {
        "key": "D",
        "text": "One of the VPCs was not shared through AWS Resource Access Manager"
      },
      {
        "key": "E",
        "text": "The IAM role in the peer accepter account does not have the correct permissions"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AE (84%) BE (16%)",
    "page_images": []
  },
  {
    "No": "129",
    "question": "An external audit of a company's serverless application reveals IAM policies that grant too many permissions. These policies are attached to the\ncompany's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to\nAmazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function\nneeds to complete its task.\nA solutions architect must determine which permissions each Lambda function needs.\nWhat should the solutions architect do to meet this requirement with the LEAST amount of effort?",
    "choices": [
      {
        "key": "A",
        "text": "Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and"
      },
      {
        "key": "B",
        "text": "Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access"
      },
      {
        "key": "C",
        "text": "Turn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda"
      },
      {
        "key": "D",
        "text": "Turn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "130",
    "question": "A solutions architect must analyze a company's Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine\nwhether the company is using resources eficiently. The company is running several large, high-memory EC2 instances to host database clusters\nthat are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and\nthe company has not identified a pattern.\nThe solutions architect must analyze the environment and take action based on the findings.\nWhich solution meets these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are"
      },
      {
        "key": "B",
        "text": "Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based"
      },
      {
        "key": "C",
        "text": "Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours."
      },
      {
        "key": "D",
        "text": "Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (94%) 6%",
    "page_images": []
  },
  {
    "No": "131",
    "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses\nAWS Transit Gateway for VPC connectivity across accounts.\nIn an AWS application account, the company's application team has deployed a web application that uses AWS Lambda and Amazon RDS. The\ncompany's database administrators have a separate DBA account and use the account to centrally manage all the databases across the\norganization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is\ndeployed m the application account.\nThe application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is\nmanually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in\nthe application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and\neliminates the need to manually share the secrets.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA"
      },
      {
        "key": "B",
        "text": "In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In"
      },
      {
        "key": "C",
        "text": "In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the"
      },
      {
        "key": "D",
        "text": "In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (78%) 8% 8%",
    "page_images": []
  },
  {
    "No": "132",
    "question": "A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs: Research and DataOps.\nBecause of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region.\nAdditionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types.\nA solutions architect must implement a solution that applies these restrictions. The solution must maximize operational eficiency and must\nminimize ongoing maintenance.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict"
      },
      {
        "key": "B",
        "text": "Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict"
      },
      {
        "key": "C",
        "text": "Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to"
      },
      {
        "key": "D",
        "text": "Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU,"
      },
      {
        "key": "E",
        "text": "Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "CE (100%)",
    "page_images": []
  },
  {
    "No": "133",
    "question": "A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites.\nThe company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon\nSQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an\nAmazon S3 bucket.\nThe company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the\nexisting Region. Results must be written to the existing S3 bucket in the current Region.\nWhich combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the SQS queue with the Lambda function to other Regions."
      },
      {
        "key": "B",
        "text": "Subscribe the SNS topic in each Region to the SQS queue."
      },
      {
        "key": "C",
        "text": "Subscribe the SQS queue in each Region to the SNS topic."
      },
      {
        "key": "D",
        "text": "Configure the SQS queue to publish URLs to SNS topics in each Region."
      },
      {
        "key": "E",
        "text": "Deploy the SNS topic and the Lambda function to other Regions."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (100%)",
    "page_images": []
  },
  {
    "No": "134",
    "question": "A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code\ncannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4\nhours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.\nWhich strategy should the solutions architect use?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours."
      },
      {
        "key": "B",
        "text": "Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours."
      },
      {
        "key": "C",
        "text": "Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours."
      },
      {
        "key": "D",
        "text": "Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": []
  },
  {
    "No": "135",
    "question": "A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week\nafter launch. Currently, the game consists of the following components deployed in a single AWS Region:\n• Amazon S3 bucket that stores game assets\n• Amazon DynamoDB table that stores player scores\nA solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.\nWhat should the solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new"
      },
      {
        "key": "C",
        "text": "Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront"
      },
      {
        "key": "D",
        "text": "Create another S3 bucket in the sine Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (88%) 13%",
    "page_images": []
  },
  {
    "No": "136",
    "question": "A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java\nbackend and a NoSQL MongoDB database to store subscriber data.\nThe company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and\nthe company cannot make changes to the application.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across"
      },
      {
        "key": "B",
        "text": "Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single"
      },
      {
        "key": "C",
        "text": "Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the"
      },
      {
        "key": "D",
        "text": "Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (86%) 14%",
    "page_images": []
  },
  {
    "No": "137",
    "question": "A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS\naccount to securely store images and media files that are used as content for the company's marketing campaigns. The creative team wants to\nshare the S3 bucket with the strategy team so that the strategy team can view the objects.\nA solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a\ncustom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when\nusers from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.\nThe solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only\nthe minimum permissions that they need.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the"
      },
      {
        "key": "B",
        "text": "Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key."
      },
      {
        "key": "C",
        "text": "Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role."
      },
      {
        "key": "D",
        "text": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user."
      },
      {
        "key": "E",
        "text": "Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role."
      },
      {
        "key": "F",
        "text": "Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ACF (100%)",
    "page_images": []
  },
  {
    "No": "138",
    "question": "A life sciences company is using a combination of open source tools to manage data analysis workfiows and Docker containers running on\nservers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN),\nand then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their\ngenomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days.\nThe company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual\njobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is\nexpecting 10-15 job requests each day.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge"
      },
      {
        "key": "B",
        "text": "Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to"
      },
      {
        "key": "C",
        "text": "Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS"
      },
      {
        "key": "D",
        "text": "Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (70%) D (30%)",
    "page_images": []
  },
  {
    "No": "139",
    "question": "A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application\nreads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device.\nThe company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2\ninstances across multiple Availability Zones.\nA solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also\nmust implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running\ninstances at any given point in time.\nWhich solution will meet these requirements with the LEAST management overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones"
      },
      {
        "key": "B",
        "text": "Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group"
      },
      {
        "key": "C",
        "text": "Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and"
      },
      {
        "key": "D",
        "text": "Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": []
  },
  {
    "No": "140",
    "question": "A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a\nstandalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email\ntemplate for acknowledgement email messages that populate customer data before the application sends the email message to the customer.\nThe company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3"
      },
      {
        "key": "B",
        "text": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an"
      },
      {
        "key": "C",
        "text": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple"
      },
      {
        "key": "D",
        "text": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (95%) 5%",
    "page_images": []
  },
  {
    "No": "141",
    "question": "A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a\nvideo Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.\nThe company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The\ncompany has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the\ndevelopment team when there are messages in the dead-letter queue.\nSeveral times during the day. the development team receives notification that messages are in the dead-letter queue and that videos have not\nbeen processed property. An investigation finds no errors m the application logs.\nHow can the company solve this problem?",
    "choices": [
      {
        "key": "A",
        "text": "Turn on termination protection tor the EC2 Instances"
      },
      {
        "key": "B",
        "text": "Update the visibility timeout for the SQS queue to 3 hours"
      },
      {
        "key": "C",
        "text": "Configure scale-in protection for the instances during processing"
      },
      {
        "key": "D",
        "text": "Update the redrive policy and set maxReceiveCount to 0."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (76%) D (21%)",
    "page_images": []
  },
  {
    "No": "142",
    "question": "A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API\nGateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.\nThe solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an\nauthenticated user\nWhich solution will meet these requirements with the LEAST amount of effort?",
    "choices": [
      {
        "key": "A",
        "text": "Create an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to"
      },
      {
        "key": "B",
        "text": "Remove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in"
      },
      {
        "key": "C",
        "text": "Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPCreate a resource policy, and"
      },
      {
        "key": "D",
        "text": "Deploy the Lambda functions inside the VPC Provision an EC2 instance, and install an Apache server. From the Apache server, call the"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "143",
    "question": "A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are\nupdated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.\nThe company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is\nslow from time to time.\nWhich combination of steps will resolve the us-east-1 performance issues? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-"
      },
      {
        "key": "B",
        "text": "Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1."
      },
      {
        "key": "C",
        "text": "Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1."
      },
      {
        "key": "D",
        "text": "Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1."
      },
      {
        "key": "E",
        "text": "Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BD (96%)",
    "page_images": []
  },
  {
    "No": "144",
    "question": "A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis\nindicates that the issue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as\nthe profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.\nThe solutions architect discovers that the file system has reached Its maximum capacity. The solutions architect must ensure that users can\nregain access. The solution also must prevent the problem from occurring again.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system."
      },
      {
        "key": "B",
        "text": "Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use"
      },
      {
        "key": "C",
        "text": "Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity"
      },
      {
        "key": "D",
        "text": "Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (91%) 4%",
    "page_images": []
  },
  {
    "No": "145",
    "question": "An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery.\nConfirmation includes the recipient's signature or a photo of the package with the recipient. The driver's handheld device uploads signatures and\nphotos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file\nname matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information.\nThe file is then placed in Amazon S3 for archiving.\nAs the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped\nconnections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30\nminutes. The billing team reports that files are not always in the archive and that the central system is not always updated.\nA solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems\nare always updated. The handheld devices cannot be modified, so the company cannot deploy a new application.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure"
      },
      {
        "key": "B",
        "text": "Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume"
      },
      {
        "key": "C",
        "text": "Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple"
      },
      {
        "key": "D",
        "text": "Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (91%) 9%",
    "page_images": []
  },
  {
    "No": "146",
    "question": "A company is running an application in the AWS Cloud. The application runs on containers m an Amazon Elastic Container Service (Amazon ECS)\ncluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory\nrequirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data\ncan be lost.\nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Provision an Aurora Replica in a different Region."
      },
      {
        "key": "B",
        "text": "Set up AWS DataSync for continuous replication of the data to a different Region."
      },
      {
        "key": "C",
        "text": "Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region."
      },
      {
        "key": "D",
        "text": "Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "147",
    "question": "A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15\nminutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card\nprimary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for\nadditional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format.\nAdditionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.\nWhich solutions will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda"
      },
      {
        "key": "B",
        "text": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate"
      },
      {
        "key": "C",
        "text": "Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS"
      },
      {
        "key": "D",
        "text": "Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "148",
    "question": "A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application\nruns on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL\ndatabase as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.\nWhich solution will achieve the company's goal with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test"
      },
      {
        "key": "B",
        "text": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target"
      },
      {
        "key": "C",
        "text": "Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the"
      },
      {
        "key": "D",
        "text": "Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (79%) C (21%)",
    "page_images": []
  },
  {
    "No": "149",
    "question": "A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the\ncompany's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The\nsolution must comply with AWS security best practices.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "In the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS account."
      },
      {
        "key": "B",
        "text": "In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required"
      },
      {
        "key": "C",
        "text": "In the company's AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM"
      },
      {
        "key": "D",
        "text": "In the company's AWS account, create an IAM group that has the required permissions. Create an IAM user in the company's account for"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "150",
    "question": "A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB\ntable to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The\nnew solution must ensure high availability for the trading platform.\nWhich solution will meet these requirements with the LEAST latency?",
    "choices": [
      {
        "key": "A",
        "text": "Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX."
      },
      {
        "key": "B",
        "text": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to"
      },
      {
        "key": "C",
        "text": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to"
      },
      {
        "key": "D",
        "text": "Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (87%) 13%",
    "page_images": []
  },
  {
    "No": "151",
    "question": "A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2\ninstances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind\nanother ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak\nusage of the application.\nThe application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives\nminimal trafic during the rest of the day.\nA solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances."
      },
      {
        "key": "B",
        "text": "Move the application frontend to a static website that is hosted on Amazon S3."
      },
      {
        "key": "C",
        "text": "Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes."
      },
      {
        "key": "D",
        "text": "Change all the backend EC2 instances to Spot Instances."
      },
      {
        "key": "E",
        "text": "Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BE (88%) 13%",
    "page_images": []
  },
  {
    "No": "152",
    "question": "A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on\nAmazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is\ndeveloping new application features to run on Amazon EKS with AWS Fargate.\nThe platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.\nWhich solution will provide the MOST cost-effective setup for the platform?",
    "choices": [
      {
        "key": "A",
        "text": "Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot"
      },
      {
        "key": "B",
        "text": "Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity"
      },
      {
        "key": "C",
        "text": "Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks."
      },
      {
        "key": "D",
        "text": "Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (80%) 9% 7%",
    "page_images": []
  },
  {
    "No": "153",
    "question": "A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon\nCloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured\nwith an alternate domain name that visitors use when they access the application.\nEach week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the\ncompany wants visitors to receive an informational message instead of a CloudFront error message.\nA solutions architect creates an Amazon S3 bucket as the first step in the process.\nWhich combination of steps should the solutions architect take next to meet the requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Upload static informational content to the S3 bucket."
      },
      {
        "key": "B",
        "text": "Create a new CloudFront distribution. Set the S3 bucket as the origin."
      },
      {
        "key": "C",
        "text": "Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin"
      },
      {
        "key": "D",
        "text": "During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete."
      },
      {
        "key": "E",
        "text": "During the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \\ Set the"
      },
      {
        "key": "F",
        "text": "During the weekly maintenance, configure Elastic Beanstalk to serve trafic from the S3 bucket."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACD (100%)",
    "page_images": []
  },
  {
    "No": "154",
    "question": "A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that\nprocesses and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN.\nThe Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment\nvariables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new\nfunction version with the updated environment variables after validating results. This update process also requires frequent changes to the\ncustom application to invoke the new function version ARN. These changes cause interruptions for users.\nA solutions architect needs to simplify this process to minimize disruption to users.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Directly modify the environment variables of the published Lambda function version. Use the SLATEST version to test image processing"
      },
      {
        "key": "B",
        "text": "Create an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image"
      },
      {
        "key": "C",
        "text": "Directly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function"
      },
      {
        "key": "D",
        "text": "Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": []
  },
  {
    "No": "155",
    "question": "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to\nkeep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application\nLoad Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex\ndomain.\nWhich solution will meet these requirements with the LEAST effort?",
    "choices": [
      {
        "key": "A",
        "text": "Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy"
      },
      {
        "key": "B",
        "text": "Place a Network Load Balancer (NLB) in front of the ALMigrate public DNS to Amazon Route 53. Create a CNAME record for the apex"
      },
      {
        "key": "C",
        "text": "Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the"
      },
      {
        "key": "D",
        "text": "Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route trafic"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "156",
    "question": "A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions\nwith API Gateway to use several shared libraries and custom classes.\nA solutions architect needs to simplify the deployment of the solution and optimize for code reuse.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the"
      },
      {
        "key": "B",
        "text": "Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR)."
      },
      {
        "key": "C",
        "text": "Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS"
      },
      {
        "key": "D",
        "text": "Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (65%) B (35%)",
    "page_images": []
  },
  {
    "No": "157",
    "question": "A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The\ncompany has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images.\nThe company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback\neven if the factory's internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the\nworkers.\nHow should the company deploy the ML model to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams."
      },
      {
        "key": "B",
        "text": "Deploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still"
      },
      {
        "key": "C",
        "text": "Order an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still"
      },
      {
        "key": "D",
        "text": "Deploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (91%) 9%",
    "page_images": []
  },
  {
    "No": "158",
    "question": "A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect\nwill use a configuration management database (CMDB) export of all the company's servers to create the case.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations."
      },
      {
        "key": "B",
        "text": "Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export."
      },
      {
        "key": "C",
        "text": "Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in"
      },
      {
        "key": "D",
        "text": "Use AWS Application Discovery Service to import the CMDB data to perform an analysis."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (85%) D (15%)",
    "page_images": []
  },
  {
    "No": "159",
    "question": "A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling\ngroup. The ALB is associated with an AWS WAF web ACL.\nThe website often encounters attacks in the application layer. The attacks produce sudden and significant increases in trafic on the application\nserver. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to\nmitigate these attacks.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm"
      },
      {
        "key": "B",
        "text": "Deploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource."
      },
      {
        "key": "C",
        "text": "Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm"
      },
      {
        "key": "D",
        "text": "Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": []
  },
  {
    "No": "160",
    "question": "A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and\nan Amazon Aurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already\ndeployed in two Regions.\nCompany policy states that critical applications must have application tier components and data tier components deployed across two Regions.\nThe RTO and RPO must be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant\nwith company policy.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Add another Region to the Aurora MySQL DB cluster"
      },
      {
        "key": "B",
        "text": "Add another Region to each table in the Aurora MySQL DB cluster"
      },
      {
        "key": "C",
        "text": "Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster"
      },
      {
        "key": "D",
        "text": "Convert the existing DynamoDB table to a global table by adding another Region to its configuration"
      },
      {
        "key": "E",
        "text": "Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "AD (83%) AC (17%)",
    "page_images": []
  },
  {
    "No": "161",
    "question": "A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the\ncompany's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones\nbehind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS\nterminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path.\nThe company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must\ndevelop a solution to allow trafic fiow to AWS from the on-premises network so that the clients can continue to access the application.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP"
      },
      {
        "key": "B",
        "text": "Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type"
      },
      {
        "key": "C",
        "text": "Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing"
      },
      {
        "key": "D",
        "text": "Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (93%) 3%",
    "page_images": []
  },
  {
    "No": "162",
    "question": "A company runs an application on a fieet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer\n(ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is\nassociated with the CloudFront distribution.\nThe company needs a solution that will prevent internet trafic from directly accessing the ALB.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB."
      },
      {
        "key": "B",
        "text": "Associate the existing web ACL with the ALB."
      },
      {
        "key": "C",
        "text": "Add a security group rule to the ALB to allow trafic from the AWS managed prefix list for CloudFront only."
      },
      {
        "key": "D",
        "text": "Add a security group rule to the ALB to allow only the various CloudFront IP address ranges."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "163",
    "question": "A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that\nthe company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit.\nAdditionally, users can access the cache without authentication.\nA solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with"
      },
      {
        "key": "B",
        "text": "Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure"
      },
      {
        "key": "C",
        "text": "Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the"
      },
      {
        "key": "D",
        "text": "Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "164",
    "question": "A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two\nplacement groups and a single instance type.\nRecently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The\ncompany needs to improve the overall reliability of the workload.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "key": "A",
        "text": "Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection."
      },
      {
        "key": "B",
        "text": "Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new"
      },
      {
        "key": "C",
        "text": "Update the launch template Auto Scaling group to increase the number of placement groups."
      },
      {
        "key": "D",
        "text": "Update the launch template to use a larger instance type."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "165",
    "question": "A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3\nAPI to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the\ndocument processing is finished, customers can download the documents directly from Amazon S3.\nDuring the migration, the company discovered that it could not immediately update the processing server that generates many documents to\nsupport the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server\nfinishes processing, the files must be available to the public for download within 30 minutes.\nWhich solution will meet these requirements with the LEAST amount of effort?",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company"
      },
      {
        "key": "B",
        "text": "Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2"
      },
      {
        "key": "C",
        "text": "Configure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and"
      },
      {
        "key": "D",
        "text": "Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (57%) C (38%) 6%",
    "page_images": []
  },
  {
    "No": "166",
    "question": "A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase\ndetails. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of\nthe other microservices store a copy of parts of the sensitive data in different storage services.\nThe company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other\nmicroservice must also delete its copy of the data immediately.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about"
      },
      {
        "key": "B",
        "text": "Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a"
      },
      {
        "key": "C",
        "text": "Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an"
      },
      {
        "key": "D",
        "text": "Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (71%) A (24%) 5%",
    "page_images": []
  },
  {
    "No": "167",
    "question": "A company is running a web application in a VPC. The web application runs on a group of Amazon EC2 instances behind an Application Load\nBalancer (ALB). The ALB is using AWS WAF.\nAn external customer needs to connect to the web application. The company must provide IP addresses to all external customers.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Replace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB."
      },
      {
        "key": "B",
        "text": "Allocate an Elastic IP address. Assign the Elastic IP address to the ALProvide the Elastic IP address to the customer."
      },
      {
        "key": "C",
        "text": "Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP"
      },
      {
        "key": "D",
        "text": "Configure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution's DNS name to determine the distribution's"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (87%) 9%",
    "page_images": []
  },
  {
    "No": "168",
    "question": "A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce\nAmazon Elastic Block Store (Amazon EBS) encryption at rest current production accounts and future production accounts only. The company\nneeds a solution that includes built-in blueprints and guardrails.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts."
      },
      {
        "key": "B",
        "text": "Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development"
      },
      {
        "key": "C",
        "text": "Create a new AWS Control Tower landing zone in the company's management account. Add production and development accounts to"
      },
      {
        "key": "D",
        "text": "Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance."
      },
      {
        "key": "E",
        "text": "Create a guardrail from the management account to detect EBS encryption."
      },
      {
        "key": "F",
        "text": "Create a guardrail for the production OU to detect EBS encryption."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CDF (71%) 13% Other",
    "page_images": []
  },
  {
    "No": "169",
    "question": "A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an\nAmazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must\nrecommend a solution to improve the resiliency of the application.\nThe solution must meet the following objectives:\n• Application tier: RPO of 2 minutes. RTO of 30 minutes\n• Database tier: RPO of 5 minutes. RTO of 30 minutes\nThe company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after\na failover.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an"
      },
      {
        "key": "B",
        "text": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS"
      },
      {
        "key": "C",
        "text": "Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region."
      },
      {
        "key": "D",
        "text": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross-"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (97%)",
    "page_images": []
  },
  {
    "No": "170",
    "question": "A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants\nto ensure that the instances are optimized based on CPU, memory, and network metrics.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Purchase AWS Business Support or AWS Enterprise Support for the account."
      },
      {
        "key": "B",
        "text": "Turn on AWS Trusted Advisor and review any “Low Utilization Amazon EC2 Instances” recommendations."
      },
      {
        "key": "C",
        "text": "Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances."
      },
      {
        "key": "D",
        "text": "Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations."
      },
      {
        "key": "E",
        "text": "Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CD (94%) 6%",
    "page_images": []
  },
  {
    "No": "171",
    "question": "A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS\nRegion.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region."
      },
      {
        "key": "B",
        "text": "Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region."
      },
      {
        "key": "C",
        "text": "Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the"
      },
      {
        "key": "D",
        "text": "Create an AWS Step Functions workfiow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workfiow to"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": []
  },
  {
    "No": "172",
    "question": "A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several\nVPCs that have CIDR ranges that overlap. The company's marketing team has created a new internal application and wants to make the\napplication accessible to all the other business units. The solution must use private IP addresses only.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Instruct each business unit to add a unique secondary CIDR range to the business unit's VPC. Peer the VPCs and use a private NAT gateway"
      },
      {
        "key": "B",
        "text": "Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPC. Create an AWS Site-to-Site VPN connection"
      },
      {
        "key": "C",
        "text": "Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to"
      },
      {
        "key": "D",
        "text": "Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "173",
    "question": "A company needs to audit the security posture of a newly acquired AWS account. The company's data security team requires a notification only\nwhen an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon\nSNS) topic that has the data security team's email address subscribed.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications."
      },
      {
        "key": "B",
        "text": "Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type"
      },
      {
        "key": "C",
        "text": "Create an Amazon EventBridge rule for the event type “Bucket-Level API Call via CloudTrail” with a filter for “PutBucketPolicy.” Select the"
      },
      {
        "key": "D",
        "text": "Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type “Config Rules"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": []
  },
  {
    "No": "174",
    "question": "A solutions architect needs to assess a newly acquired company's portfolio of applications and databases. The solutions architect must create a\nbusiness case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is\nnot well documented. The solutions architect cannot immediately determine how many applications and databases exist. Trafic for the\napplications is variable. Some applications are batch processes that run at the end of each month.\nThe solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service"
      },
      {
        "key": "B",
        "text": "Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub."
      },
      {
        "key": "C",
        "text": "Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use"
      },
      {
        "key": "D",
        "text": "Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (95%) 5%",
    "page_images": []
  },
  {
    "No": "175",
    "question": "A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS\ncluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances\nof the application. The company needs to back up the files and retain the backups for 1 year.\nWhich solution will meet these requirements while providing the FASTEST storage performance?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster."
      },
      {
        "key": "B",
        "text": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the"
      },
      {
        "key": "C",
        "text": "Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket."
      },
      {
        "key": "D",
        "text": "Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "176",
    "question": "A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience\nsurvey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises\ndata center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to\nmigrate the system to AWS to improve reliability.\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers."
      },
      {
        "key": "B",
        "text": "Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message"
      },
      {
        "key": "C",
        "text": "Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message"
      },
      {
        "key": "D",
        "text": "Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "177",
    "question": "A company is building a call center by using Amazon Connect. The company's operations team is defining a disaster recovery (DR) strategy across\nAWS Regions. The contact center has dozens of contact fiows, hundreds of users, and dozens of claimed phone numbers.\nWhich solution will provide DR with the LOWEST RTO?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team"
      },
      {
        "key": "B",
        "text": "Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the"
      },
      {
        "key": "C",
        "text": "Provision a new Amazon Connect instance with all existing contact fiows and claimed phone numbers in a second Region. Create an"
      },
      {
        "key": "D",
        "text": "Provision a new Amazon Connect instance with all existing users and contact fiows in a second Region. Create an Amazon Route 53 health"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (82%) B (18%)",
    "page_images": []
  },
  {
    "No": "178",
    "question": "A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to\nperform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift\ntables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits\nthe files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has\nbecome dificult.\nThe company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants\nto confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the\ncompany publishes the data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company"
      },
      {
        "key": "B",
        "text": "In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to"
      },
      {
        "key": "C",
        "text": "Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data"
      },
      {
        "key": "D",
        "text": "Publish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (83%) C (17%)",
    "page_images": []
  },
  {
    "No": "179",
    "question": "A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of\nevents that the solution receives. If a processing error occurs, the event must move into a separate queue for review.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Send event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to"
      },
      {
        "key": "B",
        "text": "Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto"
      },
      {
        "key": "C",
        "text": "Write events to an Amazon DynamoDB table. Configure a DynamoDB stream for the table. Configure the stream to invoke an AWS Lambda"
      },
      {
        "key": "D",
        "text": "Publish events to an Amazon EventBndge event bus. Create and run an application on an Amazon EC2 instance with an Auto Scaling group"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (53%) A (47%)",
    "page_images": []
  },
  {
    "No": "180",
    "question": "A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a\nsustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the\nprocessing engine through a RESTful API.\nThe API experiences unpredictable bursts of trafic. The company must implement a solution to process all data that the devices send to the\nprocessing engine. Data loss is unacceptable.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue."
      },
      {
        "key": "C",
        "text": "Create an Amazon API Gateway REST API that implements the RESTful API. Create a fieet of Amazon EC2 instances in an Auto Scaling"
      },
      {
        "key": "D",
        "text": "Create an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (84%) Other",
    "page_images": []
  },
  {
    "No": "181",
    "question": "A company is designing its network configuration in the AWS Cloud. The company uses AWS Organizations to manage a multi-account setup. The\ncompany has three OUs. Each OU contains more than 100 AWS accounts. Each account has a single VPC, and all the VPCs in each OU are in the\nsame AWS Region.\nThe CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution in which VPCs in the same OU can\ncommunicate with each other but cannot communicate with VPCs in other OUs.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS CloudFormation stack set that establishes VPC peering between accounts in each OU. Provision the stack set in each OU."
      },
      {
        "key": "B",
        "text": "In each OU, create a dedicated networking account that has a single VPC. Share this VPC with all the other accounts in the OU by using"
      },
      {
        "key": "C",
        "text": "Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access"
      },
      {
        "key": "D",
        "text": "In each OU, create a dedicated networking account that has a single VPC. Establish a VPN connection between the networking account and"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (70%) B (17%) 10%",
    "page_images": []
  },
  {
    "No": "182",
    "question": "A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company\nneeds to store large important documents within the application with the following requirements:",
    "choices": [
      {
        "key": "1",
        "text": "The data must be highly durable and available"
      },
      {
        "key": "2",
        "text": "The data must always be encrypted at rest and in transit"
      },
      {
        "key": "3",
        "text": "The encryption key must be managed by the company and rotated periodically"
      },
      {
        "key": "A",
        "text": "Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the"
      },
      {
        "key": "B",
        "text": "Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS"
      },
      {
        "key": "C",
        "text": "Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest."
      },
      {
        "key": "D",
        "text": "Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "183",
    "question": "A company's public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application\nLoad Balancer (ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for\nseveral months.\nRecently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection\nattacks had occurred against the API and that the API service had scaled to its maximum amount.\nA solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must\nallow legitimate trafic through and must maximize operational eficiency.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS"
      },
      {
        "key": "B",
        "text": "Create a new AWS WAF Bot Control implementation. Add a rule in the AWS WAF Bot Control managed rule group to monitor trafic and allow"
      },
      {
        "key": "C",
        "text": "Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all"
      },
      {
        "key": "D",
        "text": "Create a new AWS WAF web ACL. Create a new empty IP set in AWS WAF. Add a new rule to the web ACL to block requests that originate"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "184",
    "question": "An environmental company is deploying sensors in major cities throughout a country to measure air quality. The sensors connect to AWS IoT Core\nto ingest timeseries data readings. The company stores the data in Amazon DynamoDB.\nFor business continuity, the company must have the ability to ingest and store data in two AWS Regions.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core data endpoints in both Regions Migrate data to"
      },
      {
        "key": "B",
        "text": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT"
      },
      {
        "key": "C",
        "text": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain"
      },
      {
        "key": "D",
        "text": "Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Configure"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "185",
    "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application\nthat uses AWS Lambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table.\nThe DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table.\nThe finance team and the marketing team have separate AWS accounts.\nWhat should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?",
    "choices": [
      {
        "key": "A",
        "text": "Create an SCP to grant the marketing team's AWS account access to the specific attributes of the DynamoDB table. Attach the SCP to the"
      },
      {
        "key": "B",
        "text": "Create an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access"
      },
      {
        "key": "C",
        "text": "Create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). Attach the"
      },
      {
        "key": "D",
        "text": "Create an IAM role in the finance team's account to access the DynamoDB table. Use an IAM permissions boundary to limit the access to"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (92%) 8%",
    "page_images": []
  },
  {
    "No": "186",
    "question": "A solutions architect is creating an application that stores objects in an Amazon S3 bucket. The solutions architect must deploy the application in\ntwo AWS Regions that will be used simultaneously. The objects in the two S3 buckets must remain synchronized with each other.\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an S3 Multi-Region Access Point Change the application to refer to the Multi-Region Access Point"
      },
      {
        "key": "B",
        "text": "Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets"
      },
      {
        "key": "C",
        "text": "Modify the application to store objects in each S3 bucket"
      },
      {
        "key": "D",
        "text": "Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket"
      },
      {
        "key": "E",
        "text": "Enable S3 Versioning for each S3 bucket"
      },
      {
        "key": "F",
        "text": "Configure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ABE (100%)",
    "page_images": []
  },
  {
    "No": "187",
    "question": "A company has an IoT platform that runs in an on-premises environment. The platform consists of a server that connects to IoT devices by using\nthe MQTT protocol. The platform collects telemetry data from the devices at least once every 5 minutes. The platform also stores device\nmetadata in a MongoDB cluster.\nAn application that is installed on an on-premises machine runs periodic jobs to aggregate and transform the telemetry and device metadata. The\napplication creates reports that users view by using another web application that runs on the same on-premises machine. The periodic jobs take\n120-600 seconds to run. However, the web application is always running.\nThe company is moving the platform to AWS and must reduce the operational overhead of the stack.\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Lambda functions to connect to the IoT devices"
      },
      {
        "key": "B",
        "text": "Configure the IoT devices to publish to AWS IoT Core"
      },
      {
        "key": "C",
        "text": "Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance"
      },
      {
        "key": "D",
        "text": "Write the metadata to Amazon DocumentDB (with MongoDB compatibility)"
      },
      {
        "key": "E",
        "text": "Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use"
      },
      {
        "key": "F",
        "text": "Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BDE (100%)",
    "page_images": []
  },
  {
    "No": "188",
    "question": "A global manufacturing company plans to migrate the majority of its applications to AWS. However, the company is concerned about applications\nthat need to remain within a specific country or in the company's central on-premises data center because of data regulatory requirements or\nrequirements for latency of single-digit milliseconds. The company also is concerned about the applications that it hosts in some of its factory\nsites, where limited network infrastructure exists.\nThe company wants a consistent developer experience so that its developers can build applications once and deploy on premises, in the cloud, or\nin a hybrid architecture. The developers must be able to use the same tools, APIs, and services that are familiar to them.\nWhich solution will provide a consistent hybrid experience to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Migrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on-"
      },
      {
        "key": "B",
        "text": "Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency"
      },
      {
        "key": "C",
        "text": "Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds."
      },
      {
        "key": "D",
        "text": "Migrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (86%) 14%",
    "page_images": []
  },
  {
    "No": "189",
    "question": "A company is updating an application that customers use to make online orders. The number of attacks on the application by bad actors has\nincreased recently.\nThe company will host the updated application on an Amazon Elastic Container Service (Amazon ECS) cluster. The company will use Amazon\nDynamoDB to store application data. A public Application Load Balancer (ALB) will provide end users with access to the application. The company\nmust prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain."
      },
      {
        "key": "B",
        "text": "Deploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight."
      },
      {
        "key": "C",
        "text": "Configure auto scaling for Amazon ECS tasks Create a DynamoDB Accelerator (DAX) cluster."
      },
      {
        "key": "D",
        "text": "Configure Amazon ElastiCache to reduce overhead on DynamoDB."
      },
      {
        "key": "E",
        "text": "Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AE (100%)",
    "page_images": []
  },
  {
    "No": "190",
    "question": "A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon\nCloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fieet of\nAmazon EC2 instances in Auto Scaling groups. The application uses a domain name setup in Amazon Route 53.\nSome users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the\nALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors\noccur. The page should be displayed immediately for this error code.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Set up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the"
      },
      {
        "key": "B",
        "text": "Create a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy."
      },
      {
        "key": "C",
        "text": "Create a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket"
      },
      {
        "key": "D",
        "text": "Create a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "191",
    "question": "A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.\nA solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the\nunderlying infrastructure.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon"
      },
      {
        "key": "B",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx"
      },
      {
        "key": "C",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto"
      },
      {
        "key": "D",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "192",
    "question": "A company is running an application in the AWS Cloud. The core business logic is running on a set of Amazon EC2 instances in an Auto Scaling\ngroup. An Application Load Balancer (ALB) distributes trafic to the EC2 instances. Amazon Route 53 record api.example.com is pointing to the\nALB.\nThe company's development team makes major updates to the business logic. The company has a rule that when changes are deployed, only 10%\nof customers can receive the new logic during a testing window. A customer must use the same version of the business logic during the testing\nwindow.\nHow should the company deploy the updates to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling group. Configure the ALB to distribute trafic"
      },
      {
        "key": "B",
        "text": "Create a second target group that is referenced by the ALDeploy the new logic to EC2 instances in this new target group. Update the ALB"
      },
      {
        "key": "C",
        "text": "Create a new launch configuration for the Auto Scaling group. Specify the launch configuration to use the AutoScalingRollingUpdate policy,"
      },
      {
        "key": "D",
        "text": "Create a second Auto Scaling group that is referenced by the ALB. Deploy the new logic on a set of EC2 instances in this new Auto Scaling"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "193",
    "question": "A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The\ncompany is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is\nconnected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels.\nAn investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput\nof 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.\nWhat should the solutions architect do to meet these requirements with the LEAST administrative effort?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system."
      },
      {
        "key": "B",
        "text": "Disconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to"
      },
      {
        "key": "C",
        "text": "Deploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location."
      },
      {
        "key": "D",
        "text": "Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (63%) B (37%)",
    "page_images": []
  },
  {
    "No": "194",
    "question": "A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company\nmust modify the application to deploy the application in two AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region Modify the application"
      },
      {
        "key": "B",
        "text": "Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the"
      },
      {
        "key": "C",
        "text": "Create a new S3 bucket in a second Region Deploy the application in the second Region. Configure the application to use the new S3"
      },
      {
        "key": "D",
        "text": "Set up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (83%) C (17%)",
    "page_images": []
  },
  {
    "No": "195",
    "question": "An online gaming company needs to rehost its gaming platform on AWS. The company's gaming application requires high performance computing\n(HPC) processing and has a leaderboard that changes frequently. An Ubuntu instance that is optimized for compute generation hosts a Node.js\napplication for game display. Game state is tracked in an on-premises Redis instance.\nThe company needs a migration strategy that optimizes application performance.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Auto Scaling group of m5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon ElastlCache"
      },
      {
        "key": "B",
        "text": "Create an Auto Scaling group of c5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon OpenSearch"
      },
      {
        "key": "C",
        "text": "Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon"
      },
      {
        "key": "D",
        "text": "Create an Auto Scaling group of m5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "196",
    "question": "A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be\nsubmitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run\nmonthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.\nWhich combination of steps meets these requirements while minimizing operational overhead? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled"
      },
      {
        "key": "B",
        "text": "Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability"
      },
      {
        "key": "C",
        "text": "Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API"
      },
      {
        "key": "D",
        "text": "Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the"
      },
      {
        "key": "E",
        "text": "Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CE (57%) AE (17%) 13% 10%",
    "page_images": []
  },
  {
    "No": "197",
    "question": "A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the\nlogs for 5 years. The company's security team also must receive an email notification every time there is an attempt to delete data in the S3\nbucket.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS CloudTrail to log S3 data events."
      },
      {
        "key": "B",
        "text": "Configure S3 server access logging for the S3 bucket."
      },
      {
        "key": "C",
        "text": "Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES)."
      },
      {
        "key": "D",
        "text": "Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification"
      },
      {
        "key": "E",
        "text": "Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering."
      },
      {
        "key": "F",
        "text": "Configure a new S3 bucket to store the logs with an S3 Lifecycle policy."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDF (53%) ADF (45%)",
    "page_images": []
  },
  {
    "No": "198",
    "question": "A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed\nAmazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct. Connect connection to\nthe data center from the Region that is closest to the data center.\nThe company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-\npremises data center also must have access to AWS public services.\nWhich combination of steps will meet these requirements with the LEAST cost? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect"
      },
      {
        "key": "B",
        "text": "Set up additional Direct Connect connections from the on-premises data center to the other two Regions."
      },
      {
        "key": "C",
        "text": "Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions."
      },
      {
        "key": "D",
        "text": "Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions."
      },
      {
        "key": "E",
        "text": "Use VPC peering to establish a connection between the VPCs across the Regions Create a private VIF with the existing Direct Connect"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (100%)",
    "page_images": []
  },
  {
    "No": "199",
    "question": "A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to\nprovide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect\nis using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.\nWhich combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Enable AWS Config in all accounts"
      },
      {
        "key": "B",
        "text": "Enable Amazon GuardDuty in all accounts"
      },
      {
        "key": "C",
        "text": "Enable all features for the organization"
      },
      {
        "key": "D",
        "text": "Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions"
      },
      {
        "key": "E",
        "text": "Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions"
      },
      {
        "key": "F",
        "text": "Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ACD (64%) 8% 8% Other",
    "page_images": []
  },
  {
    "No": "200",
    "question": "A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to\nauthenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal,\naccess to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are\nnot able to access the AWS environment.\nWhich items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "The IAM user's permissions policy has allowed the use of SAML federation for that user."
      },
      {
        "key": "B",
        "text": "The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal."
      },
      {
        "key": "B",
        "text": "Test users are not in the AWSFederatedUsers group in the company's IdP."
      },
      {
        "key": "C",
        "text": "The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML"
      },
      {
        "key": "D",
        "text": "The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs."
      },
      {
        "key": "E",
        "text": "The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BCE (43%) B (29%) BD (29%)",
    "page_images": []
  },
  {
    "No": "201",
    "question": "A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB\ninstance that is experiencing overloaded connections. Most of the application's operations insert records into the database. The application\ncurrently stores credentials in a text-based configuration file.\nThe solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the\ncredentials secure and must provide the ability to rotate the credentials automatically on a regular basis.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy an Amazon RDS Proxy layer. In front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager."
      },
      {
        "key": "B",
        "text": "Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials in AWS Systems Manager Parameter Store"
      },
      {
        "key": "C",
        "text": "Create an Aurora Replica. Store the connection credentials as a secret in AWS Secrets Manager"
      },
      {
        "key": "D",
        "text": "Create an Aurora Replica. Store the connection credentials in AWS Systems Manager Parameter Store."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "202",
    "question": "A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fieet of t3.large\nAmazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across\nmultiple Availability Zones.\nIn the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB"
      },
      {
        "key": "B",
        "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB"
      },
      {
        "key": "C",
        "text": "Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression"
      },
      {
        "key": "D",
        "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (83%) Other",
    "page_images": []
  },
  {
    "No": "203",
    "question": "A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's\ncurrent internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a\nmonth to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database\nmore quickly.\nWhich solution will migrate the database in the LEAST amount of time?",
    "choices": [
      {
        "key": "A",
        "text": "Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service"
      },
      {
        "key": "B",
        "text": "Use AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use"
      },
      {
        "key": "C",
        "text": "Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration"
      },
      {
        "key": "D",
        "text": "Order an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (94%) 6%",
    "page_images": []
  },
  {
    "No": "204",
    "question": "A company has an application in the AWS Cloud. The application runs on a fieet of 20 Amazon EC2 instances. The EC2 instances are persistent\nand store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.\nThe company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration\nwithin 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes\noperational eficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network\nconfiguration in a secondary Region.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots"
      },
      {
        "key": "B",
        "text": "Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure,"
      },
      {
        "key": "C",
        "text": "Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in"
      },
      {
        "key": "D",
        "text": "Deploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (74%) B (26%)",
    "page_images": []
  },
  {
    "No": "205",
    "question": "A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files.\nAccording to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using\nAmazon S3 and Amazon CloudFront.\nWhich combination of steps will meet the encryption requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Turn on S3 server-side encryption for the S3 bucket that the web application uses."
      },
      {
        "key": "B",
        "text": "Add a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations in the S3 ACLs."
      },
      {
        "key": "C",
        "text": "Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses."
      },
      {
        "key": "D",
        "text": "Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS)."
      },
      {
        "key": "E",
        "text": "Configure redirection of HTTP requests to HTTPS requests in CloudFront."
      },
      {
        "key": "F",
        "text": "Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACE (93%) 7%",
    "page_images": []
  },
  {
    "No": "206",
    "question": "A company is implementing a serverless architecture by using AWS Lambda functions that need to access a Microsoft SQL Server DB instance on\nAmazon RDS. The company has separate environments for development and production, including a clone of the database system.\nThe company's developers are allowed to access the credentials for the development database. However, the credentials for the production\ndatabase must be encrypted with a key that only members of the IT security team's IAM user group can access. This key must be rotated on a\nregular basis.\nWhat should a solutions architect do in the production environment to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Store the database credentials in AWS Systems Manager Parameter Store by using a SecureString parameter that is encrypted by an AWS"
      },
      {
        "key": "B",
        "text": "Encrypt the database credentials by using the AWS Key Management Service (AWS KMS) default Lambda key. Store the credentials in the"
      },
      {
        "key": "C",
        "text": "Store the database credentials in the environment variables of each Lambda function. Encrypt the environment variables by using an AWS"
      },
      {
        "key": "D",
        "text": "Store the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS)"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (77%) A (23%)",
    "page_images": []
  },
  {
    "No": "207",
    "question": "An online retail company is migrating its legacy on-premises .NET application to AWS. The application runs on load-balanced frontend web\nservers, load-balanced application servers, and a Microsoft SQL Server database.\nThe company wants to use AWS managed services where possible and does not want to rewrite the application. A solutions architect needs to\nimplement a solution to resolve scaling issues and minimize licensing costs as the application scales.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier."
      },
      {
        "key": "B",
        "text": "Create images of all the servers by using AWS Database Migration Service (AWS DMS). Deploy Amazon EC2 instances that are based on the"
      },
      {
        "key": "C",
        "text": "Containerize the web frontend tier and the application tier. Provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create"
      },
      {
        "key": "D",
        "text": "Separate the application functions into AWS Lambda functions. Use Amazon API Gateway for the web frontend tier and the application tier."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "208",
    "question": "A software-as-a-service (SaaS) provider exposes APIs through an Application Load Balancer (ALB). The ALB connects to an Amazon Elastic\nKubernetes Service (Amazon EKS) cluster that is deployed in the us-east-1 Region. The exposed APIs contain usage of a few non-standard REST\nmethods: LINK, UNLINK, LOCK, and UNLOCK.\nUsers outside the United States are reporting long and inconsistent response times for these APIs. A solutions architect needs to resolve this\nproblem with a solution that minimizes operational overhead.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Add an Amazon CloudFront distribution. Configure the ALB as the origin."
      },
      {
        "key": "B",
        "text": "Add an Amazon API Gateway edge-optimized API endpoint to expose the APIs. Configure the ALB as the target."
      },
      {
        "key": "C",
        "text": "Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin."
      },
      {
        "key": "D",
        "text": "Deploy the APIs to two additional AWS Regions: eu-west-1 and ap-southeast-2. Add latency-based routing records in Amazon Route 53."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (66%) B (31%)",
    "page_images": []
  },
  {
    "No": "209",
    "question": "A company runs an IoT application in the AWS Cloud. The company has millions of sensors that collect data from houses in the United States. The\nsensors use the MQTT protocol to connect and send data to a custom MQTT broker. The MQTT broker stores the data on a single Amazon EC2\ninstance. The sensors connect to the broker through the domain named iot.example.com. The company uses Amazon Route 53 as its DNS\nservice. The company stores the data in Amazon DynamoDB.\nOn several occasions, the amount of data has overloaded the MQTT broker and has resulted in lost sensor data. The company must improve the\nreliability of the solution.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. Use the Auto Scaling group as the target for the"
      },
      {
        "key": "B",
        "text": "Set up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record"
      },
      {
        "key": "C",
        "text": "Create a Network Load Balancer (NLB). Set the MQTT broker as the target. Create an AWS Global Accelerator accelerator. Set the NLB as"
      },
      {
        "key": "D",
        "text": "Set up AWS IoT Greengrass to receive the sensor data. Update the DNS record in Route 53 to point to the AWS IoT Greengrass endpoint."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "210",
    "question": "A company has Linux-based Amazon EC2 instances. Users must access the instances by using SSH with EC2 SSH key pairs. Each machine\nrequires a unique EC2 key pair.\nThe company wants to implement a key rotation policy that will, upon request, automatically rotate all the EC2 key pairs and keep the keys in a\nsecurely encrypted place. The company will accept less than 1 minute of downtime during key rotation.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new"
      },
      {
        "key": "B",
        "text": "Store all the keys in Parameter Store, a capability of AWS Systems Manager, as a string. Define a Systems Manager maintenance window to"
      },
      {
        "key": "C",
        "text": "Import the EC2 key pairs into AWS Key Management Service (AWS KMS). Configure automatic key rotation for these key pairs. Create an"
      },
      {
        "key": "D",
        "text": "Add all the EC2 instances to Fleet Manager, a capability of AWS Systems Manager. Define a Systems Manager maintenance window to"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (69%) D (31%)",
    "page_images": []
  },
  {
    "No": "211",
    "question": "A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no\nconfiguration management database and has little knowledge about the utilization of the VMware portfolio.\nA solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight."
      },
      {
        "key": "B",
        "text": "Export the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the"
      },
      {
        "key": "C",
        "text": "Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive"
      },
      {
        "key": "D",
        "text": "Deploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "212",
    "question": "A company runs a microservice as an AWS Lambda function. The microservice writes data to an on-premises SQL database that supports a\nlimited number of concurrent connections. When the number of Lambda function invocations is too high, the database crashes and causes\napplication downtime. The company has an AWS Direct Connect connection between the company's VPC and the on-premises data center. The\ncompany wants to protect the database from crashes.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write"
      },
      {
        "key": "B",
        "text": "Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the data from the existing database to Aurora Serverless."
      },
      {
        "key": "C",
        "text": "Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the Amazon RDS DB instance. Reconfigure the Lambda"
      },
      {
        "key": "D",
        "text": "Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the Lambda function to write to the existing database"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (93%) 7%",
    "page_images": []
  },
  {
    "No": "213",
    "question": "A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS\nworkloads. The company has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be\nhighly available and cannot be down for longer than 10 minutes. The company needs to minimize ongoing maintenance.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Migrate to Amazon CloudWatch dashboards. Recreate the dashboards to match the existing Grafana dashboards. Use automatic"
      },
      {
        "key": "B",
        "text": "Create an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing"
      },
      {
        "key": "C",
        "text": "Create an AMI that has Grafana pre-installed. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto"
      },
      {
        "key": "D",
        "text": "Configure AWS Backup to back up the EC2 instance that runs Grafana once each hour. Restore the EC2 instance from the most recent"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "214",
    "question": "A company needs to migrate its customer transactions database from on premises to AWS. The database resides on an Oracle DB instance that\nruns on a Linux server. According to a new security requirement, the company must rotate the database password each year.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool (AWS SCT). Store the password in AWS Systems"
      },
      {
        "key": "B",
        "text": "Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a"
      },
      {
        "key": "C",
        "text": "Migrate the database to an Amazon EC2 instance. Use AWS Systems Manager Parameter Store to keep and rotate the connection string by"
      },
      {
        "key": "D",
        "text": "Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool (AWS SCT). Create an Amazon CloudWatch alarm to"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "215",
    "question": "A solutions architect is designing an AWS account structure for a company that consists of multiple teams. All the teams will work in the same\nAWS Region. The company needs a VPC that is connected to the on-premises network. The company expects less than 50 Mbps of total trafic to\nand from the on-premises network.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account."
      },
      {
        "key": "B",
        "text": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account."
      },
      {
        "key": "C",
        "text": "Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by"
      },
      {
        "key": "D",
        "text": "Use AWS Site-to-Site VPN for connectivity to the on-premises network."
      },
      {
        "key": "E",
        "text": "Use AWS Direct Connect for connectivity to the on-premises network."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BD (90%) 10%",
    "page_images": []
  },
  {
    "No": "216",
    "question": "A solutions architect at a large company needs to set up network security for outbound trafic to the internet from all AWS accounts within an\norganization in AWS Organizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a\ncentralized AWS Transit Gateway. Each account has both an internet gateway and a NAT gateway for outbound trafic to the internet. The company\ndeploys resources only into a single AWS Region.\nThe company needs the ability to add centrally managed rule-based filtering on all outbound trafic to the internet for all AWS accounts in the\norganization. The peak load of outbound trafic will not exceed 25 Gbps in each Availability Zone.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create a new VPC for outbound trafic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway."
      },
      {
        "key": "B",
        "text": "Create a new VPC for outbound trafic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway."
      },
      {
        "key": "C",
        "text": "Create an AWS Network Firewall firewall for rule-based filtering in each AWS account. Modify all default routes to point to the Network"
      },
      {
        "key": "D",
        "text": "In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "217",
    "question": "A company uses a load balancer to distribute trafic to Amazon EC2 instances in a single Availability Zone. The company is concerned about\nsecurity and wants a solutions architect to re-architect the solution to meet the following requirements:\n• Inbound requests must be filtered for common vulnerability attacks.\n• Rejected requests must be sent to a third-party auditing application.\n• All resources should be highly available.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously"
      },
      {
        "key": "B",
        "text": "Configure an Application Load Balancer (ALB) and add the EC2 instances as targets. Create a web ACL in WAF. Create an AWS WAF using"
      },
      {
        "key": "C",
        "text": "Configure an Application Load Balancer (ALB) along with a target group adding the EC2 instances as targets. Create an Amazon Kinesis"
      },
      {
        "key": "D",
        "text": "Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (80%) A (20%)",
    "page_images": []
  },
  {
    "No": "218",
    "question": "A company is running an application in the AWS Cloud. The application consists of microservices that run on a fieet of Amazon EC2 instances in\nmultiple Availability Zones behind an Application Load Balancer. The company recently added a new REST API that was implemented in Amazon\nAPI Gateway. Some of the older microservices that run on EC2 instances need to call this new API.\nThe company does not want the API to be accessible from the public internet and does not want proprietary data to traverse the public internet.\nWhat should a solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Site-to-Site VPN connection between the VPC and the API Gateway. Use API Gateway to generate a unique API Key for each"
      },
      {
        "key": "B",
        "text": "Create an interface VPC endpoint for API Gateway, and set an endpoint policy to only allow access to the specific API. Add a resource"
      },
      {
        "key": "C",
        "text": "Modify the API Gateway to use IAM authentication. Update the IAM policy for the IAM role that is assigned to the EC2 instances to allow"
      },
      {
        "key": "D",
        "text": "Create an accelerator in AWS Global Accelerator, and connect the accelerator to the API Gateway. Update the route table for all VPC subnets"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "219",
    "question": "A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses\nAmazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account.\nOccasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment.\nA solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make\nnoncompliant changes to the security settings for the EC2 instances.\nWhat is the FASTEST way for the solutions architect to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the"
      },
      {
        "key": "B",
        "text": "Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when"
      },
      {
        "key": "C",
        "text": "Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment."
      },
      {
        "key": "D",
        "text": "Enable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (93%) 7%",
    "page_images": []
  },
  {
    "No": "220",
    "question": "A company has IoT sensors that monitor trafic patterns throughout a large city. The company wants to read and collect data from the sensors and\nperform aggregations on the data.\nA solutions architect designs a solution in which the IoT devices are streaming to Amazon Kinesis Data Streams. Several applications are reading\nfrom the stream. However, several consumers are experiencing throttling and are periodically encountering a\nReadProvisionedThroughputExceeded error.\nWhich actions should the solutions architect take to resolve this issue? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Reshard the stream to increase the number of shards in the stream."
      },
      {
        "key": "B",
        "text": "Use the Kinesis Producer Library (KPL). Adjust the polling frequency."
      },
      {
        "key": "C",
        "text": "Use consumers with the enhanced fan-out feature."
      },
      {
        "key": "D",
        "text": "Reshard the stream to reduce the number of shards in the stream."
      },
      {
        "key": "E",
        "text": "Use an error retry and exponential backoff mechanism in the consumer logic."
      },
      {
        "key": "F",
        "text": "Configure the stream to use dynamic partitioning."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACE (100%)",
    "page_images": []
  },
  {
    "No": "221",
    "question": "A company uses AWS Organizations to manage its AWS accounts. The company needs a list of all its Amazon EC2 instances that have\nunderutilized CPU or memory usage. The company also needs recommendations for how to downsize these underutilized instances.\nWhich solution will meet these requirements with the LEAST effort?",
    "choices": [
      {
        "key": "A",
        "text": "Install a CPU and memory monitoring tool from AWS Marketplace on all the EC2 instances. Store the findings in Amazon S3. Implement a"
      },
      {
        "key": "B",
        "text": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization"
      },
      {
        "key": "C",
        "text": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization"
      },
      {
        "key": "D",
        "text": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Create an AWS Lambda function to extract"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "222",
    "question": "A company wants to run a custom network analysis software package to inspect trafic as trafic leaves and enters a VPC. The company has\ndeployed the solution by using AWS CloudFormation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been\nestablished to direct trafic to the EC2 instances.\nWhenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the\ninstance replacement occurs.\nWhich combination of steps will resolve this issue? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance."
      },
      {
        "key": "B",
        "text": "Update the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to"
      },
      {
        "key": "C",
        "text": "Update the CloudFormation template to install AWS Systems Manager Agent on the EC2 instances. Configure Systems Manager Agent to"
      },
      {
        "key": "D",
        "text": "Create an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an"
      },
      {
        "key": "E",
        "text": "Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out"
      },
      {
        "key": "F",
        "text": "In the CloudFormation template, write a condition that updates the network routes when a replacement instance is launched."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDE (100%)",
    "page_images": []
  },
  {
    "No": "223",
    "question": "A company is developing a new on-demand video application that is based on microservices. The application will have 5 million users at launch\nand will have 30 million users after 6 months. The company has deployed the application on Amazon Elastic Container Service (Amazon ECS) on\nAWS Fargate. The company developed the application by using ECS services that use the HTTPS protocol.\nA solutions architect needs to implement updates to the application by using blue/green deployments. The solution must distribute trafic to each\nECS service through a load balancer. The application must automatically adjust the number of tasks in response to an Amazon CloudWatch alarm.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Request increases to the service quota for"
      },
      {
        "key": "B",
        "text": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Implement Auto Scaling group for each"
      },
      {
        "key": "C",
        "text": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement an Auto Scaling group for"
      },
      {
        "key": "D",
        "text": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (81%) C (19%)",
    "page_images": []
  },
  {
    "No": "224",
    "question": "A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service\n(Amazon ECS) on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group.\nThe company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the\nnew image version receives a unique tag.\nThe company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically\ndelete new image tags that have Critical or High severity findings. The solution also must notify the development team when such a deletion\noccurs.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure scan on push on the repository. Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is"
      },
      {
        "key": "B",
        "text": "Configure scan on push on the repository. Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queue."
      },
      {
        "key": "C",
        "text": "Schedule an AWS Lambda function to start a manual image scan every hour. Configure Amazon EventBridge to invoke another Lambda"
      },
      {
        "key": "D",
        "text": "Configure periodic image scan on the repository. Configure scan results to be added to an Amazon Simple Queue Service (Amazon SQS)"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "225",
    "question": "A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. The workloads are hosted on Amazon EC2. AWS\nFargate. and AWS Lambda. Some of the workloads have unpredictable demand. Accounts record high usage in some months and low usage in\nother months.\nThe company wants to optimize its compute costs over the next 3 years. A solutions architect obtains a 6-month average for each of the accounts\nacross the organization to calculate usage.\nWhich solution will provide the MOST cost savings for all the organization's compute usage?",
    "choices": [
      {
        "key": "A",
        "text": "Purchase Reserved Instances for the organization to match the size and number of the most common EC2 instances from the member"
      },
      {
        "key": "B",
        "text": "Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management"
      },
      {
        "key": "C",
        "text": "Purchase Reserved Instances for each member account that had high EC2 usage according to the data from the last 6 months."
      },
      {
        "key": "D",
        "text": "Purchase an EC2 Instance Savings Plan for each member account from the management account based on EC2 usage data from the last 6"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "226",
    "question": "A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations to manage all the accounts. The company\nhas turned on all features.\nA finance team has allocated a daily budget for AWS costs. The finance team must receive an email notification if the organization's AWS costs\nexceed 80% of the allocated budget. A solutions architect needs to implement a solution to track the costs and deliver the notifications.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "In the organization's management account, use AWS Budgets to create a budget that has a daily period. Add an alert threshold and set the"
      },
      {
        "key": "B",
        "text": "In the organization's management account, set up the organizational view feature for AWS Trusted Advisor. Create an organizational view"
      },
      {
        "key": "C",
        "text": "Register the organization with AWS Control Tower. Activate the optional cost control (guardrail). Set a control (guardrail) parameter of 80%."
      },
      {
        "key": "D",
        "text": "Configure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket in the organization's management"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "227",
    "question": "A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon\nEC2 instances in the us-east-1 Region. Artists upload photos of their work as large-size. high-resolution image files from their mobile phones to a\ncentralized Amazon S3 bucket created in the us-east-1 Region. The users in Europe are reporting slow performance for their image uploads.\nHow can a solutions architect improve the performance of the image upload process?",
    "choices": [
      {
        "key": "A",
        "text": "Redeploy the application to use S3 multipart uploads."
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudFront distribution and point to the application as a custom origin."
      },
      {
        "key": "C",
        "text": "Configure the buckets to use S3 Transfer Acceleration."
      },
      {
        "key": "D",
        "text": "Create an Auto Scaling group for the EC2 instances and create a scaling policy."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (82%) A (18%)",
    "page_images": []
  },
  {
    "No": "228",
    "question": "A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application\nincludes web. application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed\ndata must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in\ntrafic.\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS)"
      },
      {
        "key": "B",
        "text": "Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache"
      },
      {
        "key": "C",
        "text": "Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use"
      },
      {
        "key": "D",
        "text": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (82%) Other",
    "page_images": []
  },
  {
    "No": "229",
    "question": "A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the\nsolutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero\ndowntime.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the"
      },
      {
        "key": "B",
        "text": "Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC)"
      },
      {
        "key": "C",
        "text": "Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure"
      },
      {
        "key": "D",
        "text": "Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (67%) B (33%)",
    "page_images": []
  },
  {
    "No": "230",
    "question": "A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability\nZones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has\ncreated multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created\nmultiple service consumer applications in the other organization.\nData transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect\nmust recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the\nwhole environment.\nWhich guidelines meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the"
      },
      {
        "key": "B",
        "text": "Place the service provider applications and the service consumer applications in AWS accounts in the same organization."
      },
      {
        "key": "C",
        "text": "Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments."
      },
      {
        "key": "D",
        "text": "Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS"
      },
      {
        "key": "E",
        "text": "Create a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (37%) BD (33%) CD (30%)",
    "page_images": []
  },
  {
    "No": "231",
    "question": "A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move\nthe backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-\npremises data center and AWS.\nWhich solution meets these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection."
      },
      {
        "key": "B",
        "text": "Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection."
      },
      {
        "key": "C",
        "text": "Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection."
      },
      {
        "key": "D",
        "text": "Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (92%) 8%",
    "page_images": []
  },
  {
    "No": "232",
    "question": "A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are\nlocated in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound\ntrafic costs, increase bandwidth throughput, and provide a consistent network experience for end users.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections"
      },
      {
        "key": "B",
        "text": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct"
      },
      {
        "key": "C",
        "text": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPUse a transit gateway with dynamic"
      },
      {
        "key": "D",
        "text": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "233",
    "question": "A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a\nseparate member account for development and a separate member account for production. Consolidated billing is linked to the management\naccount. In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in both member\naccounts.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "key": "A",
        "text": "Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to"
      },
      {
        "key": "B",
        "text": "Create an IAM user in each member account. In the management account, create a cross-account role that has least privilege access. Grant"
      },
      {
        "key": "C",
        "text": "Create an IAM user in the management account. In the member accounts, create an IAM group that has least privilege access. Add the IAM"
      },
      {
        "key": "D",
        "text": "Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": []
  },
  {
    "No": "234",
    "question": "A company wants to use AWS for disaster recovery for an on-premises application. The company has hundreds of Windows-based servers that run\nthe application. All the servers mount a common share.\nThe company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support native failover and fallback capabilities.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Storage Gateway File Gateway. Schedule daily Windows server backups. Save the data to Amazon S3. During a disaster,"
      },
      {
        "key": "B",
        "text": "Create a set of AWS CloudFormation templates to create infrastructure. Replicate all data to Amazon Elastic File System (Amazon EFS) by"
      },
      {
        "key": "C",
        "text": "Create an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-active environment on AWS. Replicate data into"
      },
      {
        "key": "D",
        "text": "Use AWS Elastic Disaster Recovery to replicate the on-premises servers. Replicate data to an Amazon FSx for Windows File Server file"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (100%)",
    "page_images": []
  },
  {
    "No": "235",
    "question": "A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared\nfiles stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when\nthe company increased the cluster size to 1.000 EC2 instances, overall performance was well below expectations.\nWhich collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose\nthree.)",
    "choices": [
      {
        "key": "A",
        "text": "Ensure the HPC cluster is launched within a single Availability Zone."
      },
      {
        "key": "B",
        "text": "Launch the EC2 instances and attach elastic network interfaces in multiples of four."
      },
      {
        "key": "C",
        "text": "Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled."
      },
      {
        "key": "D",
        "text": "Ensure the cluster is launched across multiple Availability Zones."
      },
      {
        "key": "E",
        "text": "Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array."
      },
      {
        "key": "F",
        "text": "Replace Amazon EFS with Amazon FSx for Lustre."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACF (80%) CDF (20%)",
    "page_images": []
  },
  {
    "No": "236",
    "question": "A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire\norganization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique\ntag values.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the"
      },
      {
        "key": "B",
        "text": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the"
      },
      {
        "key": "C",
        "text": "Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag"
      },
      {
        "key": "D",
        "text": "Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (77%) B (23%)",
    "page_images": []
  },
  {
    "No": "237",
    "question": "A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry\nTransport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket.\nRecently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new\ndesign on AWS that is highly available and scalable to prevent a similar occurrence.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a"
      },
      {
        "key": "B",
        "text": "Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer"
      },
      {
        "key": "C",
        "text": "Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data"
      },
      {
        "key": "D",
        "text": "Deploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (84%) B (16%)",
    "page_images": []
  },
  {
    "No": "238",
    "question": "A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances. Amazon Elastic\nFile System (Amazon EFS) file systems, and Amazon RDS DB instances.\nTo meet regulatory and business requirements, the company must make the following changes for data backups:\n• Backups must be retained based on custom daily, weekly, and monthly requirements.\n• Backups must be replicated to at least one other AWS Region immediately after capture.\n• The backup solution must provide a single source of backup status across the AWS environment.\n• The backup solution must send immediate notifications upon failure of any resource backup.\nWhich combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Backup plan with a backup rule for each of the retention requirements."
      },
      {
        "key": "B",
        "text": "Configure an AWS Backup plan to copy backups to another Region."
      },
      {
        "key": "C",
        "text": "Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs."
      },
      {
        "key": "D",
        "text": "Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any"
      },
      {
        "key": "E",
        "text": "Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements."
      },
      {
        "key": "F",
        "text": "Set up RDS snapshots on each database."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ABD (100%)",
    "page_images": []
  },
  {
    "No": "239",
    "question": "A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data\nfrom a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the\ndata and provide information back to researchers. The data platform must meet the following requirements:\n• Provide near-real-time analytics of the inbound genomic data\n• Ensure the data is fiexible, parallel, and durable\n• Deliver results of processing to a data warehouse\nWhich strategy should a solutions architect use to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an"
      },
      {
        "key": "B",
        "text": "Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an"
      },
      {
        "key": "C",
        "text": "Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon"
      },
      {
        "key": "D",
        "text": "Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "240",
    "question": "A solutions architect needs to define a reference architecture for a solution for three-tier applications with web. application, and NoSQL data\nlayers. The reference architecture must meet the following requirements:\n• High availability within an AWS Region\n• Able to fail over in 1 minute to another AWS Region for disaster recovery\n• Provide the most eficient solution while minimizing the impact on the user experience\nWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour."
      },
      {
        "key": "B",
        "text": "Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL)"
      },
      {
        "key": "C",
        "text": "Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions."
      },
      {
        "key": "D",
        "text": "Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3"
      },
      {
        "key": "E",
        "text": "Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the"
      },
      {
        "key": "F",
        "text": "Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the"
      }
    ],
    "answer_key": "F",
    "community_vote_distribution": "BCE (88%) 13%",
    "page_images": []
  },
  {
    "No": "241",
    "question": "A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to\nconnect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-\npremises storage. Custom applications analyze this data to detect anomalies.\nThe number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not\nable to scale for peak trafic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the\nscaling challenges.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache"
      },
      {
        "key": "B",
        "text": "Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores"
      },
      {
        "key": "C",
        "text": "Use AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose"
      },
      {
        "key": "D",
        "text": "Use Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (79%) C (21%)",
    "page_images": []
  },
  {
    "No": "242",
    "question": "During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it\nto an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.\nWhich solution will ensure that the credentials are appropriately secured automatically?",
    "choices": [
      {
        "key": "A",
        "text": "Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS"
      },
      {
        "key": "B",
        "text": "Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate"
      },
      {
        "key": "C",
        "text": "Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to"
      },
      {
        "key": "D",
        "text": "Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found,"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (87%) 13%",
    "page_images": []
  },
  {
    "No": "243",
    "question": "A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's\ninformation security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the\nminimum permissions necessary to function.\nTo meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.\nWhich combination of steps should the solutions architect take to implement this solution? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible"
      },
      {
        "key": "B",
        "text": "Create an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point."
      },
      {
        "key": "C",
        "text": "Create a gateway endpoint for Amazon S3 in each application's VPConfigure the endpoint policy to allow access to an S3 access point."
      },
      {
        "key": "D",
        "text": "Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access"
      },
      {
        "key": "E",
        "text": "Create a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "AC (68%) 14% Other",
    "page_images": []
  },
  {
    "No": "244",
    "question": "A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that\nsend application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.\nThe company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring\nsolution that uses Splunk on premises. A solutions architect needs to determine how to send networking trafic to Splunk.\nHow should the solutions architect meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Enable VPC fiows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an"
      },
      {
        "key": "B",
        "text": "Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function"
      },
      {
        "key": "C",
        "text": "Ask the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to"
      },
      {
        "key": "D",
        "text": "Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1-"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "245",
    "question": "A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the\ndevelopment teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide\nthe information to the company's finance team.\nThe company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States.\nHowever, some resources have been created in other Regions.\nA solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the\naccounts. The solution also must ensure that the company can create resources only in Regions in the United States.\nWhich combination of steps will meet these requirements in the MOST operationally eficient way? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage"
      },
      {
        "key": "B",
        "text": "Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all"
      },
      {
        "key": "C",
        "text": "Create an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the"
      },
      {
        "key": "D",
        "text": "Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the"
      },
      {
        "key": "E",
        "text": "Create an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management"
      },
      {
        "key": "F",
        "text": "Create an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDE (81%) Other",
    "page_images": []
  },
  {
    "No": "246",
    "question": "A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires\nread-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security\nteam.\nHow should a solutions architect meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a"
      },
      {
        "key": "B",
        "text": "Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a"
      },
      {
        "key": "C",
        "text": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole"
      },
      {
        "key": "D",
        "text": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "247",
    "question": "A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private\nsubnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the\ninternet from the private subnets.\nA solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route trafic to the internet through an\negress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.\nWhich set of additional steps should the solutions architect take to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet."
      },
      {
        "key": "B",
        "text": "Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required"
      },
      {
        "key": "C",
        "text": "Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access"
      },
      {
        "key": "D",
        "text": "Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "248",
    "question": "An education company is running a web application used by college students around the world. The application runs in an Amazon Elastic\nContainer Service (Amazon ECS) cluster in an Auto Scaling group behind an Application Load Balancer (ALB). A system administrator detects a\nweekly spike in the number of failed login attempts, which overwhelm the application's authentication service. All the failed login attempts\noriginate from about 500 different IP addresses that change each week. A solutions architect must prevent the failed login attempts from\noverwhelming the authentication service.\nWhich solution meets these requirements with the MOST operational eficiency?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses."
      },
      {
        "key": "B",
        "text": "Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB."
      },
      {
        "key": "C",
        "text": "Use AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges."
      },
      {
        "key": "D",
        "text": "Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "249",
    "question": "A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public\nSFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for\noutbound trafic. Changes to the SFTP endpoint IP addresses are not permitted.\nThe company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and"
      },
      {
        "key": "B",
        "text": "Add a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign"
      },
      {
        "key": "C",
        "text": "Register the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load"
      },
      {
        "key": "D",
        "text": "Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "250",
    "question": "A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high-\nthroughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the\napplication to be fault tolerant.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking."
      },
      {
        "key": "B",
        "text": "Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each"
      },
      {
        "key": "C",
        "text": "Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking."
      },
      {
        "key": "D",
        "text": "Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "251",
    "question": "A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon\nAPI Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.\nAfter initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The\ncompany believes this trafic is originating from a botnet and wants to secure its API while minimizing cost.\nWhich approach should the company take to secure its API?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit"
      },
      {
        "key": "C",
        "text": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API."
      },
      {
        "key": "D",
        "text": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (93%) 7%",
    "page_images": []
  },
  {
    "No": "252",
    "question": "A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor\nall data activity on all the databases.\nWhich solution will achieve this goal?",
    "choices": [
      {
        "key": "A",
        "text": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source."
      },
      {
        "key": "B",
        "text": "Start a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda"
      },
      {
        "key": "C",
        "text": "Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon"
      },
      {
        "key": "D",
        "text": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "253",
    "question": "An entertainment company recently launched a new game. To ensure a good experience for players during the launch period, the company\ndeployed a static quantity of 12 r6g.16xlarge (memory optimized) Amazon EC2 instances behind a Network Load Balancer. The company's\noperations team used the Amazon CloudWatch agent and a custom metric to include memory utilization in its monitoring strategy.\nAnalysis of the CloudWatch metrics from the launch period showed consumption at about one quarter of the CPU and memory that the company\nexpected. Initial demand for the game has subsided and has become more variable. The company decides to use an Auto Scaling group that\nmonitors the CPU and memory consumption to dynamically scale the instance fieet. A solutions architect needs to configure the Auto Scaling\ngroup to meet demand in the most cost-effective way.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Configure the Auto Scaling group to deploy c6g.4xlarge (compute optimized) instances. Configure a minimum capacity of 3, a desired"
      },
      {
        "key": "B",
        "text": "Configure the Auto Scaling group to deploy m6g.4xlarge (general purpose) instances. Configure a minimum capacity of 3, a desired"
      },
      {
        "key": "C",
        "text": "Configure the Auto Scaling group to deploy r6g.4xlarge (memory optimized) instances. Configure a minimum capacity of 3, a desired"
      },
      {
        "key": "D",
        "text": "Configure the Auto Scaling group to deploy r6g.8xlarge (memory optimized) instances. Configure a minimum capacity of 2, a desired"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (92%) 8%",
    "page_images": []
  },
  {
    "No": "254",
    "question": "A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity\nmode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts\nthroughout the day. and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.\nWhich strategy should a solutions architect recommend to meet this requirement?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy an Amazon ElastiCache cluster in front of the DynamoDB table"
      },
      {
        "key": "B",
        "text": "Deploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer."
      },
      {
        "key": "C",
        "text": "Use provisioned capacity mode. Purchase Savings Plans in Cost Explorer."
      },
      {
        "key": "D",
        "text": "Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (79%) C (16%)5%",
    "page_images": []
  },
  {
    "No": "255",
    "question": "A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts.\nAWS PrivateLink is being used to provide connectivity between the client services and the logging service.\nIn each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on\nEC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC\nendpoint.\nWhich combination of steps should a solutions architect take to resolve this issue? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL"
      },
      {
        "key": "B",
        "text": "Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets."
      },
      {
        "key": "C",
        "text": "Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets."
      },
      {
        "key": "D",
        "text": "Check the security group for the logging service running on EC2 instances to ensure it allows ingress from the clients."
      },
      {
        "key": "E",
        "text": "Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (52%) BD (36%) 12%",
    "page_images": []
  },
  {
    "No": "256",
    "question": "A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed\nfrequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side\nencryption with AWS KMS keys (SSE-KMS).\nA solutions architect reviews the company's monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of\nrequests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing"
      },
      {
        "key": "B",
        "text": "Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch"
      },
      {
        "key": "C",
        "text": "Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new"
      },
      {
        "key": "D",
        "text": "Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "257",
    "question": "A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions. Application state is stored in Amazon\nDynamoDB tables. Users are reporting that some uploaded photos are not being processed properly. The application developers trace the logs and\nfind that Lambda is experiencing photo processing issues when thousands of users upload photos simultaneously. The issues are the result of\nLambda concurrency limits and the performance of DynamoDB when data is saved.\nWhich combination of actions should a solutions architect take to increase the performance and reliability of the application? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Evaluate and adjust the RCUs for the DynamoDB tables."
      },
      {
        "key": "B",
        "text": "Evaluate and adjust the WCUs for the DynamoDB tables."
      },
      {
        "key": "C",
        "text": "Add an Amazon ElastiCache layer to increase the performance of Lambda functions."
      },
      {
        "key": "D",
        "text": "Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions."
      },
      {
        "key": "E",
        "text": "Use S3 Transfer Acceleration to provide lower latency to users."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BD (100%)",
    "page_images": []
  },
  {
    "No": "258",
    "question": "A company runs an application in an on-premises data center. The application gives users the ability to upload media files. The files persist in a\nfile server. The web application has many users. The application server is overutilized, which causes data uploads to fail occasionally. The\ncompany frequently adds new storage to the file server. The company wants to resolve these challenges by migrating the application to AWS.\nUsers from across the United States and Canada access the application. Only authenticated users should have the ability to access the\napplication to upload files. The company will consider a solution that refactors the application, and the company needs to accelerate application\ndevelopment.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the"
      },
      {
        "key": "B",
        "text": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the"
      },
      {
        "key": "C",
        "text": "Create a static website for uploads of media files. Store the static assets in Amazon S3. Use AWS AppSync to create an API. Use AWS"
      },
      {
        "key": "D",
        "text": "Use AWS Amplify to create a static website for uploads of media files. Use Amplify Hosting to serve the website through Amazon"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (87%) 13%",
    "page_images": []
  },
  {
    "No": "259",
    "question": "A company has an application that is deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are part of an\nAuto Scaling group. The application has unpredictable workloads and frequently scales out and in. The company's development team wants to\nanalyze application logs to find ways to improve the application's performance. However, the logs are no longer available after instances scale in.\nWhich solution will give the development team the ability to view the application logs after a scale-in event?",
    "choices": [
      {
        "key": "A",
        "text": "Enable access logs for the ALB. Store the logs in an Amazon S3 bucket."
      },
      {
        "key": "B",
        "text": "Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent."
      },
      {
        "key": "C",
        "text": "Modify the Auto Scaling group to use a step scaling policy."
      },
      {
        "key": "D",
        "text": "Instrument the application with AWS X-Ray tracing."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "260",
    "question": "A company runs an unauthenticated static website (www.example.com) that includes a registration form for users. The website uses Amazon S3\nfor hosting and uses Amazon CloudFront as the content delivery network with AWS WAF configured. When the registration form is submitted, the\nwebsite calls an Amazon API Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward the payload to an\nexternal API call.\nDuring testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutions architect confirms that the CloudFront\ndistribution origin has the Access-Control-Allow-Origin header set to www.example.com.\nWhat should the solutions architect do to resolve the error?",
    "choices": [
      {
        "key": "A",
        "text": "Change the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com."
      },
      {
        "key": "B",
        "text": "Enable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com."
      },
      {
        "key": "C",
        "text": "Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the"
      },
      {
        "key": "D",
        "text": "Enable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (92%) 8%",
    "page_images": []
  },
  {
    "No": "261",
    "question": "A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different\ndepartments in the company. The company has a Microsoft Azure Active Directory that is deployed.\nA solutions architect needs to centralize billing and management of the company's AWS accounts. The company wants to start using identity\nfederation instead of manual user management. The company also wants to use temporary credentials instead of long-lived access keys.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS"
      },
      {
        "key": "B",
        "text": "Configure each AWS account's email address to be aws+@example.com so that account management email messages and invoices are"
      },
      {
        "key": "C",
        "text": "Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active"
      },
      {
        "key": "D",
        "text": "Deploy an AWS Managed Microsoft AD directory in the management account. Share the directory with all other accounts in the organization"
      },
      {
        "key": "E",
        "text": "Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center"
      },
      {
        "key": "F",
        "text": "Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ACE (100%)",
    "page_images": []
  },
  {
    "No": "262",
    "question": "A company wants to manage the costs associated with a group of 20 applications that are infrequently used, but are still business-critical, by\nmigrating to AWS. The applications are a mix of Java and Node.js spread across different instance clusters. The company wants to minimize\ncosts while standardizing by using a single deployment methodology.\nMost of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other\ntimes. Average application memory consumption is less than 1 GB. though some applications use as much as 2.5 GB of memory during peak\nprocessing. The most important application in the group is a billing report written in Java that accesses multiple data sources and often runs for\nseveral hours.\nWhich is the MOST cost-effective solution?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a separate AWS Lambda function for each application. Use AWS CloudTrail logs and Amazon CloudWatch alarms to verify"
      },
      {
        "key": "B",
        "text": "Deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. Deploy an ECS task for each"
      },
      {
        "key": "C",
        "text": "Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have suficient resources. Monitor each"
      },
      {
        "key": "D",
        "text": "Deploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling and Application Load Balancers. Scale"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "263",
    "question": "A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs\ntasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, primary, and core\nnodes. The EMR tasks run each morning, starting at 1:00 AM. and take 6 hours to finish running. The amount of time to complete the processing is\nnot a priority because the data is not referenced until late in the day.\nThe solutions architect must review the architecture and suggest a solution to minimize the compute costs.\nWhich solution should the solutions architect recommend to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Launch all task, primary, and core nodes on Spot Instances in an instance fieet. Terminate the cluster, including all instances, when the"
      },
      {
        "key": "B",
        "text": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fieet. Terminate the"
      },
      {
        "key": "C",
        "text": "Continue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed."
      },
      {
        "key": "D",
        "text": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fieet. Terminate only"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (50%) D (50%)",
    "page_images": []
  },
  {
    "No": "264",
    "question": "A company has migrated a legacy application to the AWS Cloud. The application runs on three Amazon EC2 instances that are spread across three\nAvailability Zones. One EC2 instance is in each Availability Zone. The EC2 instances are running in three private subnets of the VPC and are set up\nas targets for an Application Load Balancer (ALB) that is associated with three public subnets.\nThe application needs to communicate with on-premises systems. Only trafic from IP addresses in the company's IP address range are allowed to\naccess the on-premises systems. The company's security team is bringing only one IP address from its internal IP address range to the cloud. The\ncompany has added this IP address to the allow list for the company firewall. The company also has created an Elastic IP address for this IP\naddress.\nA solutions architect needs to create a solution that gives the application the ability to communicate with the on-premises systems. The solution\nalso must be able to mitigate failures automatically.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy three NAT gateways, one in each public subnet. Assign the Elastic IP address to the NAT gateways. Turn on health checks for the"
      },
      {
        "key": "B",
        "text": "Replace the ALB with a Network Load Balancer (NLB). Assign the Elastic IP address to the NLTurn on health checks for the NLIn the case of"
      },
      {
        "key": "C",
        "text": "Deploy a single NAT gateway in a public subnet. Assign the Elastic IP address to the NAT gateway. Use Amazon CloudWatch with a custom"
      },
      {
        "key": "D",
        "text": "Assign the Elastic IP address to the ALB. Create an Amazon Route 53 simple record with the Elastic IP address as the value. Create a Route"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "265",
    "question": "A company uses AWS Organizations to manage more than 1,000 AWS accounts. The company has created a new developer organization. There\nare 540 developer member accounts that must be moved to the new developer organization. All accounts are set up with all the required\ninformation so that each account can be operated as a standalone account.\nWhich combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Choose\nthree.)",
    "choices": [
      {
        "key": "A",
        "text": "Call the MoveAccount operation in the Organizations API from the old organization's management account to migrate the developer"
      },
      {
        "key": "B",
        "text": "From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization"
      },
      {
        "key": "C",
        "text": "From each developer account, remove the account from the old organization using the RemoveAccountFromOrganization operation in the"
      },
      {
        "key": "D",
        "text": "Sign in to the new developer organization's management account and create a placeholder member account that acts as a target for the"
      },
      {
        "key": "E",
        "text": "Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to"
      },
      {
        "key": "F",
        "text": "Have each developer sign in to their account and confirm to join the new developer organization."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "BEF (81%) Other",
    "page_images": []
  },
  {
    "No": "266",
    "question": "A company's interactive web application uses an Amazon CloudFront distribution to serve images from an Amazon S3 bucket. Occasionally, third-\nparty tools ingest corrupted images into the S3 bucket. This image corruption causes a poor user experience in the application later. The company\nhas successfully implemented and tested Python logic to detect corrupt images.\nA solutions architect must recommend a solution to integrate the detection logic with minimal latency between the ingestion and serving.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use a Lambda@Edge function that is invoked by a viewer-response event."
      },
      {
        "key": "B",
        "text": "Use a Lambda@Edge function that is invoked by an origin-response event."
      },
      {
        "key": "C",
        "text": "Use an S3 event notification that invokes an AWS Lambda function."
      },
      {
        "key": "D",
        "text": "Use an S3 event notification that invokes an AWS Step Functions state machine."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "267",
    "question": "A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group. The company uses AWS CodePipeline to\ndeploy the application. The instances that run in the Auto Scaling group are constantly changing because of scaling events.\nWhen the company deploys new application code versions, the company installs the AWS CodeDeploy agent on any new target EC2 instances and\nassociates the instances with the CodeDeploy deployment group. The application is set to go live within the next 24 hours.\nWhat should a solutions architect recommend to automate the application deployment process with the LEAST amount of operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Configure Amazon EventBridge to invoke an AWS Lambda function when a new EC2 instance is launched into the Auto Scaling group. Code"
      },
      {
        "key": "B",
        "text": "Write a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new code. When the deployment is complete,"
      },
      {
        "key": "C",
        "text": "Create a new AWS CodeBuild project that creates a new AMI that contains the new code. Configure CodeBuild to update the Auto Scaling"
      },
      {
        "key": "D",
        "text": "Create a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group's launch template to use the new AMI."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (90%) 10%",
    "page_images": []
  },
  {
    "No": "268",
    "question": "A company has a website that runs on four Amazon EC2 instances that are behind an Application Load Balancer (ALB). When the ALB detects that\nan EC2 instance is no longer available, an Amazon CloudWatch alarm enters the ALARM state. A member of the company's operations team then\nmanually adds a new EC2 instance behind the ALB.\nA solutions architect needs to design a highly available solution that automatically handles the replacement of EC2 instances. The company\nneeds to minimize downtime during the switch to the new solution.\nWhich set of steps should the solutions architect take to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Delete the existing ALB. Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template"
      },
      {
        "key": "B",
        "text": "Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template to the Auto Scaling"
      },
      {
        "key": "C",
        "text": "Delete the existing ALB and the EC2 instances. Create an Auto Scaling group that is configured to handle the web application trafic. Attach"
      },
      {
        "key": "D",
        "text": "Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template to the Auto Scaling"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "269",
    "question": "A company wants to optimize AWS data-transfer costs and compute costs across developer accounts within the company's organization in AWS\nOrganizations. Developers can configure VPCs and launch Amazon EC2 instances in a single AWS Region. The EC2 instances retrieve\napproximately 1 TB of data each day from Amazon S3.\nThe developer activity leads to excessive monthly data-transfer charges and NAT gateway processing charges between EC2 instances and S3\nbuckets, along with high compute costs. The company wants to proactively enforce approved architectural patterns for any EC2 instance and VPC\ninfrastructure that developers deploy within the AWS accounts. The company does not want this enforcement to negatively affect the speed at\nwhich the developers can perform their tasks.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Create SCPs to prevent developers from launching unapproved EC2 instance types. Provide the developers with an AWS CloudFormation"
      },
      {
        "key": "B",
        "text": "Create a daily forecasted budget with AWS Budgets to monitor EC2 compute costs and S3 data-transfer costs across the developer"
      },
      {
        "key": "C",
        "text": "Create an AWS Service Catalog portfolio that users can use to create an approved VPC configuration with S3 gateway endpoints and"
      },
      {
        "key": "D",
        "text": "Create and deploy AWS Config rules to monitor the compliance of EC2 and VPC resources in the developer AWS accounts. If developers"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "270",
    "question": "A company is expanding. The company plans to separate its resources into hundreds of different AWS accounts in multiple AWS Regions. A\nsolutions architect must recommend a solution that denies access to any operations outside of specifically designated Regions.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create IAM roles for each account. Create IAM policies with conditional allow permissions that include only approved Regions for the"
      },
      {
        "key": "B",
        "text": "Create an organization in AWS Organizations. Create IAM users for each account. Attach a policy to each user to block access to Regions"
      },
      {
        "key": "C",
        "text": "Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions."
      },
      {
        "key": "D",
        "text": "Enable AWS Security Hub in each account. Create controls to specify the Regions where an account can deploy infrastructure."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "271",
    "question": "A company wants to refactor its retail ordering web application that currently has a load-balanced Amazon EC2 instance fieet for web hosting,\ndatabase API services, and business logic. The company needs to create a decoupled, scalable architecture with a mechanism for retaining failed\norders while also minimizing operational costs.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon S3 for web hosting with Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS)"
      },
      {
        "key": "B",
        "text": "Use AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API services. Use Amazon MQ for order queuing. Use"
      },
      {
        "key": "C",
        "text": "Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order"
      },
      {
        "key": "D",
        "text": "Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Amazon Simple Email Service (Amazon SES) for"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (90%) 10%",
    "page_images": []
  },
  {
    "No": "272",
    "question": "A company hosts a web application on AWS in the us-east-1 Region. The application servers are distributed across three Availability Zones behind\nan Application Load Balancer. The database is hosted in a MySQL database on an Amazon EC2 instance. A solutions architect needs to design a\ncross-Region data recovery solution using AWS services with an RTO of less than 5 minutes and an RPO of less than 1 minute. The solutions\narchitect is deploying application servers in us-west-2, and has configured Amazon Route 53 health checks and DNS failover to us-west-2.\nWhich additional step should the solutions architect take?",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica in us-west-2."
      },
      {
        "key": "B",
        "text": "Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2."
      },
      {
        "key": "C",
        "text": "Migrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment."
      },
      {
        "key": "D",
        "text": "Create a MySQL standby database on an Amazon EC2 instance in us-west-2."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "273",
    "question": "A company is using AWS Organizations to manage multiple accounts. Due to regulatory requirements, the company wants to restrict specific\nmember accounts to certain AWS Regions, where they are permitted to deploy resources. The resources in the accounts must be tagged, enforced\nbased on a group standard, and centrally managed with minimal configuration.\nWhat should a solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy."
      },
      {
        "key": "B",
        "text": "From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and"
      },
      {
        "key": "C",
        "text": "Associate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions."
      },
      {
        "key": "D",
        "text": "Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (100%)",
    "page_images": []
  },
  {
    "No": "274",
    "question": "A company has an application that generates reports and stores them in an Amazon S3 bucket. When a user accesses their report, the application\ngenerates a signed URL to allow the user to download the report. The company's security team has discovered that the files are public and that\nanyone can download them without authentication. The company has suspended the generation of new reports until the problem is resolved.\nWhich set of actions will immediately remediate the security issue without impacting the application's normal workfiow?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Lambda function that applies a deny all policy for users who are not authenticated. Create a scheduled event to invoke the"
      },
      {
        "key": "B",
        "text": "Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions."
      },
      {
        "key": "C",
        "text": "Run a script that puts a private ACL on all of the objects in the bucket."
      },
      {
        "key": "D",
        "text": "Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcIs option to TRUE on the bucket."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (78%) C (22%)",
    "page_images": []
  },
  {
    "No": "275",
    "question": "A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance in another AWS account. A solutions\narchitect needs to design a migration strategy that will require no downtime and that will minimize the amount of time necessary to complete the\nmigration. The migration strategy must replicate all existing data and any new data that is created during the migration. The target database must\nbe identical to the source database at completion of the migration process.\nAll applications currently use an Amazon Route 53 CNAME record as their endpoint for communication with the RDS for Oracle DB instance. The\nRDS for Oracle DB instance is in a private subnet.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the"
      },
      {
        "key": "B",
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema"
      },
      {
        "key": "C",
        "text": "Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account."
      },
      {
        "key": "D",
        "text": "Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account. Configure the"
      },
      {
        "key": "E",
        "text": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration"
      },
      {
        "key": "F",
        "text": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a change data capture (CDC) migration from the source"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ACE (93%) 7%",
    "page_images": []
  },
  {
    "No": "276",
    "question": "A company has implemented an ordering system using an event-driven architecture. During initial testing, the system stopped processing orders.\nFurther log analysis revealed that one order message in an Amazon Simple Queue Service (Amazon SQS) standard queue was causing an error on\nthe backend and blocking all subsequent order messages. The visibility timeout of the queue is set to 30 seconds, and the backend processing\ntimeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages and ensure that the system continues to process\nsubsequent messages.\nWhich step should the solutions architect take to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Increase the backend processing timeout to 30 seconds to match the visibility timeout."
      },
      {
        "key": "B",
        "text": "Reduce the visibility timeout of the queue to automatically remove the faulty message."
      },
      {
        "key": "C",
        "text": "Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages."
      },
      {
        "key": "D",
        "text": "Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (80%) C (20%)",
    "page_images": []
  },
  {
    "No": "277",
    "question": "A company has automated the nightly retraining of its machine learning models by using AWS Step Functions. The workfiow consists of multiple\nsteps that use AWS Lambda. Each step can fail for various reasons, and any failure causes a failure of the overall workfiow.\nA review reveals that the retraining has failed multiple nights in a row without the company noticing the failure. A solutions architect needs to\nimprove the workfiow so that notifications are sent for all types of failures in the retraining process.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type \"Email\" that targets the team's mailing list."
      },
      {
        "key": "B",
        "text": "Create a task named \"Email\" that forwards the input arguments to the SNS topic."
      },
      {
        "key": "C",
        "text": "Add a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.ALL\" ] and \"Next”: \"Email\"."
      },
      {
        "key": "D",
        "text": "Add a new email address to Amazon Simple Email Service (Amazon SES). Verify the email address."
      },
      {
        "key": "E",
        "text": "Create a task named \"Email\" that forwards the input arguments to the SES email address."
      },
      {
        "key": "F",
        "text": "Add a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.Runtime\" ] and \"Next\": \"Email\"."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ABC (86%) 7%",
    "page_images": []
  },
  {
    "No": "278",
    "question": "A company plans to deploy a new private intranet service on Amazon EC2 instances inside a VPC. An AWS Site-to-Site VPN connects the VPC to\nthe company's on-premises network. The new service must communicate with existing on-premises services. The on-premises services are\naccessible through the use of hostnames that reside in the company.example DNS zone. This DNS zone is wholly hosted on premises and is\navailable only on the company's private network.\nA solutions architect must ensure that the new service can resolve hostnames on the company.example domain to integrate with existing\nservices.\nWhich solution meets these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an empty private zone in Amazon Route 53 for company.example. Add an additional NS record to the company's on-premises"
      },
      {
        "key": "B",
        "text": "Turn on DNS hostnames for the VPC. Configure a new outbound endpoint with Amazon Route 53 Resolver. Create a Resolver rule to forward"
      },
      {
        "key": "C",
        "text": "Turn on DNS hostnames for the VPConfigure a new inbound resolver endpoint with Amazon Route 53 Resolver. Configur&the on-premises"
      },
      {
        "key": "D",
        "text": "Use AWS Systems Manager to configure a run document that will install a hosts file that contains any required hostnames. Use an Amazon"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "279",
    "question": "A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached to a transit gateway. Each VPC that sends\ntrafic to the public internet must send the trafic through a shared services VPC. Each subnet within a VPC uses the default VPC route table, and\nthe trafic is routed to the transit gateway. The transit gateway uses its default route table for any VPC attachment.\nA security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate with an EC2 instance that is deployed in any\nof the company's other VPCs. A solutions architect needs to limit the trafic between the VPCs. Each VPC must be able to communicate only with\na predefined, limited set of authorized VPCs.\nWhat should the solutions architect do to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Update the network ACL of each subnet within a VPC to allow outbound trafic only to the authorized VPCs. Remove all deny rules except"
      },
      {
        "key": "B",
        "text": "Update all the security groups that are used within a VPC to deny outbound trafic to security groups that are used within the unauthorized"
      },
      {
        "key": "C",
        "text": "Create a dedicated transit gateway route table for each VPC attachment. Route trafic only to the authorized VPCs."
      },
      {
        "key": "D",
        "text": "Update the main route table of each VPC to route trafic only to the authorized VPCs through the transit gateway."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "280",
    "question": "A company has a Windows-based desktop application that is packaged and deployed to the users' Windows machines. The company recently\nacquired another company that has employees who primarily use machines with a Linux operating system. The acquiring company has decided to\nmigrate and rehost the Windows-based desktop application to AWS.\nAll employees must be authenticated before they use the application. The acquiring company uses Active Directory on premises but wants a\nsimplified way to manage access to the application on AWS for all the employees.\nWhich solution will rehost the application on AWS with the LEAST development effort?",
    "choices": [
      {
        "key": "A",
        "text": "Set up and provision an Amazon Workspaces virtual desktop for every employee. Implement authentication by using Amazon Cognito"
      },
      {
        "key": "B",
        "text": "Create an Auto Scaling group of Windows-based Amazon EC2 instances. Join each EC2 instance to the company's Active Directory domain."
      },
      {
        "key": "C",
        "text": "Use an Amazon AppStream 2.0 image builder to create an image that includes the application and the required configurations. Provision an"
      },
      {
        "key": "D",
        "text": "Refactor and containerize the application to run as a web-based application. Run the application in Amazon Elastic Container Service"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (91%) 9%",
    "page_images": []
  },
  {
    "No": "281",
    "question": "A company is collecting a large amount of data from a fieet of IoT devices. Data is stored as Optimized Row Columnar (ORC) files in the Hadoop\nDistributed File System (HDFS) on a persistent Amazon EMR cluster. The company's data analytics team queries the data by using SQL in Apache\nPresto deployed on the same EMR cluster. Queries scan large amounts of data, always run for less than 15 minutes, and run only between 5 PM\nand 10 PM.\nThe company is concerned about the high cost associated with the current solution. A solutions architect must propose the most cost-effective\nsolution that will allow SQL data queries.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Store data in Amazon S3. Use Amazon Redshift Spectrum to query data."
      },
      {
        "key": "B",
        "text": "Store data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data."
      },
      {
        "key": "C",
        "text": "Store data in EMR File System (EMRFS). Use Presto in Amazon EMR to query data."
      },
      {
        "key": "D",
        "text": "Store data in Amazon Redshift. Use Amazon Redshift to query data."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "282",
    "question": "A large company recently experienced an unexpected increase in Amazon RDS and Amazon DynamoDB costs. The company needs to increase\nvisibility into details of AWS Billing and Cost Management. There are various accounts associated with AWS Organizations, including many\ndevelopment and production accounts. There is no consistent tagging strategy across the organization, but there are guidelines in place that\nrequire all infrastructure to be deployed using AWS CloudFormation with consistent tagging. Management requires cost center numbers and\nproject ID numbers for all existing and future DynamoDB tables and RDS instances.\nWhich strategy should the solutions architect provide to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to"
      },
      {
        "key": "B",
        "text": "Use an AWS Config rule to alert the finance team of untagged resources. Create a centralized AWS Lambda based solution to tag untagged"
      },
      {
        "key": "C",
        "text": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID. Use SCPs to restrict resource"
      },
      {
        "key": "D",
        "text": "Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources. Update"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (80%) A (20%)",
    "page_images": []
  },
  {
    "No": "283",
    "question": "A company wants to send data from its on-premises systems to Amazon S3 buckets. The company created the S3 buckets in three different\naccounts. The company must send the data privately without the data traveling across the internet. The company has no existing dedicated\nconnectivity to AWS.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect"
      },
      {
        "key": "B",
        "text": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect"
      },
      {
        "key": "C",
        "text": "Create an Amazon S3 interface endpoint in the networking account."
      },
      {
        "key": "D",
        "text": "Create an Amazon S3 gateway endpoint in the networking account."
      },
      {
        "key": "E",
        "text": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Peer VPCs from the accounts that host"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "AC (80%) 7% 7%",
    "page_images": []
  },
  {
    "No": "284",
    "question": "A company operates quick-service restaurants. The restaurants follow a predictable model with high sales trafic for 4 hours daily. Sales trafic is\nlower outside of those peak hours.\nThe point of sale and management platform is deployed in the AWS Cloud and has a backend that is based on Amazon DynamoDB. The database\ntable uses provisioned throughput mode with 100,000 RCUs and 80,000 WCUs to match known peak resource consumption.\nThe company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.\nWhich solution meets these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Reduce the provisioned RCUs and WCUs."
      },
      {
        "key": "B",
        "text": "Change the DynamoDB table to use on-demand capacity."
      },
      {
        "key": "C",
        "text": "Enable Dynamo DB auto scaling for the table."
      },
      {
        "key": "D",
        "text": "Purchase 1-year reserved capacity that is suficient to cover the peak load for 4 hours each day."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (80%) D (20%)",
    "page_images": []
  },
  {
    "No": "285",
    "question": "A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application currently\ndoes not use API keys to authorize requests. The API model is as follows:\nGET /posts/{postId}: to get post details\nGET /users/{userId}: to get user details\nGET /comments/{commentId}: to get comments details\nThe company has noticed users are actively discussing topics in the comments section, and the company wants to increase user engagement by\nmaking the comments appear in real time.\nWhich design should be used to reduce comment latency and improve user experience?",
    "choices": [
      {
        "key": "A",
        "text": "Use edge-optimized API with Amazon CloudFront to cache API responses."
      },
      {
        "key": "B",
        "text": "Modify the blog application code to request GET/comments/{commentId} every 10 seconds."
      },
      {
        "key": "C",
        "text": "Use AWS AppSync and leverage WebSockets to deliver comments."
      },
      {
        "key": "D",
        "text": "Change the concurrency limit of the Lambda functions to lower the API response time."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "286",
    "question": "A company manages hundreds of AWS accounts centrally in an organization in AWS Organizations. The company recently started to allow product\nteams to create and manage their own S3 access points in their accounts. The S3 access points can be accessed only within VPCs, not on the\ninternet.\nWhat is the MOST operationally eficient way to enforce this requirement?",
    "choices": [
      {
        "key": "A",
        "text": "Set the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key"
      },
      {
        "key": "B",
        "text": "Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin"
      },
      {
        "key": "C",
        "text": "Use AWS CloudFormation StackSets to create a new IAM policy in each AWS account that allows the s3:CreateAccessPoint action only if"
      },
      {
        "key": "D",
        "text": "Set the S3 bucket policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (92%) 8%",
    "page_images": []
  },
  {
    "No": "287",
    "question": "A solutions architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The\nsolutions architect creates an environment that is identical to the existing application environment and deploys the application to the new\nenvironment.\nWhat should be done next to complete the update?",
    "choices": [
      {
        "key": "A",
        "text": "Redirect to the new environment using Amazon Route 53."
      },
      {
        "key": "B",
        "text": "Select the Swap Environment URLs option."
      },
      {
        "key": "C",
        "text": "Replace the Auto Scaling launch configuration."
      },
      {
        "key": "D",
        "text": "Update the DNS records to point to the green environment."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": []
  },
  {
    "No": "288",
    "question": "A company is building an image service on the web that will allow users to upload and search random photos. At peak usage, up to 10,000 users\nworldwide will upload their images. The will then overlay text on the uploaded images, which will then be published on the company website.\nWhich design should a solutions architect implement?",
    "choices": [
      {
        "key": "A",
        "text": "Store the uploaded images in Amazon Elastic File System (Amazon EFS). Send application log information about each image to Amazon"
      },
      {
        "key": "B",
        "text": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to Amazon Simple"
      },
      {
        "key": "C",
        "text": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to the Amazon Simple"
      },
      {
        "key": "D",
        "text": "Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume mounted to a fieet of Amazon EC2 Spot"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": []
  },
  {
    "No": "289",
    "question": "A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data\navailable to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not\ntolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of\ncustomers need to see updates from the other group in real time.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the"
      },
      {
        "key": "B",
        "text": "Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the"
      },
      {
        "key": "C",
        "text": "Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1"
      },
      {
        "key": "D",
        "text": "Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (58%) D (42%)",
    "page_images": []
  },
  {
    "No": "290",
    "question": "A company is serving files to its customers through an SFTP server that is accessible over the internet. The SFTP server is running on a single\nAmazon EC2 instance with an Elastic IP address attached. Customers connect to the SFTP server through its Elastic IP address and use SSH for\nauthentication. The EC2 instance also has an attached security group that allows access from all customer IP addresses.\nA solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the\ndisruption to customers who access files. The solution must not change the way customers connect.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS"
      },
      {
        "key": "B",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS"
      },
      {
        "key": "C",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create a new Amazon Elastic File System (Amazon EFS) file system to be used"
      },
      {
        "key": "D",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create a multi-attach Amazon Elastic Block Store (Amazon EBS) volume to be"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (88%) 13%",
    "page_images": []
  },
  {
    "No": "291",
    "question": "A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics takes 4\nhours to complete. The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run\nfails.\nThe current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations. These EC2 instances run full time to ingest and\nstore the streaming data in attached Amazon Elastic Block Store (Amazon EBS) volumes. A scheduled script launches EC2 On-Demand Instances\neach night to perform the nightly processing. The instances access the stored data from NFS shares on the ingestion servers. The script\nterminates the instances when the processing is complete.\nThe Reserved Instance reservations are expiring. The company needs to determine whether to purchase new reservations or implement a new\ndesign.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a scheduled script to launch a fieet of"
      },
      {
        "key": "B",
        "text": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to"
      },
      {
        "key": "C",
        "text": "Update the ingestion process to use a fieet of EC2 Reserved Instances with 3-year reservations behind a Network LoadBalancer. Use AWS"
      },
      {
        "key": "D",
        "text": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use Amazon EventBridge to schedule"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (91%) 9%",
    "page_images": []
  },
  {
    "No": "292",
    "question": "A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a Linux VM. Uploaded files are made available to\ndownstream applications through an NFS share.\nAs part of the migration to AWS, a solutions architect must implement high availability. The solution must provide external vendors with a set of\nstatic public IP addresses that the vendors can allow. The company has set up an AWS Direct Connect connection between its on-premises data\ncenter and its VPC.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Transfer Family server. Configure an internet-facing VPC endpoint for the Transfer Family server. Specify an Elastic IP"
      },
      {
        "key": "B",
        "text": "Create an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family"
      },
      {
        "key": "C",
        "text": "Use AWS Application Migration Service to migrate the existing Linux VM to an Amazon EC2 instance. Assign an Elastic IP address to the"
      },
      {
        "key": "D",
        "text": "Use AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Family server. Configure a publicly accessible"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "293",
    "question": "A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two\nAvailability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and\nconnectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as\nfollows:\nVPC CIDR: 10.0.0.0/23 -\nAZ1 subnet CIDR: 10.0.0.0/24 -\nAZ2 subnet CIDR: 10.0.1.0/24 -\nSince deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional\nIPv4 address space and without service downtime. Which solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space."
      },
      {
        "key": "B",
        "text": "Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling"
      },
      {
        "key": "C",
        "text": "Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling"
      },
      {
        "key": "D",
        "text": "Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have half the previous address space. Adjust the Auto"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (82%) D (18%)",
    "page_images": []
  },
  {
    "No": "294",
    "question": "A company uses an organization in AWS Organizations to manage the company's AWS accounts. The company uses AWS CloudFormation to\ndeploy all infrastructure. A finance team wants to build a chargeback model. The finance team asked each business unit to tag resources by using\na predefined list of project values.\nWhen the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based on project, the team noticed noncompliant\nproject values. The company wants to enforce the use of project tags for new resources.\nWhich solution will meet these requirements with the LEAST effort?",
    "choices": [
      {
        "key": "A",
        "text": "Create a tag policy that contains the allowed project tag values in the organization's management account. Create an SCP that denies the"
      },
      {
        "key": "B",
        "text": "Create a tag policy that contains the allowed project tag values in each OU. Create an SCP that denies the cloudformation:CreateStack API"
      },
      {
        "key": "C",
        "text": "Create a tag policy that contains the allowed project tag values in the AWS management account. Create an IAM policy that denies the"
      },
      {
        "key": "D",
        "text": "Use AWS Service Catalog to manage the CloudFormation stacks as products. Use a TagOptions library to control project tag values. Share"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "295",
    "question": "An application is deployed on Amazon EC2 instances that run in an Auto Scaling group. The Auto Scaling group configuration uses only one type\nof instance.\nCPU and memory utilization metrics show that the instances are underutilized. A solutions architect needs to implement a solution to permanently\nreduce the EC2 cost and increase the utilization.\nWhich solution will meet these requirements with the LEAST number of configuration changes in the future?",
    "choices": [
      {
        "key": "A",
        "text": "List instance types that have properties that are similar to the properties that the current instances have. Modify the Auto Scaling group's"
      },
      {
        "key": "B",
        "text": "Use the information about the application's CPU and memory utilization to select an instance type that matches the requirements. Modify"
      },
      {
        "key": "C",
        "text": "Use the information about the application's CPU and memory utilization to specify CPU and memory requirements in a new revision of the"
      },
      {
        "key": "D",
        "text": "Create a script that selects the appropriate instance types from the AWS Price List Bulk API. Use the selected instance types to create a"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (70%) B (30%)",
    "page_images": []
  },
  {
    "No": "296",
    "question": "A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway The\napplication data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning\nby using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.\nA solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary"
      },
      {
        "key": "B",
        "text": "Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS Lambda to replicate the Aurora databases to a secondary"
      },
      {
        "key": "C",
        "text": "Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary"
      },
      {
        "key": "D",
        "text": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (57%) D (39%)",
    "page_images": []
  },
  {
    "No": "297",
    "question": "A company has a complex web application that leverages Amazon CloudFront for global scalability and performance. Over time, users report that\nthe web application is slowing down.\nThe company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. The cache metrics report indicates that\nquery strings on some URLs are inconsistently ordered and are specified sometimes in mixed-case letters and sometimes in lowercase letters.\nWhich set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger"
      },
      {
        "key": "B",
        "text": "Update the CloudFront distribution to disable caching based on query string parameters."
      },
      {
        "key": "C",
        "text": "Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase."
      },
      {
        "key": "D",
        "text": "Update the CloudFront distribution to specify casing-insensitive query string processing."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": []
  },
  {
    "No": "298",
    "question": "A company runs an ecommerce application in a single AWS Region. The application uses a five-node Amazon Aurora MySQL DB cluster to store\ninformation about customers and their recent orders. The DB cluster experiences a large number of write transactions throughout the day.\nThe company needs to replicate the data in the Aurora database to another Region to meet disaster recovery requirements. The company has an\nRPO of 1 hour.\nWhich solution will meet these requirements with the LOWEST cost?",
    "choices": [
      {
        "key": "A",
        "text": "Modify the Aurora database to be an Aurora global database. Create a second Aurora database in another Region."
      },
      {
        "key": "B",
        "text": "Enable the Backtrack feature for the Aurora database. Create an AWS Lambda function that runs daily to copy the snapshots of the"
      },
      {
        "key": "C",
        "text": "Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from"
      },
      {
        "key": "D",
        "text": "Turn off automated Aurora backups. Configure Aurora backups with a backup frequency of 1 hour. Specify another Region as the"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (77%) A (23%)",
    "page_images": []
  }
]