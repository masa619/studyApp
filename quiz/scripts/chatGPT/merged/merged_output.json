[
  {
    "No": "1",
    "question": "A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain\ncloud.example.com for the resources stored within VPCs.\nThe company has the following DNS resolution requirements:\nOn-premises systems should be able to resolve and connect to cloud.example.com.\nAll VPCs should be able to resolve cloud.example.com.\nThere is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.\nWhich architecture should the company use to meet these requirements with the HIGHEST performance?",
    "question_jp": "企業はハイブリッドDNSソリューションを設計する必要があります。このソリューションは、VPC内に保存されているリソースに対して、cloud.example.comのドメイン用にAmazon Route 53プライベートホステッドゾーンを使用します。企業には以下のDNS解決要件があります：\nオンプレミスのシステムがcloud.example.comを解決し、接続できること。\nすべてのVPCがcloud.example.comを解決できること。\nオンプレミスの企業ネットワークとAWS Transit Gatewayの間にはすでにAWS Direct Connect接続があります。\nどのアーキテクチャを使用すべきか、最高のパフォーマンスでこれらの要件を満たすためには？",
    "choices": [
      {
        "key": "A",
        "text": "Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the",
        "text_jp": "すべてのVPCにプライベートホステッドゾーンを関連付けます。共有サービスVPCにRoute 53のインバウンドリゾルバーを作成します。すべてのVPCを接続します。"
      },
      {
        "key": "B",
        "text": "Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all",
        "text_jp": "すべてのVPCにプライベートホステッドゾーンを関連付けます。共有サービスVPCにAmazon EC2の条件付きフォワーダをデプロイします。すべてのVPCを接続します。"
      },
      {
        "key": "C",
        "text": "Associate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs",
        "text_jp": "プライベートホステッドゾーンを共有サービスVPCに関連付けます。共有サービスVPCにRoute 53のアウトバウンドリゾルバーを作成します。すべてのVPCを接続します。"
      },
      {
        "key": "D",
        "text": "Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the",
        "text_jp": "プライベートホステッドゾーンを共有サービスVPCに関連付けます。共有サービスVPCにRoute 53のインバウンドリゾルバーを作成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (74%) D (26%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, which ensures that all DNS queries from the on-premises systems and VPCs are handled efficiently by the inbound resolver tailored for the private hosted zone.",
        "situation_analysis": "The requirement is to have both on-premises systems and all VPCs resolving cloud.example.com. The best option should allow direct resolution from both environments with optimal performance.",
        "option_analysis": "Option D provides a clear setup where the private hosted zone is associated with the shared services VPC and includes an inbound resolver, promoting best performance for queries. Options A and B may lead to unnecessary complexity with multiple conditional forwarding, while option C mistakenly focuses only on outbound resolution, not facilitating direct DNS queries from an on-premises environment.",
        "additional_knowledge": "This architecture also allows scalability as new VPCs are added, maintaining the performance of DNS queries without extensive reconfiguration.",
        "key_terminology": "Route 53, Private Hosted Zone, Inbound Resolver, VPC, AWS Direct Connect",
        "overall_assessment": "The choice of option D aligns with AWS best practices for hybrid architecture, ensuring efficient DNS resolution for both cloud and on-premises resources."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDであり、これはオンプレミスシステムとVPCの両方からのDNSクエリが、プライベートホステッドゾーン専用のインバウンドリゾルバーによって効率的に処理されることを保証します。",
        "situation_analysis": "要求は、オンプレミスシステムとすべてのVPCがcloud.example.comを解決できることです。最良のオプションは、両方の環境からの直接解決を最適なパフォーマンスで可能にすべきです。",
        "option_analysis": "選択肢Dは、プライベートホステッドゾーンが共有サービスVPCに関連付けられ、インバウンドリゾルバーが含まれている明確な設定を提供し、クエリに対する最良のパフォーマンスを促進します。選択肢AおよびBは、複数の条件付きフォワーダーを持つ不必要な複雑さを引き起こす可能性がある一方で、選択肢Cは間違ってアウトバウンド解決のみに焦点を当てており、オンプレミス環境からの直接DNSクエリを促進しません。",
        "additional_knowledge": "このアーキテクチャは、新しいVPCが追加される際のスケーラビリティも可能にし、広範な再構成なしでDNSクエリのパフォーマンスを維持します。",
        "key_terminology": "Route 53, プライベートホステッドゾーン, インバウンドリゾルバー, VPC, AWS Direct Connect",
        "overall_assessment": "選択肢Dの選択は、ハイブリッドアーキテクチャのためのAWSのベストプラクティスと一致し、クラウドとオンプレミスリソースの両方に対する効率的なDNS解決を保証します。"
      }
    ],
    "keywords": [
      "Route 53",
      "Private Hosted Zone",
      "Inbound Resolver",
      "VPC",
      "AWS Direct Connect"
    ]
  },
  {
    "No": "2",
    "question": "A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated\nwith different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of\nweather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the\nability to fail over to a different AWS Region.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、RESTベースのAPIを通じて複数の顧客に気象データを提供しています。このAPIはAmazon API Gatewayによってホストされており、各API操作ごとに異なるAWS Lambda関数と統合されています。企業はDNSにAmazon Route 53を使用しており、weather.example.comのリソースレコードを作成しています。APIのデータはAmazon DynamoDBテーブルに保存されています。企業は、APIが別のAWSリージョンにフェイルオーバーできるソリューションを必要としています。どのソリューションがこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda",
        "text_jp": "新しいリージョンに新しいLambda関数をデプロイします。API Gateway APIをエッジ最適化APIエンドポイントに更新し、Lambdaを使用します。"
      },
      {
        "key": "B",
        "text": "Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both",
        "text_jp": "別のリージョンに新しいAPI Gateway APIとLambda関数をデプロイします。Route 53 DNSレコードをマルチバリューの応答に変更します。両方を追加します。"
      },
      {
        "key": "C",
        "text": "Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target",
        "text_jp": "別のリージョンに新しいAPI Gateway APIとLambda関数をデプロイします。Route 53 DNSレコードをフェイルオーバーレコードに変更します。ターゲットを有効にします。"
      },
      {
        "key": "D",
        "text": "Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a",
        "text_jp": "新しいリージョンに新しいAPI Gateway APIをデプロイします。Lambda関数をグローバル関数に変更します。Route 53 DNSレコードを変更します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. By deploying a new API Gateway API and Lambda functions in another Region, and then configuring Route 53 to use a failover record, the API can effectively switch over to the alternate region during failures.",
        "situation_analysis": "The company requires a failover solution that enables their API to maintain availability across multiple AWS regions. The use of Amazon Route 53 for DNS management is already established.",
        "option_analysis": "Option C is the best choice as it specifies setting up a failover record, which is designed to allow automatic switching to a standby region if the primary region becomes unavailable. Options A and B suggest using a multi-value answer, which does not provide the same reliability in failover, while Option D lacks the explicit failover configuration.",
        "additional_knowledge": "It is important to regularly test the failover mechanism to ensure it functions as expected during actual outages.",
        "key_terminology": "API Gateway, AWS Lambda, Route 53, failover routing, DynamoDB",
        "overall_assessment": "Overall, Option C aligns well with best practices for high availability and disaster recovery by ensuring that there is a clear failover mechanism in place. The community's strong preference for Option C (100%) reflects its appropriateness for the given requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。別のリージョンに新しいAPI Gateway APIとLambda関数をデプロイし、Route 53をフェイルオーバーレコードに設定することで、APIは障害時に効果的に代替リージョンに切り替えることができます。",
        "situation_analysis": "企業は、複数のAWSリージョンにわたりAPIの可用性を維持できるフェイルオーバーソリューションを必要としています。すでにAmazon Route 53を使用してDNS管理が確立されています。",
        "option_analysis": "オプションCは、フェイルオーバーレコードの設定を指定しているため、最良の選択肢です。この設定により、プライマリリージョンが利用できなくなった場合、自動的にスタンバイリージョンに切り替えることができます。オプションAやBはマルチバリューの応答を使用することを提案していますが、同じ信頼性のフェイルオーバーを提供しません。オプションDは明示的なフェイルオーバーの設定が不足しています。",
        "additional_knowledge": "フェイルオーバーメカニズムが実際の故障時に期待通りに機能することを確認するために、定期的にテストすることが重要です。",
        "key_terminology": "API Gateway、AWS Lambda、Route 53、フェイルオーバールーティング、DynamoDB",
        "overall_assessment": "全体として、オプションCは高可用性と災害回復のベストプラクティスに適しており、明確なフェイルオーバーメカニズムが確保されています。コミュニティによるオプションCへの強い支持（100%）は、与えられた条件に対する適切性を反映しています。"
      }
    ],
    "keywords": [
      "API Gateway",
      "AWS Lambda",
      "Route 53",
      "failover routing",
      "DynamoDB"
    ]
  },
  {
    "No": "3",
    "question": "A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the\nProduction OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.\nThe company recently acquired a new business unit and invited the new unit's existing AWS account to the organization. Once onboarded, the\nadministrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company's policies.\nWhich option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term\nmaintenance?",
    "question_jp": "ある企業は、複数のアカウントを管理するために、Productionという単一のOUを持つAWS Organizationsを使用しています。すべてのアカウントはProduction OUのメンバーです。管理者は、制限されたサービスへのアクセスを管理するために、組織のルートにおいてdeny list SCPsを使用しています。企業は最近、新しい事業ユニットを買収し、そのユニットの既存のAWSアカウントを組織に招待しました。オンボーディング後、事業ユニットの管理者は、企業のポリシーを満たすために既存のAWS Configルールを更新できないことを発見しました。追加の長期的なメンテナンスを導入せずに、管理者が変更できるようにし、現在のポリシーを維持するためには、どのオプションが最適でしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Remove the organization's root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company's standard",
        "text_jp": "組織のルートSCPを削除して、AWS Configへのアクセスを制限します。企業の標準用のAWS Service Catalog製品を作成します。"
      },
      {
        "key": "B",
        "text": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the",
        "text_jp": "新しいアカウント用にOnboardingという一時的なOUを作成します。Onboarding OUにAWS Configアクションを許可するSCPを適用します。"
      },
      {
        "key": "C",
        "text": "Convert the organization's root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to",
        "text_jp": "組織のルートSCPをdeny list SCPからallow list SCPに変換し、必要なサービスのみを許可します。一時的にSCPを適用します。"
      },
      {
        "key": "D",
        "text": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the",
        "text_jp": "新しいアカウント用にOnboardingという一時的なOUを作成します。Onboarding OUにAWS Configアクションを許可するSCPを適用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (86%) 14%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating a temporary OU and applying an SCP to allow AWS Config actions enables the new business unit to manage its configurations effectively without disrupting compliance.",
        "situation_analysis": "The new business unit's administrators faced restrictions imposed by deny list SCPs and needed a way to modify AWS Config rules to adhere to the company's policies.",
        "option_analysis": "Option B allows for the modification of AWS Config rules by creating a temporary OU that bypasses deny list restrictions, ensuring compliance is maintained. Options A and C would create significant long-term management overhead, while Option D is simply a repeat of B.",
        "additional_knowledge": "It is also important to regularly review SCPs and organizational structures to adapt to new business needs.",
        "key_terminology": "AWS Organizations, Service Control Policies, AWS Config, temporary Organizational Unit (OU), access management.",
        "overall_assessment": "Option B is the most efficient approach to managing the new unit's needs while ensuring minimal ongoing maintenance and sustainment of current compliance practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。一時的なOUを作成し、AWS Configアクションを許可するSCPを適用することで、新しい事業ユニットがコンフィギュレーションを効果的に管理できるようになり、コンプライアンスを妨げることなく実現できる。",
        "situation_analysis": "新しい事業ユニットの管理者は、deny list SCPsによって課された制限に直面し、企業のポリシーに従うためにAWS Configルールを変更する方法が必要だった。",
        "option_analysis": "オプションBは、deny listの制限を回避する一時的なOUを作成することでAWS Configルールを変更できるため、コンプライアンスが維持される。オプションAとCは、管理の負担を増やす結果となる一方、オプションDはBの繰り返しに過ぎない。",
        "additional_knowledge": "新しいビジネスニーズに適応するために、SCPと組織構造を定期的に見直すことも重要である。",
        "key_terminology": "AWS Organizations、サービス制御ポリシー、AWS Config、一時的な組織単位（OU）、アクセス管理。",
        "overall_assessment": "オプションBは、新しいユニットのニーズを管理し、最小限の継続的なメンテナンスを実現しつつ、現在のコンプライアンスの実践を維持する最も効率的なアプローチである。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Service Control Policies",
      "AWS Config",
      "temporary Organizational Unit",
      "access management"
    ]
  },
  {
    "No": "4",
    "question": "A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a\nstateful application. The application connects to a PostgreSQL database running on a separate server. The application's user base is expected to\ngrow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon\nEC2 Auto Scaling, and Elastic Load Balancing.\nWhich solution will provide a consistent user experience that will allow the application and database tiers to scale?",
    "question_jp": "ある企業がオンプレミスのデータセンターで二層のウェブベースのアプリケーションを運用している。このアプリケーション層は、ステートフルアプリケーションを実行する単一のサーバーで構成されている。このアプリケーションは、別のサーバー上で実行されているPostgreSQLデータベースに接続している。このアプリケーションのユーザーベースは大幅に増加する見込みであり、企業はアプリケーションとデータベースをAWSに移行している。このソリューションは、Amazon Aurora PostgreSQL、Amazon EC2 Auto Scaling、およびElastic Load Balancingを使用する。どのソリューションが、アプリケーションとデータベースの層がスケールすることを許容し、一貫したユーザーエクスペリエンスを提供するか。",
    "choices": [
      {
        "key": "A",
        "text": "Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and",
        "text_jp": "Aurora Replicasのオートスケーリングを有効にする。最小未解決要求ルーティングアルゴリズムを持つネットワークロードバランサを使用する。"
      },
      {
        "key": "B",
        "text": "Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions",
        "text_jp": "Auroraライターのオートスケーリングを有効にする。ラウンドロビンルーティングアルゴリズムとスティッキーセッションを持つアプリケーションロードバランサを使用する。"
      },
      {
        "key": "C",
        "text": "Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.",
        "text_jp": "Aurora Replicasのオートスケーリングを有効にする。ラウンドロビンルーティングのスティッキーセッションを有効にしたアプリケーションロードバランサを使用する。"
      },
      {
        "key": "D",
        "text": "Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky",
        "text_jp": "Auroraライターのオートスケーリングを有効にする。最小未解決要求ルーティングアルゴリズムを持つネットワークロードバランサを使用し、スティッキーを使用する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It ensures both scaling and a consistent user experience by utilizing Aurora Replicas and an Application Load Balancer with sticky sessions.",
        "situation_analysis": "The company is looking to scale its application and database due to a growing user base. Therefore, a solution allowing dynamic handling of requests to multiple database replicas is ideal.",
        "option_analysis": "Option A suggests a Network Load Balancer which is not ideal for HTTP/S traffic and does not provide sticky sessions. Option B suggests possibly scaling the writer nodes, which may not effectively distribute the load. Option D has the same issue as A regarding the network load balancer.",
        "additional_knowledge": "",
        "key_terminology": "Aurora Replicas, Application Load Balancer, sticky sessions, auto scaling, consistent user experience",
        "overall_assessment": "This question is well-structured, testing knowledge on AWS scalability features and load balancing solutions. The community predominantly supports answer C, demonstrating understanding."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。Aurora Replicasを利用し、スティッキーセッションを備えたアプリケーションロードバランサを使用することで、スケーリングと一貫したユーザーエクスペリエンスを確保する。",
        "situation_analysis": "企業は増加するユーザーベースに対応するため、アプリケーションとデータベースのスケールを図ろうとしている。したがって、複数のデータベースレプリカに対して動的にリクエストを処理するソリューションが理想的である。",
        "option_analysis": "選択肢Aはネットワークロードバランサを提案しているが、これはHTTP/Sトラフィックには最適でなく、スティッキーセッションを提供しない。選択肢Bはライターノードのスケーリングを提案しているが、これは負荷を効果的に分散できない可能性がある。選択肢DもAと同様の問題がある。",
        "additional_knowledge": "",
        "key_terminology": "Aurora Replicas、アプリケーションロードバランサ、スティッキーセッション、オートスケーリング、一貫したユーザーエクスペリエンス",
        "overall_assessment": "この質問はよく構成されており、AWSのスケーラビリティ機能と負荷分散ソリューションに関する知識が試される。コミュニティは主に答えCを支持しており、理解を示している。"
      }
    ],
    "keywords": [
      "Amazon Aurora PostgreSQL",
      "Elastic Load Balancing",
      "Amazon EC2 Auto Scaling"
    ]
  },
  {
    "No": "5",
    "question": "A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and\ninternet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are\npresent in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to\nolder devices, which the company identified by the User-Agent headers.\nThe company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company\nhas already migrated the applications into a set of AWS Lambda functions.\nWhich solution will meet these requirements?",
    "question_jp": "A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers. The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company has already migrated the applications into a set of AWS Lambda functions. Which solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront",
        "text_jp": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each",
        "text_jp": "Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each"
      },
      {
        "key": "C",
        "text": "Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each",
        "text_jp": "Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each"
      },
      {
        "key": "D",
        "text": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront",
        "text_jp": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (44%) D (22%) B (19%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. An Amazon API Gateway REST API can be configured to process incoming requests and route them to the appropriate AWS Lambda functions. This allows for modifications of the request headers and the ability to handle routing based on user-agent strings.",
        "situation_analysis": "The company has a requirement to migrate its metadata service to AWS while ensuring compatibility with older devices that do not support certain HTTP headers. Serverless technologies are preferred, which points towards using AWS Lambda in conjunction with API Gateway.",
        "option_analysis": "Option B is correct as it provides a mechanism to invoke Lambda functions based on HTTP requests, allowing for modification of unsupported headers. Option A and D are misleading as CloudFront distributions primarily focus on content delivery and caching rather than direct request header manipulation. Option C does not offer the full capabilities as effectively as a REST API in this use case.",
        "additional_knowledge": "Serverless architecture promotes reduced operational overhead, and using API Gateway with Lambda limits the need for managing servers.",
        "key_terminology": "API Gateway, Lambda, REST API, payload transformation, request routing",
        "overall_assessment": "Ultimately, Option B closely aligns with the company's requirements to maintain support for older devices while leveraging serverless technology on AWS. The upcoming transformation using an API Gateway allows for greater flexibility and management over headers and the routing process."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。Amazon API Gateway REST APIは、受信リクエストを処理し、適切なAWS Lambda関数にルーティングするように設定できる。この方法を使用すれば、リクエストヘッダーの変更とユーザーエージェント文字列に基づく処理が可能となる。",
        "situation_analysis": "企業は、特定のHTTPヘッダーをサポートしない古いデバイスとの互換性を確保しながら、メタデータサービスをAWSに移行する必要がある。また、サーバーレス技術が好まれており、これはAWS LambdaとAPI Gatewayを使用することを示唆している。",
        "option_analysis": "オプションBは正しい。API GatewayはHTTPリクエストに基づいてLambda関数を呼び出し、サポートされていないヘッダーを変更できる。オプションAおよびDは誤解を招く。CloudFrontディストリビューションは、主にコンテンツ配信とキャッシングに焦点を当てており、リクエストヘッダーの操作に関しては直接的ではない。オプションCはREST APIほど効果的ではない。",
        "additional_knowledge": "サーバーレスアーキテクチャは運用のオーバーヘッドを軽減し、API GatewayとLambdaを使用することでサーバーの管理が不要になる。",
        "key_terminology": "API Gateway、Lambda、REST API、ペイロード変換、リクエストルーティング",
        "overall_assessment": "結局のところ、オプションBは古いデバイスのサポートを維持しながらAWS上でサーバーレス技術を活用するという企業の要件に最も近い。API Gatewayを用いることで、ヘッダーやルーティングプロセスに関する柔軟性と管理が容易になる。"
      }
    ],
    "keywords": [
      "API Gateway",
      "Lambda",
      "REST API",
      "payload transformation",
      "request routing"
    ]
  },
  {
    "No": "6",
    "question": "A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3\nbucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to\naccess the files from its own AWS account (Account B).\nWhich combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)",
    "question_jp": "小売会社は、ビジネスパートナーである別の会社に一連のデータファイルを提供する必要があります。これらのファイルは、小売会社に属するアカウントAのAmazon S3バケットに保存されています。ビジネスパートナー会社は、自社のAWSアカウント（アカウントB）から、そのIAMユーザーであるUser_DataProcessorがファイルにアクセスできるようにしたいと考えています。User_DataProcessorがS3バケットに正常にアクセスできるようにするために、両社が取るべき手順の組み合わせはどれですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A.",
        "text_jp": "アカウントAのS3バケットに対して、クロスオリジンリソース共有（CORS）機能を有効にする。"
      },
      {
        "key": "B",
        "text": "In Account A, set the S3 bucket policy to the following: [image_4_0] [image_4_1] [image_4_2]",
        "text_jp": "アカウントAで、次のS3バケットポリシーを設定する: [image_4_0] [image_4_1] [image_4_2]"
      },
      {
        "key": "C",
        "text": "In Account A, set the S3 bucket policy to the following: [image_4_0] [image_4_1] [image_4_2]",
        "text_jp": "アカウントAで、次のS3バケットポリシーを設定する: [image_4_0] [image_4_1] [image_4_2]"
      },
      {
        "key": "D",
        "text": "In Account B, set the permissions of User_DataProcessor to the following: [image_4_0] [image_4_1] [image_4_2]",
        "text_jp": "アカウントBで、User_DataProcessorのアクセス許可を次のように設定する: [image_4_0] [image_4_1] [image_4_2]"
      },
      {
        "key": "E",
        "text": "In Account B, set the permissions of User_DataProcessor to the following: [image_4_0] [image_4_1] [image_4_2]",
        "text_jp": "アカウントBで、User_DataProcessorのアクセス許可を次のように設定する: [image_4_0] [image_4_1] [image_4_2]"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (73%) D (23%)",
    "page_images": [
      "image_4_0.png",
      "image_4_1.png",
      "image_4_2.png",
      "image_4_3.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, which indicates that permissions for User_DataProcessor must be set in Account B.",
        "situation_analysis": "The retail company has its data files stored in an S3 bucket in Account A, and the partner company needs access to these files through an IAM user in Account B.",
        "option_analysis": "Option D correctly outlines that the permissions for User_DataProcessor need to be set in Account B, which is essential for granting access. Other options either repeat or do not address necessary permissions settings.",
        "additional_knowledge": "It's essential to set up both the bucket policy and IAM user permissions correctly to enable successful access.",
        "key_terminology": "IAM, S3 bucket policy, cross-account access",
        "overall_assessment": "The community's preference for option C suggests a misunderstanding, as it may refer to bucket policies alone without including necessary permissions in Account B."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDであり、User_DataProcessorの権限をアカウントBで設定する必要があることを示しています。",
        "situation_analysis": "小売会社は、アカウントAのS3バケットにデータファイルを保存しており、パートナー会社はアカウントBのIAMユーザーを通じてこれらのファイルへのアクセスを必要としています。",
        "option_analysis": "選択肢Dは、User_DataProcessorの権限をアカウントBで設定する必要があることを正確に示しており、アクセスを付与するために不可欠です。他の選択肢は、必要な権限設定を扱っていないか、繰り返しになっています。",
        "additional_knowledge": "成功的なアクセスを実現するためには、バケットポリシーとIAMユーザー権限を正しく設定することが不可欠です。",
        "key_terminology": "IAM、S3バケットポリシー、クロスアカウントアクセス",
        "overall_assessment": "コミュニティの選好が選択肢Cに偏っていることは誤解を示しており、バケットポリシーのみを扱ってアカウントBでの必要な権限設定を含めていない可能性があります。"
      }
    ],
    "keywords": [
      "IAM",
      "S3 bucket policy",
      "cross-account access"
    ]
  },
  {
    "No": "7",
    "question": "A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices\nthat run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is\nvariable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless\narchitecture that minimizes operational complexity.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業が、Amazon EC2 インスタンス上で従来のウェブアプリケーションを運営しています。この企業は、アプリケーションをコンテナ上で動作するマイクロサービスとしてリファクタリングする必要があります。アプリケーションの別々のバージョンが2つの異なる環境（本番環境とテスト環境）に存在します。アプリケーションの負荷は変動しますが、最小負荷と最大負荷はわかっています。ソリューションアーキテクトが、運用の複雑さを最小限に抑えたサーバーレスアーキテクチャで更新されたアプリケーションを設計する必要があります。この要件を最もコスト効果の高い方法で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle",
        "text_jp": "コンテナイメージを AWS Lambda に関数としてアップロードします。関連する Lambda 関数で同時実行数の制限を設定します"
      },
      {
        "key": "B",
        "text": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container",
        "text_jp": "コンテナイメージを Amazon Elastic Container Registry (Amazon ECR) にアップロードします。2つのオートスケールされた Amazon Elastic Container を設定します"
      },
      {
        "key": "C",
        "text": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes",
        "text_jp": "コンテナイメージを Amazon Elastic Container Registry (Amazon ECR) にアップロードします。2つのオートスケールされた Amazon Elastic Kubernetes を設定します"
      },
      {
        "key": "D",
        "text": "Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production",
        "text_jp": "コンテナイメージを AWS Elastic Beanstalk にアップロードします。Elastic Beanstalk で、本番環境とテスト環境用に別々の環境とデプロイメントを作成します"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (81%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Uploading container images to Amazon Elastic Container Registry (ECR) allows for a well-integrated container management solution that supports auto scaling.",
        "situation_analysis": "The company aims to refactor its application into microservices and needs a serverless architecture. There are distinct production and testing environments with variable loads known.",
        "option_analysis": "Option A is incorrect as AWS Lambda has limitations with container image sizes and might not support the complexity of microservices. Option C, while a possible solution, introduces Kubernetes management complexity that may not align with the goal of minimizing operational complexity. Option D, Elastic Beanstalk, is a PaaS solution but may not provide the fine-tuning capabilities required for the variable loads and separate environments effectively.",
        "additional_knowledge": "Knowledge of AWS pricing models and container orchestration helps in designing cost-effective solutions.",
        "key_terminology": "Amazon ECR, Elastic Container Service, auto scaling, serverless architecture, microservices.",
        "overall_assessment": "The community has largely supported option B, which indicates a strong alignment with best practices for containerized environments with known load variability."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは B である。コンテナイメージを Amazon Elastic Container Registry (ECR) にアップロードすることにより、オートスケーリングをサポートする適切に統合されたコンテナ管理ソリューションが得られる。",
        "situation_analysis": "企業はアプリケーションをマイクロサービスにリファクタリングし、サーバーレスアーキテクチャを必要としている。本番環境とテスト環境があり、可変な負荷があることが分かっている。",
        "option_analysis": "選択肢 A は不正解である。AWS Lambda はコンテナイメージサイズに制限があるため、マイクロサービスの複雑さをサポートできない可能性がある。選択肢 C は可能な選択肢ではあるが、Kubernetes の管理の複雑さが運用の複雑さを最小化するという目標とは合致しない。選択肢 D の Elastic Beanstalk は PaaS ソリューションであるが、可変の負荷や別々の環境を効果的に処理するために必要な微調整機能を提供できない可能性がある。",
        "additional_knowledge": "AWS の料金モデルやコンテナオーケストレーションの知識は、コスト効果の高いソリューションを設計する上で役立つ。",
        "key_terminology": "Amazon ECR、Elastic Container Service、オートスケーリング、サーバーレスアーキテクチャ、マイクロサービス。",
        "overall_assessment": "コミュニティは主に選択肢 B を支持していることから、知名度の脈動が高いことを示している。"
      }
    ],
    "keywords": [
      "Amazon ECR",
      "Elastic Container Service",
      "auto scaling",
      "serverless architecture",
      "microservices"
    ]
  },
  {
    "No": "8",
    "question": "A company has a multi-tier web application that runs on a fieet of Amazon EC2 instances behind an Application Load Balancer (ALB). The\ninstances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the\nmaximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application's data. The DB instance\nhas a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.\nThe company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region.\nThe company does not have a large enough budget for an active-active strategy.\nWhat should a solutions architect recommend to meet these requirements?",
    "question_jp": "ある企業が、アプリケーションロードバランサー（ALB）の背後にある一連のAmazon EC2インスタンスで動作するマルチティアWebアプリケーションを運営しています。それらのインスタンスはオートスケーリンググループに属しています。ALBとオートスケーリンググループはバックアップAWSリージョンで複製されています。オートスケーリンググループの最小値と最大値はゼロに設定されています。アプリケーションのデータは、Amazon RDS Multi-AZ DBインスタンスに格納されています。DBインスタンスにはバックアップリージョンにリードレプリカがあります。アプリケーションはAmazon Route 53レコードを使用してエンドユーザーにエンドポイントを提示します。企業は、アプリケーションがバックアップリージョンに自動的にフェイルオーバーできる能力を持たせ、RTOを15分未満に短縮する必要があります。企業はアクティブ-アクティブ戦略のための予算が十分ではありません。要件を満たすためにソリューションアーキテクトに推奨すべきはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Reconfigure the application's Route 53 record with a latency-based routing policy that load balances trafic between the two ALBs. Create",
        "text_jp": "アプリケーションのRoute 53レコードを再構成し、遅延ベースのルーティングポリシーを使用して2つのALB間でトラフィックを負荷分散する。"
      },
      {
        "key": "B",
        "text": "Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure",
        "text_jp": "バックアップリージョンにAWS Lambda関数を作成し、リードレプリカを昇格させ、オートスケーリンググループの値を変更する。"
      },
      {
        "key": "C",
        "text": "Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region.",
        "text_jp": "バックアップリージョンのオートスケーリンググループをプライマリリージョンのオートスケーリンググループと同じ値に設定する。"
      },
      {
        "key": "D",
        "text": "Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the",
        "text_jp": "AWS Global Acceleratorでエンドポイントを設定し、2つのALBを同等の重み付けターゲットとして使用する。バックアップリージョンにAWS Lambda関数を作成する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves creating an AWS Lambda function in the backup region to promote the read replica and modify the Auto Scaling group values. This allows for automatic failover to the backup region, addressing the company's requirement to reduce its RTO to less than 15 minutes.",
        "situation_analysis": "The company has a multi-tier application with a disaster recovery requirement to quickly failover to a backup region. The budget constraints also indicate a need for cost-effective solutions.",
        "option_analysis": "Option A is not viable due to the requirement for automatic failover rather than manual intervention. Option C does not address the failover of the database effectively. Option D, while it discusses Global Accelerator, does not specify how to manage the database failover adequately.",
        "additional_knowledge": "",
        "key_terminology": "AWS Lambda, Amazon RDS, Auto Scaling, failover, read replica",
        "overall_assessment": "The question effectively tests understanding of disaster recovery strategies in AWS, particularly under budget constraints while allowing for rapid failover capabilities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、バックアップリージョンでAWS Lambda関数を作成し、リードレプリカを昇格させ、オートスケーリンググループの値を変更することを含みます。これにより、バックアップリージョンへの自動フェイルオーバーが可能となり、企業の要件であるRTOを15分未満に短縮することができます。",
        "situation_analysis": "企業は、バックアップリージョンへの早急なフェイルオーバー要件を持つマルチティアアプリケーションを保有しています。予算の制約は、コスト効果の高いソリューションの必要性を示唆しています。",
        "option_analysis": "選択肢Aは、自動フェイルオーバーではなく手動介入が必要となるため実行可能ではありません。選択肢Cはデータベースのフェイルオーバーを効果的に扱っていません。選択肢DはGlobal Acceleratorを言及していますが、データベースのフェイルオーバーを適切に管理する方法を指定していません。",
        "additional_knowledge": "",
        "key_terminology": "AWS Lambda, Amazon RDS, オートスケーリング, フェイルオーバー, リードレプリカ",
        "overall_assessment": "この問題は、特に予算制約のもとでのAWSにおける災害復旧戦略の理解をテストする効果的なものであり、迅速なフェイルオーバー能力を許可しています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon RDS",
      "Auto Scaling",
      "failover",
      "read replica"
    ]
  },
  {
    "No": "9",
    "question": "A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node\ncluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application\nto function, each piece of the infrastructure must be healthy and must be in an active state.\nA solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least\npossible downtime.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "会社は、単一の Amazon EC2 インスタンスで重要なアプリケーションをホスティングしています。このアプリケーションは、インメモリ データ ストアとして Amazon ElastiCache for Redis の単一ノードクラスタを使用しています。アプリケーションは、リレーショナル データベース用の Amazon RDS for MariaDB DB インスタンスを使用しています。アプリケーションが機能するためには、インフラストラクチャの各部分が正常で、アクティブな状態である必要があります。ソリューション アーキテクトは、アプリケーションのアーキテクチャを改善し、インフラストラクチャが故障から自動的に回復できるようにし、ダウンタイムを最小限に抑える必要があります。この要件を満たすために必要な手順の組み合わせはどれですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Use an Elastic Load Balancer to distribute trafic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling",
        "text_jp": "Elastic Load Balancer を使用して、複数の EC2 インスタンスに負荷を分散します。EC2 インスタンスが Auto Scaling の一部であることを確認します。"
      },
      {
        "key": "B",
        "text": "Use an Elastic Load Balancer to distribute trafic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited",
        "text_jp": "Elastic Load Balancer を使用して、複数の EC2 インスタンスに負荷を分散します。EC2 インスタンスが無制限に構成されていることを確認します。"
      },
      {
        "key": "C",
        "text": "Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in",
        "text_jp": "DB インスタンスを修正して、同じアベイラビリティ ゾーンにリード レプリカを作成します。リード レプリカをプライマリ DB インスタンスに昇格させます。"
      },
      {
        "key": "D",
        "text": "Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones.",
        "text_jp": "DB インスタンスを修正して、2 つのアベイラビリティ ゾーンにまたがる Multi-AZ 展開を作成します。"
      },
      {
        "key": "E",
        "text": "Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum",
        "text_jp": "ElastiCache for Redis クラスタの複製グループを作成します。クラスターが最小限の Auto Scaling グループを使用するように構成します。"
      },
      {
        "key": "F",
        "text": "Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster.",
        "text_jp": "ElastiCache for Redis クラスタの複製グループを作成します。クラスターで Multi-AZ を有効にします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ADF (96%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, C, and D. These steps are essential for building a resilient architecture that can automatically recover from failures.",
        "situation_analysis": "The application relies on a single EC2 instance, ElastiCache for Redis single-node cluster, and RDS for MariaDB, making it vulnerable to single points of failure.",
        "option_analysis": "Option A introduces load balancing and auto-scaling for EC2 instances; Option C provides read replicas for database redundancy; Option D enhances database availability with Multi-AZ deployment.",
        "additional_knowledge": "Consider the use of AWS Backup for comprehensive backup strategies.",
        "key_terminology": "Elastic Load Balancer, Auto Scaling, Multi-AZ, Read Replica, High Availability",
        "overall_assessment": "The voting indicates that answers A, D, and F (with F less popular) reflect a strong community understanding of resilient architectures."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA、C、Dです。これらの手順は、障害から自動的に回復できる回復力のあるアーキテクチャを構築するために不可欠です。",
        "situation_analysis": "アプリケーションは、単一のEC2インスタンス、単一ノードのElastiCache for Redisクラスター、およびMariaDB用のRDSに依存しているため、単一障害点に対して脆弱です。",
        "option_analysis": "AオプションはEC2インスタンスの負荷バランスとオートスケーリングを導入し、Cオプションはデータベース冗長性のためのリードレプリカを提供し、DオプションはMulti-AZ展開でデータベースの可用性を向上させます。",
        "additional_knowledge": "包括的なバックアップ戦略のためにAWS Backupの使用を検討してください。",
        "key_terminology": "Elastic Load Balancer、Auto Scaling、Multi-AZ、リードレプリカ、高可用性",
        "overall_assessment": "投票は、A、D、F（Fの支持が少ない）が、堅牢なアーキテクチャの理解を強く反映していることを示しています。"
      }
    ],
    "keywords": [
      "Elastic Load Balancer",
      "Auto Scaling",
      "Multi-AZ",
      "Read Replica",
      "High Availability"
    ]
  },
  {
    "No": "10",
    "question": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load\nBalancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that\npoints to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.\nAfter an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP\nheaders that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the\nerror occurs.\nWhile the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page\nto visitors.\nWhich combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)",
    "question_jp": "小売企業はAWS上でeコマースアプリケーションを運営しています。このアプリケーションは、Application Load Balancer (ALB)の背後でAmazon EC2インスタンス上で実行されています。企業は、データベースバックエンドとしてAmazon RDS DBインスタンスを使用しています。Amazon CloudFrontは、ALBを指す1つのオリジンで設定されています。静的コンテンツはキャッシュされています。Amazon Route 53はすべての公開ゾーンをホストするために使用されています。アプリケーションの更新後、ALBは時折502ステータスコード（Bad Gateway）エラーを返します。根本的な原因は、ALBに返される不正なHTTPヘッダーです。エラーが発生した後、ソリューションアーキテクトがウェブページを再読み込みすると、ウェブページは正常に表示されます。企業が問題に取り組んでいる間、ソリューションアーキテクトは訪問者に標準のALBエラーページの代わりにカスタムエラーページを提供する必要があります。この要件を最小限の運用負荷で満たすための手順の組み合わせはどれですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.",
        "text_jp": "Amazon S3バケットを作成します。S3バケットを静的ウェブページをホストするように構成します。カスタムエラーページをAmazon S3にアップロードします。"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is",
        "text_jp": "ALBのヘルスチェック応答がTarget.FailedHealthChecksのときにAWS Lambda関数を呼び出すAmazon CloudWatchアラームを作成します。"
      },
      {
        "key": "C",
        "text": "Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS",
        "text_jp": "既存のAmazon Route 53レコードを修正し、ヘルスチェックを追加します。ヘルスチェックが失敗した場合にフォールバック対象を構成します。DNSを修正します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than",
        "text_jp": "ALBのヘルスチェック応答がElb.InternalErrorより大きい場合にAWS Lambda関数を呼び出すAmazon CloudWatchアラームを作成します。"
      },
      {
        "key": "0",
        "text": "Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.",
        "text_jp": "ALBの転送ルールを変更して、パブリックにアクセス可能なウェブサーバーを指すようにLambda関数を構成します。"
      },
      {
        "key": "E",
        "text": "Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web",
        "text_jp": "CloudFrontカスタムエラーページを構成してカスタムエラーレスポンスを追加します。DNSレコードをパブリックにアクセス可能なウェブを指すように変更します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "AE (96%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are C and A. This approach allows for less operational overhead while providing a custom error page.",
        "situation_analysis": "The retail company needs to serve custom error pages during application issues while minimizing the complexity of the solution.",
        "option_analysis": "Option C effectively uses Route 53 health checks to manage where traffic is directed in case of failure, keeping the operational needs low. Option A involves setting up S3, but does not directly solve the issue of delivering these pages without operational complexity.",
        "additional_knowledge": "Options to configure CloudFront or ALB directly would generally require more continuous management and layering of resources, which is generally not favored here.",
        "key_terminology": "Route 53, health checks, custom error pages, operational overhead, S3",
        "overall_assessment": "Answer C is confirmed as relevant by both its operation simplicity and effectiveness in addressing the problem. The community votes largely favor Option A, which although relevant, might introduce more overhead than just using Route 53 adjustments."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCとAである。このアプローチは、カスタムエラーページを提供しながら運用負荷を軽減することを可能にする。",
        "situation_analysis": "小売企業はアプリケーションの問題が発生した際にカスタムエラーページを提供する必要があり、解決策の複雑さを最小限に抑えたいと考えている。",
        "option_analysis": "選択肢Cは、Route 53のヘルスチェックを効果的に使用して猶予が生じた場合のトラフィックの方向を管理し、運用上のニーズを低く抑えている。選択肢AはS3の設定を含むが、運用の複雑さなくこれらのページを提供する問題の解決には直接繋がらない。",
        "additional_knowledge": "CloudFrontやALBを直接設定するオプションは、一般的により多くの管理が必要とされるため、ここではあまり好まれない。",
        "key_terminology": "Route 53、ヘルスチェック、カスタムエラーページ、運用負荷、S3",
        "overall_assessment": "回答Cはその運用の簡潔さと問題に対処する効果から有効であることが確認されている。コミュニティの票は大部分が選択肢Aを支持しているが、関連はあるものの、Route 53の調整のみよりも運用負荷を増加させる可能性がある。"
      }
    ],
    "keywords": [
      "Route 53",
      "health checks",
      "custom error pages",
      "operational overhead",
      "S3"
    ]
  },
  {
    "No": "11",
    "question": "A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the\ncompany can use to share a common network across multiple accounts.\nThe company's infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to\nmanage the network. Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to\ncreate AWS resources within subnets.\nWhich combination of actions should the solutions architect perform to meet these requirements? (Choose two.)",
    "question_jp": "ある企業は多くのAWSアカウントを持ち、AWS Organizationsを使用してそれらを管理しています。ソリューションアーキテクトは、企業が複数のアカウント間で共通のネットワークを共有できるようにするソリューションを実装する必要があります。企業のインフラストラクチャチームは、VPCを持つ専用のインフラストラクチャアカウントを持っています。インフラストラクチャチームは、このアカウントを使用してネットワークを管理しなければなりません。個々のアカウントは、自身のネットワークを管理する能力を持つことができません。ただし、個々のアカウントはサブネット内にAWSリソースを作成できる必要があります。この要件を満たすために、ソリューションアーキテクトはどの組み合わせのアクションを実行する必要がありますか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a transit gateway in the infrastructure account.",
        "text_jp": "インフラストラクチャアカウントにトランジットゲートウェイを作成する。"
      },
      {
        "key": "B",
        "text": "Enable resource sharing from the AWS Organizations management account.",
        "text_jp": "AWS Organizationsの管理アカウントからリソース共有を有効にする。"
      },
      {
        "key": "C",
        "text": "Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and",
        "text_jp": "AWS Organizations内の各AWSアカウントにVPCを作成する。同じCIDR範囲を共有するようにVPCを構成する。"
      },
      {
        "key": "D",
        "text": "Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will",
        "text_jp": "インフラストラクチャアカウントでAWS Resource Access Managerにリソース共有を作成する。特定のAWS OrganizationsのOUを選択する。"
      },
      {
        "key": "E",
        "text": "Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will",
        "text_jp": "インフラストラクチャアカウントでAWS Resource Access Managerにリソース共有を作成する。特定のAWS OrganizationsのOUを選択する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BD (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D. A transit gateway allows for the central management of network connections, which fits the company's requirement for a common network.",
        "situation_analysis": "The infrastructure team needs a centralized way to control networking across multiple accounts while allowing individual accounts to create resources within subnets.",
        "option_analysis": "Option A is important because a transit gateway facilitates communication between VPCs across multiple AWS accounts. Option D is also necessary as it allows resource sharing specifically across defined organizational units.",
        "additional_knowledge": "Understanding AWS Organizations and the benefits of centralized networking is crucial for architects.",
        "key_terminology": "AWS Organizations, Transit Gateway, AWS Resource Access Manager, VPC, Resource Sharing.",
        "overall_assessment": "The question is designed accurately to test knowledge on multi-account network strategies in AWS. The community vote indicates a strong preference for options B and D, which may reflect common misunderstandings about the use of transit gateways."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとDである。トランジットゲートウェイを使用することで、ネットワーク接続の集中管理が可能になり、企業の共通ネットワークの要件に適合する。",
        "situation_analysis": "インフラストラクチャチームは、複数のアカウント間でのネットワークを中央集権的に管理する方法が必要であり、その一方で個々のアカウントはサブネット内にリソースを作成できる必要がある。",
        "option_analysis": "選択肢Aは、トランジットゲートウェイが複数のAWSアカウント間でVPC間の通信を可能にするため重要である。選択肢Dも特定の組織単位間でのリソース共有を可能にするため必要である。",
        "additional_knowledge": "AWS Organizationsおよび集中型ネットワーク管理の利点を理解することは、アーキテクトにとって重要である。",
        "key_terminology": "AWS Organizations、トランジットゲートウェイ、AWSリソースアクセス管理、VPC、リソース共有。",
        "overall_assessment": "この問題は、AWSにおける複数アカウントネットワーク戦略の知識をテストするために正確に設計されている。コミュニティの投票は、選択肢BおよびDに対して強い支持を示しており、トランジットゲートウェイ使用に関する誤解を反映している可能性がある。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Transit Gateway",
      "AWS Resource Access Manager",
      "VPC",
      "Resource Sharing"
    ]
  },
  {
    "No": "12",
    "question": "A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API\ncalls. The third-party SaaS application also runs on AWS inside a VPC.\nThe company will consume the third-party SaaS application from inside a VPC. The company has internal security policies that mandate the use of\nprivate connectivity that does not traverse the internet. No resources that run in the company VPC are allowed to be accessed from outside the\ncompany's VPC. All permissions must conform to the principles of least privilege.\nWhich solution meets these requirements?",
    "question_jp": "ある企業がサードパーティ製のソフトウェア・アズ・ア・サービス（SaaS）アプリケーションを利用したいと考えています。このサードパーティ製SaaSアプリケーションは、いくつかのAPIコールを通じて消費されます。また、このサードパーティ製SaaSアプリケーションはAWS内のVPCで実行されています。企業は、VPC内からサードパーティ製SaaSアプリケーションを利用します。企業には、インターネットを通らないプライベートな接続を使用することを義務付ける内部セキュリティポリシーがあります。企業のVPC内で動作するリソースは、企業のVPC外からアクセスされることは許可されていません。すべての権限は最小特権の原則に準拠する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application",
        "text_jp": "AWS PrivateLinkインターフェースVPCエンドポイントを作成します。このエンドポイントをサードパーティ製SaaSアプリケーションのエンドポイントサービスに接続します。"
      },
      {
        "key": "B",
        "text": "Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. Configure network ACLs to limit",
        "text_jp": "サードパーティ製SaaSアプリケーションと企業のVPC間にAWS Site-to-Site VPN接続を作成します。ネットワークACLを構成して制限します。"
      },
      {
        "key": "C",
        "text": "Create a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed",
        "text_jp": "サードパーティ製SaaSアプリケーションと企業のVPC間にVPCピアリング接続を作成します。必要なルートテーブルを更新します。"
      },
      {
        "key": "D",
        "text": "Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service.",
        "text_jp": "AWS PrivateLinkエンドポイントサービスを作成します。サードパーティ製SaaSプロバイダーにこのエンドポイントサービス用のインターフェースVPCエンドポイントを作成するように依頼します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create an AWS PrivateLink interface VPC endpoint. This solution allows private connectivity to the third-party SaaS application while adhering to the company's security policies.",
        "situation_analysis": "The company requires a secure and private method to connect to a third-party SaaS application, which is hosted within an AWS VPC and does not use the internet for connectivity.",
        "option_analysis": "Option A is the best choice since AWS PrivateLink provides private connectivity to services hosted on AWS without exposing the traffic to the internet. Option B (VPN) introduces additional complexity and isn't necessary for this use case. Option C (VPC peering) may expose resources in a way that violates security policies. Option D requires intervention from the SaaS provider, which can result in delays.",
        "additional_knowledge": "This approach is widely recommended for secure communications between AWS resources and third-party services.",
        "key_terminology": "PrivateLink, VPC endpoint, API, security policies",
        "overall_assessment": "Given the company's internal security policies and requirement for private connectivity, AWS PrivateLink is indeed the ideal solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA：AWS PrivateLinkインターフェースVPCエンドポイントを作成することである。このソリューションは、企業のセキュリティポリシーに沿った形でサードパーティ製SaaSアプリケーションへのプライベート接続を提供する。",
        "situation_analysis": "企業は、AWSのVPC内にホストされ、インターネットを経由しないサードパーティ製SaaSアプリケーションに接続するための安全でプライベートな方法を必要としている。",
        "option_analysis": "選択肢Aが最も適切である。なぜなら、AWS PrivateLinkはAWSでホストされるサービスに対してインターネットへの露出なしでプライベート接続を提供するからである。選択肢B（VPN）は追加の複雑さをもたらし、このユースケースには必要ない。選択肢C（VPCピアリング）は、リソースがセキュリティポリシーに反して露出する可能性がある。選択肢DはSaaSプロバイダーの介入を必要とし、遅延を引き起こす可能性がある。",
        "additional_knowledge": "このアプローチは、AWSリソースとサードパーティサービス間の安全な通信に対して広く推奨されている。",
        "key_terminology": "PrivateLink、VPCエンドポイント、API、セキュリティポリシー",
        "overall_assessment": "企業の内部セキュリティポリシーとプライベート接続の要件を考慮すると、AWS PrivateLinkは確かに理想的なソリューションである。"
      }
    ],
    "keywords": [
      "PrivateLink",
      "VPC endpoint",
      "API",
      "security policies"
    ]
  },
  {
    "No": "13",
    "question": "A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to\nperform patching. Management requires a single report showing the patch status of all the servers and instances.\nWhich set of actions should a solutions architect take to meet these requirements?",
    "question_jp": "ある企業は、サーバーのパッチ管理プロセスを実装する必要があります。オンプレミスのサーバーとAmazon EC2インスタンスは、さまざまなツールを使用してパッチを適用しています。経営陣は、すべてのサーバーとインスタンスのパッチ状況を示す単一のレポートを要求しています。これらの要件を満たすために、ソリューションアーキテクトはどの一連のアクションを取るべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch",
        "text_jp": "AWS Systems Managerを使用してオンプレミスのサーバーとEC2インスタンスのパッチを管理します。Systems Managerを使用してパッチ"
      },
      {
        "key": "B",
        "text": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks",
        "text_jp": "AWS OpsWorksを使用してオンプレミスのサーバーとEC2インスタンスのパッチを管理します。OpsWorksとのAmazon QuickSight統合を使用します"
      },
      {
        "key": "C",
        "text": "Use an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to",
        "text_jp": "Amazon EventBridgeルールを使用して、AWS Systems Managerパッチ修正ジョブをスケジュールしてパッチを適用します。Amazon Inspectorを使用します"
      },
      {
        "key": "D",
        "text": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS",
        "text_jp": "AWS OpsWorksを使用してオンプレミスのサーバーとEC2インスタンスのパッチを管理します。AWS X-Rayを使用してパッチ状況をAWSに投稿します"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances.",
        "situation_analysis": "The company requires a centralized solution to manage and report the patch status of both on-premises servers and EC2 instances.",
        "option_analysis": "Option A is suitable because AWS Systems Manager provides a unified interface to manage patches across different environments. Other options may not offer the same level of integration and reporting.",
        "additional_knowledge": "Implementation of AWS Systems Manager can significantly simplify the patch process and reporting requirements.",
        "key_terminology": "AWS Systems Manager, Patch Manager, Amazon EC2, compliance report, automation.",
        "overall_assessment": "Option A aligns with AWS best practices for patch management, providing a comprehensive solution for both on-premises and cloud-based systems."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA: AWS Systems Managerを使用してオンプレミスのサーバーとEC2インスタンスのパッチを管理することです。",
        "situation_analysis": "企業は、オンプレミスのサーバーとEC2インスタンスのパッチ状況を管理し、報告するための中央集権的なソリューションを必要としています。",
        "option_analysis": "選択肢Aは適切です。AWS Systems Managerは、異なる環境全体のパッチを管理するための統合インターフェースを提供します。他のオプションは、同じレベルの統合と報告を提供しないかもしれません。",
        "additional_knowledge": "AWS Systems Managerの実装により、パッチプロセスと報告要件が大幅に簡素化される可能性があります。",
        "key_terminology": "AWS Systems Manager、Patch Manager、Amazon EC2、コンプライアンスレポート、自動化。",
        "overall_assessment": "選択肢Aは、パッチ管理に関するAWSのベストプラクティスに沿っており、オンプレミスとクラウドベースのシステムの両方に対して包括的なソリューションを提供しています。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "Patch Manager",
      "Amazon EC2"
    ]
  },
  {
    "No": "14",
    "question": "A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on\nthe application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied\nto a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2\ninstances.\nWhich set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?",
    "question_jp": "ある企業は、アプリケーションを複数の Amazon EC2 インスタンスで実行し、アプリケーションロードバランサーの背後にある Auto Scaling グループに配置しています。アプリケーションの負荷は日中さまざまに変化し、EC2 インスタンスは定期的にスケールインおよびスケールアウトしています。EC2 インスタンスからのログファイルは、15 分ごとに中央の Amazon S3 バケットにコピーされます。セキュリティチームは、いくつかの終了した EC2 インスタンスからログファイルが欠落していることを発見しました。終了した EC2 インスタンスから中央 S3 バケットにログファイルがコピーされることを確実にするためのアクションのセットはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and",
        "text_jp": "Amazon S3 にログファイルをコピーするスクリプトを作成し、そのスクリプトを EC2 インスタンス上のファイルに保存します。Auto Scaling ライフサイクルフックを作成し"
      },
      {
        "key": "B",
        "text": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an",
        "text_jp": "AWS Systems Manager ドキュメントを作成し、Amazon S3 にログファイルをコピーするスクリプトを作成します。Auto Scaling ライフサイクルフックと"
      },
      {
        "key": "C",
        "text": "Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data.",
        "text_jp": "ログ配信レートを 5 分ごとに変更します。ログファイルを Amazon S3 にコピーするスクリプトを作成し、そのスクリプトを EC2 インスタンスのユーザーデータに追加します。"
      },
      {
        "key": "D",
        "text": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that",
        "text_jp": "AWS Systems Manager ドキュメントを作成し、Amazon S3 にログファイルをコピーするスクリプトを作成します。Auto Scaling ライフサイクルフックを作成して"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating an AWS Systems Manager document with a script allows for automated log file copying even from terminated EC2 instances using the lifecycle hook.",
        "situation_analysis": "The application is running on EC2 instances with an Auto Scaling group behind a load balancer, and there are issues with lost log files during instance termination.",
        "option_analysis": "Option B is correct because it leverages Systems Manager for executing scripts that can run before instance termination. Other options fail to provide a comprehensive solution for capturing logs from terminated instances.",
        "additional_knowledge": "It's also worth considering that other methods like direct scripts may not ensure reliability for instance terminations.",
        "key_terminology": "AWS Systems Manager, lifecycle hooks, Auto Scaling, EC2 instances, log file management.",
        "overall_assessment": "The question is clear in its requirements, and the community overwhelmingly supports option B as it aligns with best practices in managing log files from EC2 instances."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。AWS Systems Manager ドキュメントを使ってスクリプトを作成することで、ライフサイクルフックを使用して終了した EC2 インスタンスからも自動的にログファイルをコピーできる。",
        "situation_analysis": "アプリケーションはロードバランサーの背後にある Auto Scaling グループの EC2 インスタンス上で実行されており、インスタンス終了時にログファイルが失われる問題が発生している。",
        "option_analysis": "選択肢Bは、インスタンス終了前に実行できるスクリプトを使用し、Systems Managerを活用するため正しい。その他の選択肢は、終了したインスタンスからログを取得する包括的なソリューションを提供しないため不適切である。",
        "additional_knowledge": "他の方法のような直接のスクリプトでは、インスタンスの終了時に信頼性が保証されない可能性があることも考慮する必要がある。",
        "key_terminology": "AWS Systems Manager, ライフサイクルフック, Auto Scaling, EC2 インスタンス, ログファイル管理。",
        "overall_assessment": "質問は要件が明確であり、コミュニティは選択肢Bを圧倒的に支持している。これは、EC2 インスタンスのログファイル管理のベストプラクティスに沿っている。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "lifecycle hooks",
      "Auto Scaling",
      "EC2 instances",
      "log file management"
    ]
  },
  {
    "No": "15",
    "question": "A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The\ncompany's applications and databases are running in Account B.\nA solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the\nAmazon RDS endpoint was created in a private hosted zone for Amazon Route 53.\nDuring deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance.\nThe solutions architect confirmed that the record set was created correctly in Route 53.\nWhich combination of steps should the solutions architect take to resolve this issue? (Choose two.)",
    "question_jp": "ある企業が複数のAWSアカウントを使用しています。DNSレコードは、アカウントAのAmazon Route 53のプライベートホステッドゾーンに保存されています。企業のアプリケーションとデータベースは、アカウントBで実行されています。ソリューションアーキテクトは、新しいVPCに2層のアプリケーションをデプロイします。構成を簡略化するために、Amazon RDSエンドポイント用のdb.example.com CNAMEレコードセットがAmazon Route 53のプライベートホステッドゾーンに作成されました。デプロイ中に、アプリケーションが起動できなかったことが判明しました。トラブルシューティングの結果、db.example.comはAmazon EC2インスタンス上で解決されていないことが明らかになりました。ソリューションアーキテクトは、Route 53でレコードセットが正しく作成されていることを確認しました。この問題を解決するために、ソリューションアーキテクトはどのステップの組み合わせを実施すべきですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance's private IP in the private hosted zone.",
        "text_jp": "データベースを新しいVPCの別のEC2インスタンスにデプロイします。インスタンスのプライベートIPのためのレコードセットをプライベートホステッドゾーンに作成します。"
      },
      {
        "key": "B",
        "text": "Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.",
        "text_jp": "SSHを使用してアプリケーション層のEC2インスタンスに接続します。/etc/resolv.confファイルにRDSエンドポイントのIPアドレスを追加します。"
      },
      {
        "key": "C",
        "text": "Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.",
        "text_jp": "アカウントAのプライベートホステッドゾーンをアカウントBの新しいVPCと関連付けるための承認を作成します。"
      },
      {
        "key": "D",
        "text": "Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts.",
        "text_jp": "アカウントBでexample.comドメインのプライベートホステッドゾーンを作成します。AWSアカウント間でRoute 53のレプリケーションを構成します。"
      },
      {
        "key": "E",
        "text": "Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.",
        "text_jp": "アカウントBの新しいVPCをアカウントAのホステッドゾーンと関連付けます。アカウントAの関連付け承認を削除します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are B and C. Option B allows direct resolution of the RDS endpoint IP on the EC2 instance, facilitating application startup.",
        "situation_analysis": "The application in Account B needs to resolve db.example.com to connect to the RDS instance in Account A, but it currently cannot due to the VPC separation.",
        "option_analysis": "Option B is crucial as it resolves the DNS directly from the EC2 instance. Option C is also necessary to enable communication between the hosted zone in Account A and the VPC in Account B.",
        "additional_knowledge": "A typical setup involves ensuring that relevant VPCs have the right DNS settings and that route tables allow proper communication.",
        "key_terminology": "DNS resolution, Route 53 private hosted zone, VPC association, cross-account access.",
        "overall_assessment": "Both B and C are valid steps. B solves the immediate issue of DNS resolution, while C creates necessary permissions for further visibility."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBとCである。選択肢Bは、EC2インスタンス上でRDSエンドポイントのIPを直接解決できるようにし、アプリケーションの起動を容易にする。",
        "situation_analysis": "アカウントBのアプリケーションは、アカウントAのRDSインスタンスに接続するためにdb.example.comを解決する必要があるが、現在VPCの分離のため解決できていない。",
        "option_analysis": "選択肢Bは、EC2インスタンスからDNSを直接解決するために重要である。一方、選択肢CもアカウントAのホステッドゾーンとアカウントBのVPC間の通信を可能にするために必要である。",
        "additional_knowledge": "通常の設定では、関連するVPCが正しいDNS設定を持ち、ルートテーブルが適切な通信を許可することが重要である。",
        "key_terminology": "DNS解決、Route 53プライベートホステッドゾーン、VPC関連付け、アカウント間アクセス。",
        "overall_assessment": "BとCはともに有効なステップである。BはDNS解決の即時的な問題を解決し、Cはさらなる可視性のための必要な権限を作成する。"
      }
    ],
    "keywords": [
      "Route 53",
      "EC2",
      "RDS",
      "DNS Resolution",
      "VPC Association"
    ]
  },
  {
    "No": "16",
    "question": "A company used Amazon EC2 instances to deploy a web fieet to host a blog site. The EC2 instances are behind an Application Load Balancer\n(ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.\nThe company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user trafic. At peak times of day,\nusers report buffering and timeout issues while attempting to reach the site or watch videos.\nWhich is the MOST cost-eficient and scalable deployment that will resolve the issues for users?",
    "question_jp": "ある企業が、ブログサイトをホストするためにAmazon EC2インスタンスを使用してウェブファイトを展開しました。EC2インスタンスはアプリケーションロードバランサー（ALB）の背後にあり、オートスケーリンググループに設定されています。ウェブアプリケーションは、すべてのブログコンテンツをAmazon EFSボリュームに保存しています。企業は最近、ブロガーが投稿にビデオを追加できる機能を追加し、以前の10倍のユーザー traficを引き寄せています。ピーク時には、ユーザーがサイトにアクセスしたりビデオを視聴したりする際にバッファリングやタイムアウトの問題を報告しています。ユーザーの問題を解決するために、最もコスト効率が高くスケーラブルな展開はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Reconfigure Amazon EFS to enable maximum I/O.",
        "text_jp": "Amazon EFSを再構成して最大I/Oを有効にする。"
      },
      {
        "key": "B",
        "text": "Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at",
        "text_jp": "ストレージ用にインスタンスストアボリュームを使用するようにブログサイトを更新する。起動時にサイトコンテンツをボリュームにコピーし、Amazon S3にコピーする。"
      },
      {
        "key": "C",
        "text": "Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.",
        "text_jp": "Amazon CloudFrontディストリビューションを構成する。ディストリビューションをS3バケットにポイントし、EFSからAmazon S3にビデオを移行する。"
      },
      {
        "key": "D",
        "text": "Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB.",
        "text_jp": "すべてのサイトコンテンツのためにAmazon CloudFrontディストリビューションを設定し、ディストリビューションをALBにポイントする。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (98%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Configuring an Amazon CloudFront distribution and migrating videos from EFS to S3 is the most effective way to ensure scalability and reduce buffering issues.",
        "situation_analysis": "The company is experiencing high traffic and performance issues due to loading video content from EFS. The existing architecture needs enhancement to handle increased user demand effectively.",
        "option_analysis": "Option A will not resolve the latency issues associated with EFS. Option B does not provide a scalable solution for video storage, and it adds complexities. Option D, while using CloudFront, does not effectively address video content delivery since the source remains EFS.",
        "additional_knowledge": "Implementing a CDN like CloudFront is a common best practice for enhancing web performance, especially for media content.",
        "key_terminology": "Amazon CloudFront, Amazon S3, Amazon EFS, CDN, content delivery",
        "overall_assessment": "The choice of C is strongly supported by the community, reflecting a consensus on the efficiency and effectiveness of using CloudFront with S3 for video content."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。Amazon CloudFrontディストリビューションを構成し、ビデオをEFSからS3に移行することにより、スケーラビリティを確保し、バッファリング問題を軽減する最も効果的な方法である。",
        "situation_analysis": "企業は、EFSからのビデオコンテンツの読み込みのために、高トラフィックとパフォーマンスの問題に直面している。増加したユーザーの需要に効果的に対処するために、既存のアーキテクチャの強化が必要である。",
        "option_analysis": "選択肢AはEFSのレイテンシ問題を解決しない。選択肢Bは動画ストレージのためにスケーラブルな解決策を提供せず、複雑さを加える。選択肢DはCloudFrontを使用しているが、ソースがEFSのままであるため、ビデオコンテンツの配信を効果的に解決しない。",
        "additional_knowledge": "CloudFrontのようなCDNを実装することは、特にメディアコンテンツのWebパフォーマンスを向上させるための一般的なベストプラクティスである。",
        "key_terminology": "Amazon CloudFront, Amazon S3, Amazon EFS, CDN, コンテンツ配信",
        "overall_assessment": "Cの選択は強くコミュニティによって支持されており、ビデオコンテンツにCloudFrontとS3を使用する効率性と効果の合意を反映している。"
      }
    ],
    "keywords": [
      "Amazon CloudFront",
      "Amazon S3",
      "Amazon EFS",
      "CDN",
      "content delivery"
    ]
  },
  {
    "No": "17",
    "question": "A company with global ofices has a single 1 Gbps AWS Direct Connect connection to a single AWS Region. The company's on-premises network\nuses the connection to communicate with the company's resources in the AWS Cloud. The connection has a single private virtual interface that\nconnects to a single VPC.\nA solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. The solution also must\nprovide connectivity to other Regions through the same pair of Direct Connect connections as the company expands into other Regions.\nWhich solution meets these requirements?",
    "question_jp": "グローバルオフィスを持つ企業が1つのAWSリージョンに1GbpsのAWS Direct Connect接続を持っています。この企業のオンプレミスネットワークは、この接続を使用してAWSクラウド内の企業リソースと通信しています。接続には、単一のVPCに接続する単一のプライベート仮想インターフェースがあります。ソリューションアーキテクトは、同じリージョンに冗長なDirect Connect接続を追加するソリューションを実装しなければなりません。このソリューションは、企業が他のリージョンに拡張する際に、同じ対のDirect Connect接続を通じて他のリージョンへの接続も提供する必要があります。どのソリューションがこれらの要件を満たしていますか？",
    "choices": [
      {
        "key": "A",
        "text": "Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct",
        "text_jp": "Direct Connectゲートウェイをプロビジョニングします。既存の接続から既存のプライベート仮想インターフェースを削除します。2つ目のDirectを作成します。"
      },
      {
        "key": "B",
        "text": "Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new private virtual interface on the new",
        "text_jp": "既存のプライベート仮想インターフェースを保持します。2つ目のDirect Connect接続を作成します。新しいプライベート仮想インターフェースを新しいで作成します。"
      },
      {
        "key": "C",
        "text": "Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new public virtual interface on the new",
        "text_jp": "既存のプライベート仮想インターフェースを保持します。2つ目のDirect Connect接続を作成します。新しい公共の仮想インターフェースを新しいで作成します。"
      },
      {
        "key": "D",
        "text": "Provision a transit gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect",
        "text_jp": "トランジットゲートウェイをプロビジョニングします。既存の接続から既存のプライベート仮想インターフェースを削除します。2つ目のDirect Connectを作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Provision a Direct Connect gateway, delete the existing private virtual interface, and create the second Direct Connect connection. This solution provides redundancy and allows for connectivity to multiple AWS Regions.",
        "situation_analysis": "The company requires a redundant Direct Connect connection in the same region to ensure high availability and also needs to connect to other regions in the future.",
        "option_analysis": "Option A is correct because provisioning a Direct Connect gateway enables the connection of multiple VPCs and regions. Option B and C do not provide redundancy as they rely on the existing private interface without utilizing a Direct Connect gateway. Option D suggests using a transit gateway but does not clarify the creation of a second connection correctly.",
        "additional_knowledge": "Implementing a Direct Connect gateway is considered best practice for organizations with multi-region needs.",
        "key_terminology": "Direct Connect, Direct Connect gateway, VPC, redundancy, transit gateway",
        "overall_assessment": "The question is well-structured to assess knowledge of AWS networking options and the importance of redundancy in cloud architectures."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はA：Direct Connectゲートウェイをプロビジョニングし、既存のプライベート仮想インターフェースを削除し、2つ目のDirect Connect接続を作成します。このソリューションは冗長性を提供し、複数のAWSリージョンへの接続を可能にします。",
        "situation_analysis": "企業は、高可用性を確保するために同じリージョンに冗長なDirect Connect接続を必要とし、将来的に他のリージョンに接続する必要があります。",
        "option_analysis": "オプションAが正しいのは、Direct Connectゲートウェイをプロビジョニングすることで複数のVPCやリージョンへの接続が可能になるためです。オプションBおよびCは、既存のプライベートインターフェースを維持するため、冗長性を提供しません。オプションDはトランジットゲートウェイの使用を提案しますが、2つ目の接続の作成を正しく説明していません。",
        "additional_knowledge": "Direct Connectゲートウェイの実装は、マルチリージョンのニーズを持つ組織にとってベストプラクティスと見なされます。",
        "key_terminology": "Direct Connect、Direct Connectゲートウェイ、VPC、冗長性、トランジットゲートウェイ",
        "overall_assessment": "この質問は、AWSのネットワーキングオプションとクラウドアーキテクチャにおける冗長性の重要性についての知識を評価するのに適切に構築されています。"
      }
    ],
    "keywords": [
      "Direct Connect",
      "Direct Connect gateway",
      "VPC",
      "redundancy",
      "transit gateway"
    ]
  },
  {
    "No": "18",
    "question": "A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by\ncustom recognition software for categorization.\nThe website contains static content that has variable trafic with peaks in certain months. The architecture consists of Amazon EC2 instances\nrunning in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue.\nThe company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove\ndependencies on third-party software.\nWhich solution meets these requirements?",
    "question_jp": "ある企業がユーザーが短い動画をアップロードできるウェブアプリケーションを持っています。動画はAmazon EBSボリュームに保存され、分類のためにカスタム認識ソフトウェアによって分析されています。ウェブサイトには一定の月にピークのある変動するトラフィックを持つ静的コンテンツが含まれています。アーキテクチャは、ウェブアプリケーション用にAuto Scalingグループで実行されるAmazon EC2インスタンスと、Amazon SQSキューを処理するためにAuto Scalingグループで実行されるEC2インスタンスで構成されています。企業は運用オーバーヘッドを削減するためにアプリケーションを再設計したいと考えており、可能な限りAWSの管理サービスを利用し、サードパーティのソフトウェアへの依存を排除したいと考えています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace",
        "text_jp": "Amazon ECSコンテナをウェブアプリケーションに使用し、SQSキューを処理するAuto Scalingグループにスポットインスタンスを使用します。"
      },
      {
        "key": "B",
        "text": "Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue",
        "text_jp": "アップロードされた動画をAmazon EFSに保存し、ウェブアプリケーション用のEC2インスタンスにファイルシステムをマウントします。SQSキューを処理します。"
      },
      {
        "key": "C",
        "text": "Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS",
        "text_jp": "ウェブアプリケーションをAmazon S3にホストします。アップロードされた動画をAmazon S3に保存します。S3イベント通知を使用してイベントをSQSに公開します。"
      },
      {
        "key": "D",
        "text": "Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to",
        "text_jp": "AWS Elastic Beanstalkを使用してウェブアプリケーションのためのEC2インスタンスをAuto Scalingグループで起動し、ワーカー環境を立ち上げます。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (85%) D (15%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using AWS Elastic Beanstalk allows for easier management of the web application and automatically handles everything needed to launch and scale the application.",
        "situation_analysis": "The company wants to decrease operational overhead and utilize managed services where possible. Elastic Beanstalk provides a scalable environment without requiring the management of the underlying EC2 instances.",
        "option_analysis": "Option D supports the requirements of reducing operational overhead. Other options either create more overhead or do not fully remove dependencies on third-party services.",
        "additional_knowledge": "It is important to note that while S3 is excellent for static content, it cannot fully replace the dynamic capabilities of a complete managed service like Elastic Beanstalk.",
        "key_terminology": "AWS Elastic Beanstalk, Amazon EC2, Auto Scaling, managed services",
        "overall_assessment": "Given the requirements for operational efficiency and reduced complexity, answer D is the optimal solution despite community preference for C. C does not fully address the need for reduced operational management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。AWS Elastic Beanstalkを使用することで、ウェブアプリケーションの管理が簡素化され、アプリケーションの起動とスケーリングに必要なすべてを自動的に処理することができる。",
        "situation_analysis": "企業は運用オーバーヘッドを減少させることを望んでおり、可能な限り管理サービスを利用したいと考えている。Elastic Beanstalkは、基盤となるEC2インスタンスの管理を必要とせずに、スケーラブルな環境を提供する。",
        "option_analysis": "選択肢Dは運用オーバーヘッドの削減という要件をサポートしている。他の選択肢は、運用負荷を増大させるか、サードパーティサービスへの依存を完全には排除していない。",
        "additional_knowledge": "S3が静的コンテンツに優れているのは重要だが、Elastic Beanstalkのような完全な管理サービスの動的機能を完全に代替することはできないことを指摘することが重要である。",
        "key_terminology": "AWS Elastic Beanstalk, Amazon EC2, Auto Scaling, 管理サービス",
        "overall_assessment": "運用効率と複雑さの削減に関する要件を考慮すると、答えDは最適なソリューションである。コミュニティがCを好むにもかかわらず、Cは運用管理の削減必要性に完全には対応していない。"
      }
    ],
    "keywords": [
      "AWS Elastic Beanstalk",
      "Amazon EC2",
      "Auto Scaling",
      "managed services"
    ]
  },
  {
    "No": "19",
    "question": "A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current\ndeployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the\nnew function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to\ndecrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert\nwhen errors are identified.\nHow can this be accomplished?",
    "question_jp": "ある企業は、Amazon CloudFront、Amazon API Gateway、およびAWS Lambda関数から構成されるサーバーレスアプリケーションを持っています。アプリケーションコードの現在のデプロイメントプロセスは、Lambda関数の新しいバージョン番号を作成し、AWS CLIスクリプトを実行して更新することです。新しい関数バージョンにエラーがある場合、別のCLIスクリプトが実行され、前の動作しているバージョンの関数を再デプロイします。企業は、Lambda関数によって提供されるアプリケーションロジックの新しいバージョンのデプロイにかかる時間を短縮し、エラーが発見された場合の検出およびロールバックにかかる時間を短縮したいと考えています。これはどのように実現できますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API",
        "text_jp": "AWS CloudFrontディストリビューションとAPIで構成される親スタックを持つネストされたAWS CloudFormationスタックを作成してデプロイする"
      },
      {
        "key": "B",
        "text": "Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift trafic to the new version, and use pre-trafic",
        "text_jp": "AWS SAMと組み込みのAWS CodeDeployを使用して新しいLambdaバージョンをデプロイし、徐々に新しいバージョンへのトラフィックをシフトし、トラフィック前にテストを行う"
      },
      {
        "key": "C",
        "text": "Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests",
        "text_jp": "AWS CLIスクリプトをリファクタリングして新しいLambdaバージョンをデプロイする単一のスクリプトにまとめる。デプロイが完了したら、スクリプトがテストを行う"
      },
      {
        "key": "D",
        "text": "Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version.",
        "text_jp": "新しいLambdaバージョンを参照する新しいAPI Gatewayエンドポイントで構成されるAWS CloudFormationスタックを作成してデプロイする"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Using AWS SAM and AWS CodeDeploy allows for more efficient deployment and error handling.",
        "situation_analysis": "The current process involves manual scripts that can be error-prone and slow. A more automated solution is needed.",
        "option_analysis": "Option B supports gradual traffic shifting and provides built-in rollback capabilities, which are essential for minimizing downtime during deployment.",
        "additional_knowledge": "Understanding how to implement AWS SAM and CodeDeploy can significantly streamline deployment pipelines.",
        "key_terminology": "AWS SAM, AWS CodeDeploy, traffic shifting, rollback, serverless applications.",
        "overall_assessment": "Option B aligns best with AWS best practices for serverless application deployment, facilitating quicker rollbacks and reduced deployment times."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、AWS SAMとAWS CodeDeployを使用することで、より効率的なデプロイメントとエラーハンドリングが可能です。",
        "situation_analysis": "現在のプロセスは手動スクリプトであり、エラーが発生しやすく、遅いです。より自動化された解決策が必要です。",
        "option_analysis": "選択肢Bは、段階的なトラフィックシフトをサポートし、デプロイメント中のダウンタイムを最小限にするために重要なロールバック機能を提供します。",
        "additional_knowledge": "AWS SAMとCodeDeployの実装方法を理解することで、デプロイメントパイプラインを大幅に効率化できます。",
        "key_terminology": "AWS SAM、AWS CodeDeploy、トラフィックシフト、ロールバック、サーバーレスアプリケーション。",
        "overall_assessment": "選択肢BはサーバーレスアプリケーションのデプロイメントにおけるAWSのベストプラクティスに最も適合し、迅速なロールバックと短縮されたデプロイメント時間を促進します。"
      }
    ],
    "keywords": [
      "AWS SAM",
      "AWS CodeDeploy",
      "traffic shifting",
      "rollback",
      "serverless applications"
    ]
  },
  {
    "No": "20",
    "question": "A company is planning to store a large number of archived documents and make the documents available to employees through the corporate\nintranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible\nto the public.\nThe documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low.\nAvailability and speed of retrieval are not concerns of the company.\nWhich solution will meet these requirements at the LOWEST cost?",
    "question_jp": "企業は大量のアーカイブ文書を保存し、文書を企業のイントラネットを通じて従業員に提供する計画を立てています。従業員は、VPCに接続されたクライアントVPNサービスを介してシステムにアクセスします。データは公開されてはなりません。企業が保存している文書は、他の場所にある物理メディアに保存されたデータのコピーです。リクエスト数は少なくなります。可用性と取得速度は企業にとっての懸念事項ではありません。どのソリューションが最低コストでこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default.",
        "text_jp": "Amazon S3バケットを作成します。S3バケットのデフォルトをS3 One Zone-インフリークエントアクセス（S3 One Zone-IA）ストレージクラスに設定します。"
      },
      {
        "key": "B",
        "text": "Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the",
        "text_jp": "ウェブサーバーを実行するAmazon EC2インスタンスを起動します。アーカイブ用にAmazon Elastic File System（Amazon EFS）ファイルシステムを添付します。"
      },
      {
        "key": "C",
        "text": "Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived",
        "text_jp": "ウェブサーバーを実行するAmazon EC2インスタンスを起動します。アーカイブ用にAmazon Elastic Block Store（Amazon EBS）ボリュームを添付します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket",
        "text_jp": "Amazon S3バケットを作成します。S3バケットのデフォルトをS3 Glacier Deep Archiveストレージクラスに設定します。S3バケットを設定します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (65%) D (34%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using S3 Glacier Deep Archive is the most economical solution for long-term storage of infrequently accessed documents.",
        "situation_analysis": "The company is seeking a low-cost solution for storing archived documents safely and without public access. The volume of access will be low, and retrieval speed is not a primary concern.",
        "option_analysis": "Option D provides a suitable cost-efficient solution with S3 Glacier Deep Archive, whereas options A, B, and C would incur higher costs due to their storage and retrieval features that are not necessary for the company's needs.",
        "additional_knowledge": "S3 Glacier provides various retrieval options that allow users to select their required speed, even if speed is not a concern, it provides ample flexibility.",
        "key_terminology": "Amazon S3, S3 Glacier Deep Archive, data archival",
        "overall_assessment": "Despite the community vote distribution showing a preference for option A, it's incorrect because S3 One Zone-IA is more expensive compared to S3 Glacier Deep Archive for lower frequency access. Option D clearly meets the requirements at the lowest cost."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。S3 Glacier Deep Archiveを使用することが、まれにアクセスされる文書の長期保存に最も経済的なソリューションである。",
        "situation_analysis": "企業は、アーカイブ文書を安全に保存し、公開アクセスなしで低コストで解決策を求めている。アクセス数は少なく、取得速度は主な懸念事項ではない。",
        "option_analysis": "選択肢Dは、S3 Glacier Deep Archiveによるコスト効率の良いソリューションを提供しているが、選択肢A、B、Cは、企業のニーズには必要ないストレージと取得機能のために高いコストが発生する。",
        "additional_knowledge": "S3 Glacierは、ユーザーが必要な速度を選択できるさまざまな取得オプションを提供し、速度が重要ではない場合でも十分な柔軟性を提供している。",
        "key_terminology": "Amazon S3、S3 Glacier Deep Archive、データアーカイブ",
        "overall_assessment": "コミュニティの投票分布は選択肢Aを支持しているが、選択肢Aは誤りであり、S3 One Zone-IAはS3 Glacier Deep Archiveよりもコストが高い。選択肢Dは明確に最低コストで要件を満たしている。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "data archival"
    ]
  },
  {
    "No": "21",
    "question": "A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to\nsign in to the company's AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-\npremises environment and all the company's AWS accounts.\nThe company's security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a\nsingle location.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がユーザー認証のためにオンプレミスのActive Directoryサービスを使用しています。この企業は、同じ認証サービスを使用して、AWS Organizationsを使用している自社のAWSアカウントにサインインしたいと考えています。オンプレミス環境とすべてのAWSアカウントの間には、AWS Site-to-Site VPN接続がすでに存在しています。この企業のセキュリティポリシーは、ユーザーグループや役割に基づいてアカウントへの条件付きアクセスを要求しています。ユーザーのアイデンティティは、単一の場所で管理する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0. Enable automatic provisioning",
        "text_jp": "AWS IAM Identity Center (AWS Single Sign-On)をSAML 2.0を使用してActive Directoryに接続するように設定します。自動プロビジョニングを有効にします。"
      },
      {
        "key": "B",
        "text": "Configure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source. Enable automatic provisioning",
        "text_jp": "AWS IAM Identity Center (AWS Single Sign-On)を使用して、IAM Identity Centerをアイデンティティソースとして設定します。自動プロビジョニングを有効にします。"
      },
      {
        "key": "C",
        "text": "In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provider.",
        "text_jp": "自社のAWSアカウントの1つで、AWS Identity and Access Management (IAM)を使用してSAML 2.0アイデンティティプロバイダを設定します。"
      },
      {
        "key": "D",
        "text": "In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity",
        "text_jp": "自社のAWSアカウントの1つで、AWS Identity and Access Management (IAM)を使用してOpenID Connect (OIDC)アイデンティティを設定します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (82%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using OpenID Connect with IAM allows integration with the existing Active Directory for user management and conditional access.",
        "situation_analysis": "The company has on-premises Active Directory and requires conditional access based on user roles and groups for AWS accounts managed under AWS Organizations.",
        "option_analysis": "Option D is appropriate because it allows the integration with existing identity services through OIDC. Other options may not meet the condition of managing user identities in one place effectively.",
        "additional_knowledge": "Community preferences can sometimes reflect common misconceptions or familiarity rather than alignment with the specific requirements.",
        "key_terminology": "OpenID Connect, AWS IAM, AWS Organizations, Active Directory, user management.",
        "overall_assessment": "Despite a community vote suggesting a preference for option A, option D remains the most aligned with the requirements as it explicitly states the use of OpenID Connect, thereby allowing for external identity federation."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです。IAMを使用したOpenID Connectの利用により、既存のActive Directoryとの統合が可能になり、ユーザー管理と条件付きアクセスを実現します。",
        "situation_analysis": "この企業はオンプレミスのActive Directoryを保有しており、AWS Organizationsで管理されているAWSアカウントへの条件付きアクセスをユーザーの役割やグループに基づいて要求しています。",
        "option_analysis": "選択肢Dは、既存のアイデンティティサービスとの統合が可能であるため適切です。他の選択肢は、ユーザーアイデンティティを効果的に一箇所で管理する要件を満たさないかもしれません。",
        "additional_knowledge": "コミュニティの好みは時に共通の誤解や親しみから生じることがあり、特定の要件との整合性を欠く場合があります。",
        "key_terminology": "OpenID Connect, AWS IAM, AWS Organizations, Active Directory, ユーザー管理。",
        "overall_assessment": "コミュニティの投票は選択肢Aに対する好みを示していますが、選択肢DはOpenID Connectの明示的な利用を示しており、要求に最も適しているため正しい選択肢です。"
      }
    ],
    "keywords": [
      "OpenID Connect",
      "AWS IAM",
      "AWS Organizations",
      "Active Directory",
      "user management"
    ]
  },
  {
    "No": "22",
    "question": "A software company has deployed an application that consumes a REST API by using Amazon API Gateway, AWS Lambda functions, and an\nAmazon DynamoDB table. The application is showing an increase in the number of errors during PUT requests. Most of the PUT calls come from a\nsmall number of clients that are authenticated with specific API keys.\nA solutions architect has identified that a large number of the PUT requests originate from one client. The API is noncritical, and clients can\ntolerate retries of unsuccessful calls. However, the errors are displayed to customers and are causing damage to the API's reputation.\nWhat should the solutions architect recommend to improve the customer experience?",
    "question_jp": "ソフトウェア会社は、Amazon API Gateway、AWS Lambda関数、およびAmazon DynamoDBテーブルを使用してREST APIを消費するアプリケーションを展開しました。このアプリケーションでは、PUTリクエスト中にエラーの数が増加しています。ほとんどのPUT呼び出しは、特定のAPIキーで認証された少数のクライアントから来ています。ソリューションアーキテクトは、多くのPUTリクエストが1つのクライアントから発生していることを特定しました。APIは重要ではなく、クライアントは不成功な呼び出しの再試行に耐えることができます。しかし、エラーは顧客に表示されており、APIの評判に悪影響を及ぼしています。顧客経験を改善するために、ソリューションアーキテクトが推奨すべきは何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Implement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and",
        "text_jp": "クライアントアプリケーションで指数バックオフと不規則な変動を伴う再試行ロジックを実装します。エラーが捕捉されることを確認します。"
      },
      {
        "key": "B",
        "text": "Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without",
        "text_jp": "API Gatewayレベルで使用プランを介してAPIスロットリングを実装します。クライアントアプリケーションがコード429の応答を処理します。"
      },
      {
        "key": "C",
        "text": "Turn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is",
        "text_jp": "APIキャッシングをオンにして、プロダクションステージの応答性を向上させます。10分間の負荷テストを実行します。キャッシュ容量を確認します。"
      },
      {
        "key": "D",
        "text": "Implement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in trafic.",
        "text_jp": "Lambda関数レベルでの予約同時実行性を実装して、トラフィックの急増時に必要なリソースを提供します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (68%) A (31%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Implementing API throttling at the API Gateway level helps manage the number of requests from any given client and prevents overwhelming the resources.",
        "situation_analysis": "A significant number of errors in PUT requests originate from a single client, affecting the customer experience and API reputation. The API is noncritical, so some retries are acceptable.",
        "option_analysis": "Option A deals with error handling but does not address the root cause of the problem - too many requests from a client. Option C improves performance but does not resolve the error issue. Option D ensures availability but doesn't limit overload from specific clients. Hence, Option B is the best choice.",
        "additional_knowledge": "It may also be beneficial to monitor usage and adjust the throttling settings as necessary based on client behavior.",
        "key_terminology": "API Gateway, API Throttling, Usage Plan, Code 429, Rate Limiting",
        "overall_assessment": "Option B is the best recommendation given the circumstances. It focuses on controlling the request rate from the problematic client while allowing the API to remain reliable and improving the user experience."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。API GatewayレベルでAPIスロットリングを実装することで、特定のクライアントからのリクエスト数を管理し、リソースが過負荷になるのを防ぐことができる。",
        "situation_analysis": "PUTリクエストにおけるエラーの多くが1つのクライアントから発生しており、顧客体験とAPIの評判に悪影響を与えている。APIは重要でないため、ある程度の再試行は許容される。",
        "option_analysis": "選択肢Aはエラーハンドリングに対処するが、問題の根本原因である特定クライアントからの過剰なリクエストには対処していない。選択肢Cはパフォーマンスを向上させるが、エラー問題の解決にはならない。選択肢Dは可用性を確保するが、特定のクライアントからの過負荷を制限しない。したがって、選択肢Bが最適である。",
        "additional_knowledge": "クライアントの動作に基づいて監視とスロットリング設定の調整も有益である場合がある。",
        "key_terminology": "API Gateway, APIスロットリング, 使用プラン, コード429, レートリミティング",
        "overall_assessment": "選択肢Bは状況に最も適している推奨事項である。問題のあるクライアントからのリクエスト率を制御することで、APIの信頼性を保ちながらユーザー体験を向上させることができる。"
      }
    ],
    "keywords": [
      "API Gateway",
      "API Throttling",
      "Usage Plan",
      "Code 429",
      "Rate Limiting"
    ]
  },
  {
    "No": "23",
    "question": "A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file\nsystem also runs on several EC2 instances that store 200 TB of data. The application reads and modifies the data on the shared file system and\ngenerates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete. The\ncompute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage\ninstances are all in the same AWS Region.\nA solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access\nto the needed data for the duration of the 72-hour run.\nWhich solution will provide the LARGEST overall cost reduction while meeting these requirements?",
    "question_jp": "ある企業がAWS上でデータ集約型アプリケーションを運用している。このアプリケーションは、数百のAmazon EC2インスタンスのクラスター上で動作している。共有ファイルシステムも複数のEC2インスタンス上で動作し、200TBのデータを保存している。アプリケーションは、共有ファイルシステム上のデータを読み取り、変更し、レポートを生成する。このジョブは毎月1回実行され、共有ファイルシステムからファイルのサブセットを読み込み、完了には約72時間かかる。コンピュートインスタンスはAuto Scalingグループでスケーリングされるが、共有ファイルシステムをホストするインスタンスは常時稼働している。コンピュートおよびストレージインスタンスはすべて同じAWSリージョン内にある。ソリューションアーキテクトは、コストを削減するために共有ファイルシステムインスタンスを置き換える必要がある。ファイルシステムは、72時間の実行期間中に必要なデータへの高パフォーマンスアクセスを提供しなければならない。どのソリューションが、これらの要件を満たしつつ、最も大きな全体的コスト削減を提供するであろうか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the",
        "text_jp": "既存の共有ファイルシステムからデータをAmazon S3バケットに移行し、S3 Intelligent-Tieringストレージクラスを使用する。ジョブが実行される前に"
      },
      {
        "key": "B",
        "text": "Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach",
        "text_jp": "既存の共有ファイルシステムからデータを大規模なAmazon Elastic Block Store（Amazon EBS）ボリュームにマルチアタッチを使用して移行する。"
      },
      {
        "key": "C",
        "text": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs",
        "text_jp": "既存の共有ファイルシステムからデータをAmazon S3バケットに移行し、S3 Standardストレージクラスを使用する。ジョブが実行される前に"
      },
      {
        "key": "D",
        "text": "Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway",
        "text_jp": "既存の共有ファイルシステムからデータをAmazon S3バケットに移行する。ジョブが毎月実行される前に、AWS Storage Gatewayを使用して"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, migrating data to Amazon S3 and using AWS Storage Gateway for pre-job access.",
        "situation_analysis": "The requirement is to replace an existing high-performance shared file system while achieving cost reduction. The application only processes the data once a month.",
        "option_analysis": "Option D allows for cost savings and uses Amazon S3, which is scalable and cheaper than running multiple EC2 instances for a shared file system. Options A, B, and C do not provide the same level of cost efficiency or may lack the necessary performance required during the processing period.",
        "additional_knowledge": "AWS Storage Gateway serves as a bridge between on-premises infrastructure and AWS cloud storage.",
        "key_terminology": "Amazon S3, AWS Storage Gateway, cost optimization, data-intensive applications, object storage",
        "overall_assessment": "Given the community vote heavily favors option A, it is vital to highlight that while S3 Intelligent-Tiering might seem beneficial for cost, it does not guarantee the performance needed for data access during the job execution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDであり、データをAmazon S3に移行し、AWS Storage Gatewayを利用してジョブ実行前にアクセスする。",
        "situation_analysis": "既存の高パフォーマンス共有ファイルシステムを置き換えつつコスト削減する必要がある。アプリケーションは月に一度データを処理するだけである。",
        "option_analysis": "選択肢Dはコスト削減を可能にし、スケーラブルで共有ファイルシステム用のEC2インスタンスを複数運用するよりも安価なAmazon S3を使用している。選択肢A、B、Cは同じレベルのコスト効率を提供しないか、処理期間中に必要なパフォーマンスを欠いている可能性がある。",
        "additional_knowledge": "AWS Storage Gatewayは、オンプレミスインフラストラクチャとAWSクラウドストレージの橋渡しを行う。",
        "key_terminology": "Amazon S3、AWS Storage Gateway、コスト最適化、データ集約型アプリケーション、オブジェクトストレージ",
        "overall_assessment": "コミュニティの投票が選択肢Aを大きく支持しているが、データアクセスのパフォーマンスが必要なため、S3 Intelligent-Tieringは一見コストにプラスに思えたとしてもジョブ実行中のアクセスには必ずしも適していないことを強調することが重要である。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "AWS Storage Gateway",
      "cost optimization",
      "data-intensive applications",
      "object storage"
    ]
  },
  {
    "No": "24",
    "question": "A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is\nhighly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible.\nThe service must use fixed address assignments so other companies can add the addresses to their allow lists.\nAssuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?",
    "question_jp": "ある企業がTCPを使用して静的ポートでアクセスされる新しいサービスを開発しています。ソリューションアーキテクトは、そのサービスが高可用性を持ち、可用性ゾーン全体に冗長性があり、DNS名my.service.comを使用してアクセスできることを保証しなければなりません。このサービスは固定アドレス割り当てを使用する必要があり、他の企業がアドレスを許可リストに追加できるようにします。リソースが単一のリージョン内の複数の可用性ゾーンにデプロイされていると仮定した場合、どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static",
        "text_jp": "各インスタンスにElastic IPアドレスを割り当てたAmazon EC2インスタンスを作成します。Network Load Balancer（NLB）を作成し、静的を公開します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create",
        "text_jp": "Amazon ECSクラスターを作成し、アプリケーションのサービス定義を作成します。ECSクラスターにパブリックIPアドレスを作成して割り当てます。"
      },
      {
        "key": "C",
        "text": "Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer",
        "text_jp": "サービスのためにAmazon EC2インスタンスを作成します。各可用性ゾーンにElastic IPアドレスを1つ作成します。Network Load Balancerを作成します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster.",
        "text_jp": "Amazon ECSクラスターを作成し、アプリケーションのサービス定義を作成します。クラスター内の各ホストにパブリックIPアドレスを作成して割り当てます。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It involves creating Amazon EC2 instances with one Elastic IP address for each Availability Zone, ensuring high availability and static IP allocation.",
        "situation_analysis": "The company requires a highly available service with redundancy across Availability Zones and fixed IP addresses for external security settings.",
        "option_analysis": "Option C meets all requirements. Creating one Elastic IP for each Availability Zone ensures that the service remains accessible with static IPs. Options A and D do not provide sufficient static IP guarantees, and option B introduces unnecessary complexity.",
        "additional_knowledge": "Elastic IPs can be quickly reassigned if instances fail, providing an additional layer of resilience.",
        "key_terminology": "Elastic IP, Network Load Balancer, Availability Zones, high availability.",
        "overall_assessment": "Option C is the best approach to meet the outlined requirements effectively, while considering AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。これは、各可用性ゾーンにElastic IPアドレスを1つ割り当てたAmazon EC2インスタンスを作成することを含み、高可用性と静的IPの割り当てを保証する。",
        "situation_analysis": "この企業は、可用性ゾーン全体に冗長性を持ち、外部のセキュリティ設定のために固定IPアドレスを使用する必要がある高可用サービスを求めている。",
        "option_analysis": "オプションCはすべての要件を満たしている。各可用性ゾーンに1つのElastic IPを作成することで、サービスは静的IPでアクセス可能なままとなる。オプションAとDは静的IPの保証が不十分であり、オプションBは不必要な複雑さを持ち込む。",
        "additional_knowledge": "Elastic IPはインスタンスが失敗した場合に迅速に再割り当てができ、追加の耐障害性を提供する。",
        "key_terminology": "Elastic IP、Network Load Balancer、可用性ゾーン、高可用性。",
        "overall_assessment": "オプションCは、提示された要件を効果的に満たすための最良のアプローチであり、AWSのベストプラクティスを考慮している。"
      }
    ],
    "keywords": [
      "Elastic IP",
      "Network Load Balancer",
      "Availability Zones",
      "high availability"
    ]
  },
  {
    "No": "25",
    "question": "A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the\ncompany's data center.\nThe system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes\nand 2 hours to finish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in\nless than 5 minutes and have no SLA. The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to\nmeet SLAs. However, user jobs can be delayed.\nA solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-\nterm commitments. The solution must maintain high availability and must not affect the SLAs.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある会社がオンプレミスのデータ分析プラットフォームを使用しています。このシステムは、会社のデータセンター内の12台のサーバーにわたって完全冗長構成で非常に高い可用性を維持しています。システムは、毎時および毎日のスケジュールされたジョブを実行するほか、ユーザーからの一度きりのリクエストも処理します。スケジュールされたジョブの実行には20分から2時間かかることがあり、厳しいSLA（サービスレベル合意）があります。スケジュールされたジョブはシステム使用率の65%を占めます。ユーザーのジョブは通常5分未満で完了し、SLAはありません。ユーザーのジョブはシステム使用率の35%を占めています。システム障害時には、スケジュールされたジョブは引き続きSLAを維持する必要がありますが、ユーザーのジョブは遅延が許容されます。ソリューションアーキテクトは、システムをAmazon EC2インスタンスに移行し、長期的なコミットメントなしでコストを削減するために消費ベースのモデルを採用する必要があります。このソリューションは、高可用性を維持し、SLAに影響を与えない必要があります。どのソリューションがこれらの要件を最もコスト効果的に満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Split the 12 instances across two Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand",
        "text_jp": "選択したAWSリージョン内の2つのアベイラビリティゾーンに12台のインスタンスを分割します。各アベイラビリティゾーンで2台のインスタンスをオンデマンドとして実行します。"
      },
      {
        "key": "B",
        "text": "Split the 12 instances across three Availability Zones in the chosen AWS Region. In one of the Availability Zones, run all four instances as",
        "text_jp": "選択したAWSリージョン内の3つのアベイラビリティゾーンに12台のインスタンスを分割します。1つのアベイラビリティゾーン内で全4台のインスタンスを実行します。"
      },
      {
        "key": "C",
        "text": "Split the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand",
        "text_jp": "選択したAWSリージョン内の3つのアベイラビリティゾーンに12台のインスタンスを分割します。各アベイラビリティゾーンで2台のインスタンスをオンデマンドとして実行します。"
      },
      {
        "key": "D",
        "text": "Split the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On-",
        "text_jp": "選択したAWSリージョン内の3つのアベイラビリティゾーンに12台のインスタンスを分割します。各アベイラビリティゾーンで3台のインスタンスをオンデマンドとして実行します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Running two instances in each of the three Availability Zones provides the necessary redundancy while also being cost-effective by using On-Demand instances.",
        "situation_analysis": "The system requires high availability and must ensure scheduled jobs meet strict SLAs. The user jobs have no SLA and can be delayed, which allows for flexible resource allocation.",
        "option_analysis": "Option A proposes only 4 instances across 2 Availability Zones which may not meet the high availability requirement. Option B attempts to run all instances in one Zone which creates a single point of failure. Option D exceeds the job requirement by providing too many instances leading to unnecessary costs.",
        "additional_knowledge": "It is essential to analyze the load patterns to further optimize the instance usage.",
        "key_terminology": "Amazon EC2, Availability Zones, On-Demand instances, SLAs, High Availability",
        "overall_assessment": "Choosing option C aligns well with AWS best practices for maintaining high availability while controlling costs. Despite community votes favoring D, C adequately meets system requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。各アベイラビリティゾーンに2台のインスタンスを実行することは、コスト効果を保ちながら必要な冗長性を提供する。",
        "situation_analysis": "システムは高可用性を必要とし、スケジュールされたジョブが厳格なSLAを満たすことを確保しなければならない。ユーザーのジョブにはSLAがなく遅延が許可されているため、リソースの柔軟な割り当てが可能である。",
        "option_analysis": "選択肢Aは、2つのアベイラビリティゾーンに4台のインスタンスしか提案しておらず、高可用性要件を満たさない可能性がある。選択肢Bは、全てのインスタンスを1つのゾーンで実行するため、単一障害点を生じさせる。選択肢Dは必要以上にインスタンスを提供し、コストの無駄を招く。",
        "additional_knowledge": "インスタンスの使用率のパターンを分析し、さらに最適化することが重要である。",
        "key_terminology": "Amazon EC2, アベイラビリティゾーン, オンデマンドインスタンス, SLA, 高可用性",
        "overall_assessment": "選択肢Cは、高可用性を維持しつつコストを管理する上で、AWSのベストプラクティスに合致している。コミュニティの投票がDを支持していても、Cはシステム要件を十分に満たしている。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Availability Zones",
      "On-Demand instances",
      "SLAs",
      "High Availability"
    ]
  },
  {
    "No": "26",
    "question": "A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in\nAmazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve\nsecurity:\nThe database must use strong, randomly generated passwords stored in a secure AWS managed service.\nThe application resources must be deployed through AWS CloudFormation.\nThe application must rotate credentials for the database every 90 days.\nA solutions architect will generate a CloudFormation template to deploy the application.\nWhich resources specified in the CloudFormation template will meet the security engineer's requirements with the LEAST amount of operational\noverhead?",
    "question_jp": "セキュリティエンジニアは、既存のアプリケーションが暗号化されたファイルからAmazon RDS for MySQLデータベースの認証情報を取得していることを確認しました。 アプリケーションの次のバージョンのために、セキュリティエンジニアは次のアプリケーション設計の変更を実装して、セキュリティを向上させたいと考えています。データベースは、セキュアなAWSマネージドサービスに保存された強力でランダムに生成されたパスワードを使用しなければなりません。アプリケーションリソースはAWS CloudFormationを通じて展開される必要があります。アプリケーションは、データベースの認証情報を90日ごとにローテーションしなければなりません。ソリューションアーキテクトは、アプリケーションを展開するためのCloudFormationテンプレートを生成します。CloudFormationテンプレートに指定されたリソースのどれが、セキュリティエンジニアの要件を運用オーバーヘッドが最も少なく満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the",
        "text_jp": "データベースパスワードをAWS Secrets Managerを使用してシークレットリソースとして生成します。 AWS Lambda関数リソースを作成してローテーションします。"
      },
      {
        "key": "B",
        "text": "Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda",
        "text_jp": "データベースパスワードをAWS Systems Manager Parameter Storeを使用してSecureStringパラメータタイプとして生成します。 AWS Lambda関数を作成する"
      },
      {
        "key": "C",
        "text": "Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the",
        "text_jp": "データベースパスワードをAWS Secrets Managerを使用してシークレットリソースとして生成します。 AWS Lambda関数リソースを作成してローテーションします。"
      },
      {
        "key": "D",
        "text": "Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync",
        "text_jp": "データベースパスワードをAWS Systems Manager Parameter Storeを使用してSecureStringパラメータタイプとして生成します。 AWS AppSyncを指定します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Using AWS Systems Manager Parameter Store for storing database credentials enables secure storage and ease of access by the application without additional overhead.",
        "situation_analysis": "The security engineer aims to enhance security by using strong, randomly generated passwords stored in a secure service and ensuring minimal operational overhead.",
        "option_analysis": "Option A and C use AWS Secrets Manager, which is a secure method, but the requirement of an AWS Lambda function to rotate the password adds operational overhead. Option D does not address automatic credential rotation adequately.",
        "additional_knowledge": "Credential rotation can be automated directly with Parameter Store configuration.",
        "key_terminology": "AWS Systems Manager, SecureString, Parameter Store, AWS Lambda, credential rotation",
        "overall_assessment": "Option B meets all requirements with the least operational overhead. Despite the community vote distribution favoring option A (100%), option B correctly implements the stated requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。AWS Systems Manager Parameter Storeを使用してデータベースの認証情報を保存すると、追加のオーバーヘッドなしでアプリケーションによる安全なストレージとアクセスが可能になる。",
        "situation_analysis": "セキュリティエンジニアは、強力でランダムに生成されたパスワードを安全なサービスに保存し、運用オーバーヘッドを最小限に抑えることを目指している。",
        "option_analysis": "オプションAとCはAWS Secrets Managerを使用しており、安全な方法であるが、パスワードをローテーションするためのAWS Lambda関数の要件は運用オーバーヘッドを追加する。オプションDは、自動的な認証情報のローテーションを十分に扱っていない。",
        "additional_knowledge": "認証情報のローテーションはParameter Storeの設定で自動化できる。",
        "key_terminology": "AWS Systems Manager, SecureString, Parameter Store, AWS Lambda, 認証情報のローテーション",
        "overall_assessment": "オプションBは、最小限の運用オーバーヘッドで全ての要件を満たしている。コミュニティの投票分布がオプションA（100％）を支持しているにもかかわらず、オプションBが述べた要件を正しく実装している。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "SecureString",
      "Parameter Store",
      "AWS Lambda",
      "credential rotation"
    ]
  },
  {
    "No": "27",
    "question": "A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data\naccessible publicly through a simple API over HTTPS. The solution must scale automatically in response to demand.\nWhich solutions meet these requirements? (Choose two.)",
    "question_jp": "ある企業が複数の Amazon DynamoDB テーブルにデータを保存しています。ソリューションアーキテクトは、サーバーレスアーキテクチャを使用してデータに HTTPS 経由で簡単な API を介して公開できるようにする必要があります。このソリューションは、需要に応じて自動的にスケールする必要があります。どのソリューションがこれらの要件を満たしますか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway's AWS integration",
        "text_jp": "Amazon API Gateway REST API を作成します。この API を API Gateway の AWS 統合を使用して DynamoDB に直接統合します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to Dynamo DB by using API Gateway's AWS",
        "text_jp": "Amazon API Gateway HTTP API を作成します。この API を API Gateway の AWS 統合を使用して DynamoDB に直接統合します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the",
        "text_jp": "Amazon API Gateway HTTP API を作成します。この API を AWS Lambda 関数に統合し、データを返します。"
      },
      {
        "key": "D",
        "text": "Create an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data",
        "text_jp": "AWS Global Accelerator にアクセラレータを作成します。このアクセラレータを AWS Lambda@Edge 関数統合で構成し、データを返します。"
      },
      {
        "key": "E",
        "text": "Create a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions.",
        "text_jp": "Network Load Balancer を作成します。リスナールールを設定してリクエストを適切な AWS Lambda 関数に転送します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "AC (83%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are C and A, as both solutions utilize API Gateway to access DynamoDB and are serverless.",
        "situation_analysis": "The company requires a scalable serverless architecture that allows public access to data through a simple HTTPS API.",
        "option_analysis": "Option C is correct because using AWS Lambda with API Gateway allows for data processing and return in a serverless manner. Option A is also viable as it provides a direct integration for REST APIs, but option C is more flexible with HTTP APIs.",
        "additional_knowledge": "Utilizing HTTP APIs can reduce cost and improve performance for certain use cases compared to REST APIs.",
        "key_terminology": "API Gateway, DynamoDB, AWS Lambda, serverless architecture, HTTPS.",
        "overall_assessment": "Both chosen solutions are aligned with AWS best practices for building public APIs. Community support aligns with these recommendations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答は C と A で、両方のソリューションは API Gateway を利用して DynamoDB にアクセスし、サーバーレスであるためです。",
        "situation_analysis": "企業は、シンプルな HTTPS API を介してデータに公然とアクセスできる、スケーラブルなサーバーレスアーキテクチャを必要としています。",
        "option_analysis": "選択肢 C は、API Gateway と AWS Lambda を使用することで、サーバーレスでデータの処理と返却が可能であるため正しいです。選択肢 A も適切ですが、REST API に直接統合するため、C の方が柔軟性があります。",
        "additional_knowledge": "HTTP API を利用することで、REST API に比べてコストを削減し、特定のユースケースでパフォーマンスを向上させることができます。",
        "key_terminology": "API Gateway、DynamoDB、AWS Lambda、サーバーレスアーキテクチャ、HTTPS。",
        "overall_assessment": "選択された2つのソリューションは、公開APIの構築に関するAWSのベストプラクティスに沿っています。コミュニティの支援もこれらの推奨に一致しています。"
      }
    ],
    "keywords": [
      "API Gateway",
      "DynamoDB",
      "AWS Lambda",
      "serverless architecture",
      "HTTPS"
    ]
  },
  {
    "No": "28",
    "question": "A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will\nredirect online visitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are\nmanaged by Amazon Route 53.\nA solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.\nWhich combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose\nthree.)",
    "question_jp": "ある企業が新たに10のドメイン名を登録しました。この企業は、オンラインマーケティングのためにドメインを使用しています。企業は、すべてのドメインとターゲットURLがJSON文書に定義されている状況下で、オンライン訪問者を各ドメインの特定のURLにリダイレクトするソリューションを必要としています。すべてのDNSレコードはAmazon Route 53で管理されています。\nソリューションアーキテクトは、HTTPおよびHTTPSリクエストを受け入れるリダイレクトサービスを実装しなければなりません。\n運用負荷が最も少ない方法でこれらの要件を満たすために、ソリューションアーキテクトが取るべきステップの組み合わせはどれですか？（3つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with",
        "text_jp": "動的ウェブページを作成し、Amazon EC2インスタンス上で実行させます。ウェブページをJSON文書と組み合わせて使用するように構成します。"
      },
      {
        "key": "B",
        "text": "Create an Application Load Balancer that includes HTTP and HTTPS listeners.",
        "text_jp": "HTTPおよびHTTPSリスナーを含むアプリケーションロードバランサーを作成します。"
      },
      {
        "key": "C",
        "text": "Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a",
        "text_jp": "JSON文書とイベントメッセージを組み合わせて使用するAWS Lambda関数を作成し、検索して応答します。"
      },
      {
        "key": "D",
        "text": "Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.",
        "text_jp": "カスタムドメインを使用したAmazon API Gateway APIを作成し、AWS Lambda関数を公開します。"
      },
      {
        "key": "E",
        "text": "Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.",
        "text_jp": "Amazon CloudFrontディストリビューションを作成します。Lambda@Edge関数を展開します。"
      },
      {
        "key": "F",
        "text": "Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.",
        "text_jp": "AWS Certificate Manager (ACM)を使用してSSL証明書を作成します。ドメインを代替名として含めます。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CEF (68%) BCF (22%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Create an Application Load Balancer that includes HTTP and HTTPS listeners.",
        "situation_analysis": "The requirement is to redirect traffic from multiple domains to specific URLs with minimal operational effort. Using Route 53 for DNS management indicates the solution should integrate well with AWS services.",
        "option_analysis": "Option B is best since an Application Load Balancer can handle HTTP/HTTPS traffic and perform URL redirects efficiently. Option A increases complexity and operational overhead. Option C is valid but involves more management of Lambda functions. Option D provides a way to create an API but is more than what is needed. Option E introduces complexity and requires additional setup for caching. Option F is necessary for SSL, but it's only part of the full solution.",
        "additional_knowledge": "ALB also supports SSL termination, simplifying certificate management.",
        "key_terminology": "Application Load Balancer, URL Redirection, Amazon Route 53, DNS Management, HTTPS Listener",
        "overall_assessment": "The majority of community votes also support option B, highlighting its effectiveness and alignment with AWS best practices for low operational overhead. The combined use of Route 53 and ALB achieves the goal efficiently."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はB: HTTPおよびHTTPSリスナーを含むアプリケーションロードバランサーを作成することです。",
        "situation_analysis": "複数のドメインから特定のURLにトラフィックをリダイレクトする必要があり、運用負荷を最小限に抑えることが求められています。DNS管理がRoute 53で行われていることから、AWSサービスとの統合が望まれます。",
        "option_analysis": "オプションBが最適です。アプリケーションロードバランサーはHTTP/HTTPSトラフィックを扱い、効率的にURLのリダイレクトを実行できます。オプションAは複雑性が増し、運用負荷が高まります。オプションCは妥当ですが、Lambda関数の管理が必要です。オプションDはAPIを作成する方法ですが、必要以上の構成になります。オプションEは複雑性を増し、キャッシングのための追加設定が必要です。オプションFはSSLに必要ですが、全体的なソリューションの一部に過ぎません。",
        "additional_knowledge": "ALBはSSL終端もサポートしており、証明書管理を簡素化します。",
        "key_terminology": "アプリケーションロードバランサー、URLリダイレクション、Amazon Route 53、DNS管理、HTTPSリスナー",
        "overall_assessment": "コミュニティ投票の大多数もオプションBを支持しており、その効果と運用負荷の低さが確認されています。Route 53とALBの組み合わせが効率的に目標に到達します。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "URL Redirection",
      "Amazon Route 53",
      "DNS Management",
      "HTTPS Listener"
    ]
  },
  {
    "No": "29",
    "question": "A company that has multiple AWS accounts is using AWS Organizations. The company's AWS accounts host VPCs, Amazon EC2 instances, and\ncontainers.\nThe company's compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2\ninstances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-\nrelated resources with a key of “costCenter” and a value or “compliance”.\nThe company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the\ncompliance team's AWS account. The cost calculation must be as accurate as possible.\nWhat should a solutions architect do to meet these requirements?",
    "question_jp": "複数のAWSアカウントを持つ企業がAWS Organizationsを使用しています。企業のAWSアカウントはVPC、Amazon EC2インスタンス、およびコンテナをホストしています。\n企業のコンプライアンスチームは、企業がデプロイを行っている各VPCにセキュリティツールを展開しました。セキュリティツールはEC2インスタンス上で実行され、コンプライアンスチーム向けの専用AWSアカウントに情報を送信しています。企業はすべてのコンプライアンス関連リソースに、「costCenter」というキーと「compliance」という値でタグ付けを行っています。\n企業は、セキュリティツールがEC2インスタンス上で実行されているコストを特定したいと考えており、コンプライアンスチームのAWSアカウントに請求を行えるようにしたいと考えています。コスト計算はできるだけ正確である必要があります。\nこの要件を満たすために、ソリューションアーキテクトは何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports",
        "text_jp": "組織の管理アカウントで、costCenterユーザー定義タグを有効にし、毎月のAWSコストおよび使用状況レポートを設定します"
      },
      {
        "key": "B",
        "text": "In the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to",
        "text_jp": "組織のメンバーアカウントで、costCenterユーザー定義タグを有効にし、毎月のAWSコストおよび使用状況レポートを設定します"
      },
      {
        "key": "C",
        "text": "In the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly",
        "text_jp": "組織のメンバーアカウントでcostCenterユーザー定義タグを有効にし、管理アカウントから毎月のスケジュール"
      },
      {
        "key": "D",
        "text": "Create a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the",
        "text_jp": "AWS Trusted Advisorの組織ビューにカスタムレポートを作成し、請求概要を毎月生成するように設定します"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (96%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, which suggests activating the costCenter tag in the management account and configuring monthly AWS Cost and Usage Reports. This allows for accurate cost allocation based on the user-defined tags.",
        "situation_analysis": "The company has multiple AWS accounts managed under AWS Organizations, with compliance-related resources tagged to identify costs accurately.",
        "option_analysis": "Option A directly addresses the requirement to enable the costCenter tag in the management account, ensuring that costs can be tracked and attributed correctly. Options B, C, and D do not adequately meet the precise need for organization-wide cost reporting using the costCenter tag.",
        "additional_knowledge": "Ensuring that cost allocation tags are properly set up at the management account level allows for a clearer view of spending across different business units or compliance needs.",
        "key_terminology": "AWS Organizations, Cost and Usage Reports, user-defined tags, EC2 instances, compliance.",
        "overall_assessment": "Option A aligns with AWS best practices for cost management. The community strongly supports this choice, indicated by a 96% vote, which suggests consensus on its correctness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAであり、管理アカウントでcostCenterタグを有効にし、毎月のAWSコストおよび使用状況レポートを設定することを提案しています。これにより、ユーザー定義タグに基づいて正確なコスト配分が可能になります。",
        "situation_analysis": "企業は複数のAWSアカウントを持ち、AWS Organizationsの下で管理されています。コンプライアンス関連のリソースにはコストを正確に特定するためのタグが付けられています。",
        "option_analysis": "選択肢Aは、管理アカウントでcostCenterタグを有効にする必要性に直接対応しており、コストの追跡と適切な帰属を保証します。選択肢B、C、Dは、costCenterタグを使用した組織全体のコスト報告のニーズに十分に対処していません。",
        "additional_knowledge": "管理アカウントレベルでコスト配分タグが正しく設定されることで、異なるビジネスユニットやコンプライアンスニーズにおける支出のより明確なビューが提供されます。",
        "key_terminology": "AWS Organizations、コストおよび使用状況レポート、ユーザー定義タグ、EC2インスタンス、コンプライアンス。",
        "overall_assessment": "選択肢Aは、コスト管理に関するAWSのベストプラクティスに沿っています。コミュニティはこの選択肢を強く支持しており、96%の投票によってその正しさが示されています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Cost and Usage Reports",
      "user-defined tags",
      "EC2 instances",
      "compliance"
    ]
  },
  {
    "No": "30",
    "question": "A company has 50 AWS accounts that are members of an organization in AWS Organizations. Each account contains multiple VPCs. The company\nwants to use AWS Transit Gateway to establish connectivity between the VPCs in each member account. Each time a new member account is\ncreated, the company wants to automate the process of creating a new VPC and a transit gateway attachment.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "question_jp": "ある企業は、AWS Organizationsに所属する50のAWSアカウントを持っています。各アカウントには複数のVPCがあります。企業は、AWS Transit Gatewayを使用して、各メンバーアカウント内のVPC間の接続を確立したいと考えています。新しいメンバーアカウントが作成されるたびに、企業は新しいVPCとトランジットゲートウェイのアタッチメントを自動的に作成するプロセスを自動化したいと考えています。これらの要件を満たすための手順の組み合わせはどれですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.",
        "text_jp": "管理アカウントから、AWS Resource Access Managerを使用してメンバーアカウントとトランジットゲートウェイを共有する。"
      },
      {
        "key": "B",
        "text": "From the management account, share the transit gateway with member accounts by using an AWS Organizations SCP.",
        "text_jp": "管理アカウントから、AWS Organizations SCPを使用してメンバーアカウントとトランジットゲートウェイを共有する。"
      },
      {
        "key": "C",
        "text": "Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway",
        "text_jp": "管理アカウントからAWS CloudFormationスタックセットを起動し、VPCとVPCトランジットゲートウェイを自動的に作成する。"
      },
      {
        "key": "D",
        "text": "Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a peering transit",
        "text_jp": "管理アカウントからAWS CloudFormationスタックセットを起動し、新しいVPCとピアリングトランジットを自動的に作成する。"
      },
      {
        "key": "E",
        "text": "From the management account, share the transit gateway with member accounts by using AWS Service Catalog.",
        "text_jp": "管理アカウントから、AWS Service Catalogを使用してメンバーアカウントとトランジットゲートウェイを共有する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and C. Answer A involves sharing the transit gateway using AWS Resource Access Manager, allowing member accounts to establish connections to the shared transit gateway. Answer C involves using AWS CloudFormation stack sets to automate the creation of new VPCs and attachments.",
        "situation_analysis": "The company needs to create a new VPC and transit gateway attachment each time a new member account is added, indicating a need for automation and resource sharing across multiple accounts.",
        "option_analysis": "Option A is correct as it enables resource sharing between accounts; option C is also correct as it automates the setup process. Option B is incorrect since SCPs are not used for resource sharing. Options D and E do not meet the requirements for establishing transit gateway connectivity across VPCs.",
        "additional_knowledge": "",
        "key_terminology": "AWS Organizations, AWS Transit Gateway, AWS Resource Access Manager, AWS CloudFormation, VPC.",
        "overall_assessment": "The question accurately represents a practical scenario involving AWS Organizations and resource management. Community voting supports the chosen correct answers."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAとCである。Aは、AWS Resource Access Managerを使用してトランジットゲートウェイを共有し、メンバーアカウントが共有トランジットゲートウェイに接続を確立できるようにする。Cは、AWS CloudFormationスタックセットを使用して新しいVPCとアタッチメントを自動的に作成する。",
        "situation_analysis": "新しいメンバーアカウントが追加されるたびに新しいVPCとトランジットゲートウェイアタッチメントを作成する必要があり、自動化と複数アカウント間のリソース共有が求められている。",
        "option_analysis": "Aは、アカウント間でのリソース共有を可能にするため正しい。Cも自動化プロセスを実現するため正しい。Bは、SCPがリソース共有に使われることはないため不正解。DとEは、VPC間のトランジットゲートウェイ接続を確立する要件を満たしていない。",
        "additional_knowledge": "",
        "key_terminology": "AWS Organizations、AWS Transit Gateway、AWS Resource Access Manager、AWS CloudFormation、VPC。",
        "overall_assessment": "この質問は、AWS Organizationsとリソース管理に関する実践的なシナリオを正確に表している。コミュニティ投票も選択した正解を支持している。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS Transit Gateway",
      "AWS Resource Access Manager",
      "AWS CloudFormation",
      "VPC"
    ]
  },
  {
    "No": "31",
    "question": "An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS\nOrganizations account structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by\nprocurement managers. The procurement team's policy indicates that developers should be able to obtain third-party software from an approved\nlist only and use Private Marketplace in AWS Marketplace to achieve this requirement. The procurement team wants administration of Private\nMarketplace to be restricted to a role named procurement-manager-role, which could be assumed by procurement managers. Other IAM users,\ngroups, roles, and account administrators in the company should be denied Private Marketplace administrative access.\nWhat is the MOST eficient way to design an architecture to meet these requirements?",
    "question_jp": "企業が開発者にAWS Marketplaceを通じてサードパーティソフトウェアを購入させたいと考えています。この企業は、フル機能が有効化されたAWS Organizationsアカウント構造を使用しており、調達担当者が使用するための各組織単位（OU）に共有サービスアカウントがあります。調達チームのポリシーでは、開発者は承認されたリストからのみサードパーティソフトウェアを取得でき、AWS Marketplaceのプライベートマーケットプレイスを利用してこの要件を達成することが求められています。調達チームは、プライベートマーケットプレイスの管理をprocurement-manager-roleという役割に制限し、調達マネージャーがその役割を引き受けることができるようにしたいと考えています。会社のその他のIAMユーザー、グループ、ロール、およびアカウント管理者は、プライベートマーケットプレイスに対する管理アクセスを拒否されるべきです。これらの要件を満たすために、最も効率的なアーキテクチャの設計方法は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the PowerUserAccess managed policy to",
        "text_jp": "全てのAWSアカウントにprocurement-manager-roleというIAMロールを作成します。PowerUserAccess管理ポリシーを追加します。"
      },
      {
        "key": "B",
        "text": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the AdministratorAccess managed",
        "text_jp": "全てのAWSアカウントにprocurement-manager-roleというIAMロールを作成します。AdministratorAccess管理ポリシーを追加します。"
      },
      {
        "key": "C",
        "text": "Create an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the",
        "text_jp": "全ての共有サービスアカウントにprocurement-manager-roleというIAMロールを作成します。"
      },
      {
        "key": "D",
        "text": "Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developers. Add the",
        "text_jp": "開発者が使用する全てのAWSアカウントにprocurement-manager-roleというIAMロールを作成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct answer: D. This option allows the management of Private Marketplace to be restricted to the procurement-manager-role in the accounts used by developers.",
        "situation_analysis": "The enterprise utilizes AWS Organizations, necessitating a solution that permits specific roles limited access while ensuring compliance with procurement policies.",
        "option_analysis": "Option D is correct as it assigns the role in the accounts of developers, ensuring they use the Private Marketplace appropriately. Options A, B, and C do not restrict access effectively as required.",
        "additional_knowledge": "Understanding how AWS IAM roles can provide fine-grained access control is critical for effective governance.",
        "key_terminology": "AWS Organizations, IAM Role, Private Marketplace, procurement-policy, PowerUserAccess, AdministratorAccess.",
        "overall_assessment": "The correct choice aligns with AWS best practices by delegating specific responsibilities effectively while ensuring adherence to procurement regulations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解: D。このオプションは、開発者が使用するアカウントにおいてprocurement-manager-roleにプライベートマーケットプレイスの管理を制限できるようにします。",
        "situation_analysis": "企業はAWS Organizationsを利用しており、特定のロールに限られたアクセスを許可しながら、調達ポリシーに準拠する解決策が必要です。",
        "option_analysis": "Dオプションが正しいのは、開発者のアカウントにおいてロールを割り当てることにより、プライベートマーケットプレイスを適切に使用することができるからです。A、B、およびCオプションは、必要とされるようにアクセスを効果的に制限しません。",
        "additional_knowledge": "AWS IAMロールがどのように細かなアクセス制御を提供できるかを理解することは、効果的なガバナンスのために重要です。",
        "key_terminology": "AWS Organizations、IAMロール、プライベートマーケットプレイス、調達ポリシー、PowerUserAccess、AdministratorAccess。",
        "overall_assessment": "正しい選択肢は、AWSのベストプラクティスに沿って、特定の責任を効果的に委任し、中でも調達規則に従うようにしています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "IAM Role",
      "Private Marketplace",
      "procurement-policy",
      "PowerUserAccess",
      "AdministratorAccess"
    ]
  },
  {
    "No": "32",
    "question": "A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon\nDynamoDB. The developers account resides in a dedicated organizational unit (OU). The solutions architect has implemented the following SCP\non the developers account:\nWhen this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy.\nWhat should the solutions architect do to eliminate the developers' ability to use services outside the scope of this policy?",
    "question_jp": "ある企業は、AWS Organizationsを実装して、開発者がAmazon EC2、Amazon S3、およびAmazon DynamoDBのみを使用できるように制限しようとしています。開発者のアカウントは、専用の組織単位（OU）に存在します。ソリューションアーキテクトは、次のSCPを開発者アカウントに実装しました。このポリシーがデプロイされると、開発者アカウントのIAMユーザーは、ポリシーに記載されていないAWSサービスを引き続き使用することができます。ソリューションアーキテクトは、ポリシーの範囲外のサービスを使用する開発者の能力を排除するために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an explicit deny statement for each AWS service that should be constrained.",
        "text_jp": "制限すべきAWSサービスごとに明示的な拒否ステートメントを作成する。"
      },
      {
        "key": "B",
        "text": "Remove the FullAWSAccess SCP from the developers account's OU.",
        "text_jp": "開発者アカウントのOUからFullAWSAccess SCPを削除する。"
      },
      {
        "key": "C",
        "text": "Modify the FullAWSAccess SCP to explicitly deny all services.",
        "text_jp": "FullAWSAccess SCPを修正して、すべてのサービスを明示的に拒否する。"
      },
      {
        "key": "D",
        "text": "Add an explicit deny statement using a wildcard to the end of the SCP. [image_20_0]",
        "text_jp": "SCPの末尾にワイルドカードを使用した明示的な拒否ステートメントを追加する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (81%) Other",
    "page_images": [
      "image_20_0.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create an explicit deny statement for each AWS service that should be constrained.",
        "situation_analysis": "The company has a requirement to limit its developers to specific AWS services, and the current SCP is not restrictive enough.",
        "option_analysis": "Option A directly addresses the need for explicit denial of all other services not mentioned in the allowed services. Options B and C fail to achieve that, and D may not be specific enough.",
        "additional_knowledge": "To effectively constrain access, using explicit deny statements ensures that only the permitted services are accessible.",
        "key_terminology": "AWS Organizations, Service Control Policies (SCPs), Explicit Deny, IAM, AWS Services",
        "overall_assessment": "Answer A is the most precise approach to achieving the desired restriction. The community vote shows a strong lean towards option B, which suggests a misunderstanding of SCP behavior."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA: 制限すべきAWSサービスごとに明示的な拒否ステートメントを作成することです。",
        "situation_analysis": "企業は開発者が特定のAWSサービスのみに制限される必要があり、現在のSCPは十分に制限的ではありません。",
        "option_analysis": "選択肢Aは、明示的に他のサービスの拒否を必要とするこの要求に直接対処しています。選択肢BとCはそれを達成できず、選択肢Dは具体性に欠ける可能性があります。",
        "additional_knowledge": "アクセスを効果的に制限するために、明示的な拒否ステートメントを使用すると、許可されたサービスのみがアクセス可能であることが保証されます。",
        "key_terminology": "AWS Organizations、Service Control Policies (SCPs)、明示的拒否、IAM、AWSサービス",
        "overall_assessment": "答えAは、望ましい制限を達成するための最も正確なアプローチです。コミュニティの投票は選択肢Bに強く傾いており、SCPの動作に対する誤解を示唆しています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Service Control Policies",
      "IAM",
      "Explicit Deny",
      "AWS Services"
    ]
  },
  {
    "No": "33",
    "question": "A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC. Mobile clients\nconnect to the API by using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing\npolicy with the IP addresses of all the EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to trafic. The app\nhas not been able to keep up with the trafic.\nA solutions architect needs to implement a solution so that the app can handle the new and varying load.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業が、VPCのパブリックサブネットにある5つのAmazon EC2インスタンス上で、モノリシックなRESTベースのAPIをモバイルアプリのためにホスティングしています。モバイルクライアントは、Amazon Route 53でホストされているドメイン名を使用してAPIに接続します。企業は、すべてのEC2インスタンスのIPアドレスを使用してRoute 53のマルチバリュー応答ルーティングポリシーを作成しました。最近、アプリは大きく突然のトラフィック増加に圧倒されています。アプリはトラフィックに追いつくことができませんでした。ソリューションアーキテクトは、アプリが新しい変動する負荷に対応できるようにソリューションを実施する必要があります。どのソリューションが最小の運用負荷でこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Separate the API into individual AWS Lambda functions. Configure an Amazon API Gateway REST API with Lambda integration for the",
        "text_jp": "APIを個別のAWS Lambda関数に分割します。Lambda統合を使用してAmazon API Gateway REST APIを構成します。"
      },
      {
        "key": "B",
        "text": "Containerize the API logic. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Run the containers in the cluster by using",
        "text_jp": "APIロジックをコンテナ化します。Amazon Elastic Kubernetes Service（Amazon EKS）クラスターを作成します。クラスター内でコンテナを実行します。"
      },
      {
        "key": "C",
        "text": "Create an Auto Scaling group. Place all the EC2 instances in the Auto Scaling group. Configure the Auto Scaling group to perform scaling",
        "text_jp": "Auto Scalingグループを作成します。すべてのEC2インスタンスをAuto Scalingグループに配置します。Auto Scalingグループをスケーリングを実行するように構成します。"
      },
      {
        "key": "D",
        "text": "Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances",
        "text_jp": "APIの前にアプリケーションロードバランサー（ALB）を作成します。EC2インスタンスをVPC内のプライベートサブネットに移動します。EC2インスタンスを追加します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (56%) C (24%) D (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances into the ALB for load balancing.",
        "situation_analysis": "The application is overwhelmed by traffic, indicating a need for improved traffic management. Moving to private subnets provides additional security and allows the use of an ALB to distribute traffic efficiently.",
        "option_analysis": "Option A (AWS Lambda) may introduce higher operational overhead due to the requirement of rewriting the code into functions. Option B (EKS) will require additional management of Kubernetes, leading to more complexity. Option C (Auto Scaling) is beneficial but may not address sudden large traffic spikes effectively without a load balancer.",
        "additional_knowledge": "No additional knowledge is needed.",
        "key_terminology": "Application Load Balancer, Auto Scaling, EC2, security, traffic management",
        "overall_assessment": "While many options could provide scalability, Option D is the most straightforward and offers essential features for managing traffic flow with the least operational overhead. The community votes suggest a strong support for this option, but there might be a misunderstanding of the actual implementation required."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです：APIの前にアプリケーションロードバランサー（ALB）を作成します。EC2インスタンスをVPC内のプライベートサブネットに移動します。EC2インスタンスをALBに追加してロードバランシングを行います。",
        "situation_analysis": "アプリケーションがトラフィックに圧倒されており、トラフィック管理の改善が必要であることを示しています。プライベートサブネットへの移動は、セキュリティ向上を図り、ALBを使用して効果的にトラフィックを分配することを可能にします。",
        "option_analysis": "選択肢A（AWS Lambda）は、コードを関数に再構築する必要があるため、高い運用負荷をもたらす可能性があります。選択肢B（EKS）は、Kubernetesの追加管理が必要であり、複雑さが増します。選択肢C（Auto Scaling）は有益ですが、ロードバランサーなしでは突然の大規模なトラフィックスパイクに効果的に対処できません。",
        "additional_knowledge": "追加の知識は必要ありません。",
        "key_terminology": "アプリケーションロードバランサー、Auto Scaling、EC2、セキュリティ、トラフィック管理",
        "overall_assessment": "多くのオプションがスケーラビリティを提供できますが、選択肢Dが最も簡潔で、最小限の運用負荷でトラフィックフローの管理に必要な機能を提供します。コミュニティの投票はこの選択肢を強く支持していますが、実際に必要な実装について誤解があるかもしれません。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "Auto Scaling",
      "EC2",
      "security",
      "traffic management"
    ]
  },
  {
    "No": "34",
    "question": "A company has created an OU in AWS Organizations for each of its engineering teams. Each OU owns multiple AWS accounts. The organization\nhas hundreds of AWS accounts.\nA solutions architect must design a solution so that each OU can view a breakdown of usage costs across its AWS accounts.\nWhich solution meets these requirements?",
    "question_jp": "ある企業は、AWS Organizations内に各エンジニアリングチームのためのOU（組織単位）を作成しました。各OUは複数のAWSアカウントを所有しています。この組織は数百のAWSアカウントを持っています。ソリューションアーキテクトは、各OUがそのAWSアカウントの使用コストの内訳を表示できるようにするソリューションを設計しなければなりません。どのソリューションがこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access Manager. Allow each team to visualize the CUR",
        "text_jp": "AWSリソースアクセスマネージャを使用して各OUのためにAWSコストと使用状況レポート（CUR）を作成し、各チームがCURを視覚化できるようにします"
      },
      {
        "key": "B",
        "text": "Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR",
        "text_jp": "AWS Organizations管理アカウントからAWSコストと使用状況レポート（CUR）を作成し、各チームがCURを視覚化できるようにします"
      },
      {
        "key": "C",
        "text": "Create an AWS Cost and Usage Report (CUR) in each AWS Organizations member account. Allow each team to visualize the CUR through an",
        "text_jp": "各AWS OrganizationsのメンバーアカウントにAWSコストと使用状況レポート（CUR）を作成し、各チームがCURを視覚化できるようにします"
      },
      {
        "key": "D",
        "text": "Create an AWS Cost and Usage Report (CUR) by using AWS Systems Manager. Allow each team to visualize the CUR through Systems",
        "text_jp": "AWS Systems Managerを使用してAWSコストと使用状況レポート（CUR）を作成し、各チームがSystemsを介してCURを視覚化できるようにします"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating an AWS Cost and Usage Report (CUR) from the AWS Organizations management account allows for a centralized view of cost data across all accounts in the organization, facilitating visibility for each OU.",
        "situation_analysis": "The organization has multiple AWS accounts under various OUs and requires a way to analyze and visualize cost data effectively for each OU.",
        "option_analysis": "Option A suggests using AWS Resource Access Manager, which is not needed for CUR. Option C creates CURs in each member account, complicating aggregation. Option D is irrelevant as it suggests using Systems Manager, which does not apply to CUR directly.",
        "additional_knowledge": "AWS Cost Explorer can also be utilized for further visualization and analysis.",
        "key_terminology": "AWS Cost and Usage Report, AWS Organizations, centralized view, cost allocation, visualization tools",
        "overall_assessment": "The community supports option B unanimously, affirming it as the most effective approach to meet the requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。AWS Organizations管理アカウントからAWSコストと使用状況レポート（CUR）を作成することで、組織内のすべてのアカウントのコストデータを中央で確認でき、各OUに対する可視性が促進される。",
        "situation_analysis": "この組織には、さまざまなOUの下に複数のAWSアカウントがあり、各OUのコストデータを効果的に分析し、視覚化する方法が必要である。",
        "option_analysis": "選択肢AはAWSリソースアクセスマネージャを使用することを提案しているが、それはCURには必要ない。選択肢Cは各メンバーアカウントにCURを作成することを提案しており、集計が複雑になる。選択肢Dは、CURに直接関係しないAWS Systems Managerを使用することを示唆しているため、無関係である。",
        "additional_knowledge": "AWSコストエクスプローラーは、さらなる視覚化と分析のためにも利用できる。",
        "key_terminology": "AWSコストと使用状況レポート、AWS Organizations、中央集約ビュー、コスト配分、視覚化ツール",
        "overall_assessment": "コミュニティは選択肢Bを一貫して支持しており、これが要件を満たすための最も効果的なアプローチであることを確認している。"
      }
    ],
    "keywords": [
      "AWS Cost and Usage Report",
      "AWS Organizations",
      "cost allocation",
      "resource visibility",
      "centralized management"
    ]
  },
  {
    "No": "35",
    "question": "A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily.\nThe company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company\nalready has established an AWS Direct Connect connection between the on-premises network and AWS.\nWhich data migration strategy should the company use?",
    "question_jp": "ある企業が、オンプレミスのWindowsファイルサーバーにデータを保存しています。この企業は、毎日5GBの新しいデータを生成しています。\nこの企業は、Windowsベースのワークロードの一部をAWSに移行し、データをクラウド内のファイルシステムで利用可能にする必要があります。この企業はすでにオンプレミスネットワークとAWSの間にAWS Direct Connect接続を確立しています。\nどのデータ移行戦略をこの企業は使用すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new",
        "text_jp": "AWS Storage Gatewayのファイルゲートウェイオプションを使用して既存のWindowsファイルサーバーに置き換え、既存のファイル共有を新しいものにポイントします。"
      },
      {
        "key": "B",
        "text": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.",
        "text_jp": "AWS DataSyncを使用して、オンプレミスのWindowsファイルサーバーとAmazon FSxの間でデータを複製するための日次タスクをスケジュールします。"
      },
      {
        "key": "C",
        "text": "Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File",
        "text_jp": "AWS Data Pipelineを使用して、オンプレミスのWindowsファイルサーバーとAmazon Elastic Fileの間でデータを複製するための日次タスクをスケジュールします。"
      },
      {
        "key": "D",
        "text": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System",
        "text_jp": "AWS DataSyncを使用して、オンプレミスのWindowsファイルサーバーとAmazon Elastic File Systemの間でデータを複製するための日次タスクをスケジュールします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (63%) A (38%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. AWS DataSync is designed for transferring large amounts of data efficiently between on-premises storage and AWS services.",
        "situation_analysis": "The company generates 5 GB of new data daily and requires a solution to move this data to AWS while maintaining availability on a file system in the cloud.",
        "option_analysis": "Option A is incorrect because AWS Storage Gateway as a file gateway cannot replicate data from the Windows file server to FSx. Option C is incorrect as AWS Data Pipeline is not ideal for regular file synchronization tasks. Option D is a plausible choice but option B specifically mentions replication to Amazon FSx, which aligns with the company's needs for a Windows file system compatible solution.",
        "additional_knowledge": "Data transfers can be scheduled and monitored through the AWS Management Console, providing transparency and reliability.",
        "key_terminology": "AWS DataSync, Amazon FSx, file server replication, file system integration.",
        "overall_assessment": "The question accurately tests knowledge of AWS data migration strategies and highlights key service capabilities. Community support for option B shows that it is widely accepted as the best solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。AWS DataSyncは、オンプレミスストレージとAWSサービス間で大量のデータを効率的に転送するために設計されている。",
        "situation_analysis": "この企業は毎日5GBの新しいデータを生成しており、データをAWSに移動させる必要があり、クラウド内のファイルシステムでの可用性を維持する必要がある。",
        "option_analysis": "選択肢Aは不正解である。AWS Storage Gatewayのファイルゲートウェイは、WindowsファイルサーバーからFSxへのデータ複製を行えない。選択肢Cは不正解であり、AWS Data Pipelineは定期的なファイル同期タスクには適していない。選択肢Dは妥当な選択肢であるが、選択肢Bは具体的にAmazon FSxへの複製について触れており、企業のWindowsファイルシステム適合ソリューションのニーズと一致している。",
        "additional_knowledge": "データ転送はAWS Management Consoleを通じてスケジュールおよび監視でき、透明性と信頼性を提供する。",
        "key_terminology": "AWS DataSync、Amazon FSx、ファイルサーバーの複製、ファイルシステムの統合。",
        "overall_assessment": "この問題はAWSのデータ移行戦略に関する知識を正確にテストし、主要なサービス機能を強調している。選択肢Bへのコミュニティの支持は、それが最良の解決策として広く受け入れられていることを示している。"
      }
    ],
    "keywords": [
      "AWS DataSync",
      "Amazon FSx",
      "file server replication",
      "file system integration"
    ]
  },
  {
    "No": "36",
    "question": "A company's solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3\nbucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a\nsecond Region.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業のソリューションアーキテクトが、AWS上で実行されているウェブアプリケーションをレビューしています。アプリケーションは、us-east-1リージョンのAmazon S3バケットにある静的アセットを参照しています。企業は複数のAWSリージョンにわたるレジリエンシーが必要です。企業はすでに2番目のリージョンにS3バケットを作成しています。この要件を最も運用のオーバーヘッドを少なくして満たすソリューションはどれでしょうか。",
    "choices": [
      {
        "key": "A",
        "text": "Configure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set by using",
        "text_jp": "アプリケーションを構成して各オブジェクトを両方のS3バケットに書き込む。Amazon Route 53のパブリックホストゾーンを設定し、レコードセットを使用する。"
      },
      {
        "key": "B",
        "text": "Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda",
        "text_jp": "AWS Lambda関数を作成して、us-east-1のS3バケットから2番目のリージョンのS3バケットにオブジェクトをコピーする。Lambdaを呼び出す。"
      },
      {
        "key": "C",
        "text": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront",
        "text_jp": "us-east-1のS3バケットでレプリケーションを構成し、オブジェクトを2番目のリージョンのS3バケットにレプリケートする。Amazon CloudFrontを設定する。"
      },
      {
        "key": "D",
        "text": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update",
        "text_jp": "us-east-1のS3バケットでレプリケーションを構成し、オブジェクトを2番目のリージョンのS3バケットにレプリケートする。フェイルオーバーが必要な場合は、更新する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (94%) 3%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Configuring replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region provides a resilient architecture with minimal operational overhead.",
        "situation_analysis": "The company needs to ensure that their static assets are available across multiple AWS regions, which is crucial for redundancy and reduced latency for users located in various geographical areas.",
        "option_analysis": "Option A introduces more manual processes and increases operational overhead by requiring applications to explicitly write to two buckets. Option B also involves additional operational overhead by requiring a Lambda function. Option C, while it sets up replication, may involve more complexity with CloudFront setup that is not directly related to the primary requirement of replication. Option D directly meets the requirement of replication with minimal changes to the application.",
        "additional_knowledge": "",
        "key_terminology": "Amazon S3, replication, AWS regions, CloudFront, operational overhead.",
        "overall_assessment": "While community voting shows a strong preference for option C, option D provides a more straightforward solution with the least operational overhead as it focuses solely on replication without additional complexities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。us-east-1のS3バケットでレプリケーションを構成して、オブジェクトを2番目のリージョンのS3バケットにレプリケートすることは、最小限の運用オーバーヘッドでレジリエントなアーキテクチャを提供する。",
        "situation_analysis": "企業は静的アセットが複数のAWSリージョンにわたって利用可能であることを確保する必要があり、これは冗長性と様々な地理的地域にいるユーザーのためのレイテンシーの軽減にとって重要である。",
        "option_analysis": "選択肢Aは、アプリケーションが明示的に2つのバケットに書き込むことを必要とし、運用オーバーヘッドを増加させるため、手動プロセスが増える。選択肢BもLambda関数を必要とするため、追加の運用オーバーヘッドを伴う。選択肢Cはレプリケーションを設定するが、CloudFrontの設定は、レプリケーションの主な要件に直接関係しないため、より複雑さを伴うことになる。選択肢Dは、アプリケーションへの変更を最小限に抑えながらレプリケーション要件を直接満たす。",
        "additional_knowledge": "",
        "key_terminology": "Amazon S3, レプリケーション, AWSリージョン, CloudFront, 運用オーバーヘッド。",
        "overall_assessment": "コミュニティ投票は選択肢Cに対する強力な支持を示しているが、選択肢Dは追加の複雑さを持たず、最小限の運用オーバーヘッドでレプリケーションに専念しているため、より直接的な解決策を提供している。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "replication",
      "AWS regions",
      "CloudFront",
      "operational overhead"
    ]
  },
  {
    "No": "37",
    "question": "A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in trafic that resulted in downtime and a\nsignificant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a\ndependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily\nusers.\nWhich steps should the solutions architect take to design an appropriate solution?",
    "question_jp": "ある企業は、オンプレミス環境で三層のウェブアプリケーションをホストしています。最近のトラフィックの急増によりダウンタイムが発生し、重要な財務的影響が出たため、企業の経営陣はアプリケーションをAWSに移行するよう命じました。アプリケーションは.NETで書かれており、MySQLデータベースに依存しています。ソリューションアーキテクトは、20万人のユーザーの需要を満たすために、スケーラブルで高可用性のソリューションを設計する必要があります。ソリューションアーキテクトは、適切なソリューションを設計するためにどのステップを実施すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance.",
        "text_jp": "AWS Elastic Beanstalkを使用して、ウェブサーバー環境とAmazon RDS MySQL Multi-AZ DBインスタンスを持つ新しいアプリケーションを作成する。"
      },
      {
        "key": "B",
        "text": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group",
        "text_jp": "AWS CloudFormationを使用して、Amazon EC2 Auto Scalingグループの前にApplication Load Balancer (ALB)を配置したスタックを起動する。"
      },
      {
        "key": "C",
        "text": "Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application",
        "text_jp": "AWS Elastic Beanstalkを使用して、2つの異なるリージョンに跨る自動スケーリングウェブサーバー環境を作成する。"
      },
      {
        "key": "D",
        "text": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot",
        "text_jp": "AWS CloudFormationを使用して、Amazon ECSのスポットインスタンスの前にApplication Load Balancer (ALB)を配置したスタックを起動する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (94%) 4%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which proposes using AWS Elastic Beanstalk to create an automatically scaling web server environment spanning two separate regions, supporting high availability and scalability.",
        "situation_analysis": "The company needs to handle 200,000 daily users, so a solution must be capable of scaling and ensuring high availability to avoid downtime and financial loss.",
        "option_analysis": "Option A suggests using Elastic Beanstalk with RDS, but doesn't address multi-region deployment. Option B focuses on Auto Scaling and ALB but could be limited to a single region. Option D doesn't leverage elastic workloads optimally for high availability and auto scaling.",
        "additional_knowledge": "It's essential to design applications that can failover and be resilient to regional outages.",
        "key_terminology": "AWS Elastic Beanstalk, Auto Scaling, Multi-AZ Deployment, High Availability, RDS, Application Load Balancer.",
        "overall_assessment": "Considering the community vote indicates a significant preference for option B, it emphasizes a common solution pattern; however, given the context (the need for a multi-region scalable architecture), option C would still be more aligned with the specific requirements of high availability and scalability."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCで、AWS Elastic Beanstalkを使用して、2つの異なるリージョンにまたがる自動スケーリングウェブサーバー環境を作成することを提案しています。これは、高可用性とスケーラビリティをサポートします。",
        "situation_analysis": "企業は、20万人の日常的なユーザーを処理する必要があるため、ソリューションはスケーリングが可能で、高可用性を確保し、ダウンタイムや財務的損失を回避するものでなければなりません。",
        "option_analysis": "選択肢Aは、Elastic BeanstalkとRDSを使用することを提案していますが、マルチリージョン展開に触れていません。選択肢BはAuto ScalingとALBに焦点を当てていますが、単一リージョンに制限される可能性があります。選択肢Dは、可用性と自動スケーリングに最適にエラスティックなワークロードを活用していません。",
        "additional_knowledge": "地域的な障害に耐えられる冗長性を持つアプリケーションを設計することが重要です。",
        "key_terminology": "AWS Elastic Beanstalk、Auto Scaling、Multi-AZデプロイメント、高可用性、RDS、Application Load Balancer。",
        "overall_assessment": "コミュニティ投票が選択肢Bを示しているが、一般的なソリューションパターンを強調しています。ただし、特定の要件である高可用性とスケーラビリティを考えると、選択肢Cがより適しているでしょう。"
      }
    ],
    "keywords": [
      "AWS Elastic Beanstalk",
      "Auto Scaling",
      "Multi-AZ Deployment",
      "High Availability",
      "RDS",
      "Application Load Balancer"
    ]
  },
  {
    "No": "38",
    "question": "A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an\nAmazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations\nmember accounts.\nA solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of\nCloudFormation stacks. Trusted access has been enabled in Organizations.\nWhat should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
    "question_jp": "企業は、複数のAWSアカウントを管理するためにAWS Organizationsを使用しています。セキュリティ上の理由から、企業はすべてのOrganizationsメンバーアカウントでサードパーティの警告システムとの統合を可能にするAmazon Simple Notification Service（Amazon SNS）トピックの作成を要求しています。ソリューションアーキテクトは、SNSトピックを作成するためにAWS CloudFormationテンプレートを使用し、CloudFormationスタックのデプロイを自動化するためにスタックセットを使用しました。信頼されたアクセスはOrganizationsにおいて有効化されています。ソリューションアーキテクトは、すべてのAWSアカウントでCloudFormation StackSetsをデプロイするために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an",
        "text_jp": "組織メンバーアカウントにスタックセットを作成します。サービス管理の権限を使用します。デプロイオプションを設定して、次のようにデプロイします"
      },
      {
        "key": "B",
        "text": "Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization.",
        "text_jp": "組織メンバーアカウントにスタックを作成します。自己管理の権限を使用します。デプロイオプションを設定して、組織にデプロイします"
      },
      {
        "key": "C",
        "text": "Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the",
        "text_jp": "組織の管理アカウントにスタックセットを作成します。サービス管理の権限を使用します。デプロイオプションを設定して次のようにデプロイします"
      },
      {
        "key": "D",
        "text": "Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the",
        "text_jp": "組織の管理アカウントにスタックを作成します。サービス管理の権限を使用します。デプロイオプションを設定してデプロイします"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C because the stack set should be created in the management account of the AWS Organization, which allows it to deploy across all member accounts using service-managed permissions.",
        "situation_analysis": "The company is using AWS Organizations with multiple accounts and requires a centralized way to deploy resources, specifically an SNS topic, to all member accounts.",
        "option_analysis": "Option A is incorrect because stack sets should not be created in member accounts for this scenario. Option B is incorrect as it suggests using self-service permissions which is not necessary. Option D is incorrect because it also mentions stacks rather than stack sets, which is not suitable for deploying to multiple accounts.",
        "additional_knowledge": "Understanding the differences between stack sets and stacks is critical in cloud deployment scenarios.",
        "key_terminology": "AWS Organizations, CloudFormation, StackSets, service-managed permissions, SNS topic",
        "overall_assessment": "The question effectively tests knowledge of how to deploy resources across multiple AWS accounts using CloudFormation StackSets in an AWS Organizations context. The community consensus supports the correct answer."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCです。スタックセットはAWS Organizationの管理アカウントで作成する必要があり、これによりサービス管理の権限を使用してすべてのメンバーアカウントにデプロイすることができます。",
        "situation_analysis": "企業は複数のアカウントを持つAWS Organizationsを使用しており、特にSNSトピックをすべてのメンバーアカウントにデプロイするための集中管理された方法を要求しています。",
        "option_analysis": "選択肢Aは、スタックセットがこのシナリオでメンバーアカウントに作成されるべきではないため、誤っています。選択肢Bは、自己管理の権限を使用することを提案しているため、誤りです。選択肢Dもメンバーアカウントにデプロイするためには適していないスタックを参照しているため、誤りです。",
        "additional_knowledge": "スタックセットとスタックの違いを理解することは、クラウドデプロイメントシナリオで重要です。",
        "key_terminology": "AWS Organizations, CloudFormation, StackSets, サービス管理の権限, SNSトピック",
        "overall_assessment": "この質問は、AWS Organizationsの文脈において、CloudFormation StackSetsを利用してリソースを複数のAWSアカウントにデプロイする方法についての知識を効果的にテストしています。コミュニティの合意が正しい回答を支持しています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "CloudFormation",
      "StackSets",
      "service-managed permissions",
      "SNS topic"
    ]
  },
  {
    "No": "39",
    "question": "A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux and Windows. The company has a large on-\npremises infrastructure that consists of physical machines and VMs that host numerous applications.\nThe company must capture details about the system configuration, system performance, running processes, and network connections of its on-\npremises workloads. The company also must divide the on-premises applications into groups for AWS migrations. The company needs\nrecommendations for Amazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effective manner.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "ある企業がオンプレミスからAWSへのワークロードの移行を希望しています。ワークロードはLinuxとWindowsで実行されます。この企業には、多数のアプリケーションをホストする物理マシンと仮想マシンから成る大規模なオンプレミス環境があります。企業は、オンプレミスのワークロードのシステム構成、システム性能、実行中のプロセス、ネットワーク接続に関する詳細をキャプチャする必要があります。また、企業はAWSへの移行のためにオンプレミスのアプリケーションをグループ化する必要があります。企業は、AWS上でコスト効率の高い方法でワークロードを実行できるようにするため、Amazon EC2インスタンスタイプに関する推奨を必要としています。要件を満たすために、ソリューションアーキテクトが取るべき手順の組み合わせはどれですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs.",
        "text_jp": "物理マシンとVMにAWSアプリケーション発見エージェントをインストールして既存のアプリケーションを評価する。"
      },
      {
        "key": "B",
        "text": "Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs.",
        "text_jp": "物理マシンとVMにAWS Systems Managerエージェントをインストールして既存のアプリケーションを評価する。"
      },
      {
        "key": "C",
        "text": "Group servers into applications for migration by using AWS Systems Manager Application Manager.",
        "text_jp": "AWS Systems Managerアプリケーションマネージャーを使用して、移行のためのアプリケーションとしてサーバーをグループ化する。"
      },
      {
        "key": "D",
        "text": "Group servers into applications for migration by using AWS Migration Hub.",
        "text_jp": "AWS Migration Hubを使用して、移行のためのアプリケーションとしてサーバーをグループ化する。"
      },
      {
        "key": "E",
        "text": "Generate recommended instance types and associated costs by using AWS Migration Hub.",
        "text_jp": "AWS Migration Hubを使用して、推奨されるインスタンスタイプと関連コストを生成する。"
      },
      {
        "key": "F",
        "text": "Import data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization.",
        "text_jp": "サーバーサイズに関するデータをAWS Trusted Advisorにインポートし、コスト最適化の推奨事項に従う。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ADE (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Installing the AWS Systems Manager Agent on both physical machines and VMs is essential for gathering detailed information about the existing applications, such as system configuration and performance metrics. This data is critical for planning the migration accurately.",
        "situation_analysis": "The company has a combination of Linux and Windows workloads with complex on-premises infrastructure. It requires comprehensive insights into its existing applications to make informed decisions about migration.",
        "option_analysis": "Option A focuses on the AWS Application Discovery Agent, which is relevant but not the best first step. Options C and D correctly describe how to categorize applications, but the assessment must come first. Option E is misleading because generating instance types is not the first step after assessment - it comes later. Option F should not be prioritized until the assessment phase is completed.",
        "additional_knowledge": "After installing the Systems Manager Agent, AWS recommends using the Application Manager and Migration Hub for effective application grouping and migration tracking.",
        "key_terminology": "AWS Systems Manager, AWS Migration Hub, migration assessment, application grouping",
        "overall_assessment": "The question is comprehensive and requires an understanding of AWS services related to migration. While community votes indicate support for options A, D, and E, ensuring a correct assessment through option B is vital as the foundation for the rest of the process."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。物理マシンとVMの両方にAWS Systems Managerエージェントをインストールすることは、システム構成や性能メトリクスなど、既存のアプリケーションに関する詳細情報を収集するために重要です。このデータは、正確な移行計画に必要です。",
        "situation_analysis": "この企業は、LinuxとWindowsのワークロードを組み合わせた複雑なオンプレミスインフラストラクチャを持っています。適切な移行に関する決定を下すには、既存のアプリケーションに関する包括的な洞察が必要です。",
        "option_analysis": "選択肢AはAWSアプリケーション発見エージェントに焦点を当てていますが、最初のステップとしては最適ではありません。選択肢CおよびDはアプリケーションをカテゴライズする方法を正しく説明していますが、評価が最初である必要があります。選択肢Eは、インスタンスタイプを生成することが重要ですが、評価の後に来るべきです。選択肢Fは、評価フェーズが完了するまで優先されるべきではありません。",
        "additional_knowledge": "Systems Managerエージェントをインストールした後、AWSは効果的なアプリケーショングループ化と移行追跡のためにApplication ManagerやMigration Hubの使用を推奨しています。",
        "key_terminology": "AWS Systems Manager、AWS Migration Hub、移行評価、アプリケーショングループ化",
        "overall_assessment": "この質問は包括的で、移行に関連するAWSサービスの理解が必要です。コミュニティ投票では選択肢A、D、Eが支持されていますが、正しい評価がプロセス全体に必要です。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "AWS Migration Hub",
      "Migration Assessment",
      "Application Grouping"
    ]
  },
  {
    "No": "40",
    "question": "A company is hosting an image-processing service on AWS in a VPC. The VPC extends across two Availability Zones. Each Availability Zone\ncontains one public subnet and one private subnet.\nThe service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service.\nThe service needs to communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The\nEC2 instances retrieve approximately 1 ТВ of data from an S3 bucket each day.\nThe company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without\ncompromising the service's security posture or increasing the time spent on ongoing operations.\nWhich solution will meet these requirements?",
    "question_jp": "ある会社がAWS上で画像処理サービスをVPC内でホストしています。このVPCは二つのアベイラビリティゾーンにまたがっています。各アベイラビリティゾーンには、1つのパブリックサブネットと1つのプライベートサブネットがあります。サービスはプライベートサブネット内のAmazon EC2インスタンス上で実行されています。パブリックサブネットに配置されたアプリケーションロードバランサーがサービスの前にあります。サービスはインターネットと通信する必要があり、2つのNATゲートウェイを介して通信します。また、サービスは画像ストレージにAmazon S3を使用しています。EC2インスタンスは毎日約1TBのデータをS3バケットから取得します。会社はこのサービスを非常に安全であると宣伝しています。ソリューションアーキテクトは、サービスのセキュリティポスチャーを損なうことなく、運営にかかる時間を増やすことなく、クラウド支出を可能な限り削減しなければなりません。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Replace the NAT gateways with NAT instances. In the VPC route table, create a route from the private subnets to the NAT instances.",
        "text_jp": "NATゲートウェイをNATインスタンスに置き換えます。VPCルートテーブルにプライベートサブネットからNATインスタンスへのルートを作成します。"
      },
      {
        "key": "B",
        "text": "Move the EC2 instances to the public subnets. Remove the NAT gateways.",
        "text_jp": "EC2インスタンスをパブリックサブネットに移動します。NATゲートウェイを削除します。"
      },
      {
        "key": "C",
        "text": "Set up an S3 gateway VPC endpoint in the VPAttach an endpoint policy to the endpoint to allow the required actions on the S3 bucket.",
        "text_jp": "S3ゲートウェイVPCエンドポイントをVPCに設定します。エンドポイントポリシーをエンドポイントに追加して、S3バケットに対する必要なアクションを許可します。"
      },
      {
        "key": "D",
        "text": "Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances. Host the images on the EFS volume.",
        "text_jp": "Amazon Elastic File System（Amazon EFS）ボリュームをEC2インスタンスにアタッチします。画像をEFSボリュームにホストします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Setting up an S3 gateway VPC endpoint allows EC2 instances in the private subnet to access S3 directly without going through the NAT gateway, reducing costs significantly.",
        "situation_analysis": "The service is hosted in a highly secure environment using private subnets for EC2 instances. Currently, it incurs costs due to NAT gateways when accessing S3.",
        "option_analysis": "Option A would reduce costs but doesn't maintain the security posture as required. Option B compromises the security by moving EC2 instances to public subnets. Option D introduces additional complexity and cost without directly addressing the NAT costs.",
        "additional_knowledge": "AWS recommends using VPC endpoints to securely extend services into private subnets without utilizing NAT.",
        "key_terminology": "VPC endpoint, NAT gateway, S3, cost reduction, security posture",
        "overall_assessment": "Option C is optimal as it enhances security by maintaining a private subnet architecture while reducing NAT costs. Community voting aligns with this assessment, indicating strong support for this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCである。S3ゲートウェイVPCエンドポイントを設定することで、プライベートサブネットのEC2インスタンスがS3に直接アクセスでき、NATゲートウェイを介さないため、コストを大幅に削減できる。",
        "situation_analysis": "サービスは、EC2インスタンス用にプライベートサブネットを使用している高いセキュリティ環境でホストされている。現在、S3にアクセスするためにNATゲートウェイにコストがかかっている。",
        "option_analysis": "選択肢Aはコスト削減にはなるが、求められているセキュリティポスチャーを維持できない。選択肢Bは、EC2インスタンスをパブリックサブネットに移動させるため、セキュリティを損なう。選択肢Dは、追加の複雑さとコストを持ち込むが、NATのコストに直接アプローチしていない。",
        "additional_knowledge": "AWSは、NATを使用せずにプライベートサブネットからサービスを安全に拡張するためにVPCエンドポイントの使用を推奨している。",
        "key_terminology": "VPCエンドポイント、NATゲートウェイ、S3、コスト削減、セキュリティポスチャー",
        "overall_assessment": "選択肢Cが最適であり、プライベートサブネットアーキテクチャを維持しながらNATのコストを削減し、セキュリティを強化する。コミュニティ投票はこの評価に沿っており、この選択肢への強い支持を示している。"
      }
    ],
    "keywords": [
      "VPC endpoint",
      "NAT gateway",
      "S3",
      "cost reduction",
      "security posture"
    ]
  },
  {
    "No": "41",
    "question": "A company recently deployed an application on AWS. The application uses Amazon DynamoDB. The company measured the application load and\nconfigured the RCUs and WCUs on the DynamoDB table to match the expected peak load. The peak load occurs once a week for a 4-hour period\nand is double the average load. The application load is close to the average load for the rest of the week. The access pattern includes many more\nwrites to the table than reads of the table.\nA solutions architect needs to implement a solution to minimize the cost of the table.\nWhich solution will meet these requirements?",
    "question_jp": "企業は最近、AWS上にアプリケーションを展開しました。このアプリケーションは、Amazon DynamoDBを使用しています。企業はアプリケーションの負荷を測定し、DynamoDBテーブルのRCUおよびWCUを予想されるピーク負荷に合わせて設定しました。ピーク負荷は週に一度、4時間の期間で発生し、平均負荷の2倍です。アプリケーションの負荷は、週の残りの部分では平均負荷に近い状態です。アクセスパターンは、テーブルの読み取りよりも書き込みがはるかに多いです。ソリューションアーキテクトは、テーブルのコストを最小限に抑えるための解決策を実装する必要があります。どの解決策がこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average",
        "text_jp": "ピーク期間中に容量を増加させるために、AWS Application Auto Scalingを使用します。平均に合わせてRCUおよびWCUを予約購入します。"
      },
      {
        "key": "B",
        "text": "Configure on-demand capacity mode for the table.",
        "text_jp": "テーブルのオンデマンド容量モードを設定します。"
      },
      {
        "key": "C",
        "text": "Configure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table.",
        "text_jp": "テーブルの前にDynamoDB Accelerator (DAX)を設定します。新しいピーク負荷に合わせてプロビジョニングされた読み取り容量を減少させます。"
      },
      {
        "key": "D",
        "text": "Configure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table.",
        "text_jp": "テーブルの前にDynamoDB Accelerator (DAX)を設定します。テーブルのオンデマンド容量モードを設定します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (72%) B (17%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. By configuring DynamoDB Accelerator (DAX) in front of the table and setting the table to on-demand capacity mode, the company can minimize costs while handling fluctuating peak loads efficiently.",
        "situation_analysis": "The company has a weekly peak load that lasts for 4 hours and is double the average load. Given the write-heavy access pattern, it is vital to use a cost-effective solution.",
        "option_analysis": "Option A involves auto-scaling which could be costly due to high RCU and WCU rates; Option B is effective but doesn't use DAX which can provide caching for improved read performance; Option C would reduce read capacity but does not account for the load spike effectively like DAX with on-demand mode.",
        "additional_knowledge": "Implementing DAX with on-demand capacity can take advantage of DynamoDB's scaling capabilities more effectively than others.",
        "key_terminology": "DynamoDB, DynamoDB Accelerator (DAX), On-Demand Capacity Mode, Read Capacity Unit (RCU), Write Capacity Unit (WCU)",
        "overall_assessment": "D is superior as it combines read optimization with cost effectiveness, while the community votes suggest a preference for option A likely due to a misunderstanding of cost dynamics."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。テーブルの前にDynamoDB Accelerator (DAX)を設定し、テーブルをオンデマンド容量モードにすることで、企業はコストを最小限に抑えながら、変動するピーク負荷に効率的に対応できる。",
        "situation_analysis": "企業は、毎週ピーク負荷が4時間続き、平均負荷の2倍である状況である。書き込み中心のアクセスパターンを考えると、コスト効果の高い解決策を使用することが重要である。",
        "option_analysis": "選択肢Aはオートスケーリングを含むが、RCUおよびWCUの高い料金によるコストがかかる可能性がある。選択肢Bは効果的だが、DAXを使用していないため、ピーク時の読み取り性能を向上させることができない。選択肢Cは読み取り容量を削減するが、ピーク負荷に対する対応は不十分である。",
        "additional_knowledge": "DAXとオンデマンド容量の実装は、件に最も効果的なDynamoDBのスケーリング機能を最大限に活用することができる。",
        "key_terminology": "DynamoDB、DynamoDB Accelerator (DAX)、オンデマンド容量モード、読み取り容量単位 (RCU)、書き込み容量単位 (WCU)",
        "overall_assessment": "Dの選択肢は、コスト効果を考慮しつつ、読み取りの最適化を組み合わせる点で優れている。コミュニティの投票は選択肢Aに偏っているが、コストの動態に対する誤解から来ている可能性がある。"
      }
    ],
    "keywords": [
      "DynamoDB",
      "DynamoDB Accelerator",
      "On-Demand Capacity Mode",
      "Read Capacity Unit",
      "Write Capacity Unit"
    ]
  },
  {
    "No": "42",
    "question": "A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users\nupload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a\nmessage queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting\nprocessing is significantly higher during business hours, with the number of files rapidly declining after business hours.\nWhat is the MOST cost-effective migration recommendation?",
    "question_jp": "ソリューションアーキテクトは、企業に対してオンプレミスのデータ処理アプリケーションをAWSクラウドに移行する方法を助言する必要があります。現在、ユーザーはウェブポータルを通じて入力ファイルをアップロードしており、ウェブサーバーはアップロードされたファイルをNASに保存し、メッセージキューを介して処理サーバーにメッセージを送信します。各メディアファイルの処理には最大1時間かかる場合があります。企業は、ビジネス時間中に処理を待っているメディアファイルの数が大幅に多く、ビジネス時間後にはファイルの数が急速に減少することを確認しました。最もコスト効果の高い移行推奨は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue,",
        "text_jp": "Amazon SQSを使用してキューを作成します。既存のウェブサーバーを構成して新しいキューに発行します。キューにメッセージがある場合、"
      },
      {
        "key": "B",
        "text": "Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue,",
        "text_jp": "Amazon MQを使用してキューを作成します。既存のウェブサーバーを構成して新しいキューに発行します。キューにメッセージがある場合、"
      },
      {
        "key": "C",
        "text": "Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue,",
        "text_jp": "Amazon MQを使用してキューを作成します。既存のウェブサーバーを構成して新しいキューに発行します。キューにメッセージがある場合、"
      },
      {
        "key": "D",
        "text": "Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2",
        "text_jp": "Amazon SQSを使用してキューを作成します。既存のウェブサーバーを構成して新しいキューに発行します。Amazon EC2インスタンスを使用してEC2で"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (94%) 3%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. By creating a queue using Amazon SQS and utilizing EC2 instances, the company can efficiently handle varying workloads during business hours and minimize costs by scaling down after hours.",
        "situation_analysis": "The high volume of media files during business hours indicates the need for scalable and flexible processing. SQS enables effective message queuing, facilitating demand spikes without pre-provisioning resources.",
        "option_analysis": "Option A would not utilize EC2, leading to potential performance bottlenecks. Options B and C propose using Amazon MQ, which is less cost-effective for this use case compared to SQS.",
        "additional_knowledge": "Using Amazon EC2 instances can help in dynamically adjusting processing capacity based on the queue length.",
        "key_terminology": "Amazon SQS, EC2, message queue, scalable architecture",
        "overall_assessment": "This question tests the understanding of scalable architecture and cost-effective AWS services. The overwhelming community support for option D aligns with AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。Amazon SQSを使用してキューを作成し、EC2インスタンスを利用することで、企業はビジネス時間中の変動するワークロードを効率的に処理し、営業時間後はコストを最小限に抑えることができる。",
        "situation_analysis": "ビジネス時間中のメディアファイルの高いボリュームは、スケーラブルで柔軟な処理の必要性を示している。SQSは効果的なメッセージキューイングを可能にし、事前にリソースをプロビジョニングせずに需要の急増に対処できる。",
        "option_analysis": "選択肢AはEC2を活用せず、パフォーマンスのボトルネックにつながる可能性がある。選択肢BとCはAmazon MQの利用を提案しているが、SQSに比べてコスト効果が低い。",
        "additional_knowledge": "Amazon EC2インスタンスを使用することで、キューの長さに基づいて処理能力を動的に調整することができる。",
        "key_terminology": "Amazon SQS, EC2, メッセージキュー, スケーラブルアーキテクチャ",
        "overall_assessment": "この質問は、スケーラブルなアーキテクチャとコスト効果の高いAWSサービスについての理解をテストしている。選択肢Dへの圧倒的なコミュニティの支持は、AWSのベストプラクティスと一致している。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "EC2",
      "message queue",
      "scalable architecture"
    ]
  },
  {
    "No": "43",
    "question": "A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes\nfrom an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the\ncompany deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.\nThe company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "企業はAmazon OpenSearch Serviceを使用してデータを分析しています。企業はデータをAmazon S3バケットから10のデータノードを持つOpenSearch Serviceクラスターにロードしています。このデータは1か月間、読み取り専用の分析のためにクラスターに保持されます。1か月後、企業はクラスタからデータを含むインデックスを削除します。コンプライアンスのために、企業はすべての入力データのコピーを保持する必要があります。企業は継続的なコストを懸念しており、ソリューションアーキテクトに新しいソリューションを推奨するよう依頼しています。どのソリューションがこれらの要件を最もコスト効率よく満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Replace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier",
        "text_jp": "すべてのデータノードをUltraWarmノードに置き換え、予想される容量を処理します。入力データをS3 StandardからS3 Glacierに移行します。"
      },
      {
        "key": "B",
        "text": "Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to",
        "text_jp": "クラスター内のデータノードの数を2に減らし、予想される容量を処理するためにUltraWarmノードを追加します。インデックスを構成します。"
      },
      {
        "key": "C",
        "text": "Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to",
        "text_jp": "クラスター内のデータノードの数を2に減らし、予想される容量を処理するためにUltraWarmノードを追加します。インデックスを構成します。"
      },
      {
        "key": "D",
        "text": "Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input",
        "text_jp": "クラスター内のデータノードの数を2に減らし、予想される容量を処理するためにインスタンスバックデータノードを追加します。入力を移行します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Reduce the number of data nodes in the cluster to 2 and add UltraWarm nodes to handle the expected capacity. This solution effectively lowers costs while still meeting the requirements for data retention.",
        "situation_analysis": "The company needs to retain data at a lower cost after a one-month retention period in OpenSearch. This necessitates a cost-effective storage solution for the data that has already been analyzed.",
        "option_analysis": "Option A is expensive due to S3 Glacier costs for frequent access. Option C does not fully articulate solutions so it is less clear. Option D does not utilize UltraWarm nodes, which can save costs. Hence, option B is best.",
        "additional_knowledge": "Companies often face challenges balancing data analysis needs with cost, making choices like UltraWarm critical in financial planning.",
        "key_terminology": "OpenSearch Service, UltraWarm nodes, S3 Glacier, data retention, cost-effective storage",
        "overall_assessment": "The answer B has strong support from the community and aligns well with AWS best practices for cost management and data compliance. The high community vote indicates broad agreement with this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。クラスター内のデータノードの数を2に減らし、予想される容量を処理するためにUltraWarmノードを追加する。このソリューションは、データ保持の要件を満たしながらコストを効果的に削減する。",
        "situation_analysis": "企業は、OpenSearch内での1か月の保持期間後に、低コストでデータを保持する必要がある。これには、既に分析されたデータのためのコスト効率の良いストレージソリューションが必要である。",
        "option_analysis": "選択肢Aは、頻繁なアクセスのためのS3 Glacierコストが高くなるため高価である。選択肢Cは具体的な解決策を十分に示していないため不明瞭である。選択肢Dは、コストを節約できるUltraWarmノードを活用していない。したがって、選択肢Bが最適である。",
        "additional_knowledge": "企業は、分析ニーズとコストのバランスを取ることにしばしば課題を抱えており、UltraWarmのような選択肢が財務計画において重要である。",
        "key_terminology": "OpenSearch Service、UltraWarmノード、S3 Glacier、データ保持、コスト効率の良いストレージ",
        "overall_assessment": "答えBはコミュニティから強い支持を受けており、コスト管理やデータコンプライアンスに関するAWSのベストプラクティスと良く一致している。高いコミュニティ投票は、この選択について広く合意があることを示している。"
      }
    ],
    "keywords": [
      "OpenSearch Service",
      "UltraWarm nodes",
      "S3 Glacier",
      "data retention",
      "cost-effective storage"
    ]
  },
  {
    "No": "44",
    "question": "A company has 10 accounts that are part of an organization in AWS Organizations. AWS Config is configured in each account. All accounts belong\nto either the Prod OU or the NonProd OU.\nThe company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic\nwhen an Amazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source. The company's security team is subscribed to the SNS\ntopic.\nFor all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0\nas the source.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "question_jp": "ある企業には、AWS Organizationsに所属する10のアカウントがあります。AWS Configは各アカウントで設定されています。すべてのアカウントは、Prod OUまたはNonProd OUのいずれかに属しています。\nこの企業は、Amazon EC2のセキュリティグループのインバウンドルールが0.0.0.0/0をソースとして作成されたときに、Amazon Simple Notification Service (Amazon SNS)トピックに通知するために、各AWSアカウントにAmazon EventBridgeルールを設定しました。\n企業のセキュリティチームはSNSトピックに加入しています。\nNonProd OU内のすべてのアカウントについて、セキュリティチームは、0.0.0.0/0をソースとして含むセキュリティグループのインバウンドルールを作成する能力を削除する必要があります。\n最も運用上のオーバーヘッドが少ない要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic.",
        "text_jp": "イベントブリッジルールを変更して、AWS Lambda関数を呼び出し、セキュリティグループのインバウンドルールを削除し、SNSトピックに公開する。"
      },
      {
        "key": "B",
        "text": "Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU.",
        "text_jp": "NonProd OUにvpc-sg-open-only-to-authorized-ports AWS Config管理ルールを追加する。"
      },
      {
        "key": "C",
        "text": "Configure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0.",
        "text_jp": "aws:SourceIp条件キーの値が0.0.0.0/0ではない場合に、ec2:AuthorizeSecurityGroupIngressアクションを許可するSCPを設定する。"
      },
      {
        "key": "D",
        "text": "Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0.",
        "text_jp": "aws:SourceIp条件キーの値が0.0.0.0/0である場合に、ec2:AuthorizeSecurityGroupIngressアクションを拒否するSCPを設定する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (58%) A (39%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This solution uses Service Control Policies (SCPs) to manage permissions effectively and ensures that the security group inbound rules cannot be created with a 0.0.0.0/0 source.",
        "situation_analysis": "The requirement is to prevent the creation of security group inbound rules with a source of 0.0.0.0/0 in all accounts under the NonProd OU, thereby improving security and reducing exposure to potential threats.",
        "option_analysis": "Option A introduces a Lambda function which adds operational complexity. Option B does not directly meet the requirement of preventing the creation of the rule. Option D, while it may seem appropriate, does not align with best practices since it's generally more effective to manage permissions at a broader level using SCPs.",
        "additional_knowledge": "It's crucial to continuously review and adjust policies in AWS Organizations to ensure they enforce the desired security posture.",
        "key_terminology": "Service Control Policy, AWS Organizations, Security Group, Inbound Rule, IAM Role",
        "overall_assessment": "The analysis indicates that Option C is the most efficient and least operationally burdensome solution that meets the specified requirement. The community votes suggest misunderstanding, as they favor options that don't directly restrict the rule creation."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。このソリューションはサービスコントロールポリシー（SCP）を使用して権限を効果的に管理し、セキュリティグループのインバウンドルールが0.0.0.0/0をソースとすることを防ぐ。",
        "situation_analysis": "今回の要件は、NonProd OU下のすべてのアカウントで0.0.0.0/0をソースとするセキュリティグループのインバウンドルールを作成できないようにすることで、セキュリティを向上させ、潜在的な脅威への露出を減少させることである。",
        "option_analysis": "オプションAはLambda関数を導入するため、運用上の複雑さが増す。オプションBはルール作成を直接防ぐものではない。オプションDは適切に思えるが、SCPを使用してより広範囲に権限を管理する方がベストプラクティスに沿っている。",
        "additional_knowledge": "AWS Organizations内のポリシーは継続的にレビューおよび調整し、望まれるセキュリティ体制を強化することが重要である。",
        "key_terminology": "サービスコントロールポリシー、AWS Organizations、セキュリティグループ、インバウンドルール、IAMロール",
        "overall_assessment": "分析の結果、オプションCが要件に対して最も効率的で運用的負担が少ないことが明らかになった。コミュニティの投票は誤解を示唆しており、ルール作成を直接制限しないオプションを支持している。"
      }
    ],
    "keywords": [
      "Service Control Policy",
      "AWS Organizations",
      "Security Group",
      "Inbound Rule",
      "IAM Role"
    ]
  },
  {
    "No": "45",
    "question": "A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud.\nThe company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an\nApplication Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a\nserverless architecture.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "企業はオンプレミスのデータセンターにGitリポジトリをホストしています。企業はWebフックを使用して、AWSクラウドで実行される機能を呼び出します。企業は、WebフックのロジックをAmazon EC2インスタンスのAuto Scalingグループにホストしており、そのグループがApplication Load Balancer（ALB）のターゲットとして設定されています。GitサーバーはALBを呼び出して設定されたWebフックを実行します。企業はこのソリューションをサーバーレスアーキテクチャに移行したいと考えています。最小の運用オーバーヘッドでこれらの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "For each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs.",
        "text_jp": "各Webフックに対してAWS Lambda関数のURLを作成して設定します。Gitサーバーを個々のLambda関数のURLを呼び出すように更新します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to",
        "text_jp": "Amazon API GatewayのHTTP APIを作成します。各Webフックロジックを別々のAWS Lambda関数に実装します。Gitサーバーを更新します。"
      },
      {
        "key": "C",
        "text": "Deploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB",
        "text_jp": "WebhookロジックをAWS App Runnerにデプロイします。ALBを作成し、ターゲットとしてApp Runnerを設定します。Gitサーバーを呼び出します。"
      },
      {
        "key": "D",
        "text": "Containerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS",
        "text_jp": "Webhookロジックをコンテナ化します。Amazon Elastic Container Service（Amazon ECS）クラスターを作成し、AWSでWebhookロジックを実行します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (89%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Deploying the webhook logic to AWS App Runner allows for a serverless architecture with minimal operational management, as AWS handles the scaling and maintenance.",
        "situation_analysis": "The company seeks to migrate from a traditional EC2 Auto Scaling setup to a serverless solution to reduce operational burden while maintaining webhook functionality.",
        "option_analysis": "Option A requires updating Git servers to use individual Lambda URLs, which can become cumbersome. Option B may require complex management of multiple APIs and Lambda functions. Option D entails container management, which involves more operational overhead than App Runner.",
        "additional_knowledge": "Understanding the benefits of serverless architectures is crucial for optimizing cloud operations.",
        "key_terminology": "AWS App Runner, Application Load Balancer, serverless architecture, operational overhead, webhook.",
        "overall_assessment": "Option C represents the most efficient transition to a serverless model, focusing on reduced operational overhead compared to the alternatives. The community question responses may favor option B due to its flexibility in function implementation, yet it doesn’t match the requirement for minimizing overhead."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。WebhookロジックをAWS App Runnerにデプロイすることで、AWSがスケーリングやメンテナンスを処理するため、最小の運用管理で済むサーバーレスアーキテクチャが実現されます。",
        "situation_analysis": "企業は、ウェブフックの機能を維持しつつ、従来のEC2 Auto Scaling構成からサーバーレスソリューションに移行し、運用コストを削減したいと考えています。",
        "option_analysis": "選択肢Aは、 Gitサーバーが個々のLambdaのURLを使用するように更新する必要があり、手間がかかる可能性があります。選択肢Bは複数のAPIとLambda関数の管理が必要になる可能性があります。選択肢Dはコンテナ管理を含むため、App Runnerよりも運用オーバーヘッドが多くなります。",
        "additional_knowledge": "サーバーレスアーキテクチャの利点を理解することは、クラウド運用の最適化に非常に重要です。",
        "key_terminology": "AWS App Runner、Application Load Balancer、サーバーレスアーキテクチャ、運用オーバーヘッド、Webフック。",
        "overall_assessment": "選択肢Cは、代替案に比べて運用オーバーヘッドの最小化に着目したサーバーレスモデルへの移行を最も効率的に示しています。コミュニティの質問への回答は選択肢Bに偏るかもしれませんが、柔軟性はあるものの、オーバーヘッド最小化の要件には合致しません。"
      }
    ],
    "keywords": [
      "AWS App Runner",
      "Application Load Balancer",
      "serverless architecture",
      "operational overhead",
      "webhook"
    ]
  },
  {
    "No": "46",
    "question": "A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company's data center. As\npart of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and\nrunning processes. The company then wants to query and analyze the data.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が1,000台のオンプレミスサーバーをAWSに移行する計画を立てています。これらのサーバーは、企業のデータセンターにあるいくつかのVMwareクラスター上で動作しています。移行計画の一環として、企業はCPUの詳細、RAMの使用状況、オペレーティングシステム情報、実行中のプロセスなどのサーバーメトリックを収集したいと考えています。その後、企業はデータをクエリし、分析したいと考えています。これらの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in",
        "text_jp": "AWS Agentless Discovery Connector仮想アプライアンスをオンプレミスホストに展開し、構成する。データ探索をAWSに設定する。"
      },
      {
        "key": "B",
        "text": "Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update",
        "text_jp": "オンプレミスホストからVMパフォーマンス情報のみをエクスポートし、必要なデータをAWS Migration Hubに直接インポートする。"
      },
      {
        "key": "C",
        "text": "Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-",
        "text_jp": "サーバー情報をオンプレミスホストから自動的に収集するスクリプトを作成する。AWS CLIを使用してput-resource-を実行する。"
      },
      {
        "key": "D",
        "text": "Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon",
        "text_jp": "AWS Application Discovery Agentを各オンプレミスサーバーに展開し、AWS Migration Hubでデータ探索を設定する。Amazonを使用する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (89%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It allows for the automatic gathering of server information, which is essential for migration planning.",
        "situation_analysis": "The company needs to collect detailed metrics from 1,000 on-premises servers to effectively analyze their environment before migration.",
        "option_analysis": "Option A involves the Agentless Discovery Connector, which might not fully cover detailed server metrics. Option B only exports VM performance data, lacking comprehensive insight. Option D suggests using AWS Application Discovery Agent but does not mention gathering information automatically.",
        "additional_knowledge": "Automation in data collection prevents human errors and enhances the accuracy of migration planning.",
        "key_terminology": "AWS CLI, AWS Application Discovery Agent, AWS Migration Hub, server metrics",
        "overall_assessment": "While option D has community support, option C provides a practical method for gathering comprehensive server data necessary for migration analysis."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。これは、移行計画に不可欠なサーバー情報の自動収集を可能にします。",
        "situation_analysis": "企業は、1,000台のオンプレミスサーバーから詳細なメトリックを収集し、移行前に環境を効果的に分析する必要があります。",
        "option_analysis": "選択肢Aは、Agentless Discovery Connectorを利用していますが、詳細なサーバーメトリックを完全にはカバーできません。選択肢BはVMパフォーマンスデータのみをエクスポートし、包括的な洞察が欠けています。選択肢DはAWS Application Discovery Agentを使用することを提案していますが、情報を自動的に収集することには言及していません。",
        "additional_knowledge": "データ収集の自動化は人的エラーを防ぎ、移行計画の正確性を高めます。",
        "key_terminology": "AWS CLI、AWS Application Discovery Agent、AWS Migration Hub、サーバーメトリック",
        "overall_assessment": "選択肢Dはコミュニティの支持がありますが、選択肢Cは移行分析に必要な包括的なサーバーデータを収集する実用的な方法を提供します。"
      }
    ],
    "keywords": [
      "AWS CLI",
      "AWS Application Discovery Agent",
      "AWS Migration Hub",
      "server metrics"
    ]
  },
  {
    "No": "47",
    "question": "A company is building a serverless application that runs on an AWS Lambda function that is attached to a VPC. The company needs to integrate\nthe application with a new service from an external provider. The external provider supports only requests that come from public IPv4 addresses\nthat are in an allow list.\nThe company must provide a single public IP address to the external provider before the application can start using the new service.\nWhich solution will give the application the ability to access the new service?",
    "question_jp": "会社は、VPC に接続された AWS Lambda 関数上で実行されるサーバーレスアプリケーションを構築しています。会社は、外部プロバイダーからの新しいサービスとアプリケーションを統合する必要があります。外部プロバイダーは、許可リストにあるパブリック IPv4 アドレスからのリクエストのみをサポートしています。会社は、新しいサービスを使用できるようになる前に、外部プロバイダーに単一のパブリック IP アドレスを提供する必要があります。どのソリューションがアプリケーションに新しいサービスにアクセスする能力を与えるのでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.",
        "text_jp": "NAT ゲートウェイを展開し、NAT ゲートウェイに Elastic IP アドレスを関連付け、VPC を NAT ゲートウェイを使用するように構成する。"
      },
      {
        "key": "B",
        "text": "Deploy an egress-only internet gateway. Associate an Elastic IP address with the egress-only internet gateway. Configure the elastic",
        "text_jp": "Egress-only インターネットゲートウェイを展開し、Egress-only インターネットゲートウェイに Elastic IP アドレスを関連付ける。"
      },
      {
        "key": "C",
        "text": "Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the Lambda function to use the internet",
        "text_jp": "インターネットゲートウェイを展開し、インターネットゲートウェイに Elastic IP アドレスを関連付け、Lambda 関数をインターネットを使用するように構成する。"
      },
      {
        "key": "D",
        "text": "Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the default route in the public VPC route",
        "text_jp": "インターネットゲートウェイを展開し、インターネットゲートウェイに Elastic IP アドレスを関連付け、パブリック VPC ルートのデフォルトルートを構成する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (94%) 4%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Deploying an internet gateway and associating an Elastic IP allows the Lambda function to send requests using a public IPv4 address, which is required by the external provider.",
        "situation_analysis": "The application needs to integrate with an external service requiring requests from a public IP address. An AWS Lambda function in a VPC does not have direct internet access unless configured properly.",
        "option_analysis": "Option C allows the Lambda function to connect to the internet and utilize a designated Elastic IP for requests. Option A does not suit this requirement as it only allows for external traffic from private addresses. Option B is incorrect as it applies to IPv6 traffic. Option D could allow internet access but does not specifically mention associating the Elastic IP in a way that complies with the requirement.",
        "additional_knowledge": "When using an internet gateway, the VPC route tables must also be configured to allow traffic to flow correctly.",
        "key_terminology": "AWS Lambda, VPC, Elastic IP, Internet Gateway, NAT Gateway",
        "overall_assessment": "The various options analyzed indicate that option C is the best solution as it directly addresses the need for a public IP address for external services."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは C である。インターネットゲートウェイを展開し、Elastic IP を関連付けることで、Lambda 関数は外部プロバイダーが要求するパブリック IPv4 アドレスを使用してリクエストを送信できる。",
        "situation_analysis": "アプリケーションは、外部サービスと統合する必要があり、そのサービスはパブリック IP アドレスからのリクエストを必要としている。VPC 内の AWS Lambda 関数は、適切に設定しない限り、直接インターネットにアクセスできない。",
        "option_analysis": "選択肢 C は、Lambda 関数がインターネットに接続し、指定された Elastic IP を使用してリクエストを送信することを可能にする。他の選択肢 A はプライベートアドレスからの外部トラフィックのみを許可するため、この要件には適していない。選択肢 B は IPv6 トラフィックに関するもので、誤りである。選択肢 D はインターネットアクセスを許す可能性があるが、Elastic IP を適切に関連付けていない。",
        "additional_knowledge": "インターネットゲートウェイを使用する場合は、VPC ルートテーブルも正しくトラフィックが流れるように構成する必要がある。",
        "key_terminology": "AWS Lambda、VPC、Elastic IP、インターネットゲートウェイ、NAT ゲートウェイ",
        "overall_assessment": "いくつかの選択肢を分析した結果、選択肢 C が外部サービスへのパブリック IP アドレスの要件に直接対応しているため、最良のソリューションである。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "VPC",
      "Elastic IP",
      "Internet Gateway",
      "NAT Gateway"
    ]
  },
  {
    "No": "48",
    "question": "A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The\nconsumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an\nAmazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.\nDuring testing, the application does not meet performance requirements. Under high load, the application opens a large number of database\nconnections. The solutions architect must improve the application's performance.\nWhich actions should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "あるソリューションアーキテクトが、Amazon API GatewayのリージョナルエンドポイントとAWS Lambda関数を使用したWebアプリケーションを開発しました。このWebアプリケーションの消費者は、アプリケーションが展開されるAWSリージョンの近くにいます。Lambda関数はAmazon Aurora MySQLデータベースをクエリするのみであり、データベースは3つのリードレプリカを持つように設定されています。テスト中、アプリケーションはパフォーマンス要件を満たしません。高負荷時にアプリケーションは多くのデータベース接続を開き、ソリューションアーキテクトはアプリケーションのパフォーマンスを向上させる必要があります。これらの要件を満たすために、ソリューションアーキテクトはどのアクションを取るべきでしょうか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Use the cluster endpoint of the Aurora database.",
        "text_jp": "Auroraデータベースのクラスタエンドポイントを使用する。"
      },
      {
        "key": "B",
        "text": "Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.",
        "text_jp": "RDS Proxyを使用してAuroraデータベースのリーダーエンドポイントへの接続プールを設定する。"
      },
      {
        "key": "C",
        "text": "Use the Lambda Provisioned Concurrency feature.",
        "text_jp": "Lambdaのプロビジョンドコンカレンシー機能を使用する。"
      },
      {
        "key": "D",
        "text": "Move the code for opening the database connection in the Lambda function outside of the event handler.",
        "text_jp": "Lambda関数のイベントハンドラーの外でデータベース接続を開くコードを移動する。"
      },
      {
        "key": "E",
        "text": "Change the API Gateway endpoint to an edge-optimized endpoint.",
        "text_jp": "API Gatewayのエンドポイントをエッジ最適化エンドポイントに変更する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BD (98%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct actions are to use RDS Proxy to manage database connections and improve scalability.",
        "situation_analysis": "The application is experiencing performance issues due to a large number of database connections during high load, necessitating a solution to optimize connection usage.",
        "option_analysis": "Option B is correct because RDS Proxy manages database connections efficiently. Option A is not sufficiently helpful for connection issues alone. Option C is relevant for concurrency but does not address database connection limits. Option D could reduce connection overhead but needs to be combined with a connection management layer like RDS Proxy. Option E does not address the primary issue of database connection management.",
        "additional_knowledge": "",
        "key_terminology": "RDS Proxy, connection pooling, Aurora MySQL, scalability, performance optimization",
        "overall_assessment": "The question accurately assesses knowledge of optimizing AWS architecture, particularly around managing database connections. Community voting aligns with the identified correct answer, affirming the choice of RDS Proxy as a best practice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しいアクションは、RDS Proxyを使用してデータベース接続を管理し、スケーラビリティを向上させることです。",
        "situation_analysis": "アプリケーションは、高負荷時に多くのデータベース接続が開かれるため、パフォーマンスの問題を抱えており、接続の使用を最適化するソリューションが必要です。",
        "option_analysis": "選択肢Bは、RDS Proxyがデータベース接続を効率的に管理できるため正しいです。選択肢Aは接続問題の改善には不十分です。選択肢Cは同時実行性に関連しますが、データベース接ッション制限の問題に対処しません。選択肢Dは接続のオーバーヘッドを減らす可能性がありますが、RDS Proxyのような接続管理レイヤーとの組み合わせが必要です。選択肢Eは、データベース接続管理の主要な問題を解決しません。",
        "additional_knowledge": "",
        "key_terminology": "RDS Proxy, 接続プーリング, Aurora MySQL, スケーラビリティ, パフォーマンス最適化",
        "overall_assessment": "この質問は、AWSアーキテクチャを最適化する知識を適切に評価しています。特にデータベース接続の管理について。コミュニティの投票は、正しい回答としてRDS Proxyの選択を確認し合っています。"
      }
    ],
    "keywords": [
      "RDS Proxy",
      "connection pooling",
      "Aurora MySQL",
      "scalability",
      "performance optimization"
    ]
  },
  {
    "No": "49",
    "question": "A company is planning to host a web application on AWS and wants to load balance the trafic across a group of Amazon EC2 instances. One of\nthe security requirements is to enable end-to-end encryption in transit between the client and the web server.\nWhich solution will meet this requirement?",
    "question_jp": "ある企業がAWS上でウェブアプリケーションをホストする計画をしており、複数のAmazon EC2インスタンスにトラフィックを負荷分散したいと考えています。セキュリティ要件の一つは、クライアントとウェブサーバー間のトランジットでエンドツーエンドの暗号化を有効にすることです。この要件を満たす解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM),",
        "text_jp": "EC2インスタンスをアプリケーションロードバランサー（ALB）の背後に配置する。AWS Certificate Manager (ACM)を使用してSSL証明書をプロビジョニングする。"
      },
      {
        "key": "B",
        "text": "Associate the EC2 instances with a target group. Provision an SSL certificate using AWS Certificate Manager (ACM). Create an Amazon",
        "text_jp": "EC2インスタンスをターゲットグループに関連付ける。AWS Certificate Manager (ACM)を使用してSSL証明書をプロビジョニングする。Amazonを作成する。"
      },
      {
        "key": "C",
        "text": "Place the EC2 instances behind an Application Load Balancer (ALB) Provision an SSL certificate using AWS Certificate Manager (ACM), and",
        "text_jp": "EC2インスタンスをアプリケーションロードバランサー（ALB）の背後に配置し、AWS Certificate Manager (ACM)を使用してSSL証明書をプロビジョニングし、"
      },
      {
        "key": "D",
        "text": "Place the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each",
        "text_jp": "EC2インスタンスをネットワークロードバランサー（NLB）の背後に配置する。サードパーティのSSL証明書をプロビジョニングし、NLBおよび各インスタンスにインストールする。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (56%) D (34%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This solution effectively uses AWS services to enable end-to-end encryption via SSL.",
        "situation_analysis": "Secure communication is a key requirement for the application, particularly for user data safety.",
        "option_analysis": "Option C clearly outlines the use of ALB and ACM, which allows for secure SSL management and provides the necessary end-to-end encryption.",
        "additional_knowledge": "When implementing SSL with ALB, it's crucial to establish listener rules for handling SSL termination.",
        "key_terminology": "Application Load Balancer, SSL certificate, AWS Certificate Manager, end-to-end encryption.",
        "overall_assessment": "This question highlights critical AWS components for achieving security in web applications. The provided options are relevant, though only C effectively meets the security requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。このソリューションはAWSサービスを効果的に利用してSSLを介してエンドツーエンドの暗号化を有効にする。",
        "situation_analysis": "安全な通信はアプリケーションの重要な要件であり、特にユーザーデータの安全性に関わる。",
        "option_analysis": "選択肢Cは、ALBとACMを使用することを明記しており、安全なSSL管理を可能にし、必要なエンドツーエンドの暗号化を提供する。",
        "additional_knowledge": "ALBでSSLを実装する際には、SSLターミネーションを処理するためのリスナールールを設定することが重要である。",
        "key_terminology": "アプリケーションロードバランサー、SSL証明書、AWS Certificate Manager、エンドツーエンドの暗号化。",
        "overall_assessment": "この質問は、ウェブアプリケーションのセキュリティを達成するための重要なAWSコンポーネントを浮き彫りにしている。提供された選択肢は関連性があるが、セキュリティ要件を満たすのはCだけである。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "SSL certificate",
      "AWS Certificate Manager",
      "end-to-end encryption"
    ]
  },
  {
    "No": "50",
    "question": "A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js\napplications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into\nreports. When the aggregation jobs run, some of the load jobs fail to run correctly.\nThe company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the\ncompany's customers.\nWhat should a solutions architect do to meet these requirements?",
    "question_jp": "ある企業がオンプレミスからAWSへのデータ分析環境の移行を希望しています。この環境は、2つのシンプルなNode.jsアプリケーションで構成されています。1つのアプリケーションはセンサーデータを収集し、それをMySQLデータベースにロードします。もう1つのアプリケーションは、データを集計してレポートにします。集計ジョブが実行されると、いくつかのロードジョブが正しく実行されません。企業はデータのロードの問題を解決しなければなりません。また、企業の顧客に対して中断や変更なく移行を行う必要があります。ソリューションアーキテクトはこれらの要件を満たすために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora",
        "text_jp": "Amazon Aurora MySQLデータベースをオンプレミスデータベースのレプリケーションターゲットとして設定します。Aurora用のAuroraレプリカを作成します"
      },
      {
        "key": "B",
        "text": "Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from",
        "text_jp": "Amazon Aurora MySQLデータベースを設定します。AWS Database Migration Service (AWS DMS)を使用して持続的なデータレプリケーションを行います"
      },
      {
        "key": "C",
        "text": "Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from",
        "text_jp": "Amazon Aurora MySQLデータベースを設定します。AWS Database Migration Service (AWS DMS)を使用して持続的なデータレプリケーションを行います"
      },
      {
        "key": "D",
        "text": "Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run",
        "text_jp": "Amazon Aurora MySQLデータベースを設定します。Aurora MySQLデータベースのためにAuroraレプリカを作成し、集計ジョブを移動して実行します"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Setting up an Amazon Aurora MySQL database and using AWS DMS ensures continuous data replication without downtime.",
        "situation_analysis": "The company requires a seamless migration with no changes for customers. Ongoing data issues during aggregation jobs hinder their operation.",
        "option_analysis": "Option C is superior because it ensures continuous replication, which resolves data loading issues and avoids service interruptions. Options A, B, and D do not provide the same level of reliability or data integrity during migration.",
        "additional_knowledge": "Continuous replication not only addresses the current data loading issues but also prepares the system for future scalability.",
        "key_terminology": "Amazon Aurora, AWS Database Migration Service (AWS DMS), continuous data replication, MySQL, data aggregation.",
        "overall_assessment": "Given the high community support (94%) for Option C, along with its alignment with best practices for smooth database migration, it is highly recommended."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。Amazon Aurora MySQLデータベースを設定し、AWS DMSを使用することで、ダウンタイムなしで持続的なデータレプリケーションを確保できます。",
        "situation_analysis": "企業は顧客に影響を与えないシームレスな移行を必要としています。集計ジョブ中の継続的なデータの問題が運用を妨げています。",
        "option_analysis": "選択肢Cは、持続的なレプリケーションを確保するため、データのロードの問題を解決し、サービスの中断を回避します。他の選択肢A、B、Dは同等の信頼性やデータの整合性を提供しません。",
        "additional_knowledge": "継続的なレプリケーションは、現在のデータのロード問題に対処するだけでなく、将来の拡張性のための準備にもなります。",
        "key_terminology": "Amazon Aurora、AWS Database Migration Service (AWS DMS)、持続的なデータレプリケーション、MySQL、データ集計。",
        "overall_assessment": "選択肢Cに対するコミュニティの支持率が94%であることからも、この選択肢がベストプラクティスに合致していることが分かります。非常に推奨されます。"
      }
    ],
    "keywords": [
      "Amazon Aurora",
      "AWS Database Migration Service",
      "continuous data replication",
      "MySQL",
      "data aggregation"
    ]
  },
  {
    "No": "51",
    "question": "A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption\nwith S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket\nmust be encrypted by keys that the company's security team manages. The S3 bucket does not have versioning enabled.\nWhich solution will meet these requirements?",
    "question_jp": "ヘルスケア保険会社は、Amazon S3 バケットに個人を特定可能な情報 (PII) を保存しています。会社は、サーバー側の暗号化を使用して、S3 管理暗号化キー (SSE-S3) を使用してオブジェクトを暗号化しています。新しい要件により、S3 バケット内の現在および将来のオブジェクトはすべて、会社のセキュリティチームが管理するキーで暗号化されなければなりません。S3 バケットではバージョニングが無効になっています。この要件を満たすためのソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all",
        "text_jp": "S3 バケットのプロパティで、デフォルトの暗号化を SSE-S3 と顧客管理キーに変更します。すべてを再アップロードするために AWS CLI を使用してください。"
      },
      {
        "key": "B",
        "text": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS).",
        "text_jp": "S3 バケットのプロパティで、デフォルトの暗号化を AWS KMS 管理暗号化キー (SSE-KMS) に変更します。"
      },
      {
        "key": "C",
        "text": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS).",
        "text_jp": "S3 バケットのプロパティで、デフォルトの暗号化を AWS KMS 管理暗号化キー (SSE-KMS) に変更します。"
      },
      {
        "key": "D",
        "text": "In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted",
        "text_jp": "S3 バケットのプロパティで、デフォルトの暗号化を顧客管理キーによる AES-256 に変更します。暗号化されていないものを拒否するポリシーを添付します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (58%) D (41%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. It ensures that all objects are encrypted using a customer-managed key, which meets the security team's requirement.",
        "situation_analysis": "The company needs to secure PII data in accordance with new regulatory requirements, mandating the use of managed keys for encryption.",
        "option_analysis": "Option D aligns with the requirement as it utilizes customer-managed encryption keys, while options A, B, and C either do not allow proper management or are misconfigured.",
        "additional_knowledge": "In scenarios where data security is paramount, shifting to KMS is essential for compliance.",
        "key_terminology": "SSE-S3, SSE-KMS, AES-256, encryption, customer-managed keys",
        "overall_assessment": "Option D is the most appropriate solution, while community votes indicate some confusion between options B and D regarding managed keys."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは D です。これにより、すべてのオブジェクトが顧客管理キーを使用して暗号化され、セキュリティチームの要件を満たします。",
        "situation_analysis": "会社は、新たな規制要件に従って、PII データを安全に保護する必要があります。この要件では、暗号化に管理されたキーを使用することが義務付けられています。",
        "option_analysis": "選択肢 D は顧客管理の暗号化キーを使用して要件に合致しており、選択肢 A、B、および C は適切な管理を行えないか、設定が誤っています。",
        "additional_knowledge": "データのセキュリティが重要なシナリオでは、KMS への移行はコンプライアンスにとって不可欠です。",
        "key_terminology": "SSE-S3, SSE-KMS, AES-256, 暗号化, 顧客管理キー",
        "overall_assessment": "選択肢 D が最も適切な解決策であり、コミュニティの投票はオプション B と D の間で混乱を示しています。"
      }
    ],
    "keywords": [
      "SSE-S3",
      "SSE-KMS",
      "AES-256",
      "encryption",
      "customer-managed keys"
    ]
  },
  {
    "No": "52",
    "question": "A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2\ninstances. The EC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).\nThe company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an\norigin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.\nA solutions architect must configure the application so that itis highly available and fault tolerant.\nWhich solution meets these requirements?",
    "question_jp": "企業はAWSクラウドでWebアプリケーションを運用しています。このアプリケーションは、一連のAmazon EC2インスタンスで生成された動的コンテンツで構成されています。EC2インスタンスは、アプリケーションロードバランサ（ALB）のターゲットグループとして構成されたAuto Scalingグループ内で実行されています。企業は、Amazon CloudFrontディストリビューションを使用してアプリケーションをグローバルに配信しています。CloudFrontディストリビューションは、ALBをオリジンとして使用しています。企業はAmazon Route 53をDNSとして使用し、CloudFrontディストリビューションのためのAレコード www.example.com を作成しています。ソリューションアーキテクトは、アプリケーションを高可用性かつフォールトトレラントに構成しなければなりません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Provision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add",
        "text_jp": "異なるAWSリージョンにフルのセカンダリアプリケーション展開を提供します。Route 53のAレコードをフェイルオーバーレコードに更新します。追加する"
      },
      {
        "key": "B",
        "text": "Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a",
        "text_jp": "異なるAWSリージョンにALB、Auto Scalingグループ、EC2インスタンスを提供します。CloudFrontディストリビューションを更新し、新しい"
      },
      {
        "key": "C",
        "text": "Provision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the",
        "text_jp": "異なるAWSリージョンにAuto ScalingグループとEC2インスタンスを提供します。新しいAuto Scalingグループのためのセカンドターゲットを作成します"
      },
      {
        "key": "D",
        "text": "Provision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new",
        "text_jp": "異なるAWSリージョンにフルのセカンダリアプリケーション展開を提供します。二つ目のCloudFrontディストリビューションを作成し、新しい"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This option ensures availability and fault tolerance by setting up a separate application environment in a different region.",
        "situation_analysis": "The application runs on EC2 instances in an Auto Scaling group and serves dynamic content through CloudFront. It needs to be highly available across multiple regions.",
        "option_analysis": "Option B establishes a complete environment in another region, ensuring redundancy. Options A and D lack a CloudFront update which would be crucial for global distribution. Option C falls short as it only provides an Auto Scaling group without a complete ALB setup.",
        "additional_knowledge": "",
        "key_terminology": "Auto Scaling, ELB, Route 53, CloudFront, High Availability, Fault Tolerance",
        "overall_assessment": "Option B is the best solution as it addresses both high availability and fault tolerance effectively. The community support for this option (100%) indicates strong consensus."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。このオプションは、異なるリージョンに別のアプリケーション環境を設置することで可用性とフォールトトレラントを確保します。",
        "situation_analysis": "アプリケーションは、Auto Scalingグループ内のEC2インスタンスで実行され、CloudFrontを通じて動的コンテンツを提供しています。複数リージョンにわたる高可用性が必要です。",
        "option_analysis": "Bオプションは、他のリージョンに完全な環境を構築し、冗長性を確保します。AおよびDオプションは、CloudFrontの更新が必要なため不十分です。Cオプションは、ALBの設定なしでAuto Scalingグループのみを提供するため不十分です。",
        "additional_knowledge": "",
        "key_terminology": "Auto Scaling、ELB、Route 53、CloudFront、高可用性、フォールトトレランス",
        "overall_assessment": "Bオプションは、高可用性とフォールトトレラントの両方に効果的に対処しているため、最良の解決策です。このオプションに対するコミュニティの支持（100%）は強い合意を示しています。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "ALB",
      "Route 53",
      "CloudFront",
      "High Availability",
      "Fault Tolerance"
    ]
  },
  {
    "No": "53",
    "question": "A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a\ntransit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured\nbetween all of the company's global ofices and the transit account. The company has AWS Config enabled on all of its accounts.\nThe company's networking team needs to centrally manage a list of internal IP address ranges that belong to the global ofices. Developers will\nreference this list to gain access to their applications securely.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "question_jp": "ある会社には、多数のAWSアカウントを持つAWS Organizationsがあります。その中の1つのAWSアカウントはトランジットアカウントとして指定され、すべての他のAWSアカウントと共有されているトランジットゲートウェイを持っています。AWS Site-to-Site VPN接続は、会社のすべてのグローバルオフィスとトランジットアカウントの間に構成されています。会社はすべてのアカウントでAWS Configを有効にしています。会社のネットワーキングチームは、グローバルオフィスに属する内部IPアドレス範囲のリストを中央管理する必要があります。開発者は、このリストを参照してアプリケーションに安全にアクセスします。どのソリューションが最小限の運用負荷でこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification",
        "text_jp": "内部IPアドレス範囲をリストしたJSONファイルをAmazon S3にホストし、それを通知するAmazon Simple Notificationを設定します。"
      },
      {
        "key": "B",
        "text": "Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each",
        "text_jp": "すべての内部IPアドレス範囲を含む新しいAWS Config管理ルールを作成します。このルールを使用して、各セキュリティグループをチェックします。"
      },
      {
        "key": "C",
        "text": "In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the",
        "text_jp": "トランジットアカウントで、すべての内部IPアドレス範囲を含むVPCプレフィックスリストを作成します。AWSリソースアクセスマネージャーを使用して、これを共有します。"
      },
      {
        "key": "D",
        "text": "In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts",
        "text_jp": "トランジットアカウントで、すべての内部IPアドレス範囲を含むセキュリティグループを作成します。他のアカウントのセキュリティグループを設定します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating a VPC prefix list in the transit account allows for centralized management of IP address ranges.",
        "situation_analysis": "The company requires a feasible method for the networking team to manage internal IP address ranges easily while ensuring developers can access it securely.",
        "option_analysis": "Option C directly addresses the requirement for centralized management with minimal operational overhead, while other options involve more complex management or reliance on additional services.",
        "additional_knowledge": "Using a prefix list ensures that any changes to the IP address ranges automatically propagate to security groups and routes that reference the prefix list.",
        "key_terminology": "VPC Prefix List, AWS Resource Access Manager, IP address management",
        "overall_assessment": "Option C is the most efficient solution that reduces operational overhead compared to the others, making it the best choice in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCである。トランジットアカウントにVPCプレフィックスリストを作成することで、IPアドレス範囲の中央管理が可能となる。",
        "situation_analysis": "会社は、ネットワーキングチームが内部IPアドレス範囲を簡単に管理できる方法を必要としており、開発者が安全にアクセスできることを確保する必要がある。",
        "option_analysis": "選択肢Cは、操作負荷を最小限に抑えながら中央管理の要件に直接対応しているが、他の選択肢はより複雑な管理や追加サービスへの依存がある。",
        "additional_knowledge": "プレフィックスリストを使用することで、IPアドレス範囲の変更が自動的にプレフィックスリストを参照するセキュリティグループやルートに伝播される。",
        "key_terminology": "VPCプレフィックスリスト、AWSリソースアクセスマネージャー、IPアドレス管理",
        "overall_assessment": "選択肢Cは、他の選択肢に比べて操作負荷を減らす最も効率的なソリューションであり、このシナリオにおいて最良の選択となる。"
      }
    ],
    "keywords": [
      "VPC Prefix List",
      "AWS Resource Access Manager",
      "IP address management"
    ]
  },
  {
    "No": "54",
    "question": "A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and\nuses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API\nmethod.\nThe company wants to create a CSV report every 2 weeks to show each API Lambda function's recommended configured memory, recommended\ncost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.\nWhich solution will meet these requirements with the LEAST development time?",
    "question_jp": "ある企業がAmazon S3で静的ウェブサイトとして新しいアプリケーションを運営しています。企業はプロダクションAWSアカウントにアプリケーションをデプロイし、ウェブサイトを配信するためにAmazon CloudFrontを使用しています。このウェブサイトは、Amazon API Gateway REST APIを呼び出し、各APIメソッドをバックするAWS Lambda関数があります。企業は、各API Lambda関数の推奨される設定メモリ、推奨コスト、および現在の設定と推奨の間の価格差を示すCSVレポートを2週間ごとに作成したいと考えています。企業はレポートをS3バケットに保存します。この要件を最も少ない開発時間で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period.",
        "text_jp": "Amazon CloudWatch Logsから各API Lambda関数のメトリクスデータを抽出するLambda関数を作成します。"
      },
      {
        "key": "B",
        "text": "Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the",
        "text_jp": "AWS Compute Optimizerにオプトインします。ExportLambdaFunctionRecommendations操作を呼び出すLambda関数を作成します。"
      },
      {
        "key": "C",
        "text": "Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export",
        "text_jp": "AWS Compute Optimizerにオプトインします。拡張インフラストラクチャメトリクスを設定します。Compute Optimizerコンソール内でレポートをエクスポートするジョブをスケジュールします。"
      },
      {
        "key": "D",
        "text": "Purchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In",
        "text_jp": "プロダクションアカウントのAWSビジネスサポートプランを購入します。AWS Trusted AdvisorチェックのためにAWS Compute Optimizerにオプトインします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. By opting into AWS Compute Optimizer and using the ExportLambdaFunctionRecommendations operation in a Lambda function, the company can easily get recommended memory configurations and cost analysis for their API Lambda functions.",
        "situation_analysis": "The company needs a solution that delivers recommendations for Lambda configurations with minimal development effort while leveraging existing AWS services.",
        "option_analysis": "Option B is the most efficient because it utilizes AWS Compute Optimizer’s functionality designed specifically for this situation. Option A requires extracting data manually, which increases development time. Option C also necessitates additional metric configurations, and Option D adds cost without direct benefit for this requirement.",
        "additional_knowledge": "Opting into AWS Compute Optimizer also provides ongoing recommendations for all the supported services, thus offering continuous optimization opportunities.",
        "key_terminology": "AWS Lambda, AWS Compute Optimizer, Amazon API Gateway, CSV report, CloudWatch Logs.",
        "overall_assessment": "The solution in option B is streamlined, leveraging an existing AWS service reducing manual workload, which aligns well with the requirements outlined in the question. Community support for this option also indicates its validity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。AWS Compute Optimizerにオプトインし、Lambda関数内でExportLambdaFunctionRecommendations操作を利用することで、企業はAPI Lambda関数の推奨メモリ構成とコスト分析を簡単に取得できます。",
        "situation_analysis": "企業は、最小限の開発労力でLambda構成に関する推奨事項を提供するソリューションが必要です。既存のAWSサービスを活用することが求められています。",
        "option_analysis": "選択肢Bが最も効率的である理由は、特にこの状況に適したAWS Compute Optimizerの機能を利用するからです。選択肢Aは手動でデータを抽出する必要があり、開発時間が増加します。選択肢Cも追加のメトリクス構成が必要であり、選択肢Dは直接的な利点なしにコストを追加します。",
        "additional_knowledge": "AWS Compute Optimizerにオプトインすると、サポートされているすべてのサービスについて継続的な推奨も提供され、最適化の機会を常に得られます。",
        "key_terminology": "AWS Lambda, AWS Compute Optimizer, Amazon API Gateway, CSVレポート, CloudWatch Logs.",
        "overall_assessment": "選択肢Bのソリューションは、既存のAWSサービスを活用し、手作業の負荷を軽減するため、問題に記載された要件と良く一致しています。この選択肢に対するコミュニティの支持も、その妥当性を示しています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "AWS Compute Optimizer",
      "Amazon API Gateway",
      "CSV report",
      "CloudWatch Logs"
    ]
  },
  {
    "No": "55",
    "question": "A company's factory and automation applications are running in a single VPC. More than 20 applications run on a combination of Amazon EC2,\nAmazon Elastic Container Service (Amazon ECS), and Amazon RDS.\nThe company has software engineers spread across three teams. One of the three teams owns each application, and each time is responsible for\nthe cost and performance of all of its applications. Team resources have tags that represent their application and team. The teams use IAM\naccess for daily activities.\nThe company needs to determine which costs on the monthly AWS bill are attributable to each application or team. The company also must be\nable to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must\nrecommend an AWS Billing and Cost Management solution that provides these cost reports.\nWhich combination of actions will meet these requirements? (Choose three.)",
    "question_jp": "ある企業の工場と自動化アプリケーションが単一のVPC内で稼働している。20以上のアプリケーションがAmazon EC2、Amazon Elastic Container Service (Amazon ECS)、およびAmazon RDSの組み合わせで運営されている。この企業には3つのチームに分かれたソフトウェアエンジニアがいる。3つのチームのうちの1つが各アプリケーションを所有し、各チームは自分のアプリケーションのコストとパフォーマンスに責任を持っている。チームのリソースにはそれぞれのアプリケーションとチームを表すタグが付けられている。チームは、日々の業務でIAMアクセスを使用している。この企業は、AWSの月次請求書において、どのコストが各アプリケーションまたはチームに起因するかを特定する必要がある。また、過去12か月のコストを比較し、次の12か月のコストを予測するためのレポートを作成できる必要がある。ソリューションアーキテクトは、これらのコストレポートを提供するAWS Billing and Cost Managementソリューションを推奨しなければならない。どのアクションの組み合わせがこれらの要件を満たすか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Activate the user-define cost allocation tags that represent the application and the team.",
        "text_jp": "アプリケーションとチームを表すユーザー定義コスト配分タグを有効にする。"
      },
      {
        "key": "B",
        "text": "Activate the AWS generated cost allocation tags that represent the application and the team.",
        "text_jp": "アプリケーションとチームを表すAWS生成コスト配分タグを有効にする。"
      },
      {
        "key": "C",
        "text": "Create a cost category for each application in Billing and Cost Management.",
        "text_jp": "Billing and Cost Managementにおいて各アプリケーションのコストカテゴリーを作成する。"
      },
      {
        "key": "D",
        "text": "Activate IAM access to Billing and Cost Management.",
        "text_jp": "Billing and Cost ManagementへのIAMアクセスを有効にする。"
      },
      {
        "key": "E",
        "text": "Create a cost budget.",
        "text_jp": "コスト予算を作成する。"
      },
      {
        "key": "F",
        "text": "Enable Cost Explorer.",
        "text_jp": "Cost Explorerを有効にする。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACF (58%) ADF (42%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer involves activating user-defined cost allocation tags, which allows the company to correctly identify and attribute costs to applications and teams.",
        "situation_analysis": "The company has multiple applications and teams that need cost tracking and reporting capabilities to ensure accountability for expenses and performance.",
        "option_analysis": "Option A is essential for tagging for accurate cost attribution. Option B does not provide the needed granularity of control. Option C is beneficial but not sufficient alone. Option D relates to access but does not entail cost reporting. Option E is necessary for budget management but not directly related to cost attribution. Option F enables advanced reporting features but does not directly ensure cost attribution.",
        "additional_knowledge": "Implementing a robust tagging strategy is crucial for financial transparency and accountability within cloud environments.",
        "key_terminology": "Cost Allocation Tags, Cost Explorer, AWS Billing and Cost Management",
        "overall_assessment": "The community vote leans towards combining A, C, and F for a comprehensive solution. A strong recommendation is made towards using user-defined tags to ensure cost accountability."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は、アプリケーションとチームにコストを正確に識別し、割り当てることを可能にするユーザー定義コスト配分タグを有効にすることである。",
        "situation_analysis": "企業には、支出とパフォーマンスに対する責任を確保するために、コスト追跡とレポート機能が必要な複数のアプリケーションとチームが存在する。",
        "option_analysis": "選択肢Aは、正確なコスト割当てのためにタグ付けが必須である。選択肢Bは、必要な詳細度を提供しない。選択肢Cは有益であるが、単独では不十分である。選択肢Dはアクセスに関するものであり、コストレポートには関与しない。選択肢Eは予算管理に必要であるが、コスト割当てとは直接関連しない。選択肢Fは高度なレポート機能を有効にするが、コスト割当てを直接保証するものではない。",
        "additional_knowledge": "クラウド環境内での財務透明性と責任感を確保するためには、堅固なタグ付け戦略を実施することが重要である。",
        "key_terminology": "コスト配分タグ、Cost Explorer、AWS Billing and Cost Management",
        "overall_assessment": "コミュニティの投票は、A、C、Fの組み合わせが包括的なソリューションとして支持されている。ユーザー定義のタグを使用してコストの責任を確実にすることを強く推奨する。"
      }
    ],
    "keywords": [
      "Cost Allocation Tags",
      "Cost Explorer",
      "AWS Billing and Cost Management"
    ]
  },
  {
    "No": "56",
    "question": "An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall.\nThe third party accepts only one public CIDR block in each client's allow list.\nThe customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind\nan Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT\ngateways provide internet access to the private subnets.\nHow should a solutions architect ensure that the web application can continue to call the third-party API after the migration?",
    "question_jp": "AWSの顧客は、オンプレミスで実行されるWebアプリケーションを持っています。このWebアプリケーションは、ファイアウォールの背後にあるサードパーティAPIからデータを取得します。サードパーティは、各クライアントの許可リストに対して、1つのパブリックCIDRブロックのみを受け入れます。顧客は、WebアプリケーションをAWSクラウドに移行したいと考えています。アプリケーションは、VPC内のApplication Load Balancer（ALB）の背後にある一連のAmazon EC2インスタンスでホストされます。ALBはパブリックサブネットにあり、EC2インスタンスはプライベートサブネットにあります。NATゲートウェイがプライベートサブネットにインターネットアクセスを提供しています。ソリューションアーキテクトは、移行後にWebアプリケーションがサードパーティAPIを引き続き呼び出すことができるようにするには、どうすれば良いのでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.",
        "text_jp": "顧客所有のパブリックIPアドレスのブロックをVPCに関連付ける。VPCのパブリックサブネットにパブリックIPアドレッシングを有効にする。"
      },
      {
        "key": "B",
        "text": "Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and",
        "text_jp": "顧客所有のパブリックIPアドレスのブロックをAWSアカウントに登録する。アドレスブロックからElastic IPアドレスを作成する。"
      },
      {
        "key": "C",
        "text": "Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB.",
        "text_jp": "顧客所有のIPアドレスのブロックからElastic IPアドレスを作成する。静的Elastic IPアドレスをALBに割り当てる。"
      },
      {
        "key": "D",
        "text": "Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses",
        "text_jp": "顧客所有のパブリックIPアドレスのブロックをAWSアカウントに登録する。AWS Global AcceleratorをセットアップしてElastic IPアドレスを使用する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, as it allows the customer to utilize their own public IP address block for Elastic IP addresses they will create, ensuring that the external API recognizes the requests coming from a whitelisted IP.",
        "situation_analysis": "The application needs to maintain the ability to access a third-party API that requires whitelisting a public CIDR block. The migration involves leveraging AWS services while adhering to these constraints.",
        "option_analysis": "Option A is incorrect as it does not involve public IP blocks being registered in AWS. Option C does not register the block correctly, and Elastic IPs need to come from registered blocks. Option D introduces Global Accelerator unnecessarily.",
        "additional_knowledge": "Elastic IPs can be associated with resources, allowing them to retain fixed public addresses that facilitate uninterrupted connectivity.",
        "key_terminology": "Elastic IP, Public IP, VPC, CIDR Block, NAT Gateway",
        "overall_assessment": "Option B aligns correctly with the requirement, using customer-owned IPs and ensuring API connectivity post-migration. Community support for this answer indicates consensus on its correctness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、これにより顧客はElastic IPアドレスを作成する際に自分のパブリックIPアドレスブロックを利用でき、サードパーティAPIがホワイトリストに登録されたIPからのリクエストを認識できるようになります。",
        "situation_analysis": "アプリケーションは、パブリックCIDRブロックをホワイトリストに必要とするサードパーティAPIへのアクセス能力を維持する必要があります。移行では、これらの制約を遵守しながらAWSサービスを活用します。",
        "option_analysis": "選択肢Aは、AWSにパブリックIPブロックが登録されていないため不正解です。選択肢Cは、ブロックが適切に登録されておらず、Elastic IPは登録されたブロックから作成する必要があります。選択肢Dは、Global Acceleratorを不要に導入しています。",
        "additional_knowledge": "Elastic IPはリソースと関連付けることができ、固定のパブリックアドレスを維持させることで、途切れのない接続を可能にします。",
        "key_terminology": "Elastic IP, パブリックIP, VPC, CIDRブロック, NATゲートウェイ",
        "overall_assessment": "選択肢Bは、顧客が所有するIPを使用し、移行後にAPI接続を確保するという要件に正しく一致します。この答えに対するコミュニティの支持は、その正確性を示しています。"
      }
    ],
    "keywords": [
      "Elastic IP",
      "Public IP",
      "VPC",
      "CIDR Block",
      "NAT Gateway"
    ]
  },
  {
    "No": "57",
    "question": "A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An administrator created the following\nSCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:\nDevelopers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the administrator address this\nproblem?",
    "question_jp": "複数の AWS アカウントを持つ企業が AWS Organizations とサービスコントロールポリシー (SCP) を使用しています。管理者は次の SCP を作成し、AWS アカウント 1111-1111-1111 を含む組織単位 (OU) にアタッチしました: アカウント 1111-1111-1111 で作業している開発者は、Amazon S3 バケットを作成できないと不満を言っています。管理者はこの問題にどのように対処すべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Add s3:CreateBucket with “Allow” effect to the SCP. [image_34_0]",
        "text_jp": "SCP に s3:CreateBucket を「Allow」効果で追加する。 [image_34_0]"
      },
      {
        "key": "B",
        "text": "Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.",
        "text_jp": "アカウントを OU から削除し、SCP をアカウント 1111-1111-1111 に直接アタッチする。"
      },
      {
        "key": "C",
        "text": "Instruct the developers to add Amazon S3 permissions to their IAM entities.",
        "text_jp": "開発者に IAM エンティティに Amazon S3 の権限を追加するよう指示する。"
      },
      {
        "key": "D",
        "text": "Remove the SCP from account 1111-1111-1111.",
        "text_jp": "アカウント 1111-1111-1111 から SCP を削除する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (84%) A (16%)",
    "page_images": [
      "image_34_0.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. The developers need to have the appropriate IAM permissions enabled to create S3 buckets. SCPs do not grant permissions; they only define the maximum boundary of permissions.",
        "situation_analysis": "The situation involves AWS Organizations where SCPs are being used to control permissions at the organizational unit level.",
        "option_analysis": "Option A is incorrect because adding s3:CreateBucket to the SCP would still require the IAM users to have necessary permissions. Option B is unnecessary since SCPs are meant for organizational management, not direct account permissions. Option D would entirely remove the constraints set by the SCP, likely not a desired solution.",
        "additional_knowledge": "AWS best practices suggest clearly separating IAM policies and SCPs for effective permissions management.",
        "key_terminology": "AWS Organizations, Service Control Policies, IAM permissions, Amazon S3, SCP.",
        "overall_assessment": "The question effectively tests the understanding of SCPs and IAM permissions in an AWS account management scenario. The community votes reflect a correct understanding of the topic, with Option C receiving the most votes."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答は C です。開発者は S3 バケットを作成するために適切な IAM 権限が必要です。SCP は権限を付与するものではなく、権限の最大限の境界を定義するものです。",
        "situation_analysis": "この状況は、AWS Organizations で SCP を使用して、組織単位レベルでの権限を管理するものです。",
        "option_analysis": "選択肢 A は誤りです。s3:CreateBucket を SCP に追加しても、IAM ユーザーに必要な権限がない限り、バケットを作成することはできません。選択肢 B は不必要です。SCP は組織の管理を目的としているため、アカウント権限の直接付与には使用しません。選択肢 D は SCP による制約全体を削除することになるため、望ましい解決策ではないでしょう。",
        "additional_knowledge": "AWS のベストプラクティスでは、権限管理を効率的に行うために、IAM ポリシーと SCP を明確に分けることが推奨されています。",
        "key_terminology": "AWS Organizations、サービスコントロールポリシー、IAM 権限、Amazon S3、SCP。",
        "overall_assessment": "この質問は、AWS アカウント管理シナリオにおける SCP と IAM 権限の理解を効果的に試すものである。コミュニティの投票はトピックに対する正しい理解を反映しており、選択肢 C が最も多くの票を得ている。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Service Control Policies",
      "IAM permissions",
      "Amazon S3",
      "SCP"
    ]
  },
  {
    "No": "58",
    "question": "A company has a monolithic application that is critical to the company's business. The company hosts the application on an Amazon EC2\ninstance that runs Amazon Linux 2. The company's application team receives a directive from the legal department to back up the data from the\ninstance's encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the\nadministrative SSH key pair for the instance. The application must continue to serve the users.\nWhich solution will meet these requirements?",
    "question_jp": "企業は、企業のビジネスにとって重要なモノリシックアプリケーションを持っています。企業は、Amazon Linux 2を実行しているAmazon EC2インスタンス上でアプリケーションをホストしています。企業のアプリケーションチームは、法務部門からの指示を受けて、インスタンスの暗号化されたAmazon Elastic Block Store (Amazon EBS) ボリュームからデータをAmazon S3バケットにバックアップする必要があります。アプリケーションチームは、インスタンスの管理SSHキーペアを持っていません。アプリケーションは引き続きユーザーにサービスを提供する必要があります。どの解決策がこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain [image_34_0]",
        "text_jp": "インスタンスにAmazon S3への書き込み権限を持つロールをアタッチします。AWS Systems Manager Session Managerオプションを使用してアクセスします。"
      },
      {
        "key": "B",
        "text": "Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new",
        "text_jp": "再起動オプションをオンにしてインスタンスのイメージを作成します。イメージから新しいEC2インスタンスを起動します。新しいインスタンスにロールをアタッチします。"
      },
      {
        "key": "C",
        "text": "Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.",
        "text_jp": "Amazon Data Lifecycle Manager (Amazon DLM) を使用して EBS ボリュームのスナップショットを取得します。データをAmazon S3にコピーします。"
      },
      {
        "key": "D",
        "text": "Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to",
        "text_jp": "インスタンスのイメージを作成します。イメージから新しいEC2インスタンスを起動します。新しいインスタンスに書き込み権限を持つロールをアタッチします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (63%) C (36%)",
    "page_images": [
      "image_34_0.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This solution allows the application team to access the EC2 instance without needing SSH access, which is not possible in this scenario. By attaching an IAM role to the instance that has write permissions to S3, they can use AWS Systems Manager Session Manager to manage the instance securely.",
        "situation_analysis": "The company needs to back up data from an encrypted EBS volume to an S3 bucket while ensuring that the application remains operational. The lack of SSH access prevents conventional direct access.",
        "option_analysis": "Option A is valid as it allows management without SSH. Option B requires access to the instance, which is not available. Option C provides a way to back up but does not ensure ongoing access for the application. Option D also requires instance access, making it impractical.",
        "additional_knowledge": "Having a robust backup mechanism in place for critical applications is essential to ensure business continuity.",
        "key_terminology": "Amazon EC2, Amazon EBS, Amazon S3, AWS Systems Manager, IAM Role.",
        "overall_assessment": "Considering that community votes are strongly in favor of Option A, it not only meets the requirements but also aligns with AWS best practices for managing access and maintaining application availability."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。この解決策により、アプリケーションチームはSSHアクセスなしでEC2インスタンスにアクセスでき、これはこのシナリオでは不可能です。S3への書き込み権限を持つIAMロールをインスタンスにアタッチすることで、AWS Systems Manager Session Managerを使用してインスタンスを安全に管理できます。",
        "situation_analysis": "企業は、暗号化されたEBSボリュームからS3バケットにデータをバックアップする必要があり、アプリケーションが稼働したままであることを確認する必要があります。SSHアクセスがないため、従来の直接アクセスが不可能です。",
        "option_analysis": "選択肢AはSSHなしでの管理を可能にするため、有効です。選択肢Bはインスタンスへのアクセスを必要としますが、これは利用できません。選択肢Cはバックアップ方法を提供しますが、アプリケーションへの継続的なアクセスを保証しません。選択肢Dもインスタンスへのアクセスを必要とし、実用的ではありません。",
        "additional_knowledge": "重要なアプリケーションのバックアップメカニズムを確立することは、ビジネスの継続性を確保するために必須です。",
        "key_terminology": "Amazon EC2, Amazon EBS, Amazon S3, AWS Systems Manager, IAMロール。",
        "overall_assessment": "コミュニティの投票が選択肢Aを強く支持していることを考慮すると、要件を満たすだけでなく、アクセス管理とアプリケーションの可用性を維持するためのAWSのベストプラクティスにも合致しています。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Amazon EBS",
      "Amazon S3",
      "AWS Systems Manager",
      "IAM Role"
    ]
  },
  {
    "No": "59",
    "question": "A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucket in a new AWS account. The solutions\narchitect must implement a solution that uses the AWS CLI.\nWhich combination of steps will successfully copy the data? (Choose three.)",
    "question_jp": "ソリューションアーキテクトは、AWSアカウントのAmazon S3バケットから新しいAWSアカウントの新しいS3バケットにデータをコピーする必要があります。ソリューションアーキテクトは、AWS CLIを使用してソリューションを実装する必要があります。データを正常にコピーするための手順の組み合わせはどれですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket.",
        "text_jp": "ソースバケットがその内容をリストし、オブジェクトを置くことと、宛先バケットでオブジェクトのACLを設定することを許可するバケットポリシーを作成する。"
      },
      {
        "key": "B",
        "text": "Create a bucket policy to allow a user in the destination account to list the source bucket's contents and read the source bucket's objects.",
        "text_jp": "宛先アカウントのユーザーがソースバケットの内容をリストし、ソースバケットのオブジェクトを読み取ることを許可するバケットポリシーを作成する。"
      },
      {
        "key": "C",
        "text": "Create an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the",
        "text_jp": "ソースアカウントでIAMポリシーを作成する。ユーザーが内容をリストおよびオブジェクトを取得できるようにポリシーを構成する。"
      },
      {
        "key": "D",
        "text": "Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get",
        "text_jp": "宛先アカウントでIAMポリシーを作成する。ユーザーが内容をリストおよびオブジェクトを取得できるようにポリシーを構成する。"
      },
      {
        "key": "E",
        "text": "Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data.",
        "text_jp": "ソースアカウントのユーザーとしてaws s3 syncコマンドを実行する。データをコピーするためにソースバケットと宛先バケットを指定する。"
      },
      {
        "key": "F",
        "text": "Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data.",
        "text_jp": "宛先アカウントのユーザーとしてaws s3 syncコマンドを実行する。データをコピーするためにソースバケットと宛先バケットを指定する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDF (92%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer for the steps to copy data from one S3 bucket to another across AWS accounts involves implementing proper permissions and executing the commands correctly.",
        "situation_analysis": "The architect needs to ensure that the source S3 bucket can grant access to the destination bucket and that users have the correct permissions to perform copy operations.",
        "option_analysis": "Option A is essential because it allows for proper access permissions to the destination bucket. Option B is helpful but not a direct requirement for copying. Options C and D deal with IAM policies which could be used but aren’t directly needed as A and E suffice. Option E is necessary to execute the data copy operation.",
        "additional_knowledge": "It is essential to review AWS documentation on cross-account access to ensure a correct and secure implementation.",
        "key_terminology": "AWS CLI, S3, bucket policy, IAM policy",
        "overall_assessment": "While the community has voted for option B, the correct sequence starts with securing proper S3 bucket policies, making A a central response. Options should be evaluated based on the permissions they enable."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "データをAWSアカウント間でS3バケットから別のバケットにコピーするための正しい手順は、適切な権限を実装し、コマンドを正しく実行することが含まれます。",
        "situation_analysis": "アーキテクトは、ソースのS3バケットが宛先バケットへのアクセスを許可し、ユーザーがコピー操作を実行するための正しい権限を持っていることを確認する必要があります。",
        "option_analysis": "オプションAは、宛先バケットへの適切なアクセス権を許可するために不可欠です。オプションBは有益ですが、コピーの直接要件ではありません。オプションCとDはIAMポリシーについてですが、AとEが十分なため必須ではありません。オプションEはデータコピー操作を実行するために必要です。",
        "additional_knowledge": "正しい安全な実装を確保するために、クロスアカウントアクセスに関するAWSの文書を確認することが重要です。",
        "key_terminology": "AWS CLI, S3, バケットポリシー, IAMポリシー",
        "overall_assessment": "コミュニティはオプションBに投票しましたが、正しい順序は最初にS3バケットポリシーを確保し、Aが中心的な回答となるべきです。オプションは、有効にする権限に基づいて評価されるべきです。"
      }
    ],
    "keywords": [
      "AWS CLI",
      "S3",
      "bucket policy",
      "IAM policy"
    ]
  },
  {
    "No": "60",
    "question": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web\napplication introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to\nsupport a canary release.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWS Lambdaに基づいて構築したアプリケーションをAWS CloudFormationスタックにデプロイしました。ウェブアプリケーションの最新の本番リリースでは、数分間の停止を引き起こす問題が発生しました。ソリューションアーキテクトはカナリアリリースをサポートするためにデプロイメントプロセスを調整する必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config",
        "text_jp": "新しくデプロイされたLambda関数のバージョンごとにエイリアスを作成する。AWS CLIのupdate-aliasコマンドをrouting-configで使用する"
      },
      {
        "key": "B",
        "text": "Deploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load.",
        "text_jp": "アプリケーションを新しいCloudFormationスタックにデプロイします。Amazon Route 53の加重ルーティングポリシーを使用して負荷を分散します。"
      },
      {
        "key": "C",
        "text": "Create a version for every new deployed Lambda function. Use the AWS CLI update-function-configuration command with the routing-config",
        "text_jp": "新しくデプロイされたLambda関数ごとにバージョンを作成します。AWS CLIのupdate-function-configurationコマンドをrouting-configで使用する"
      },
      {
        "key": "D",
        "text": "Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load.",
        "text_jp": "AWS CodeDeployを設定し、Deployment configurationでCodeDeployDefault.OneAtATimeを使用して負荷を分散する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating an alias for each deployed version allows the organization to implement canary releases easily by routing a small amount of traffic to the new version while monitoring its behavior before scaling up.",
        "situation_analysis": "The application is already deployed on AWS Lambda, and a reliable deployment strategy is needed to minimize impact from future issues during releases.",
        "option_analysis": "Option A is correct because it allows controlled traffic routing to new versions using an alias. Option B involves creating a new stack, which does not effectively support canary releases. Option C lacks the alias feature. Option D uses a deployment style not tailored for canary releases.",
        "additional_knowledge": "Understanding the deployment options in Lambda is crucial for developing resilient applications.",
        "key_terminology": "AWS Lambda, canary release, aliases, AWS CLI, CloudFormation.",
        "overall_assessment": "Option A is the clear best practice for implementing a canary release in AWS Lambda. The community overwhelmingly supports this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。新しくデプロイされた各バージョンのエイリアスを作成することで、カナリアリリースを容易に実装でき、新しいバージョンに少量のトラフィックをルーティングし、その挙動を監視した後に規模を拡大することができます。",
        "situation_analysis": "アプリケーションはすでにAWS Lambda上にデプロイされており、将来のリリース中の問題の影響を最小限に抑えるための信頼できるデプロイ戦略が必要です。",
        "option_analysis": "選択肢Aは新バージョンへの制御されたトラフィックルーティングを可能にするため正解です。選択肢Bは新しいスタックを作成することを伴い、カナリアリリースを効果的にはサポートしません。選択肢Cはエイリアス機能が欠けています。選択肢Dはカナリアリリースに特化していないデプロイスタイルを使用しています。",
        "additional_knowledge": "Lambdaのデプロイオプションを理解することは、回復力のあるアプリケーションを開発するために重要です。",
        "key_terminology": "AWS Lambda、カナリアリリース、エイリアス、AWS CLI、CloudFormation。",
        "overall_assessment": "選択肢AはAWS Lambdaにおけるカナリアリリースの実装において明確で最良の実践です。コミュニティはこの選択を圧倒的に支持しています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "canary release",
      "aliases",
      "AWS CLI",
      "CloudFormation"
    ]
  },
  {
    "No": "61",
    "question": "A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties.\nThe company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the files are uploaded, they are moved to the\ndata lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route\n53.\nWhat should a solutions architect do to improve the reliability and scalability of the SFTP solution?",
    "question_jp": "ある金融会社がAmazon S3にデータレイクをホストしています。この会社は、毎晩、複数の第三者からSFTP経由で財務データレコードを受け取っています。\nこの会社は、自社のSFTPサーバーをVPCのパブリックサブネット内のAmazon EC2インスタンス上で運用しています。ファイルがアップロードされた後、同じインスタンス上で実行されているcronジョブによってデータレイクに移動されます。SFTPサーバーは、Amazon Route 53を使用してDNS sftp.example.comでアクセス可能です。\nソリューションアーキテクトは、SFTPソリューションの信頼性とスケーラビリティを向上させるために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS",
        "text_jp": "EC2インスタンスをAuto Scalingグループに移動させ、EC2インスタンスをアプリケーションロードバランサー（ALB）の背後に置き、DNSを更新する"
      },
      {
        "key": "B",
        "text": "Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint",
        "text_jp": "SFTPサーバーをAWS Transfer for SFTPに移行し、Route 53のDNSレコードsftp.example.comをサーバーエンドポイントを指すように更新する"
      },
      {
        "key": "C",
        "text": "Migrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the file",
        "text_jp": "SFTPサーバーをAWS Storage Gatewayのファイルゲートウェイに移行し、Route 53のDNSレコードsftp.example.comをファイルを指すように更新する"
      },
      {
        "key": "D",
        "text": "Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB.",
        "text_jp": "EC2インスタンスをネットワークロードバランサー（NLB）の背後に置き、Route 53のDNSレコードsftp.example.comをNLBを指すように更新する"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Migrate the SFTP server to AWS Transfer for SFTP.",
        "situation_analysis": "The company needs a reliable, scalable solution for its SFTP server that can handle varying loads and potential deaths of the EC2 instance.",
        "option_analysis": "Option A does not provide a fully managed solution and still relies on the EC2 instance. Option C uses Storage Gateway, which is not designed for SFTP operations directly. Option D improves some aspects but does not address underlying reliability issues.",
        "additional_knowledge": "N/A",
        "key_terminology": "AWS Transfer for SFTP, Amazon S3, fully managed service",
        "overall_assessment": "Option B aligns best with AWS best practices. It improves the current architecture by migrating to a service tailored for SFTP needs which ensures higher availability and ease of management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはB：SFTPサーバーをAWS Transfer for SFTPに移行することです。",
        "situation_analysis": "この会社は、SFTPサーバーの信頼性が高く、スケール可能なソリューションが必要です。これにより、負荷の変動やEC2インスタンスの死に対処できます。",
        "option_analysis": "オプションAは完全に管理されたソリューションを提供せず、依然としてEC2インスタンスに依存します。オプションCは、SFTP操作に直接設計されていないStorage Gatewayを使用しています。オプションDは、いくつかの側面を改善しますが、基本的な信頼性の問題には対処していません。",
        "additional_knowledge": "N/A",
        "key_terminology": "AWS Transfer for SFTP, Amazon S3, 完全管理されたサービス",
        "overall_assessment": "オプションBはAWSのベストプラクティスに最も合致しています。SFTPニーズに特化したサービスに移行することで、現在のアーキテクチャが改善され、可用性の向上と管理の容易さが実現されます。"
      }
    ],
    "keywords": [
      "AWS Transfer for SFTP",
      "Amazon S3",
      "fully managed service"
    ]
  },
  {
    "No": "62",
    "question": "A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions\narchitect must preserve the software and configuration settings during the migration.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "企業は、オンプレミスのデータセンターで実行されているVMwareインフラストラクチャからAmazon EC2にアプリケーションを移行したいと考えています。ソリューションアーキテクトは、移行中にソフトウェアや構成設定を保持する必要があります。ソリューションアーキテクトはこれらの要件を満たすために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the",
        "text_jp": "AWS DataSyncエージェントを設定してデータストアをAmazon FSx for Windows File Serverに複製し、SMB共有をホストします。"
      },
      {
        "key": "B",
        "text": "Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3",
        "text_jp": "VMware vSphereクライアントを使用して、アプリケーションをOpen Virtualization Format (OVF)形式のイメージとしてエクスポートします。Amazon S3を作成します。"
      },
      {
        "key": "C",
        "text": "Configure AWS Storage Gateway for files service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared",
        "text_jp": "ファイルサービス用のAWS Storage Gatewayを設定し、Common Internet File System (CIFS)共有をエクスポートします。共有先にバックアップコピーを作成します。"
      },
      {
        "key": "D",
        "text": "Create a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on",
        "text_jp": "AWS Systems Managerでハイブリッド環境用のマネージドインスタンスのアクティベーションを作成します。Systems Managerエージェントをダウンロードしてインストールします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, which involves creating a managed-instance activation for a hybrid environment in AWS Systems Manager. This enables the installation of the Systems Manager Agent on the EC2 instance, allowing the migration process to maintain software and configuration settings effectively.",
        "situation_analysis": "The requirement is to migrate an application from an on-premises VMware infrastructure to Amazon EC2 while preserving software and configuration settings. This necessitates a solution that integrates with AWS management tools, ensuring seamless migration.",
        "option_analysis": "Option A focuses on data replication and storage but does not address software configuration preservation. Option B mentions exporting an image but lacks direct integration with AWS services required for maintaining configurations. Option C addresses file sharing but does not ensure the application settings are preserved. In contrast, Option D provides the necessary framework for migration through Systems Manager.",
        "additional_knowledge": "The use of AWS Systems Manager is a common practice for hybrid cloud management, enabling organizations to maintain consistency between on-premises and cloud resources.",
        "key_terminology": "AWS Systems Manager,Hybrid Environment,Systems Manager Agent,Configuration Management,Application Migration",
        "overall_assessment": "The question effectively assesses AWS migration strategies. Although there is a discrepancy in community voting, the correct approach aligns with AWS best practices for migrating applications while ensuring configuration fidelity. It is essential to consider options that facilitate both data and configuration preservation during migration."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDで、AWS Systems Managerでハイブリッド環境用のマネージドインスタンスのアクティベーションを作成することです。これにより、EC2インスタンスにSystems Managerエージェントをインストールでき、移行プロセスでソフトウェアと構成設定を効果的に保持できます。",
        "situation_analysis": "要件は、オンプレミスのVMwareインフラストラクチャからAmazon EC2にアプリケーションを移行し、ソフトウェアと構成設定を保持することです。これには、AWS管理ツールと統合されたソリューションが必要であり、シームレスな移行を保証します。",
        "option_analysis": "選択肢Aはデータの複製とストレージに焦点を当てていますが、ソフトウェアの構成保持には対応していません。選択肢Bはイメージのエクスポートに言及しますが、構成維持に必要なAWSサービスとの直接的な統合が欠けています。選択肢Cはファイル共有に対応していますが、アプリケーション設定の保持を保証していません。一方、選択肢Dは、Systems Managerを通じて移行に必要なフレームワークを提供します。",
        "additional_knowledge": "AWS Systems Managerの使用は、ハイブリッドクラウド管理の一般的な慣行であり、組織がオンプレミスとクラウドのリソース間での整合性を維持できるようにします。",
        "key_terminology": "AWS Systems Manager,ハイブリッド環境,Systems Managerエージェント,構成管理,アプリケーション移行",
        "overall_assessment": "この質問はAWSの移行戦略を効果的に評価しています。コミュニティの投票には不一致がありますが、正しいアプローチは、構成の忠実性を保ちながらアプリケーションを移行するためのAWSのベストプラクティスに沿っています。移行中にデータと構成の両方を保持するオプションを考慮することが重要です。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "Hybrid Environment",
      "Systems Manager Agent",
      "Configuration Management",
      "Application Migration"
    ]
  },
  {
    "No": "63",
    "question": "A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed\nimage in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and\nruns by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.\nThe application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing\nfrequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application's\narchitecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "あるビデオ処理会社は、Amazon S3バケットから画像をダウンロードし、画像を処理し、変換した画像を第2のS3バケットに保存し、画像に関するメタデータをAmazon DynamoDBテーブルで更新するアプリケーションを持っている。このアプリケーションはNode.jsで書かれており、AWS Lambda関数を使用して実行されている。Lambda関数は、Amazon S3に新しい画像がアップロードされるたびに呼び出される。このアプリケーションはしばらくの間問題なく動作していた。しかし、画像のサイズが大きくなったため、Lambda関数がタイムアウトエラーで頻繁に失敗するようになった。関数のタイムアウトは、その最大値に設定されている。ソリューションアーキテクトは、呼び出しの失敗を防ぐためにアプリケーションのアーキテクチャをリファクタリングする必要がある。会社は基盤となるインフラストラクチャを管理したくない。これらの要件を満たすために、ソリューションアーキテクトはどの組み合わせの手順を取るべきか？（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic",
        "text_jp": "アプリケーションコードを含むDockerイメージを構築することによってアプリケーションのデプロイを変更する。そのイメージをAmazon Elasticに公開する"
      },
      {
        "key": "B",
        "text": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task",
        "text_jp": "AWS Fargateの互換性タイプを持つ新しいAmazon Elastic Container Service (Amazon ECS) タスク定義を作成する。タスクを構成する"
      },
      {
        "key": "C",
        "text": "Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of",
        "text_jp": "Lambda関数を呼び出すためのParallelステートを持つAWS Step Functionsステートマシンを作成する。プロビジョニングされた同時実行数を増やす"
      },
      {
        "key": "D",
        "text": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task",
        "text_jp": "Amazon EC2の互換性タイプを持つ新しいAmazon Elastic Container Service (Amazon ECS) タスク定義を作成する。タスクを構成する"
      },
      {
        "key": "E",
        "text": "Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB",
        "text_jp": "アプリケーションを修正して、画像をAmazon Elastic File System (Amazon EFS)に保存し、メタデータをAmazon RDS DBに保存する"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "AB (90%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are D and B. Creating an ECS task definition with Amazon EC2 compatibility allows the application to run in a container without Lambda limitations.",
        "situation_analysis": "The current setup is facing timeout issues due to increased image sizes. The company seeks a serverless architecture without managing infrastructure.",
        "option_analysis": "Option D is correct as it allows using Amazon ECS with EC2, providing more resources and control. Option B is also valid as it utilizes AWS Fargate for serverless container management. Options A, C, and E do not directly address the timeout issues effectively.",
        "additional_knowledge": "",
        "key_terminology": "AWS Fargate, Amazon ECS, Docker, EC2, container orchestration",
        "overall_assessment": "The question is relevant for scenario-based problem solving in AWS environments. The community's vote may suggest preferences, but official guidance leans towards D and B for resolving the timeout issue."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDとBである。Amazon EC2との互換性を持つECSタスク定義を作成すると、Lambdaの制限なくコンテナでアプリケーションを実行できるようになる。",
        "situation_analysis": "現在の設定は、画像サイズの増加によりタイムアウトの問題に直面している。会社はインフラを管理せず、サーバーレスアーキテクチャを求めている。",
        "option_analysis": "Dの選択肢は正しい。なぜなら、Amazon ECSをEC2で使用することにより、より多くのリソースと制御を提供するからである。Bの選択肢も有効であり、サーバーレスコンテナ管理のためにAWS Fargateを利用する。A、C、Eの選択肢は、タイムアウト問題に効果的に対処していない。",
        "additional_knowledge": "",
        "key_terminology": "AWS Fargate, Amazon ECS, Docker, EC2, コンテナオーケストレーション",
        "overall_assessment": "この質問は、AWS環境におけるシナリオベースの問題解決に関連している。コミュニティの投票は好みを示唆するかもしれないが、公式ガイダンスはタイムアウト問題を解決するためにDとBに傾いている。"
      }
    ],
    "keywords": [
      "AWS Fargate",
      "Amazon ECS",
      "Docker",
      "EC2",
      "container orchestration"
    ]
  },
  {
    "No": "64",
    "question": "A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization.\nThe company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB\ninstances that are not encrypted at rest in the company's production OU.\nWhich solution will meet this requirement?",
    "question_jp": "ある企業がAWS Organizationsを使用して組織を管理しています。この企業は、AWS Control Towerを利用して、組織のためにランディングゾーンを展開しています。\n企業はガバナンスとポリシーの強制を実施したいと考えています。企業は、プロダクションOU内で暗号化されていないAmazon RDS DBインスタンスを検出するポリシーを実装しなければなりません。\nどのソリューションがこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU.",
        "text_jp": "AWS Control Towerで必須ガードレールをオンにします。必須ガードレールをプロダクションOUに適用します。"
      },
      {
        "key": "B",
        "text": "Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the",
        "text_jp": "AWS Control Towerの強く推奨されるガードレールのリストから適切なガードレールを有効にします。ガードレールをプロダクションOUに適用します。"
      },
      {
        "key": "C",
        "text": "Use AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU.",
        "text_jp": "AWS Configを使用して新しい必須ガードレールを作成します。このルールをプロダクションOUのすべてのアカウントに適用します。"
      },
      {
        "key": "D",
        "text": "Create a custom SCP in AWS Control Tower. Apply the SCP to the production OU.",
        "text_jp": "AWS Control TowerでカスタムSCPを作成します。SCPをプロダクションOUに適用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Enabling and applying the appropriate guardrail will enforce encryption at rest for Amazon RDS databases.",
        "situation_analysis": "The company requires a policy that detects unencrypted Amazon RDS instances in its production OU. Therefore, the solution must provide automated governance.",
        "option_analysis": "Option B directly references AWS Control Tower's strong recommended guardrails, which are designed to enforce good practices such as encryption.",
        "additional_knowledge": "",
        "key_terminology": "AWS Control Tower, guardrails, Amazon RDS, encryption at rest, governance",
        "overall_assessment": "Option B aligns perfectly with AWS best practices for governance as it leverages the built-in capabilities of AWS Control Tower."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。適切なガードレールを有効にし適用することで、Amazon RDSデータベースの暗号化を強制することができる。",
        "situation_analysis": "企業は、プロダクションOU内の暗号化されていないAmazon RDSインスタンスを検出するポリシーを必要としている。そのため、ソリューションは自動的なガバナンスを提供する必要がある。",
        "option_analysis": "選択肢Bは、AWS Control Towerの強く推奨されるガードレールを直接参照しており、暗号化などの良い実践を強制するように設計されている。",
        "additional_knowledge": "",
        "key_terminology": "AWS Control Tower、ガードレール、Amazon RDS、静止データの暗号化、ガバナンス",
        "overall_assessment": "選択肢Bは、AWS Control Towerの組み込み機能を活用しており、ガバナンスのためのAWSのベストプラクティスと完全に一致している。"
      }
    ],
    "keywords": [
      "AWS Control Tower",
      "guardrails",
      "Amazon RDS",
      "encryption at rest",
      "governance"
    ]
  },
  {
    "No": "65",
    "question": "A startup company hosts a fieet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company's engineers rely\nheavily on SSH access to the instances for troubleshooting.\nThe company's existing architecture includes the following:\n• A VPC with private and public subnets, and a NAT gateway.\n• Site-to-Site VPN for connectivity with the on-premises environment.\n• EC2 security groups with direct SSH access from the on-premises environment.\nThe company needs to increase security controls around SSH access and provide auditing of commands run by the engineers.\nWhich strategy should a solutions architect use?",
    "question_jp": "スタートアップ企業は、最新のAmazon Linux 2 AMIを使用してプライベートサブネットにAmazon EC2インスタンスのファイートをホストしています。この企業のエンジニアは、トラブルシューティングのためにインスタンスへのSSHアクセスに大きく依存しています。この企業の既存のアーキテクチャには、以下の要素が含まれています： • プライベートサブネットとパブリックサブネットを持つVPCおよびNATゲートウェイ。 • オンプレミス環境との接続のためのSite-to-Site VPN。 • オンプレミス環境からの直接SSHアクセスを提供するEC2セキュリティグループ。 この企業は、SSHアクセスに関するセキュリティ制御を強化し、エンジニアが実行したコマンドの監査を提供する必要があります。ソリューションアーキテクトはどの戦略を使用すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Install and configure EC2 Instance Connect on the fieet of EC2 instances. Remove all security group rules attached to EC2 instances that",
        "text_jp": "EC2インスタンスのファイートにEC2 Instance Connectをインストールおよび構成します。EC2インスタンスに付属するすべてのセキュリティグループルールを削除します。"
      },
      {
        "key": "B",
        "text": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Install the Amazon",
        "text_jp": "EC2セキュリティグループを更新して、エンジニアのデバイスのIPアドレスに対してのみポート22の着信TCPを許可します。Amazonをインストールします。"
      },
      {
        "key": "C",
        "text": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Enable AWS Config for",
        "text_jp": "EC2セキュリティグループを更新して、エンジニアのデバイスのIPアドレスに対してのみポート22の着信TCPを許可します。AWS Configを有効にします。"
      },
      {
        "key": "D",
        "text": "Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances.",
        "text_jp": "AmazonSSMManagedInstanceCore管理ポリシーがアタッチされたIAMロールを作成します。そのIAMロールをすべてのEC2インスタンスにアタッチします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Creating an IAM role with the AmazonSSMManagedInstanceCore managed policy allows for enhanced security and auditing capabilities.",
        "situation_analysis": "The company requires improved security around SSH access and must track the commands executed by engineers.",
        "option_analysis": "Option D directly addresses security by employing IAM roles and SSM, whereas other options do not adequately enhance security or provide command auditing.",
        "additional_knowledge": "Using SSM can also reduce exposure to potential threats from public IP access.",
        "key_terminology": "IAM roles, Amazon SSM, command auditing, security groups, EC2 instances",
        "overall_assessment": "The majority of the community supports Option D (91%). This validates its acceptance as the most secure and auditable approach for the use case."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。AmazonSSMManagedInstanceCore管理ポリシーを持つIAMロールを作成することで、セキュリティと監査能力を強化できます。",
        "situation_analysis": "企業はSSHアクセスに関するセキュリティを向上させ、エンジニアが実行したコマンドの追跡が必要です。",
        "option_analysis": "オプションDはIAMロールとSSMを用いることで直接的にセキュリティを強化しますが、他の選択肢は十分にセキュリティを強化したり、コマンド監査を提供したりしていません。",
        "additional_knowledge": "SSMを使用することで、パブリックIPアクセスからの潜在的な脅威への露出を減少させることができます。",
        "key_terminology": "IAMロール、Amazon SSM、コマンド監査、セキュリティグループ、EC2インスタンス",
        "overall_assessment": "コミュニティの大多数がオプションD（91％）を支持しています。これは、このソリューションがユースケースに最も安全で監査可能なアプローチとして受け入れられていることを裏付けています。"
      }
    ],
    "keywords": [
      "IAM roles",
      "Amazon SSM",
      "command auditing",
      "security groups",
      "EC2 instances"
    ]
  },
  {
    "No": "66",
    "question": "A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed,\ndevelopers use their company email address to request an account. The company wants to ensure that developers are not launching costly\nservices or running services unnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "AWS Organizationsを使用する企業が、開発者がAWS上で実験できるようにしています。企業が展開したランディングゾーンの一環として、開発者は会社のメールアドレスを使用してアカウントをリクエストします。企業は、開発者がコストのかかるサービスを起動したり、不必要なサービスを運用したりしないようにすることを望んでいます。企業は、AWSのコストを制限するために、開発者に固定の月間予算を与える必要があります。この要件を満たすための手順の組み合わせはどれですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.",
        "text_jp": "固定の月間アカウント使用制限を設定するSCPを作成します。そのSCPを開発者アカウントに適用します。"
      },
      {
        "key": "B",
        "text": "Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process.",
        "text_jp": "アカウント作成プロセスの一環として、各開発者のアカウントに固定の月間予算を作成するためにAWS Budgetsを使用します。"
      },
      {
        "key": "C",
        "text": "Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.",
        "text_jp": "コストのかかるサービスとコンポーネントへのアクセスを拒否するSCPを作成します。そのSCPを開発者アカウントに適用します。"
      },
      {
        "key": "D",
        "text": "Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts.",
        "text_jp": "コストのかかるサービスとコンポーネントへのアクセスを拒否するIAMポリシーを作成します。そのIAMポリシーを開発者アカウントに適用します。"
      },
      {
        "key": "E",
        "text": "Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all",
        "text_jp": "予算額に達したときにサービスを終了するためのAWS Budgetsアラートアクションを作成します。すべてのサービスを終了するようにアクションを構成します。"
      },
      {
        "key": "F",
        "text": "Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount",
        "text_jp": "予算額に達したときにAmazon Simple Notification Service（Amazon SNS）通知を送信するAWS Budgetsアラートアクションを作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BCF (77%) BDF (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct answer includes using AWS Budgets to set a monthly budget for developers, ensuring they do not exceed AWS costs.",
        "situation_analysis": "Developers need controlled access to AWS resources with a fixed budget to prevent unexpected cost increases.",
        "option_analysis": "Option B is correct because it directly addresses setting a budget for each developer's account effectively. Other options may help but do not guarantee budget enforcement.",
        "additional_knowledge": "Understanding of the limitations of SCP and IAM policy within an organization context is crucial.",
        "key_terminology": "AWS Budgets, AWS Organizations, SCP, IAM Policy, cost management",
        "overall_assessment": "The question is well-structured and tests knowledge on AWS budgeting and access control. The answer selected reflects robust cost management practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えには、開発者のためにAWS Budgetsを使用して月間予算を設定し、AWSコストを超えないようにすることが含まれています。",
        "situation_analysis": "開発者は予期しないコストの増加を防ぐために、固定予算でAWSリソースへの制御されたアクセスが必要です。",
        "option_analysis": "選択肢Bが正しいのは、各開発者のアカウントに予算を設定することに直接対処しており、効果的だからです。他の選択肢は役立つかもしれませんが、予算の強制を保証しません。",
        "additional_knowledge": "組織の文脈におけるSCPとIAMポリシーの制限を理解することが重要です。",
        "key_terminology": "AWS Budgets, AWS Organizations, SCP, IAMポリシー, コスト管理",
        "overall_assessment": "この質問はよく構成されており、AWSの予算管理とアクセス制御に関する知識をテストしています。選択された答えは堅牢なコスト管理の実践を反映しています。"
      }
    ],
    "keywords": [
      "AWS Budgets",
      "AWS Organizations",
      "SCP",
      "IAM Policy",
      "cost management"
    ]
  },
  {
    "No": "67",
    "question": "A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the\napplications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions\nby using a deployment package. The company has configured automated backups for Aurora.\nThe company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application\nprocesses critical data, so the company must minimize downtime.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWSアカウント「Source」にアプリケーションを持っています。このアカウントはAWS Organizationsの組織内にあります。アプリケーションの1つはAWS Lambda関数を使用し、Amazon Auroraデータベースに在庫データを保存しています。アプリケーションはデプロイメントパッケージを使用してLambda関数をデプロイしています。企業はAuroraの自動バックアップを構成しています。企業はLambda関数とAuroraデータベースを新しいAWSアカウント「Target」に移行したいと考えています。アプリケーションは重要なデータを処理するため、ダウンタイムを最小限に抑える必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda",
        "text_jp": "SourceアカウントからLambda関数のデプロイメントパッケージをダウンロードし、そのパッケージを使用して新しいLambdaを作成します。"
      },
      {
        "key": "B",
        "text": "Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda",
        "text_jp": "SourceアカウントからLambda関数のデプロイメントパッケージをダウンロードし、そのパッケージを使用して新しいLambdaを作成します。"
      },
      {
        "key": "C",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant",
        "text_jp": "AWS Resource Access Manager (AWS RAM)を使用して、TargetアカウントとLambda関数およびAurora DBクラスターを共有します。"
      },
      {
        "key": "D",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB",
        "text_jp": "AWS Resource Access Manager (AWS RAM)を使用して、Lambda関数をTargetアカウントと共有します。自動化されたAurora DBを共有します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using AWS Resource Access Manager (AWS RAM) to share the Lambda functions and Aurora DB cluster allows the company to minimize downtime by maintaining access to these resources in both accounts.",
        "situation_analysis": "The company needs to migrate critical Lambda functions and an Aurora database with minimal downtime, which indicates a need for a strategy that maintains resource access during migration.",
        "option_analysis": "Option C facilitates the sharing of already deployed services without interruption, while the other options require recreating resources, which can lead to downtime.",
        "additional_knowledge": "Proper implementation of AWS RAM can significantly enhance collaboration and resource management in multi-account setups.",
        "key_terminology": "AWS Resource Access Manager, Lambda, Amazon Aurora, AWS Organizations, resource sharing",
        "overall_assessment": "While community vote heavily favors Option B, it overlooks the necessity of resource availability during migration. Option C fully meets the provided requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。AWS Resource Access Manager (AWS RAM)を使用してLambda関数とAurora DBクラスターを共有することにより、企業は両アカウントでこれらのリソースへのアクセスを維持しながら、ダウンタイムを最小限に抑えることができる。",
        "situation_analysis": "企業は、重要なLambda関数とAuroraデータベースをダウンタイムを最小限に抑えて移行する必要があるため、移行中にリソースへのアクセスを維持する必要があることを示している。",
        "option_analysis": "選択肢Cは、他の選択肢がリソースを再作成する必要があるためダウンタイムを引き起こす可能性があるのに対し、デプロイ済みサービスへのアクセスを中断することなく共有を容易にする。",
        "additional_knowledge": "AWS RAMの適切な実装は、マルチアカウント構成におけるコラボレーションとリソース管理を大幅に向上させることができる。",
        "key_terminology": "AWS Resource Access Manager, Lambda, Amazon Aurora, AWS Organizations, リソース共有",
        "overall_assessment": "コミュニティの投票ではBが大きな支持を得ているが、移行中のリソースの可用性が必要であることを見落としている。選択肢Cは提供された要件を完全に満たしている。"
      }
    ],
    "keywords": [
      "AWS Resource Access Manager",
      "Lambda",
      "Amazon Aurora",
      "AWS Organizations",
      "resource sharing"
    ]
  },
  {
    "No": "68",
    "question": "A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an\nAmazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess\na file that the script has already processed.\nThe company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the\nfile processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term\nmanagement overhead.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業が、データを処理するためにAmazon EC2インスタンス上でPythonスクリプトを実行しています。このスクリプトは10分ごとに実行されます。スクリプトはAmazon S3バケットからファイルを取り込み、これらのファイルを処理します。平均して、スクリプトは各ファイルを処理するのに約5分かかります。スクリプトは既に処理されたファイルを再処理しません。この企業は、Amazon CloudWatchメトリクスを確認したところ、ファイル処理の速度のためにEC2インスタンスが約40％の時間アイドル状態であることに気付きました。企業は、ワークロードを高可用性でスケーラブルにし、長期的な管理オーバーヘッドを削減したいと考えています。どのソリューションがこれらの要件を最もコスト効果的に満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the",
        "text_jp": "データ処理スクリプトをAWS Lambda関数に移行する。S3イベント通知を使用してLambda関数を呼び出し、ファイルを処理する。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create",
        "text_jp": "Amazon Simple Queue Service（Amazon SQS）キューを作成する。Amazon S3がイベント通知をSQSキューに送信するように設定する。"
      },
      {
        "key": "C",
        "text": "Migrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to",
        "text_jp": "データ処理スクリプトをコンテナイメージに移行する。データ処理コンテナをEC2インスタンス上で実行する。コンテナを設定して、"
      },
      {
        "key": "D",
        "text": "Migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.",
        "text_jp": "データ処理スクリプトをAmazon Elastic Container Service（Amazon ECS）上のAWS Fargateで実行されるコンテナイメージに移行する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (80%) D (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Migrating the data processing script to a container image running on Amazon ECS Fargate enables high availability and automatic scaling of the workload while minimizing management overhead.",
        "situation_analysis": "The company encounters idle EC2 instances and inefficiencies with their current processing script. By looking for a solution that is cost-effective, scalable, and manageable, they aim to optimize their data processing tasks.",
        "option_analysis": "Option A may reduce idling but doesn't provide the scalability needed. Option B introduces SQS, which adds a layer of complexity without directly addressing the processing speed limitations. Option C still involves managing EC2 instances, which isn't aligned with the goal of reducing management overhead. Option D with Amazon ECS on Fargate addresses all these concerns effectively.",
        "additional_knowledge": "In this scenario, utilizing containerization on AWS ECS with Fargate is seen as a best practice in cloud environments for workloads that require flexibility and scalability.",
        "key_terminology": "AWS Lambda, Amazon S3, Amazon ECS, AWS Fargate, EC2, containerization.",
        "overall_assessment": "Considering the community vote distribution, while many believe option A is a good approach, it lacks the efficiency and effectiveness offered by option D. The architecture with Fargate is the most aligned with AWS best practices regarding scalability and management efficiency."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDです。データ処理スクリプトをAmazon ECSのAWS Fargateで実行されるコンテナイメージに移行することで、ワークロードの高可用性と自動スケーリングが実現でき、管理オーバーヘッドを最小限に抑えることができます。",
        "situation_analysis": "企業はアイドル状態のEC2インスタンスと現在の処理スクリプトの非効率を認識しています。コスト効果があり、スケーラブルで管理しやすい解決策を模索しており、データ処理タスクの最適化を目指しています。",
        "option_analysis": "オプションAはアイドル状態を減少させる可能性がありますが、必要なスケーラビリティを提供しません。オプションBはSQSを導入し、処理速度の制限に直接対処しません。オプションCはEC2インスタンスの管理が必要であり、長期的な管理オーバーヘッドを減らすという目標には合致しません。オプションDはすべての懸念に効果的に対処します。",
        "additional_knowledge": "このシナリオでは、AWS ECS上のFargateでのコンテナ化を利用することが、柔軟性とスケーラビリティを必要とするワークロードに適したクラウド環境のベストプラクティスと見なされています。",
        "key_terminology": "AWS Lambda、Amazon S3、Amazon ECS、AWS Fargate、EC2、コンテナ化。",
        "overall_assessment": "コミュニティの投票分布を考慮すると、多くの人がオプションAが良いアプローチであると信じていますが、Dの選択肢が提供する効率性と効果には及びません。Fargateによるアーキテクチャは、スケーラビリティと管理効率に関するAWSのベストプラクティスに最も沿っています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon S3",
      "Amazon ECS",
      "AWS Fargate",
      "EC2",
      "containerization"
    ]
  },
  {
    "No": "69",
    "question": "A financial services company in North America plans to release a new online web application to its customers on AWS. The company will launch\nthe application in the us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet\nuser trafic. The company also wants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-\npassive failover.\nWhich solution will meet these requirements?",
    "question_jp": "北米の金融サービス企業は、AWS上で顧客向けに新しいオンラインWebアプリケーションをリリースする計画です。この企業は、us-east-1リージョンでAmazon EC2インスタンスにアプリケーションを立ち上げる予定です。アプリケーションは高可用性であり、ユーザーのトラフィックに応じて動的にスケールする必要があります。また、企業はus-west-1リージョンでアクティブ-パッシブフェイルオーバーを使用した災害復旧環境も実装したいと考えています。\nこれらの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB)",
        "text_jp": "us-east-1にVPCを作成し、us-west-1にVPCを作成します。VPCピアリングを構成します。us-east-1 VPCに、アプリケーションロードバランサー（ALB）を作成します。"
      },
      {
        "key": "B",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across",
        "text_jp": "us-east-1にVPCを作成し、us-west-1にVPCを作成します。us-east-1 VPCに、アプリケーションロードバランサー（ALB）を作成し、跨いで拡張可能です。"
      },
      {
        "key": "C",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across",
        "text_jp": "us-east-1にVPCを作成し、us-west-1にVPCを作成します。us-east-1 VPCに、アプリケーションロードバランサー（ALB）を作成し、跨いで拡張可能です。"
      },
      {
        "key": "D",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB)",
        "text_jp": "us-east-1にVPCを作成し、us-west-1にVPCを作成します。VPCピアリングを構成します。us-east-1 VPCに、アプリケーションロードバランサー（ALB）を作成します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating VPCs in both us-east-1 and us-west-1 allows for geographical redundancy. An Application Load Balancer (ALB) can then manage traffic effectively, supporting high availability and dynamic scaling.",
        "situation_analysis": "The requirements specify the need for high availability and dynamic scalability of the application in us-east-1, along with implementing disaster recovery in us-west-1. This necessitates the creation of separate VPCs along with load balancing capabilities.",
        "option_analysis": "Option C is the only choice that correctly describes creating an ALB in us-east-1, which is necessary for traffic distribution while considering disaster recovery in us-west-1. Options A, B, and D do not correctly implement the requirements for ALB functionality in conjunction with VPC peering.",
        "additional_knowledge": "Consider potential scenarios where database synchronization or configuration management would also need to be addressed when dealing with active-passive setups across different regions.",
        "key_terminology": "VPC, Application Load Balancer, High Availability, Disaster Recovery, Active-Passive Failover",
        "overall_assessment": "This question tests understanding of AWS networking and high availability architecture. It effectively assesses the knowledge required to implement a highly available application with a disaster recovery strategy. Community votes align with the correct answer, reinforcing its validity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。us-east-1とus-west-1にVPCを作成することで、地理的な冗長性を確保できます。アプリケーションロードバランサー（ALB）がトラフィックを効果的に管理し、高可用性と動的スケーリングを支援します。",
        "situation_analysis": "要件は、us-east-1におけるアプリケーションの高可用性と動的スケーリングが必要であることを示しており、さらにus-west-1での災害復旧の実施が求められています。このため、別々のVPCを作成し、ロードバランシング機能を持たせる必要があります。",
        "option_analysis": "選択肢Cは、us-east-1にALBを作成することを正しく記述しており、トラフィック分散の必要性とus-west-1での災害復旧を考慮しています。選択肢A、B、およびDは、VPCピアリングとALBの機能を正しく実装していません。",
        "additional_knowledge": "異なるリージョン間でのアクティブ-パッシブセットアップを扱う際に、データベースの同期や構成管理も対処する必要がある可能性を考慮してください。",
        "key_terminology": "VPC、アプリケーションロードバランサー、高可用性、災害復旧、アクティブ-パッシブフェイルオーバー",
        "overall_assessment": "この質問はAWSネットワーキングおよび高可用性アーキテクチャの理解をテストし、高可用性アプリケーションの実装と災害復旧戦略を評価する知識を効果的に評価しています。コミュニティの投票は正解と一致しており、その有効性を強化しています。"
      }
    ],
    "keywords": [
      "VPC",
      "Application Load Balancer",
      "High Availability",
      "Disaster Recovery",
      "Active-Passive Failover"
    ]
  },
  {
    "No": "70",
    "question": "A company has an environment that has a single AWS account. A solutions architect is reviewing the environment to recommend what the\ncompany could improve specifically in terms of access to the AWS Management Console. The company's IT support workers currently access the\nconsole for administrative tasks, authenticating with named IAM users that have been mapped to their job role.\nThe IT support workers no longer want to maintain both their Active Directory and IAM user accounts. They want to be able to access the console\nby using their existing Active Directory credentials. The solutions architect is using AWS IAM Identity Center (AWS Single Sign-On) to implement\nthis functionality.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "会社は単一のAWSアカウントを持つ環境を持っています。ソリューションアーキテクトがその環境をレビューし、特にAWS Management Consoleへのアクセスに関して会社が改善できる点を推奨しています。現在、ITサポート作業者は管理業務のためにコンソールにアクセスしており、職務に基づいた名前付きIAMユーザーで認証しています。ITサポート作業者は、もはやActive DirectoryとIAMユーザーアカウントの両方を維持したくありません。彼らは、既存のActive Directoryの資格情報を使用してコンソールにアクセスしたいと考えています。ソリューションアーキテクトは、AWS IAM Identity Center（AWS Single Sign-On）を使用してこの機能を実装しています。どのソリューションが最もコスト効率よくこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure a directory in",
        "text_jp": "AWS Organizationsに組織を作成します。OrganizationsでIAM Identity Center機能をオンにします。ディレクトリを作成して設定します。"
      },
      {
        "key": "B",
        "text": "Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure an AD",
        "text_jp": "AWS Organizationsに組織を作成します。OrganizationsでIAM Identity Center機能をオンにします。ADを作成して設定します。"
      },
      {
        "key": "C",
        "text": "Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure a directory in AWS Directory",
        "text_jp": "AWS Organizationsに組織を作成します。組織のすべての機能をオンにします。AWS Directoryでディレクトリを作成して設定します。"
      },
      {
        "key": "D",
        "text": "Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure an AD Connector to connect to",
        "text_jp": "AWS Organizationsに組織を作成します。組織のすべての機能をオンにします。ADコネクタを作成して設定し、接続します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (80%) B (15%)5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This option allows the integration of existing Active Directory credentials with the AWS Management Console using an AD Connector.",
        "situation_analysis": "The company wants to eliminate the management of two sets of credentials (Active Directory and IAM) and desires to streamline access using existing AD accounts.",
        "option_analysis": "Option D offers the most efficient way to connect their existing AD with AWS. Other options (A, B, C) may involve unnecessary complexity or cost with directory configurations.",
        "additional_knowledge": "This solution directly supports AWS best practices for identity management by leveraging existing infrastructure.",
        "key_terminology": "AWS Organizations, IAM Identity Center, AD Connector, Active Directory, Single Sign-On.",
        "overall_assessment": "Option D is the most cost-effective as it enables the use of existing credentials without duplicating user management efforts."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。このオプションは、ADコネクタを使用してAWS Management Consoleへの既存のActive Directoryの資格情報の統合を可能にする。",
        "situation_analysis": "会社は2つの認証情報（Active DirectoryおよびIAM）を管理することを排除したいと考えており、既存のADアカウントを用いてアクセスを簡素化したいと思っている。",
        "option_analysis": "オプションDは、既存のADをAWSと接続する最も効率的な方法を提供する。他のオプション（A、B、C）は、ディレクトリの設定を持ち込むことで不必要な複雑さやコストを伴う可能性がある。",
        "additional_knowledge": "このソリューションは、既存のインフラストラクチャを活用してAWSのアイデンティティ管理に関するベストプラクティスを直接支持する。",
        "key_terminology": "AWS Organizations、IAM Identity Center、ADコネクタ、Active Directory、シングルサインオン。",
        "overall_assessment": "オプションDは、ユーザー管理の手間を重複させることなく、既存の資格情報を使用できるため、最もコスト効率が良い。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "IAM Identity Center",
      "AD Connector",
      "Active Directory",
      "Single Sign-On"
    ]
  },
  {
    "No": "71",
    "question": "A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-\neast-1 Region. The files range in size from 1 GB to 10 GB.\nUsers who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload\nfor these users. A solutions architect must improve the app's performance for these uploads.\nWhich solutions will meet these requirements? (Choose two.)",
    "question_jp": "ビデオストリーミング会社は最近、ビデオ共有のためのモバイルアプリを立ち上げました。そのアプリは、us-east-1リージョンのAmazon S3バケットにさまざまなファイルをアップロードします。ファイルのサイズは1 GBから10 GBまでさまざまです。オーストラリアからアプリにアクセスするユーザーは、長時間かかるアップロードを体験しています。時には、ユーザーのためにファイルが完全にアップロードされないこともあります。ソリューションアーキテクトは、これらのアップロードのパフォーマンスを向上させる必要があります。どのソリューションがこれらの要件を満たしますか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.",
        "text_jp": "S3 Transfer AccelerationをS3バケットで有効にします。アップロードにTransfer Accelerationエンドポイントを使用するようにアプリを構成します。"
      },
      {
        "key": "B",
        "text": "Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3",
        "text_jp": "各リージョンにS3バケットを構成してアップロードを受け取ります。S3クロスリージョンレプリケーションを使用してファイルを配布S3にコピーします。"
      },
      {
        "key": "C",
        "text": "Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region.",
        "text_jp": "Amazon Route 53をセットアップし、レイテンシベースのルーティングを使用してアップロードを最寄りのS3バケットリージョンにルーティングします。"
      },
      {
        "key": "D",
        "text": "Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.",
        "text_jp": "アプリを構成してビデオファイルをチャンクに分割します。マルチパートアップロードを使用してAmazon S3にファイルを転送します。"
      },
      {
        "key": "E",
        "text": "Modify the app to add random prefixes to the files before uploading.",
        "text_jp": "アプリを変更して、アップロード前にファイルにランダムなプレフィックスを追加します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (96%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D. Enabling S3 Transfer Acceleration will reduce the transfer time for users in distant locations, while implementing multipart uploads allows for more efficient handling of large files, improving the reliability of uploads.",
        "situation_analysis": "The app experiences performance issues during uploads from Australia due to latency and the size of files. Users are encountering long upload times and file transfer failures, indicating a need for optimization.",
        "option_analysis": "Option A helps to optimize uploads by using Amazon's edge locations to accelerate transfers. Option B requires managing multiple buckets and is more complex. Option C improves routing but does not directly enhance upload speed. Option D directly addresses large file handling and prevents timeouts from occurring during uploads. Option E does not have a significant impact on upload performance.",
        "additional_knowledge": "S3 buckets can be configured for different regions to help with distribution and redundancy.",
        "key_terminology": "S3 Transfer Acceleration, Multipart Uploads, Amazon CloudFront, Latency, S3 Buckets.",
        "overall_assessment": "This question effectively tests the understanding of how to optimize file uploads in AWS. It challenges the examinee to analyze the best solutions based on the geographical constraints and file sizes involved."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAとDである。S3 Transfer Accelerationを有効にすることで、遠方からのユーザーの転送時間を短縮し、マルチパートアップロードを使用することで大きなファイルの処理をより効率的に行い、アップロードの信頼性を向上させることができる。",
        "situation_analysis": "このアプリは、オーストラリアからのアップロード中にパフォーマンスの問題を抱えている。遅延とファイルサイズが原因で、ユーザーは長時間のアップロードやファイル転送の失敗に直面しており、最適化の必要性が示唆されている。",
        "option_analysis": "選択肢Aは、Amazonのエッジロケーションを利用して転送を加速することで、アップロードを最適化する。選択肢Bでは、複数のバケットを管理する必要があり、複雑さが増す。選択肢Cはルーティングを改善するが、直接的なアップロード速度の向上にはつながらない。選択肢Dは、大きなファイルの処理に直接対処し、アップロード中のタイムアウトを防ぐ。選択肢Eはアップロードのパフォーマンスに対する大きな影響はない。",
        "additional_knowledge": "S3バケットは、配信と冗長性を助けるために異なるリージョンに設定することができる。",
        "key_terminology": "S3 Transfer Acceleration, マルチパートアップロード, Amazon CloudFront, レイテンシ, S3バケット。",
        "overall_assessment": "この質問は、AWSにおけるファイルアップロードの最適化方法を理解しているかを効果的にテストしている。地理的制約や関与するファイルサイズに基づいて、最良の解決策を分析するよう受験者に求めている。"
      }
    ],
    "keywords": [
      "S3 Transfer Acceleration",
      "Multipart Uploads",
      "Amazon CloudFront",
      "Latency",
      "S3 Buckets"
    ]
  },
  {
    "No": "72",
    "question": "An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After a failover test, the application lost the\nconnections to the database and could not re-establish the connections. After a restart of the application, the application re-established the\nconnections.\nA solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.\nWhich solution will meet these requirements?",
    "question_jp": "アプリケーションがus-east-1リージョンのAmazon RDS for MySQL Multi-AZ DBインスタンスを使用しています。フェイルオーバーテストの後、アプリケーションはデータベースへの接続を失い、再接続できませんでした。アプリケーションを再起動した後、アプリケーションは接続を再確立しました。ソリューションアーキテクトは、アプリケーションが再起動を要求せずにデータベースへの接続を再確立できるような解決策を実装する必要があります。この要件を満たす解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Aurora MySQL Serverless v1 DB instance. Migrate the RDS DB instance to the Aurora Serverless v1 DB instance. Update",
        "text_jp": "Amazon Aurora MySQL Serverless v1 DBインスタンスを作成します。RDS DBインスタンスをAurora Serverless v1 DBインスタンスに移行します。接続設定を更新します。"
      },
      {
        "key": "B",
        "text": "Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS",
        "text_jp": "RDSプロキシを作成します。既存のRDSエンドポイントをターゲットとして構成します。アプリケーションの接続設定をRDSにポイントするように更新します。"
      },
      {
        "key": "C",
        "text": "Create a two-node Amazon Aurora MySQL DB cluster. Migrate the RDS DB instance to the Aurora DB cluster. Create an RDS proxy. Configure",
        "text_jp": "二ノードのAmazon Aurora MySQL DBクラスターを作成します。RDS DBインスタンスをAurora DBクラスターに移行します。RDSプロキシを作成します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon S3 bucket. Export the database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Configure Amazon",
        "text_jp": "Amazon S3バケットを作成します。AWS Database Migration Service (AWS DMS)を使用してデータベースをAmazon S3にエクスポートします。Amazonを構成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating an RDS proxy allows the application to re-establish database connections without requiring a restart.",
        "situation_analysis": "The application needs a reliable way to re-establish connections after a failover without needing a restart.",
        "option_analysis": "Option B effectively addresses the requirements by using RDS proxy, while options A, C, and D do not meet the need for seamless connection restoration.",
        "additional_knowledge": "RDS proxies can be configured to point to existing RDS instances and can smooth over interruptions caused by maintenance or failovers.",
        "key_terminology": "Amazon RDS, RDS Proxy, Multi-AZ, failover, connection management.",
        "overall_assessment": "This question evaluates knowledge of AWS RDS and connection management solutions. The answer is supported by community feedback, as 100% favor option B."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。RDSプロキシを作成することで、アプリケーションは再起動を必要とせずにデータベース接続を再確立できる。",
        "situation_analysis": "アプリケーションは、再起動を伴わない信頼性のある方法でフェイルオーバー後に接続を再確立する必要がある。",
        "option_analysis": "選択肢BはRDSプロキシを使用することで要件を効果的に満たしており、選択肢A、C、Dはシームレスな接続復元のニーズに対応していない。",
        "additional_knowledge": "RDSプロキシは既存のRDSインスタンスを指すように設定でき、メンテナンスやフェイルオーバーによって引き起こされる中断をスムーズに処理する。",
        "key_terminology": "Amazon RDS, RDSプロキシ, Multi-AZ, フェイルオーバー, 接続管理。",
        "overall_assessment": "この問題はAWS RDSおよび接続管理ソリューションに関する知識を評価するものである。答えはコミュニティのフィードバックによって裏付けられており、100%が選択肢Bを支持している。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "RDS Proxy",
      "Multi-AZ",
      "failover",
      "connection management"
    ]
  },
  {
    "No": "73",
    "question": "A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution and send data. Each device needs to be able\nto send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "企業はAWSクラウドでソリューションを構築しています。何千ものデバイスがこのソリューションに接続し、データを送信します。各デバイスはMQTTプロトコルを介してリアルタイムでデータを送受信できる必要があります。各デバイスは、ユニークなX.509証明書を使用して認証されなければなりません。どのソリューションが運用負荷を最も軽減しながらこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to",
        "text_jp": "AWS IoT Coreを設定します。各デバイスに対して対応するAmazon MQキューを作成し、証明書をプロビジョニングします。各デバイスを接続します。"
      },
      {
        "key": "B",
        "text": "Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in",
        "text_jp": "Network Load Balancer (NLB)を作成し、AWS Lambdaオーサライザーで設定します。Amazon EC2インスタンス上でMQTTブローカーを実行します。"
      },
      {
        "key": "C",
        "text": "Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT",
        "text_jp": "AWS IoT Coreを設定します。各デバイスに対して対応するAWS IoT Thingを作成し、証明書をプロビジョニングします。各デバイスをAWS IoTに接続します。"
      },
      {
        "key": "D",
        "text": "Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB.",
        "text_jp": "Amazon API Gateway HTTP APIとNetwork Load Balancer (NLB)を設定します。API GatewayとNLBの間に統合を作成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Setting up an Amazon API Gateway HTTP API in conjunction with a Network Load Balancer provides a flexible and scalable solution for managing MQTT device connections with lower operational overhead.",
        "situation_analysis": "The requirement is for thousands of devices to connect and authenticate using unique X.509 certificates while communicating in real-time using the MQTT protocol. Minimizing operational overhead is crucial.",
        "option_analysis": "Option A requires managing an Amazon MQ queue for each device, which can lead to increased operational complexity. Option B involves running an MQTT broker on EC2, which necessitates more management and configuration. Option C is a plausible option but lacks the scalability and flexibility offered by API Gateway in choice D.",
        "additional_knowledge": "Always consider the total cost of ownership and the effort required for maintenance when selecting the right architecture.",
        "key_terminology": "API Gateway, Network Load Balancer, MQTT, X.509, AWS IoT Core",
        "overall_assessment": "While the community mainly voted for C, D actually matches the requirement of minimal operational overhead effectively, considering the scalability and feature set of API Gateway combined with NLB."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。Amazon API Gateway HTTP APIをNetwork Load Balancerと併用することで、運用の負荷を軽減しつつ、MQTTデバイス接続を管理するための柔軟でスケーラブルなソリューションを提供します。",
        "situation_analysis": "要求は、数千のデバイスが接続し、ユニークなX.509証明書を使用して認証しながら、MQTTプロトコルを使用してリアルタイムで通信することです。運用負荷の軽減が重要です。",
        "option_analysis": "オプションAは各デバイスごとにAmazon MQキューを管理する必要があり、運用の複雑性が増す可能性があります。オプションBはEC2上でMQTTブローカーを実行するため、より多くの管理と構成が必要です。オプションCは妥当な選択肢ですが、選択Dで提供されるAPI Gatewayのスケーラビリティと柔軟性に欠けています。",
        "additional_knowledge": "適切なアーキテクチャを選択する際には、所有コストとメンテナンスに必要な労力を常に考慮してください。",
        "key_terminology": "API Gateway、Network Load Balancer、MQTT、X.509、AWS IoT Core",
        "overall_assessment": "コミュニティの投票は主にCに集中していますが、Dは効果的に運用負荷を最小限に抑えられる要件に合致しています。API GatewayとNLBの統合の利点を考慮すべきです。"
      }
    ],
    "keywords": [
      "API Gateway",
      "Network Load Balancer",
      "MQTT",
      "X.509",
      "AWS IoT Core"
    ]
  },
  {
    "No": "74",
    "question": "A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved\nresources and that engineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to\nenforce the new restriction on the IAM role that the engineers use for access.\nWhat should the solutions architect do to create the solution?",
    "question_jp": "ある企業が単一のAWSアカウントでいくつかのワークロードを実行しています。新しい企業方針により、エンジニアは承認されたリソースのみをプロビジョニングでき、かつ、エンジニアはAWS CloudFormationを使用してこれらのリソースをプロビジョニングしなければならなくなりました。ソリューションアーキテクトは、エンジニアがアクセスするために使用するIAMロールに対して、新しい制限を強制するソリューションを作成する必要があります。ソリューションアーキテクトは、どのようにしてこのソリューションを作成すべきでしょうか。",
    "choices": [
      {
        "key": "A",
        "text": "Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket. Update the IAM policy for the engineers'",
        "text_jp": "承認されたリソースを含むAWS CloudFormationテンプレートをAmazon S3バケットにアップロードします。エンジニアのIAMポリシーを更新します。"
      },
      {
        "key": "B",
        "text": "Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS",
        "text_jp": "エンジニアのIAMロールのIAMポリシーを更新し、承認されたリソースおよびAWSのプロビジョニングのみを許可する権限を付与します。"
      },
      {
        "key": "C",
        "text": "Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy",
        "text_jp": "エンジニアのIAMロールのIAMポリシーを更新し、AWS CloudFormationアクションのみを許可する権限を付与します。新しいIAMポリシーを作成します。"
      },
      {
        "key": "D",
        "text": "Provision resources in AWS CloudFormation stacks. Update the IAM policy for the engineers' IAM role to only allow access to their own",
        "text_jp": "AWS CloudFormationスタックでリソースをプロビジョニングします。エンジニアのIAMロールのIAMポリシーを更新し、自分のスタックへのアクセスのみを許可します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "答えはBです。エンジニアのIAMロールのIAMポリシーを更新し、承認されたリソースのみをプロビジョニングできるようにします。",
        "situation_analysis": "企業は新しいポリシーを導入し、エンジニアが許可されたリソースのみをAWS CloudFormationを使用してプロビジョニングする必要があります。",
        "option_analysis": "選択肢Bは、エンジニアのIAMロールに制限を設け、許可されたリソースのみを許可するための最も効果的な方法です。他の選択肢は、十分な制限を提供しません。",
        "additional_knowledge": "IAMポリシーにおいて、条件を利用してさらに細かい制御が可能です。",
        "key_terminology": "IAM, CloudFormation, provisioning, policy, approved resources",
        "overall_assessment": "選択肢Bは明確かつ直接的にポリシーに従ったアプローチであり、他の選択肢よりも適切です。コミュニティの投票結果がCに偏っているが、正しい選択肢はBであることを理解することが重要です。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "答えはBです。エンジニアのIAMロールのIAMポリシーを更新し、承認されたリソースのみをプロビジョニングできるようにします。",
        "situation_analysis": "企業は新しいポリシーを導入し、エンジニアが許可されたリソースのみをAWS CloudFormationを使用してプロビジョニングする必要があります。",
        "option_analysis": "選択肢Bは、エンジニアのIAMロールに制限を設け、許可されたリソースのみを許可するための最も効果的な方法です。他の選択肢は、十分な制限を提供しません。",
        "additional_knowledge": "IAMポリシーにおいて、条件を利用してさらに細かい制御が可能です。",
        "key_terminology": "IAM, CloudFormation, プロビジョニング, ポリシー, 承認されたリソース",
        "overall_assessment": "選択肢Bは明確かつ直接的にポリシーに従ったアプローチであり、他の選択肢よりも適切です。コミュニティの投票結果がCに偏っているが、正しい選択肢はBであることを理解することが重要です。"
      }
    ],
    "keywords": [
      "IAM",
      "CloudFormation",
      "provisioning",
      "policy",
      "approved resources"
    ]
  },
  {
    "No": "75",
    "question": "A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The\napplication is designed to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and\nneeds to be stored in a durable location where it can be retrieved with low latency. The data is ephemeral and the company is required to store the\ndata for 120 days only, after which the data can be deleted.\nThe solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.\nWhich storage strategy is the MOST cost-effective and meets the design requirements?",
    "question_jp": "ソリューションアーキテクトは、企業が近日中に発売する新しいアプリケーションのデータストレージと取得アーキテクチャを設計しています。このアプリケーションは、世界中のデバイスから毎分何百万もの小さなレコードを取り込むように設計されています。各レコードは4KB未満のサイズで、耐障害性のある場所に保存され、低遅延で取得できる必要があります。データは一時的なものであり、会社は120日間データを保存することが義務付けられており、その後データは削除できます。ソリューションアーキテクトは、年間を通じてストレージ要件が約10〜15TBになると計算しました。どのストレージ戦略が最もコスト効果が高く、設計要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Design the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieval. Configure a",
        "text_jp": "アプリケーションを設計して、受信した各レコードを単一の.csvファイルとしてAmazon S3バケットに保存し、インデックス付きで取得できるようにします。"
      },
      {
        "key": "B",
        "text": "Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the",
        "text_jp": "アプリケーションを設計して、受信した各レコードを適切にスケーリングされたAmazon DynamoDBテーブルに保存します。"
      },
      {
        "key": "C",
        "text": "Design the application to store each incoming record in a single table in an Amazon RDS MySQL database. Run a nightly cron job that runs",
        "text_jp": "アプリケーションを設計して、受信した各レコードを単一のテーブルにAmazon RDS MySQLデータベースに保存します。夜間にcronジョブを実行します。"
      },
      {
        "key": "D",
        "text": "Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to",
        "text_jp": "アプリケーションを設計して、受信したレコードをバッチ処理してAmazon S3バケットに書き込みます。オブジェクトのメタデータを更新します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (75%) D (25%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Store each incoming record in an Amazon DynamoDB table configured for the scale. DynamoDB is ideal for handling high throughput and low-latency data retrieval, especially for ephemeral data.",
        "situation_analysis": "The application requires ingestion of millions of small records per minute, low-latency access, and storage for a limited duration (120 days). The size and volume of data dictate the storage strategy.",
        "option_analysis": "Option A (S3 with individual .csv files) would introduce higher costs and latency due to file handling overhead. Option C (RDS MySQL) is not suitable for high-velocity data ingestion at this scale. Option D (batching records for S3) may not meet latency requirements.",
        "additional_knowledge": "Understanding of DynamoDB's pricing model and performance metrics can further enhance decision-making.",
        "key_terminology": "DynamoDB, low-latency, ephemeral data, auto-scaling, on-demand capacity",
        "overall_assessment": "The question effectively assesses knowledge of AWS services relevant for data storage solutions and their respective capabilities against specific use cases. The community vote primarily supports option B, aligning with the architectural requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはB：受信した各レコードをスケーリングに適したAmazon DynamoDBテーブルに保存します。DynamoDBは高スループットと低レイテンシのデータ取得を扱うのに理想的で、特に一時的なデータに適しています。",
        "situation_analysis": "アプリケーションは毎分何百万もの小さなレコードの取り込み、低レイテンシのアクセス、限られた期間（120日）の保存を要求しています。データのサイズと量がストレージ戦略を決定します。",
        "option_analysis": "選択肢A（個別の.csvファイルでのS3）は、ファイル処理のオーバーヘッドによりコストとレイテンシが高くなる可能性があります。選択肢C（RDS MySQL）は、このスケールでの高速度データ取り込みには適していません。選択肢D（S3用のレコードバッチ処理）は、レイテンシ要件を満たさない可能性があります。",
        "additional_knowledge": "DynamoDBの価格モデルやパフォーマンス指標の理解が意思決定をさらに強化できます。",
        "key_terminology": "DynamoDB, 低レイテンシ, 一時データ, 自動スケーリング, オンデマンドキャパシティ",
        "overall_assessment": "この質問は、データストレージソリューションに関連するAWSサービスの知識を効果的に評価し、特定のユースケースに対するそれぞれの能力を比較しています。コミュニティ投票は主に選択肢Bを支持しており、アーキテクチャ要件と一致しています。"
      }
    ],
    "keywords": [
      "DynamoDB",
      "low-latency",
      "ephemeral data",
      "auto-scaling",
      "on-demand capacity"
    ]
  },
  {
    "No": "76",
    "question": "A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all\ntimes for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.\nWhich solution will provide the HIGHEST availability for the database?",
    "question_jp": "小売会社は、複数のAWSリージョンにわたってAWS上でeコマースウェブサイトをホスティングしています。この会社は、オンライン購入のためにウェブサイトが常に稼働していることを望んでいます。ウェブサイトは、Amazon RDS for MySQL DBインスタンスにデータを保存しています。データベースに対して、最も高い可用性を提供するソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance.",
        "text_jp": "Amazon RDSで自動バックアップを設定します。中断が発生した場合、自動バックアップをスタンドアロンDBインスタンスに昇格させます。"
      },
      {
        "key": "B",
        "text": "Configure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to",
        "text_jp": "Amazon RDSでグローバルテーブルとリードレプリカを設定します。クロスリージョンのスコープをアクティブにします。中断が発生した場合、AWS Lambdaを使用します。"
      },
      {
        "key": "C",
        "text": "Configure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from",
        "text_jp": "Amazon RDSでグローバルテーブルと自動バックアップを設定します。中断が発生した場合、AWS Lambdaを使用してリードレプリカをコピーします。"
      },
      {
        "key": "D",
        "text": "Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance.",
        "text_jp": "Amazon RDSでリードレプリカを設定します。中断が発生した場合、クロスリージョンのリードレプリカをスタンドアロンDBインスタンスに昇格させます。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Configuring read replicas on Amazon RDS and promoting a cross-Region read replica to a standalone DB instance during disruptions ensures the highest availability.",
        "situation_analysis": "The company operates an ecommerce site across multiple regions and needs a reliable data storage solution with continuous operation to prevent downtime.",
        "option_analysis": "Option A involves automation but does not guarantee immediate availability. Option B and C do provide redundancy but lack the ability to quickly switch to a fully operational instance during disruptions.",
        "additional_knowledge": "The importance of cross-region read replicas is significant in this context, allowing for compliant failover and operational continuity.",
        "key_terminology": "Amazon RDS, read replicas, high availability, cross-region failover, automated backups",
        "overall_assessment": "Given the high community vote for option D, it aligns with AWS recommended practices for high availability and disaster recovery."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。Amazon RDSでリードレプリカを設定し、中断時にクロスリージョンのリードレプリカをスタンドアロンのDBインスタンスに昇格させることで、最も高い可用性を確保できます。",
        "situation_analysis": "会社は複数の地域にわたってeコマースサイトを運営しており、ダウンタイムを防ぐために、継続的な運用を備えた信頼性の高いデータストレージソリューションが必要です。",
        "option_analysis": "選択肢Aは自動化を含みますが、即時の可用性を保証するものではありません。選択肢BとCは冗長性を提供しますが、障害発生時に完全に機能するインスタンスにすぐに切り替える能力が欠けています。",
        "additional_knowledge": "クロスリージョンのリードレプリカの重要性は、この文脈において重大であり、準拠したフェイルオーバーと運用の継続性を可能にします。",
        "key_terminology": "Amazon RDS、リードレプリカ、高可用性、クロスリージョンフェイルオーバー、自動バックアップ",
        "overall_assessment": "選択肢Dに対するコミュニティの高い投票は、AWSの推奨される高可用性および災害復旧の実践と一致しています。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "read replicas",
      "high availability",
      "cross-region failover",
      "automated backups"
    ]
  },
  {
    "No": "77",
    "question": "Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to\nVPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A. Example Corp. just acquired AnyCompany, which\nhas a VPC named VPC B. There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B.\nExample Corp. wants to connect from its on-premise servers to VPC B. Example Corp. has properly set up the network ACL and security groups.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "question_jp": "Example Corp. は、オンプレミスのデータセンターと、Example Corp. の AWS アカウント内の VPC A を持っています。オンプレミスのネットワークは、AWS Site-To-Site VPN を介して VPC A に接続しています。オンプレミスのサーバーは VPC A に正しくアクセスできます。Example Corp. は AnyCompany を新たに買収し、VPC B という VPC を持っています。これらのネットワーク間に IP アドレスの重複はありません。Example Corp. は VPC A と VPC B をピア接続しました。Example Corp. は、オンプレミスのサーバーから VPC B に接続したいと考えています。Example Corp. はネットワーク ACL とセキュリティグループを正しく設定しました。最も運用の手間をかけずに、この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for",
        "text_jp": "トランジットゲートウェイを作成します。Site-to-Site VPN、VPC A、および VPC B をトランジットゲートウェイに接続します。トランジットゲートウェイのルートテーブルを更新します。"
      },
      {
        "key": "B",
        "text": "Create a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN",
        "text_jp": "トランジットゲートウェイを作成します。オンプレミスネットワークと VPC B の間に Site-to-Site VPN 接続を作成し、VPN を接続します。"
      },
      {
        "key": "C",
        "text": "Update the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks.",
        "text_jp": "Site-to-Site VPN および両方の VPC のルートテーブルをすべての 3 つのネットワーク用に更新します。すべての 3 つのネットワークの BGP プロパゲーションを構成します。"
      },
      {
        "key": "D",
        "text": "Modify the Site-to-Site VPN's virtual private gateway definition to include VPC A and VPC B. Split the two routers of the virtual private",
        "text_jp": "Site-to-Site VPN の仮想プライベートゲートウェイの定義を変更して VPC A と VPC B を含めます。仮想プライベートの 2 つのルーターを分割します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (87%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Modify the Site-to-Site VPN's virtual private gateway definition to include VPC A and VPC B.",
        "situation_analysis": "Example Corp. needs its on-premises servers to access both VPC A and VPC B, which they have already configured for communication via VPN.",
        "option_analysis": "Option D allows for the expansion of an existing Site-to-Site VPN to include both VPCs, simplifying connectivity. Option A and B would require additional management and configurations. Option C would require extensive routing changes.",
        "additional_knowledge": "In some cases, AWS Transit Gateway could also facilitate such connections, but in this scenario, the existing Site-to-Site VPN is already established.",
        "key_terminology": "Site-to-Site VPN, virtual private gateway, VPC peering",
        "overall_assessment": "Option D provides the least operational effort while satisfying the connectivity needs of Example Corp. The community vote distribution is unusually skewed, which may not reflect the most efficient solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは D です：Site-to-Site VPN の仮想プライベートゲートウェイの定義を変更して VPC A と VPC B を含めます。",
        "situation_analysis": "Example Corp. は、オンプレミスのサーバーが VPC A と VPC B の両方にアクセスできる必要があり、すでに VPN を介して通信を設定しています。",
        "option_analysis": "オプション D は、既存の Site-to-Site VPN を拡張して両方の VPC を含めることを可能にし、接続を簡素化します。オプション A および B は、追加の管理および構成が必要となります。オプション C は、広範なルーティング変更が必要です。",
        "additional_knowledge": "場合によっては、AWS Transit Gateway もこのような接続を促進できますが、このシナリオでは既存の Site-to-Site VPN がすでに設定されています。",
        "key_terminology": "Site-to-Site VPN、仮想プライベートゲートウェイ、VPC ピアリング",
        "overall_assessment": "オプション D は、最小限の運用努力で Example Corp. の接続ニーズを満たします。コミュニティの投票分布は異常に偏っており、最も効率的なソリューションを反映していない可能性があります。"
      }
    ],
    "keywords": [
      "Site-to-Site VPN",
      "virtual private gateway",
      "VPC peering"
    ]
  },
  {
    "No": "78",
    "question": "A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the\nmigrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends\noutbound email messages to the company's customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The\napplication can use SMTP only.\nThe company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has\ncreated and validated the SES domain. The company has lifted the SES limits.\nWhat should the company do to modify the application to send email messages from Amazon SES?",
    "question_jp": "企業は最近、リプラットフォーミング戦略を使用してオンプレミスのデータセンターからAWSクラウドへの移行を完了しました。移行されたサーバーの1台は、重要なアプリケーションが依存しているレガシーのシンプルメール転送プロトコル（SMTP）サービスを実行しています。このアプリケーションは、企業の顧客に対して外部メールメッセージを送信します。レガシーSMTPサーバーはTLS暗号化をサポートしておらず、TCPポート25を使用しています。アプリケーションはSMTPのみを使用できます。企業はAmazonシンプルメールサービス（Amazon SES）を使用し、レガシーSMTPサーバーを廃止することを決定しました。企業はSESドメインを作成し、検証しました。企業はSESの制限を引き上げました。企業は、アプリケーションをどのように変更してAmazon SESからメールメッセージを送信すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and",
        "text_jp": "アプリケーションをTLS Wrapperを使用してAmazon SESに接続するように構成します。ses:SendEmailの権限を持つIAMロールを作成します。"
      },
      {
        "key": "B",
        "text": "Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to",
        "text_jp": "アプリケーションをSTARTTLSを使用してAmazon SESに接続するように構成します。Amazon SES SMTP認証情報を取得します。認証情報を使用して"
      },
      {
        "key": "C",
        "text": "Configure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail",
        "text_jp": "アプリケーションを構成してSES APIを使用してメールメッセージを送信します。ses:SendEmailとses:SendRawEmailの権限を持つIAMロールを作成します。"
      },
      {
        "key": "D",
        "text": "Configure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use",
        "text_jp": "アプリケーションをAWS SDKを使用してメールメッセージを送信するように構成します。Amazon SES用のIAMユーザーを作成します。APIアクセスキーを生成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (85%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Configuring the application to connect to Amazon SES using a TLS wrapper enables secure email transmission. Creating an IAM role with 'ses:SendEmail' permissions allows the application to send emails via SES.",
        "situation_analysis": "The application is currently reliant on a legacy SMTP server not supporting TLS encryption, which is insecure. Transitioning to Amazon SES, which supports TLS, resolves this security issue.",
        "option_analysis": "Option A correctly addresses the need for secure connections and permissions. Option B suggests STARTTLS, but the legacy application cannot utilize it. Option C involves using the SES API, which is more modern but does not align with the application's SMTP requirement. Option D also moves toward modern practices with AWS SDK but fails to address SMTP constraints.",
        "additional_knowledge": "Setting up IAM roles with precise permissions is crucial for maintaining security alongside functionality.",
        "key_terminology": "Amazon SES, IAM role, SMTP, TLS, secure email transmission",
        "overall_assessment": "Option A aligns best with the company's need to maintain SMTP while ensuring secure email delivery. Although there is community support for option B, it does not suit the application's requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。アプリケーションをTLS Wrapperを使用してAmazon SESに接続するように構成することで、安全なメールの送信が可能になります。'ses:SendEmail'の権限を持つIAMロールを作成することで、アプリケーションがSES経由でメールを送信できるようになります。",
        "situation_analysis": "アプリケーションは現在、TLS暗号化をサポートしていないレガシーSMTPサーバーに依存していますが、これは安全ではありません。TLSをサポートするAmazon SESに移行することで、このセキュリティの問題を解決できます。",
        "option_analysis": "選択肢Aは、安全な接続と権限の必要性を正しく扱っています。選択肢BはSTARTTLSを提案していますが、レガシーアプリケーションではそれを利用できません。選択肢CはSES APIを使用することを含んでいますが、これはより現代的で、アプリケーションのSMTP要件とは一致しません。選択肢DもAWS SDKを使用する方向に進んでいますが、SMTPの制約には対処していません。",
        "additional_knowledge": "IAMロールを正確な権限で設定することは、機能性と同時にセキュリティを維持するために重要です。",
        "key_terminology": "Amazon SES、IAMロール、SMTP、TLS、安全なメール送信",
        "overall_assessment": "選択肢Aは、SMTPを維持しながら安全なメール配信を実現するという企業のニーズに最も合致しています。コミュニティから選択肢Bに支持が集まっていますが、それはアプリケーションの要件に合致しません。"
      }
    ],
    "keywords": [
      "Amazon SES",
      "IAM role",
      "SMTP",
      "TLS",
      "secure email transmission"
    ]
  },
  {
    "No": "79",
    "question": "A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method.\nThe acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found\nit dificult to generate a cost report that contains meaningful groups for all the teams.\nThe acquiring company's finance team needs a solution to report on costs for all the companies through a self-managed application.\nWhich solution will meet these requirements?",
    "question_jp": "最近、ある会社が他の複数の会社を買収しました。それぞれの会社は、異なる請求および報告方法を持つ別々のAWSアカウントを持っています。\n買収した会社は、すべてのアカウントをAWS Organizations内の1つの組織に統合しました。しかしながら、買収した会社は、すべてのチームに対して意味のあるグループを含むコストレポートを生成することが困難であることがわかりました。\n買収した会社の財務チームは、自己管理プログラムを通じてすべての会社のコストを報告するためのソリューションを必要としています。\nどのソリューションがこれらの要求を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena.",
        "text_jp": "組織のためにAWSコストと使用状況レポートを作成します。レポート内でタグとコストカテゴリを定義します。Amazon Athenaにテーブルを作成します。"
      },
      {
        "key": "B",
        "text": "Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in",
        "text_jp": "組織のためにAWSコストと使用状況レポートを作成します。レポート内でタグとコストカテゴリを定義します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the",
        "text_jp": "AWS価格リストクエリAPIから支出情報を受け取るAmazon QuickSightデータセットを作成します。データセットを共有します。"
      },
      {
        "key": "D",
        "text": "Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the",
        "text_jp": "AWS価格リストクエリAPIを使用してアカウントの支出情報を収集します。AWSコストエクスプローラーに特化したテンプレートを作成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using the AWS Price List Query API to collect account spending information allows for a detailed and accurate report of costs, which can be tailored to specific needs.",
        "situation_analysis": "The acquiring company needs to generate cost reports that encompass all acquired accounts within a single organization, capturing meaningful cost allocation across different teams.",
        "option_analysis": "Option D is correct as it utilizes an API specifically designed for retrieving pricing and spending data, which can be integrated into customized reporting tools. Other options do not adequately address the requirement for self-management and customization.",
        "additional_knowledge": "The Price List Query API can help in creating custom templates that can aggregate and analyze costs efficiently.",
        "key_terminology": "AWS Price List Query API, AWS Cost Explorer, cost management, cost reports, financial reporting",
        "overall_assessment": "The question effectively assesses the understanding of cost reporting mechanisms within AWS and the ability to leverage APIs for customized financial insights. \"Community voted for A, which may be justified if they are favoring general reporting tools rather than specialized APIs but does not meet the specific requirement for self-managed applications as effectively as option D.\""
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。AWS価格リストクエリAPIを使用してアカウントの支出情報を収集することで、特定のニーズに合わせた詳細で正確なコストレポートが可能となる。",
        "situation_analysis": "買収した会社は、単一の組織内で買収されたすべてのアカウントを包含するコストレポートを生成する必要があり、異なるチーム間での意味のあるコスト配分を把握することが求められている。",
        "option_analysis": "選択肢Dは正しい。特に支出データを取得するために設計されたAPIを利用し、カスタマイズされた報告ツールに統合することができる。他の選択肢は、自己管理とカスタマイズの要求を十分に満たしていない。",
        "additional_knowledge": "価格リストクエリAPIは、コストを効率的に集約および分析するためのカスタムテンプレートを作成するのに役立つ。",
        "key_terminology": "AWS価格リストクエリAPI、AWSコストエクスプローラー、コスト管理、コストレポート、財務報告",
        "overall_assessment": "この問題は、AWS内のコスト報告メカニズムに対する理解と、カスタマイズされた財務インサイトのためにAPIを活用する能力を効果的に評価している。「コミュニティはAに投票したが、一般的な報告ツールを好む場合には正当化されるかもしれないが、自己管理アプリケーションの特定の要求をDほど効果的には満たしていない。」"
      }
    ],
    "keywords": [
      "AWS Price List Query API",
      "AWS Cost Explorer",
      "cost management",
      "cost reports",
      "financial reporting"
    ]
  },
  {
    "No": "80",
    "question": "A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company's Node.js API servers on Amazon EC2\ninstances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General\nPurpose SSD volume.\nThe number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are\nconsistently overloaded and RDS metrics show high write latency.\nWhich of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this\nplatform cost-eficient? (Choose two.)",
    "question_jp": "ある企業がAWS上にIoTプラットフォームを運営している。様々な場所に設置されたIoTセンサーが、Amazon EC2インスタンス上のNode.js APIサーバーにデータを送信しており、そのサーバーはアプリケーションロードバランサーの背後で稼働している。データは、4 TBの汎用SSDボリュームを使用するAmazon RDS MySQL DBインスタンスに保存されている。企業が現場に展開したセンサーの数は、時間とともに増加しており、今後も大幅に成長が見込まれている。APIサーバーは常に過負荷状態であり、RDSのメトリクスは高い書き込みレイテンシーを示している。次のうち、コスト効率を保ちながら新しいセンサーが調達されるにつれて問題を恒久的に解決し、成長を可能にするための手段はどれか。(2つ選択せよ)",
    "choices": [
      {
        "key": "A",
        "text": "Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS.",
        "text_jp": "MySQL汎用SSDストレージを6 TBにリサイズし、ボリュームのIOPSを改善する。"
      },
      {
        "key": "B",
        "text": "Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.",
        "text_jp": "データベース層をRDS MySQL DBインスタンスの代わりにAmazon Auroraを使用するように再構築し、リードレプリカを追加する。"
      },
      {
        "key": "C",
        "text": "Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.",
        "text_jp": "Amazon Kinesis Data StreamsとAWS Lambdaを活用して、生データを取り込み処理する。"
      },
      {
        "key": "D",
        "text": "Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.",
        "text_jp": "AWS X-Rayを使用してアプリケーションの問題を分析し、負荷に応じてAPIサーバーを追加する。"
      },
      {
        "key": "E",
        "text": "Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.",
        "text_jp": "データベース層をRDS MySQL DBインスタンスの代わりにAmazon DynamoDBを使用するように再構築する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "CE (63%) BC (18%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Leveraging Amazon Kinesis Data Streams and AWS Lambda allows for efficient ingestion and processing of large amounts of data from the IoT sensors, resolving the data overload issue and enabling scalability as new sensors are added.",
        "situation_analysis": "The company is facing high write latency and consistently overloaded API servers due to an increasing number of IoT sensors sending data. This indicates the current architecture is not scalable to accommodate future growth.",
        "option_analysis": "Option A merely increases storage, which may not solve the overload problem. Option B is a potential solution, but it doesn't directly address the immediate data ingestion issue. Option D focuses on debugging but won't prevent overload. Option E is an alternative but doesn't provide the same immediate value for data streaming as Kinesis.",
        "additional_knowledge": "In a fast-paced IoT environment, scalability and efficiency are paramount. Kinesis and Lambda provide a robust mechanism to handle unpredictable loads and rapid data growth.",
        "key_terminology": "Amazon Kinesis, AWS Lambda, IoT, scalability, data ingestion",
        "overall_assessment": "Community votes show a strong preference for C, as it effectively addresses the data ingestion problem. The other options may improve other aspects but do not provide a comprehensive solution for the growing data stream."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。Amazon Kinesis Data StreamsとAWS Lambdaを活用することで、IoTセンサーからの大量のデータを効率的に取り込み処理でき、データオーバーロードの問題を解決し、新しいセンサーが追加される際のスケーラビリティを実現する。",
        "situation_analysis": "企業は、IoTセンサーから送信されるデータが増加するにつれて高い書き込みレイテンシーとAPIサーバーの過負荷に直面している。これは、現在のアーキテクチャが将来の成長を考慮していないことを示唆している。",
        "option_analysis": "選択肢Aはストレージを増加させるだけであり、過負荷の問題を解決しない。選択肢Bは潜在的な解決策だが、即座にデータ取り込みの問題を解決するものではない。選択肢Dはデバッグに焦点を当てているが、過負荷を防ぐことはできない。選択肢Eは代替案だが、Kinesisほどリアルタイムのデータストリーミングの価値を提供しない。",
        "additional_knowledge": "急速に進化するIoT環境においては、スケーラビリティと効率性が重要である。KinesisとLambdaは、不規則な負荷と急速なデータ成長を処理するための強力なメカニズムを提供する。",
        "key_terminology": "Amazon Kinesis、AWS Lambda、IoT、スケーラビリティ、データ取り込み",
        "overall_assessment": "コミュニティの投票はCに強い支持を示しており、データ取り込みの問題を効果的に解決する。その他の選択肢は他の側面を改善するかもしれないが、増大するデータストリームに対する包括的な解決策を提供していない。"
      }
    ],
    "keywords": [
      "Amazon Kinesis",
      "AWS Lambda",
      "IoT",
      "scalability",
      "data ingestion"
    ]
  },
  {
    "No": "81",
    "question": "A company is building an electronic document management system in which users upload their documents. The application stack is entirely\nserverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for\ndelivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs\ncall AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.\nThe company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of\nEurope.\nWhich combination of actions will meet these requirements? (Choose two.)",
    "question_jp": "ある企業が、ユーザーがドキュメントをアップロードする電子文書管理システムを構築しています。アプリケーションスタックは完全にサーバーレスで、eu-central-1リージョンのAWS上で実行されています。このシステムには、Amazon S3をオリジンとする配信にAmazon CloudFrontディストリビューションを使用するウェブアプリケーションが含まれています。ウェブアプリケーションは、Amazon API Gatewayのリージョナルエンドポイントと通信しています。API Gateway APIは、メタデータをAmazon Aurora Serverlessデータベースに保存し、ドキュメントをS3バケットに格納するAWS Lambda関数を呼び出します。この企業は着実に成長しており、最大の顧客とのPoC（概念実証）を完了しています。企業は、ヨーロッパ以外でのレイテンシーを改善する必要があります。これらの要件を満たすためにどのアクションの組み合わせを選択できますか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs.",
        "text_jp": "S3バケットでS3 Transfer Accelerationを有効にします。ウェブアプリケーションがTransfer Accelerationの署名付きURLを使用することを確認します。"
      },
      {
        "key": "B",
        "text": "Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution.",
        "text_jp": "AWS Global Acceleratorでアクセラレーターを作成します。アクセラレーターをCloudFrontディストリビューションにアタッチします。"
      },
      {
        "key": "C",
        "text": "Change the API Gateway Regional endpoints to edge-optimized endpoints.",
        "text_jp": "API Gatewayのリージョナルエンドポイントをエッジ最適化エンドポイントに変更します。"
      },
      {
        "key": "D",
        "text": "Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.",
        "text_jp": "スタック全体を、世界中に分散した2つの他のロケーションにプロビジョニングします。Aurora Serverlessクラスターでグローバルデータベースを使用します。"
      },
      {
        "key": "E",
        "text": "Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database.",
        "text_jp": "Lambda関数とAurora Serverlessデータベースの間にAmazon RDSプロキシを追加します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (45%) CD (40%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and B. Enabling S3 Transfer Acceleration and using AWS Global Accelerator can help improve latency for users outside of Europe by reducing the transfer time of large files and optimizing the route to the closest edge location.",
        "situation_analysis": "The system must improve latency for users located outside of Europe, which is critical for global user experience and system performance.",
        "option_analysis": "Option A directly addresses the need for faster uploads to S3, particularly for large documents, while option B optimizes routing to CloudFront distribution for worse latency scenarios. Options C, D, and E do not effectively enhance the required latency improvement for users outside Europe.",
        "additional_knowledge": "Using these services helps ensure smoother operations as the company scales its application to a global audience.",
        "key_terminology": "S3 Transfer Acceleration, AWS Global Accelerator, CloudFront, edge locations, Aurora Serverless.",
        "overall_assessment": "Both selected options represent best practices for optimizing user experience across geographical locations, aligning well with AWS capabilities for latency reduction."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとBである。S3 Transfer Accelerationを有効にし、AWS Global Acceleratorを使用することで、ヨーロッパ以外のユーザーに対するレイテンシーを改善することができ、大きなファイルの転送時間を短縮し、最寄りのエッジロケーションへのルーティングを最適化する。",
        "situation_analysis": "システムはヨーロッパ以外のユーザーに対するレイテンシーを改善する必要があり、これはグローバルユーザーエクスペリエンスやシステムパフォーマンスにとって重要である。",
        "option_analysis": "選択肢Aは、特に大きなドキュメントのS3へのアップロードを高速化するニーズに直接対応しており、選択肢Bは、悪化したレイテンシーのシナリオにおけるCloudFrontディストリビューションへのルーティングを最適化する。選択肢C、D、Eは、ヨーロッパ以外のユーザーに対するレイテンシー改善には効果的でない。",
        "additional_knowledge": "これらのサービスを使用することで、企業がアプリケーションをグローバルなオーディエンスにスケーリングする際に、よりスムーズな運用が確保される。",
        "key_terminology": "S3 Transfer Acceleration, AWS Global Accelerator, CloudFront, エッジロケーション, Aurora Serverless。",
        "overall_assessment": "選択された両方のオプションは、地理的ロケーションにおけるユーザーエクスペリエンスの最適化に対するベストプラクティスを表しており、レイテンシー削減に関するAWSの能力とよくalignしている。"
      }
    ],
    "keywords": [
      "S3 Transfer Acceleration",
      "AWS Global Accelerator",
      "CloudFront",
      "edge locations",
      "Aurora Serverless"
    ]
  },
  {
    "No": "82",
    "question": "An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and\nvideos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.\nThe company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are\naccessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions\narchitect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.\nWhich solution will meet these requirements?",
    "question_jp": "アドベンチャー会社は、モバイルアプリに新しい機能を導入しました。ユーザーはこの機能を使用して、ハイキングやラフティングの写真やビデオをいつでもアップロードできます。これらの写真やビデオは、S3バケットのAmazon S3 Standardストレージに保存され、Amazon CloudFrontを通じて配信されます。この会社は、ストレージコストを最適化する必要があります。ソリューションアーキテクトは、アップロードされたほとんどの写真やビデオが30日後にアクセスされることが少ないことを発見しました。しかし、30日後には頻繁にアクセスされる写真やビデオもあります。ソリューションアーキテクトは、最低限のコストで写真やビデオのミリソ秒での取得可能性を維持するソリューションを実装する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure S3 Intelligent-Tiering on the S3 bucket.",
        "text_jp": "S3バケットにS3 Intelligent-Tieringを設定する。"
      },
      {
        "key": "B",
        "text": "Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days.",
        "text_jp": "S3ライフサイクルポリシーを設定して、画像オブジェクトとビデオオブジェクトを30日後にS3 StandardからS3 Glacier Deep Archiveに移行する。"
      },
      {
        "key": "C",
        "text": "Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances.",
        "text_jp": "Amazon S3をAmazon Elastic File System (Amazon EFS)ファイルシステムに置き換え、Amazon EC2インスタンスにマウントする。"
      },
      {
        "key": "D",
        "text": "Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days.",
        "text_jp": "S3画像オブジェクトとS3ビデオオブジェクトにCache-Control: max-ageヘッダーを追加する。このヘッダーを30日間に設定する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Configuring an S3 Lifecycle policy to transition image and video objects to S3 Glacier Deep Archive after 30 days will lower storage costs, as they are accessed infrequently in that time frame.",
        "situation_analysis": "The company faces high storage costs due to infrequent access of uploaded media after 30 days. The challenge is to balance cost savings with the need for some level of retrieval availability.",
        "option_analysis": "Option A, S3 Intelligent-Tiering, does not guarantee the lowest cost because it could involve additional fees for frequent access, making it unsuitable. Option C, EFS, is not ideal as it is generally more expensive than S3 for this use case. Option D only adds a cache header but does not address the storage cost issues; hence, it also fails to meet the requirements.",
        "additional_knowledge": "It is essential for architects to understand the retrieval times associated with various storage classes in S3 to make effective decisions.",
        "key_terminology": "S3 Lifecycle Policy, S3 Glacier Deep Archive, Storage Cost Optimization",
        "overall_assessment": "Answer B provides a balanced approach to reducing costs while retaining the necessary access level for the uploaded media files. Considering the community vote with option A receiving 95%, it highlights a misconception about frequent access within the 30-day window."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。画像オブジェクトとビデオオブジェクトを30日後にS3 Glacier Deep Archiveに移行するS3ライフサイクルポリシーを設定することで、ストレージコストを削減できる。これは、その期間中にアクセスが少ないためである。",
        "situation_analysis": "この会社は、アップロードされたメディアが30日後にアクセスされる頻度が低いため、高いストレージコストに直面している。ストレージコスト削減と一部の取得可能性の需要のバランスを取ることが課題となっている。",
        "option_analysis": "選択肢AのS3 Intelligent-Tieringは、頻繁にアクセスされる場合の追加料金が発生するため、最低コストを確保できず不向きである。選択肢CのEFSは、一般的にこのユースケースではS3よりも高価であるため、理想的ではない。選択肢Dはキャッシュヘッダーを追加するだけで、ストレージコストの問題には対処しないため、要件を満たさない。",
        "additional_knowledge": "アーキテクトは、さまざまな収納クラスの取得時間を理解し、効果的な決定を下すことが重要である。",
        "key_terminology": "S3ライフサイクルポリシー, S3 Glacier Deep Archive, ストレージコスト最適化",
        "overall_assessment": "選択肢Bは、アップロードされたメディアファイルへの必要なアクセスレベルを維持しながらコストを削減するバランスの取れたアプローチを提供する。コミュニティの投票結果では、選択肢Aが95%の支持を得ているため、30日以内の頻繁なアクセスに関する誤解が浮き彫りとなる。"
      }
    ],
    "keywords": [
      "S3 Lifecycle Policy",
      "S3 Glacier Deep Archive",
      "Storage Cost Optimization"
    ]
  },
  {
    "No": "83",
    "question": "A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during\nthe past year.\nA solutions architect needs to review data trends for the past 12 months and identity the appropriate storage class for the objects.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、さまざまなストレージクラスでファイルや画像を保存するためにAmazon S3を使用しています。この企業のS3コストは、過去1年間で大幅に増加しました。ソリューションアーキテクトは、過去12か月のデータトレンドを確認し、オブジェクトに適切なストレージクラスを特定する必要があります。どの解決策がこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Download AWS Cost and Usage Reports for the last 12 months of S3 usage. Review AWS Trusted Advisor recommendations for cost",
        "text_jp": "過去12か月のS3利用のAWSコストおよび使用レポートをダウンロードし、コストのためのAWS Trusted Advisorの推奨事項を確認します。"
      },
      {
        "key": "B",
        "text": "Use S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard to analyze storage trends.",
        "text_jp": "S3ストレージクラス分析を使用します。データトレンドをAmazon QuickSightダッシュボードにインポートして、ストレージトレンドを分析します。"
      },
      {
        "key": "C",
        "text": "Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends.",
        "text_jp": "Amazon S3 Storage Lensを使用します。ストレージトレンドのための高度なメトリクスを含めるようにデフォルトダッシュボードをアップグレードします。"
      },
      {
        "key": "D",
        "text": "Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 months. Import the .csv file to an Amazon",
        "text_jp": "S3用のアクセスアナライザーを使用します。過去12か月のためのアクセスアナライザーのレポートをダウンロードし、.csvファイルをAmazonにインポートします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (78%) 12% 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Use S3 storage class analysis.",
        "situation_analysis": "The company needs to review data trends from the past year to identify cost-effective storage classes for different objects stored in Amazon S3.",
        "option_analysis": "Option B directly addresses the need for analyzing storage data trends and allows the use of QuickSight for visualization. Option A focuses on cost reports without specific storage class insights. Option C is useful but less direct than B without initial trend analysis. Option D does not pertain to storage classes directly.",
        "additional_knowledge": "Understanding of Amazon S3 storage classes such as Standard, Intelligent-Tiering, Standard-IA, and Glacier is essential for cost management.",
        "key_terminology": "S3 storage class analysis, QuickSight, storage costs, data trends, S3 lifecycle management.",
        "overall_assessment": "The question tests knowledge of AWS tools for data analysis related to S3 usage and costs. Option B is most aligned with the requirements, while the community's choice suggests a misunderstanding of intended storage class optimization."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はB: S3ストレージクラス分析を使用します。",
        "situation_analysis": "企業は過去1年間のデータトレンドを確認し、Amazon S3に保存されているさまざまなオブジェクトのためにコスト効率の良いストレージクラスを特定する必要があります。",
        "option_analysis": "選択肢Bはストレージデータトレンドを分析するニーズに直接対応しており、QuickSightによる可視化が可能です。選択肢Aは特定のストレージクラスの洞察なしにコストレポートに焦点を当てています。選択肢Cは有用ですが、初期のトレンド分析なしではBほど直接的ではありません。選択肢Dはストレージクラスと直接関係ありません。",
        "additional_knowledge": "Amazon S3ストレージクラスの理解、例えばStandard、Intelligent-Tiering、Standard-IA、Glacierなどはコスト管理にとって重要です。",
        "key_terminology": "S3ストレージクラス分析、QuickSight、ストレージコスト、データトレンド、S3ライフサイクル管理。",
        "overall_assessment": "この質問は、S3の使用とコストに関連するデータ分析に関するAWSツールの知識をテストします。選択肢Bは要件に最も適合しており、コミュニティの選択肢は意図されたストレージクラスの最適化の誤解を示唆しています。"
      }
    ],
    "keywords": [
      "S3 storage class analysis",
      "QuickSight",
      "storage costs",
      "data trends",
      "S3 lifecycle management"
    ]
  },
  {
    "No": "84",
    "question": "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently\ndeployed in one AWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "ある企業はAWS上にクラウドインフラを構築しています。ソリューションアーキテクトは、インフラをコードとして定義する必要があります。インフラは現在、一つのAWSリージョンにデプロイされています。この企業のビジネス拡大計画には、複数のAWSアカウントにまたがる複数のリージョンへのデプロイが含まれています。これらの要件を満たすために、ソリューションアーキテクトは何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.",
        "text_jp": "AWS CloudFormationテンプレートを使用する。様々なアカウントを制御するためにIAMポリシーを追加し、複数のリージョンにわたってテンプレートをデプロイする。"
      },
      {
        "key": "B",
        "text": "Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage",
        "text_jp": "AWS Organizationsを使用する。管理アカウントからAWS CloudFormationテンプレートをデプロイし、AWS Control Towerを使用して管理する。"
      },
      {
        "key": "C",
        "text": "Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary",
        "text_jp": "AWS OrganizationsとAWS CloudFormation StackSetsを使用する。必要な権限を持つアカウントからCloudFormationテンプレートをデプロイする。"
      },
      {
        "key": "D",
        "text": "Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.",
        "text_jp": "AWS CloudFormationテンプレートを使用してネストスタックを利用する。ネストスタックを使用してリージョンを変更する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Use AWS Organizations and AWS CloudFormation StackSets. Deploy a CloudFormation template from an account that has the necessary permissions.",
        "situation_analysis": "The company needs to deploy infrastructure across multiple AWS Regions and accounts, indicating a requirement for scalability and consistent management.",
        "option_analysis": "Option A is incorrect because while AWS CloudFormation is used for infrastructure as code, it does not facilitate multi-account management as effectively as AWS Organizations with StackSets. Option B involves AWS Control Tower, which is a good practice but does not explicitly mention the use of StackSets. Option D suggests using nested stacks which complicates the deployment process across multiple accounts and regions.",
        "additional_knowledge": "StackSets functionality allows administrators to manage permissions and deployment targets efficiently across accounts.",
        "key_terminology": "AWS Organizations, AWS CloudFormation, StackSets, infrastructure as code, multi-account management",
        "overall_assessment": "This question effectively tests the candidate's understanding of managing multi-region and multi-account deployments using AWS best practices. The community overwhelmingly supports the answer C, aligning with best practices in the industry."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はC: AWS OrganizationsとAWS CloudFormation StackSetsを使用し、必要な権限を持つアカウントからCloudFormationテンプレートをデプロイすることである。",
        "situation_analysis": "この企業は複数のAWSリージョンとアカウントにわたってインフラをデプロイする必要があり、スケーラビリティと一貫した管理が求められていることを示している。",
        "option_analysis": "選択肢Aは、AWS CloudFormationがインフラをコードとして表現するために使われるが、AWS OrganizationsとStackSetsほど効果的にマルチアカウント管理を行うことができないため不正解である。選択肢BはAWS Control Towerを含んでいるが、StackSetsの利用について明示的に言及していない。選択肢Dはネストスタックの使用を提案しているが、これは複数のアカウント及びリージョンへのデプロイプロセスを複雑にする。",
        "additional_knowledge": "StackSets機能により、管理者は効率的にアカウント間の権限とデプロイターゲットを管理できる。",
        "key_terminology": "AWS Organizations, AWS CloudFormation, StackSets, インフラをコードとして管理, マルチアカウント管理",
        "overall_assessment": "この質問は、候補者のAWSのベストプラクティスに基づくマルチリージョンとマルチアカウントデプロイメントの管理に関する理解を効果的にテストしている。コミュニティも圧倒的に選択肢Cを支持しており、業界でのベストプラクティスと一致している。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS CloudFormation",
      "StackSets",
      "infrastructure as code",
      "multi-account management"
    ]
  },
  {
    "No": "85",
    "question": "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently\ndeployed in one AWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "ある会社はAWS上にクラウドインフラストラクチャを持っています。ソリューションアーキテクトは、インフラストラクチャをコードとして定義する必要があります。このインフラストラクチャは現在、1つのAWSリージョンにデプロイされています。会社のビジネス拡張計画には、複数のAWSアカウントにまたがる複数のリージョンへのデプロイメントが含まれています。これらの要件を満たすために、ソリューションアーキテクトは何を行うべきか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.",
        "text_jp": "AWS CloudFormationテンプレートを使用する。さまざまなアカウントを制御するためにIAMポリシーを追加し、複数のリージョンにわたってテンプレートをデプロイする。"
      },
      {
        "key": "B",
        "text": "Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage",
        "text_jp": "AWS Organizationsを使用する。管理アカウントからAWS CloudFormationテンプレートをデプロイし、AWS Control Towerを使用して管理する。"
      },
      {
        "key": "C",
        "text": "Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary",
        "text_jp": "AWS OrganizationsおよびAWS CloudFormation StackSetsを使用する。必要な権限を持つアカウントからCloudFormationテンプレートをデプロイする。"
      },
      {
        "key": "D",
        "text": "Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.",
        "text_jp": "AWS CloudFormationテンプレートを使用したネストスタックを利用する。ネストスタックを使用してリージョンを変更する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which involves using AWS Organizations and AWS CloudFormation StackSets.",
        "situation_analysis": "The company requires a solution to deploy its infrastructure across multiple AWS Regions and accounts efficiently.",
        "option_analysis": "Option C allows deployment of CloudFormation templates through StackSets to multiple accounts and regions simultaneously, which meets the requirements. Option A is incorrect as it lacks multi-account management. Option B also includes AWS Control Tower but does not specifically address Cross-Region deployment. Option D does not provide the necessary account management capabilities.",
        "additional_knowledge": "Utilizing AWS Organizations also allows the management of permissions and policies across multiple accounts.",
        "key_terminology": "AWS Organizations, AWS CloudFormation, StackSets",
        "overall_assessment": "This question assesses knowledge of deploying infrastructure as code in a multi-account, multi-region context. Community support for option C indicates strong agreement with best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCであり、AWS OrganizationsおよびAWS CloudFormation StackSetsを使用することです。",
        "situation_analysis": "この会社は、複数のAWSリージョンとアカウントにわたってインフラストラクチャを効率的にデプロイするソリューションを必要としています。",
        "option_analysis": "選択肢Cは、StackSetsを通じてCloudFormationテンプレートを複数のアカウントやリージョンに同時にデプロイできるため、要件を満たします。選択肢Aは、マルチアカウント管理が欠けているため不正解です。選択肢BはAWS Control Towerを含んでいますが、リージョン間デプロイメントを具体的に扱っていません。選択肢Dは、必要なアカウント管理機能を提供していません。",
        "additional_knowledge": "AWS Organizationsを利用することで、複数のアカウント間の権限やポリシーの管理も可能になります。",
        "key_terminology": "AWS Organizations、AWS CloudFormation、StackSets",
        "overall_assessment": "この問題は、マルチアカウントおよびマルチリージョンの文脈でインフラストラクチャをコードとしてデプロイする知識を評価します。選択肢Cに対するコミュニティの支持は、ベストプラクティスに対する強い合意を示しています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS CloudFormation",
      "StackSets"
    ]
  },
  {
    "No": "86",
    "question": "A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be\nupgraded to support the modern design for the application with the following requirements:\n• It should allow changes to be released several times every hour.\n• It should be able to roll back the changes as quickly as possible.\nWhich design will meet these requirements?",
    "question_jp": "企業はモノリシックアプリケーションをAWS上にデプロイされたモダンアプリケーションデザインにリファクタリングする計画を立てています。CI/CDパイプラインは、以下の要件を満たすモダンデザインのアプリケーションをサポートするようにアップグレードする必要があります：• 数時間ごとに何度でも変更をリリースできるべきである。• 変更をできるだけ早くロールバックできるべきである。どのデザインがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing",
        "text_jp": "アプリケーションとその設定を含むAMIを使用したCI/CDパイプラインを展開する。アプリケーションは置き換えによってデプロイする。"
      },
      {
        "key": "B",
        "text": "Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To",
        "text_jp": "AWS Elastic Beanstalkを指定してCI/CDパイプラインのデプロイメントターゲットとして二次環境にステージする。"
      },
      {
        "key": "C",
        "text": "Use AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code",
        "text_jp": "AWS Systems Managerを使用して各デプロイメントのためにインフラを再プロビジョニングする。最新のコードを取得するためにAmazon EC2のユーザーデータを更新する。"
      },
      {
        "key": "D",
        "text": "Roll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances. and",
        "text_jp": "Auto Scalingイベントの一環としてアプリケーションのアップデートを展開する。新しいバージョンのAMIを使用してインスタンスを追加する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves using AWS Elastic Beanstalk to create an isolated staging environment, allowing for frequent deployments and quick rollback capabilities.",
        "situation_analysis": "The requirements specify a need for rapid and frequent changes to be deployed, along with the ability to quickly revert changes if necessary.",
        "option_analysis": "A suggests using AMIs which may not allow for rapid changes. C utilizes Systems Manager for provisioning, but this could add complexity and time during deployment. D mentions Auto Scaling, but does not address the CI/CD pipeline directly.",
        "additional_knowledge": "Elastic Beanstalk also supports various deployment strategies, enhancing flexibility during the CI/CD process.",
        "key_terminology": "CI/CD, Elastic Beanstalk, AMI, Auto Scaling, infrastructure provisioning",
        "overall_assessment": "The choice of B aligns well with the needs for continuous integration and delivery, along with rollback capabilities, making it ideal for modern application deployments."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBであり、AWS Elastic Beanstalkを使用して孤立したステージング環境を作成し、頻繁なデプロイと迅速なロールバック能力を可能にします。",
        "situation_analysis": "要求事項には、迅速かつ頻繁な変更のデプロイと、必要に応じて迅速に変更を元に戻す能力が求められています。",
        "option_analysis": "AはAMIを使用することを提案していますが、これでは迅速な変更が難しいです。CはプロビジョニングのためにSystems Managerを利用しますが、デプロイ中の複雑さと時間が増える可能性があります。DはAuto Scalingに言及していますが、CI/CDパイプラインについて直接言及していません。",
        "additional_knowledge": "Elastic Beanstalkは、CI/CDプロセス中の柔軟性を高めるためにさまざまなデプロイ戦略もサポートしています。",
        "key_terminology": "CI/CD、Elastic Beanstalk、AMI、Auto Scaling、インフラストラクチャプロビジョニング",
        "overall_assessment": "Bの選択肢は、継続的な統合とデリバリーのニーズ、およびロールバック機能とよく合致しており、モダンアプリケーションデプロイメントに最適です。"
      }
    ],
    "keywords": [
      "CI/CD",
      "Elastic Beanstalk",
      "AMI",
      "Auto Scaling",
      "infrastructure provisioning"
    ]
  },
  {
    "No": "87",
    "question": "A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where\nthe application needs to access an Amazon Aurora DB Cluster. The EC2 instances are all associated with the same security group. The DB cluster\nis associated with its own security group.\nThe solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "question_jp": "ある企業がAmazon EC2インスタンス上で動作するアプリケーションを持っています。ソリューションアーキテクトは、アプリケーションがAmazon Aurora DBクラスターにアクセスする必要があるAWSリージョンのVPCインフラストラクチャを設計しています。EC2インスタンスはすべて同じセキュリティグループに関連付けられており、DBクラスターは独自のセキュリティグループに関連付けられています。ソリューションアーキテクトは、DBクラスターへのアプリケーションの最小特権アクセスを提供するためにセキュリティグループにルールを追加する必要があります。これらの要件を満たすための手順の組み合わせはどれですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Add an inbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the source over the default Aurora port.",
        "text_jp": "EC2インスタンスのセキュリティグループにインバウンドルールを追加します。デフォルトのAuroraポートを介してDBクラスターのセキュリティグループをソースとして指定します。"
      },
      {
        "key": "B",
        "text": "Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora",
        "text_jp": "EC2インスタンスのセキュリティグループにアウトバウンドルールを追加します。デフォルトのAuroraを介してDBクラスターのセキュリティグループを宛先として指定します。"
      },
      {
        "key": "C",
        "text": "Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port.",
        "text_jp": "DBクラスターのセキュリティグループにインバウンドルールを追加します。デフォルトのAuroraポートを介してEC2インスタンスのセキュリティグループをソースとして指定します。"
      },
      {
        "key": "D",
        "text": "Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the default Aurora",
        "text_jp": "DBクラスターのセキュリティグループにアウトバウンドルールを追加します。デフォルトのAuroraを介してEC2インスタンスのセキュリティグループを宛先として指定します。"
      },
      {
        "key": "E",
        "text": "Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the ephemeral",
        "text_jp": "DBクラスターのセキュリティグループにアウトバウンドルールを追加します。エフェメラルを介してEC2インスタンスのセキュリティグループを宛先として指定します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BC (76%) AC (24%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and C. Adding an inbound rule to the EC2 security group and an inbound rule to the DB cluster security group is essential for allowing communication between the EC2 instances and the DB cluster while maintaining least privilege access.",
        "situation_analysis": "The application requires access to the Aurora DB cluster, and security groups must be configured appropriately to allow only necessary traffic between the EC2 instances and the DB cluster.",
        "option_analysis": "Option A allows EC2 instances to accept traffic from the DB cluster by specifying the security group as the source. Option C allows the DB cluster to accept connections from the EC2 instances by specifying the EC2 security group as the source. Options B, D, and E are not appropriate as they do not establish the required inbound rules.",
        "additional_knowledge": "It is beneficial to periodically review and audit security group rules to ensure that they follow the least privilege principle and adjust based on usage patterns.",
        "key_terminology": "EC2, Aurora, security group, inbound rule, outbound rule",
        "overall_assessment": "The question assesses knowledge of VPC security and the least privilege principle in access control. The community vote indicates a misunderstanding, likely due to confusion between inbound and outbound rule necessities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとCです。EC2セキュリティグループにインバウンドルールを追加し、DBクラスターセキュリティグループにインバウンドルールを追加することが必要で、EC2インスタンスとDBクラスター間の通信を許可しながら最小特権アクセスを維持します。",
        "situation_analysis": "アプリケーションはAurora DBクラスターへのアクセスを必要としており、セキュリティグループはEC2インスタンスとDBクラスター間でのみ必要なトラフィックを許可するように適切に構成する必要があります。",
        "option_analysis": "選択肢Aは、EC2インスタンスがDBクラスターからのトラフィックを受け入れることを許可します。選択肢Cは、DBクラスターがEC2インスタンスからの接続を受け入れることを許可します。選択肢B、D、Eは必要なインバウンドルールを確立しないため不適切です。",
        "additional_knowledge": "セキュリティグループルールを定期的にレビューおよび監査し、最小特権原則に従っていることを確認し、使用パターンに基づいて調整することが有益です。",
        "key_terminology": "EC2、Aurora、セキュリティグループ、インバウンドルール、アウトバウンドルール",
        "overall_assessment": "この質問は、VPCのセキュリティとアクセス制御における最小特権原則の知識を評価します。コミュニティの投票は誤解を示している可能性があり、インバウンドとアウトバウンドルールの必要性の混乱によるものと思われます。"
      }
    ],
    "keywords": [
      "EC2",
      "Aurora",
      "security group",
      "inbound rule",
      "outbound rule"
    ]
  },
  {
    "No": "88",
    "question": "A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports\nfor overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for\neach business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team\nwants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for\nany cloud spending that exceeds a set threshold.\nWhich solution is the MOST cost-effective way to meet these requirements?",
    "question_jp": "ある企業は、それぞれの事業部門に対する内部クラウドの請求戦略を変更したいと考えています。現在、クラウドガバナンスチームは、それぞれの事業部門の責任者に対して全体的なクラウド支出に関する報告書を共有しています。この企業は、各事業部門のための別々のAWSアカウントを管理するためにAWS Organizationsを使用しています。Organizationsにおける既存のタグ付け基準には、アプリケーション、環境、および所有者が含まれています。クラウドガバナンスチームは、各事業部門が自動的に月次のクラウド支出報告書を受け取るための集中管理ソリューションを望んでいます。このソリューションは、設定された閾値を超えたクラウド支出に対して通知を送信する必要があります。この要件を満たす最も費用対効果の高い方法はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each",
        "text_jp": "各アカウントでAWS Budgetsを設定し、アプリケーション、環境、および所有者でグループ化された予算アラートを構成します。"
      },
      {
        "key": "B",
        "text": "Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application,",
        "text_jp": "組織の管理アカウントでAWS Budgetsを設定し、アプリケーションでグループ化された予算アラートを構成します。"
      },
      {
        "key": "C",
        "text": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each",
        "text_jp": "各アカウントでAWS Budgetsを設定し、アプリケーション、環境、および所有者でグループ化された予算アラートを構成します。"
      },
      {
        "key": "D",
        "text": "Enable AWS Cost and Usage Reports in the organization's management account and configure reports grouped by application, environment.",
        "text_jp": "組織の管理アカウントでAWS Cost and Usage Reportsを有効にし、アプリケーション、環境でグループ化されたレポートを構成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct answer is B: Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application.",
        "situation_analysis": "The requirement is for each business unit to receive monthly spending reports and alerts for expenditures that exceed a threshold. Using AWS Budgets at the management account level enables centralized management for all business units.",
        "option_analysis": "Option B is the best choice as it centralizes budget management while ensuring alerts are grouped properly, as opposed to managing budgets separately in each account as suggested by options A and C.",
        "additional_knowledge": "Understanding AWS Budgets facilitates better financial control over cloud resources.",
        "key_terminology": "AWS Budgets, AWS Organizations, Cost Management",
        "overall_assessment": "Community strongly supports option B, indicating a consensus view that centralized budget management is more effective and efficient."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はB: 組織の管理アカウントでAWS Budgetsを設定し、アプリケーションでグループ化された予算アラートを構成します。",
        "situation_analysis": "各事業部門が月次支出報告書を受け取り、閾値を超える支出についてのアラートを受け取る必要があります。管理アカウントレベルでAWS Budgetsを使用することで、すべての事業部門の集中管理が可能になります。",
        "option_analysis": "オプションBが最良の選択肢であり、予算の中央管理を実現しつつ、正しくグループ化されたアラートを確保します。オプションAおよびCは各アカウントで予算を別々に管理することを提案しています。",
        "additional_knowledge": "AWS Budgetsの理解は、クラウドリソースに対するより良い財務管理を促進します。",
        "key_terminology": "AWS Budgets, AWS Organizations, コスト管理",
        "overall_assessment": "コミュニティはオプションBを強く支持しており、集中管理された予算管理がより効果的かつ効率的であるという共通の見解を示しています。"
      }
    ],
    "keywords": [
      "AWS Budgets",
      "AWS Organizations",
      "Cost Management"
    ]
  },
  {
    "No": "89",
    "question": "A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is\ndeleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.\nHow can the company prevent users from accidentally deleting data in this way?",
    "question_jp": "ある企業はAWS CloudFormationを使用してインフラストラクチャをデプロイしています。企業は、製品のCloudFormationスタックが削除された場合、Amazon RDSデータベースやAmazon EBSボリュームに保存された重要なデータも削除される懸念があります。どのようにすれば、企業はユーザーがこのような方法でデータを誤って削除するのを防ぐことができるでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.",
        "text_jp": "CloudFormationテンプレートを修正し、RDSおよびEBSリソースにDeletionPolicy属性を追加する。"
      },
      {
        "key": "B",
        "text": "Configure a stack policy that disallows the deletion of RDS and EBS resources.",
        "text_jp": "RDSおよびEBSリソースの削除を禁止するスタックポリシーを構成する。"
      },
      {
        "key": "C",
        "text": "Modify IAM policies lo deny deleting RDS and EBS resources that are tagged with an \"aws:cloudformation:stack-name\" tag.",
        "text_jp": "IAMポリシーを修正して、\"aws:cloudformation:stack-name\"タグが付いたRDSおよびEBSリソースの削除を拒否する。"
      },
      {
        "key": "D",
        "text": "Use AWS Config rules to prevent deleting RDS and EBS resources.",
        "text_jp": "AWS Configルールを使用して、RDSおよびEBSリソースの削除を防ぐ。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (85%) B (15%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. By modifying the CloudFormation templates to include a DeletionPolicy attribute for RDS and EBS resources, the resources can be protected from deletion when the CloudFormation stack is deleted.",
        "situation_analysis": "The company wants to prevent the accidental deletion of critical data during the deletion of CloudFormation stacks. Stakeholder concerns about losing RDS and EBS data must be addressed.",
        "option_analysis": "Option A is correct as it directly utilizes CloudFormation's built-in feature to mitigate deletion risks. Option B introduces a stack policy but does not specifically lock down the individual resource properties; it could be less effective if not implemented correctly. Option C can't directly prevent resource deletion due to the lack of direct association with CloudFormation actions. Option D focuses on using AWS Config, but it does not proactively prevent deletion at the stack level.",
        "additional_knowledge": "It's critical to implement resource protection strategies in cloud architecture to ensure data durability.",
        "key_terminology": "CloudFormation, DeletionPolicy, Amazon RDS, Amazon EBS, stack policy, IAM policy",
        "overall_assessment": "The question is highly relevant for operations involving AWS CloudFormation and emphasizes the importance of protecting cloud resources. The expert community validates choice A as the best practice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAである。CloudFormationテンプレートを修正してRDSおよびEBSリソースにDeletionPolicy属性を含めることにより、CloudFormationスタックが削除される際にリソースを保護できる。これは重要な機能である。",
        "situation_analysis": "企業はCloudFormationスタックの削除中に重要なデータが誤って削除されるのを防ぎたい。RDSおよびEBSデータの喪失に対する懸念に対処する必要がある。",
        "option_analysis": "選択肢Aは、直接CloudFormationの機能を利用して削除リスクを軽減するため正しい。選択肢Bはスタックポリシーを導入するが、個々のリソースプロパティを特に制限していないため、実施に誤りがあれば効果が薄くなる可能性がある。選択肢Cは、CloudFormationアクションとの直接的な関連がないため、リソース削除を直接防止することはできない。選択肢DはAWS Configの使用に焦点を当てているが、スタックレベルでの削除を積極的に防ぐことはできない。",
        "additional_knowledge": "クラウドアーキテクチャにおけるリソース保護戦略を実施することは、データの耐久性を確保するために重要である。",
        "key_terminology": "CloudFormation、DeletionPolicy、Amazon RDS、Amazon EBS、スタックポリシー、IAMポリシー",
        "overall_assessment": "この質問はAWS CloudFormationを使用した運用に非常に関係があり、クラウドリソースを保護することの重要性を強調している。専門コミュニティは選択肢Aがベストプラクティスであると検証している。"
      }
    ],
    "keywords": [
      "CloudFormation",
      "DeletionPolicy",
      "Amazon RDS",
      "Amazon EBS",
      "stack policy",
      "IAM policy"
    ]
  },
  {
    "No": "90",
    "question": "A company has VPC fiow logs enabled for Its NAT gateway. The company is seeing Action = ACCEPT for inbound trafic that comes from public IP\naddress 198.51.100.2 destined for a private Amazon EC2 instance.\nA solutions architect must determine whether the trafic represents unsolicited inbound connections from the internet. The first two octets of the\nVPC CIDR block are 203.0.\nWhich set of steps should the solutions architect take to meet these requirements?",
    "question_jp": "ある企業がNATゲートウェイに対してVPCフローログを有効にしています。この企業は、198.51.100.2というパブリックIPアドレスからプライベートAmazon EC2インスタンスに送信される受信トラフィックのアクションがACCEPTであることを確認しています。ソリューションアーキテクトは、このトラフィックがインターネットからの不要な受信接続であるかどうかを判断しなければなりません。VPC CIDRブロックの最初の2オクテットは203.0です。これらの要件を満たすために、ソリューションアーキテクトはどの一連の手順を踏むべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's",
        "text_jp": "AWS CloudTrailコンソールを開き、NATゲートウェイのエラスティックネットワークインターフェイスとプライベートインスタンスを含むロググループを選択します。"
      },
      {
        "key": "B",
        "text": "Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private",
        "text_jp": "Amazon CloudWatchコンソールを開き、NATゲートウェイのエラスティックネットワークインターフェイスとプライベートインスタンスを含むロググループを選択します。"
      },
      {
        "key": "C",
        "text": "Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's",
        "text_jp": "AWS CloudTrailコンソールを開き、NATゲートウェイのエラスティックネットワークインターフェイスとプライベートインスタンスを含むロググループを選択します。"
      },
      {
        "key": "D",
        "text": "Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private",
        "text_jp": "Amazon CloudWatchコンソールを開き、NATゲートウェイのエラスティックネットワークインターフェイスとプライベートインスタンスを含むロググループを選択します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (66%) D (34%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Accessing the Amazon CloudWatch console allows examination of metrics and logs associated with the NAT gateway which can help in determining if the incoming traffic is unsolicited.",
        "situation_analysis": "The company is receiving ACCEPT action log entries for inbound traffic from a public IP. This indicates potential unsolicited traffic that needs further investigation.",
        "option_analysis": "Option D allows the solutions architect to access relevant logs from CloudWatch, essential for analyzing traffic patterns. Options A and C (CloudTrail) are less effective for real-time traffic analysis compared to CloudWatch.",
        "additional_knowledge": "It’s important to review any potential security groups and network ACLs that could affect the traffic as well.",
        "key_terminology": "Amazon CloudWatch, NAT gateway, inbound traffic, VPC flow logs, unsolicited connections",
        "overall_assessment": "The community vote suggests a preference for CloudWatch over CloudTrail for this scenario because CloudWatch is specifically designed for monitoring and logging resource utilization and performance."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。Amazon CloudWatchコンソールにアクセスすることで、NATゲートウェイに関連するメトリクスとログを調査でき、受信トラフィックが不要であるかどうかを判断するのに役立ちます。",
        "situation_analysis": "企業は、パブリックIPからの受信トラフィックに対してACCEPTアクションのログエントリを受け取っています。これは、さらなる調査が必要な潜在的な不要トラフィックを示しています。",
        "option_analysis": "選択肢Dは、ソリューションアーキテクトがトラフィックパターンを分析するために必要なCloudWatchの関連ログにアクセスできるようにします。選択肢AおよびC（CloudTrail）は、CloudWatchと比較してリアルタイムのトラフィック分析には効果的ではありません。",
        "additional_knowledge": "トラフィックに影響する可能性のあるセキュリティグループやネットワークACLを確認することも重要です。",
        "key_terminology": "Amazon CloudWatch, NATゲートウェイ, 受信トラフィック, VPCフローログ, 不要な接続",
        "overall_assessment": "コミュニティの投票は、この状況においてCloudTrailよりもCloudWatchを支持しており、CloudWatchがリソースの利用状況とパフォーマンスを監視およびログするために特に設計されているためです。"
      }
    ],
    "keywords": [
      "Amazon CloudWatch",
      "NAT gateway",
      "inbound traffic",
      "VPC flow logs",
      "unsolicited connections"
    ]
  },
  {
    "No": "91",
    "question": "A company consists or two separate business units. Each business unit has its own AWS account within a single organization in AWS\nOrganizations. The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3\nbucket in each account and configured low-way replication between the S3 buckets. The S3 buckets have millions of objects.\nRecently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be\nstored with encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).\nWhat is the MOST operationally eficient solution that meets these requirements?",
    "question_jp": "ある会社は二つの独立した事業部門で構成されている。それぞれの事業部門は、AWS Organizations内の単一の組織内で独自のAWSアカウントを持っている。事業部門は、定期的に敏感な文書を互いに共有している。共有を促進するために、会社は各アカウントにAmazon S3バケットを作成し、S3バケット間で低帯域幅レプリケーションを構成した。S3バケットには数百万のオブジェクトがある。最近のセキュリティ監査で、いずれのS3バケットにも静止中の暗号化が有効になっていないことが確認された。会社の方針では、すべての文書は静止中に暗号化されて保存される必要がある。会社は、Amazon S3管理の暗号化キー(SSE-S3)を使用したサーバーサイドの暗号化を実装したい。これらの要件を満たすために、最も運用効率の高いソリューションは何か？",
    "choices": [
      {
        "key": "A",
        "text": "Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location.",
        "text_jp": "両方のS3バケットでSSE-S3を有効にし、S3バッチ操作を使用して、同じ場所にオブジェクトをコピーして暗号化する。"
      },
      {
        "key": "B",
        "text": "Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on",
        "text_jp": "各アカウントでAWSキー管理サービス(AWS KMS)キーを作成し、AWS KMSキー(SSE-KMS)を使用したサーバーサイドの暗号化を有効にする。"
      },
      {
        "key": "C",
        "text": "Turn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI.",
        "text_jp": "両方のS3バケットでSSE-S3を有効にし、AWS CLIのS3コピーコマンドを使用して既存のオブジェクトを暗号化する。"
      },
      {
        "key": "D",
        "text": "Create an AWS Key Management Service, (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on",
        "text_jp": "AWSキー管理サービス(AWS KMS)キーを各アカウントに作成し、AWS KMSキー(SSE-KMS)を使用したサーバーサイドの暗号化を有効にする。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (85%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Turn on SSE-S3 on both S3 buckets and use the S3 copy command to encrypt existing objects. This is the most operationally efficient solution because it directly uses S3's built-in features without needing to manage additional KMS keys.",
        "situation_analysis": "The requirement is to enable encryption at rest for all documents in S3 buckets. The solution needs to effectively manage millions of objects while maintaining operational efficiency.",
        "option_analysis": "Option A is operationally complex as it involves batch processing and potential downtime. Option B and D require managing KMS keys which adds administrative overhead. Option C effectively meets the requirements with minimal effort.",
        "additional_knowledge": "Utilizing existing S3 copy functionality to encrypt data avoids the need for additional resources.",
        "key_terminology": "SSE-S3, S3 Batch Operations, Encryption at Rest, AWS CLI",
        "overall_assessment": "Option C is highly preferred as it aligns with best practices for simplicity and efficiency. Although community votes favor option A, option C remains the best technically according to AWS capabilities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はC: 両方のS3バケットでSSE-S3を有効にし、AWS CLIのS3コピーコマンドを使用して既存のオブジェクトを暗号化することである。これは、追加のKMSキーを管理する必要がなく、S3の組み込み機能を直接利用できるため、最も運用効率の高い解決策である。",
        "situation_analysis": "要件は、S3バケット内のすべての文書に対して静止中の暗号化を有効にすることである。解決策は、何百万ものオブジェクトを持ちながら、運用効率を維持する必要がある。",
        "option_analysis": "選択肢Aはバッチ処理と潜在的なダウンタイムを含むため、運用上の複雑さがある。選択肢BとDはKMSキーを管理する必要があり、管理オーバーヘッドが加わる。選択肢Cは、最小限の努力で要件を効果的に満たしている。",
        "additional_knowledge": "データを暗号化するために既存のS3のコピー機能を利用することで、追加のリソースの必要がなくなる。",
        "key_terminology": "SSE-S3, S3バッチ操作, 静止中の暗号化, AWS CLI",
        "overall_assessment": "選択肢Cは、シンプルさと効率の観点からベストプラクティスに沿っているため、高く評価される。コミュニティ投票は選択肢Aを好んでいるが、AWSの機能に基づく限り、選択肢Cが技術的に最も優れている。"
      }
    ],
    "keywords": [
      "SSE-S3",
      "S3 Batch Operations",
      "Encryption at Rest",
      "AWS CLI"
    ]
  },
  {
    "No": "92",
    "question": "A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3\nbucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes\nevery day.\nThe company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must\nretain all the data indefinitely for compliance reasons.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業がAWSクラウドでアプリケーションを運用しています。このアプリケーションはAmazon S3バケットに大量の非構造化データを収集・保存しています。S3バケットには数テラバイトのデータが含まれており、S3 Standardストレージクラスを使用しています。データは毎日数ギガバイトずつ増加しています。企業はデータをクエリし分析する必要がありますが、1年を超える古いデータにはアクセスしません。ただし、コンプライアンスの理由からすべてのデータを無期限に保持する必要があります。これらの要件を最もコスト効率よく満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
        "text_jp": "S3 Selectを使用してデータをクエリします。1年以上古いデータをS3 Glacier Deep Archiveに移行するS3ライフサイクルポリシーを作成します。"
      },
      {
        "key": "B",
        "text": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old 10 S3 Glacier",
        "text_jp": "Amazon Redshift Spectrumを使用してデータをクエリします。1年以上古いデータをS3 Glacierに移行するS3ライフサイクルポリシーを作成します。"
      },
      {
        "key": "C",
        "text": "Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1",
        "text_jp": "AWS GlueデータカタログとAmazon Athenaを使用してデータをクエリします。1年以上古いデータを移行するS3ライフサイクルポリシーを作成します。"
      },
      {
        "key": "D",
        "text": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3",
        "text_jp": "Amazon Redshift Spectrumを使用してデータをクエリします。1年以上古いデータをS3に移行するS3ライフサイクルポリシーを作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (87%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. S3 Select allows for efficient querying of data directly from S3, while transitioning older, infrequently accessed data to S3 Glacier Deep Archive minimizes costs.",
        "situation_analysis": "The company has a large amount of unstructured data in S3 that is increasing daily, needs to retain all data indefinitely for compliance, and does not access data older than 1 year.",
        "option_analysis": "Option A is the most cost-effective as it utilizes S3 Select and reduces retention costs by using Glacier Deep Archive for older data. Options B and D suggest transitioning data to S3 Glacier, which is more expensive than Glacier Deep Archive. Option C does not specify the lifecycle action for older data.",
        "additional_knowledge": "It's vital to note the distinctions between S3 Glacier and S3 Glacier Deep Archive, especially in a cost-focused analysis.",
        "key_terminology": "S3 Select, S3 Lifecycle policies, S3 Glacier Deep Archive",
        "overall_assessment": "While community votes heavily favor C, Option A is the best choice based on cost-efficiency and compliance needs. The misunderstanding might arise from a focus on using Athena versus Glacier costs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。S3 SelectはS3から直接データを効率的にクエリでき、古いアクセスの少ないデータをS3 Glacier Deep Archiveに移行することでコストを最小限に抑えることができます。",
        "situation_analysis": "企業は毎日増加する大量の非構造化データをS3に持ち、すべてのデータをコンプライアンスのために無期限に保持する必要があり、1年を超える古いデータにはアクセスしません。",
        "option_analysis": "選択肢AはS3 Selectを利用しており、古いデータをS3 Glacier Deep Archiveに移行することでコストを削減しています。選択肢BとDはデータをS3 Glacierに移行することを提案しており、これはDeep Archiveよりもコストが高くなります。選択肢Cは古いデータのライフサイクルアクションを具体的に述べていません。",
        "additional_knowledge": "S3 GlacierとS3 Glacier Deep Archiveの違いを理解することが、コスト重視の分析において重要です。",
        "key_terminology": "S3 Select, S3ライフサイクルポリシー, S3 Glacier Deep Archive",
        "overall_assessment": "コミュニティの投票はCに偏っていますが、コスト効率とコンプライアンスニーズに基づけば選択肢Aが最良の選択です。誤解はAthenaの使用やGlacierコストに焦点が当たっていることから生じているかもしれません。"
      }
    ],
    "keywords": [
      "S3 Select",
      "S3 Lifecycle policies",
      "S3 Glacier Deep Archive"
    ]
  },
  {
    "No": "93",
    "question": "A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of\nfiles in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises\nfor ML experiments and wants to use AWS.\nThe company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be\nencrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps. and multiple departments share the\nconnection.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ビデオ処理会社は、600 TBの圧縮データを使用して機械学習（ML）モデルを構築したいと考えています。このデータは数千のファイルとして会社のオンプレミスのネットワーク接続ストレージシステムに保存されています。会社は、ML実験に必要な計算リソースをオンプレミスで持っておらず、AWSを利用したいと考えています。会社は、データをAWSに転送するのに3週間以内で完了する必要があります。データ転送は一時的なもので、転送中にデータは暗号化される必要があります。会社のインターネット接続の測定されたアップロード速度は100 Mbpsであり、複数の部門がその接続を共有しています。この要件を最もコスト効果的に満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a",
        "text_jp": "AWS Management Consoleを使用して、複数のAWS Snowball Edge Storage Optimizedデバイスを注文します。デバイスを構成します。"
      },
      {
        "key": "B",
        "text": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN",
        "text_jp": "会社の場所と最寄りのAWSリージョンとの間に10 GbpsのAWS Direct Connect接続を設定します。VPN経由でデータを転送します。"
      },
      {
        "key": "C",
        "text": "Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN",
        "text_jp": "オンプレミスのネットワーク接続ストレージと最寄りのAWSリージョンとの間にVPN接続を作成します。VPN経由でデータを転送します。"
      },
      {
        "key": "D",
        "text": "Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file",
        "text_jp": "オンプレミスにAWS Storage Gatewayファイルゲートウェイを展開します。ファイルゲートウェイをS3バケットを宛先とするように構成します。データをファイルにコピーします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. AWS Snowball Edge is designed for large data transfers, enabling the transfer of 600 TB to AWS effectively.",
        "situation_analysis": "The company needs to transfer 600 TB of data within 3 weeks, which requires a high-speed and reliable transfer method to meet the timeline.",
        "option_analysis": "Option A is the best choice as it utilizes AWS Snowball Edge, which is specifically designed for such bulk data migrations. Options B and C require significant infrastructure setup that might not ensure completion in the given time frame, while option D involves additional configuration and might not meet the urgency.",
        "additional_knowledge": "Using Snowball also eliminates reliance on the limited bandwidth of the company’s internet connection.",
        "key_terminology": "AWS Snowball Edge, data encryption, data transfer, on-premises storage, ML model training.",
        "overall_assessment": "This question emphasizes the importance of selecting efficient data transfer solutions based on conditions such as speed, encryption, and cost. The community overwhelmingly supports option A as it directly addresses all requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。AWS Snowball Edgeは、大規模データ転送用に設計されており、600 TBをAWSに効果的に転送できます。",
        "situation_analysis": "会社は、3週間以内に600 TBのデータを転送する必要があり、タイムラインを満たすためには、高速かつ信頼性のある転送方法が必要です。",
        "option_analysis": "選択肢Aは、AWS Snowball Edgeを利用しているため、バルクデータ移行に最適です。選択肢BとCは、インフラの大規模なセットアップを要するため、指定されたタイムフレーム内での完了が保証されない可能性があります。一方、選択肢Dは追加設定が必要で、緊急性を満たさないかもしれません。",
        "additional_knowledge": "Snowballを使用することで、会社のインターネット接続の限られた帯域幅に依存する必要がなくなります。",
        "key_terminology": "AWS Snowball Edge, データ暗号化, データ転送, オンプレミスストレージ, MLモデル訓練。",
        "overall_assessment": "この質問は、速度、暗号化、コストなどの条件に基づいて効率的なデータ転送ソリューションを選択することの重要性を強調しています。コミュニティはA選択肢を全面的に支持しており、すべての要件に直接対処しています。"
      }
    ],
    "keywords": [
      "AWS Snowball Edge",
      "data encryption",
      "data transfer",
      "on-premises storage",
      "ML model training"
    ]
  },
  {
    "No": "94",
    "question": "A company has migrated Its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files\nthrough a web application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on\nAmazon EC2 instances and an Amazon RDS for PostgreSQL database.\nWhen forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team\nmember then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before entering\nthe information into another system that uses an API.\nA solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction. minimize time\nto market, and minimize tong-term operational overhead.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、フォーム処理アプリケーションをAWSに移行しました。ユーザーがアプリケーションとやり取りをする際、スキャンしたフォームをファイルとしてウェブアプリケーションを通じてアップロードします。データベースはユーザーメタデータと、Amazon S3に保存されたファイルへの参照を格納します。ウェブアプリケーションはAmazon EC2インスタンスおよびAmazon RDS for PostgreSQLデータベース上で実行されます。\nフォームがアップロードされると、アプリケーションはAmazon Simple Notification Service (Amazon SNS)を介してチームに通知を送信します。チームメンバーはログインし、各フォームを処理します。チームメンバーはフォームのデータ検証を行い、関連データを抽出して情報をAPIを使用する別のシステムに入力します。\nソリューションアーキテクトは、フォームの手動処理を自動化する必要があります。ソリューションは正確なフォーム抽出を提供し、市場投入までの時間を最小限に抑え、長期的な運用オーバーヘッドを最小限に抑える必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Develop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes",
        "text_jp": "フォーム上で光学文字認識 (OCR) を行うカスタムライブラリを開発し、それをAmazon Elastic Kubernetesにデプロイする。"
      },
      {
        "key": "B",
        "text": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence",
        "text_jp": "AWS Step FunctionsとAWS Lambdaを使用するアプリケーション層をシステムに拡張します。この層を構成して人工知能を使用する。"
      },
      {
        "key": "C",
        "text": "Host a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine teaming (AI/ML)",
        "text_jp": "EC2インスタンスで新しいアプリケーション層をホストします。この層を使用して人工知能と機械学習 (AI/ML) をホストするエンドポイントを呼び出す。"
      },
      {
        "key": "D",
        "text": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and",
        "text_jp": "AWS Step FunctionsとAWS Lambdaを使用するアプリケーション層をシステムに拡張します。この層を構成してAmazon Textractを使用します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Extend the system with an application tier that uses AWS Step Functions and AWS Lambda, and configure this tier to use Amazon Textract.",
        "situation_analysis": "The current system requires automation of manual processing of forms while ensuring accuracy and low operational overhead.",
        "option_analysis": "Option D is valid as it leverages AWS Step Functions and AWS Lambda for orchestration and serverless execution, coupled with Amazon Textract for form data extraction, which is optimized for such tasks.",
        "additional_knowledge": "Options A, B, and C do not effectively integrate existing AWS services specifically designed for document processing.",
        "key_terminology": "AWS Step Functions, AWS Lambda, Amazon Textract, automation, OCR",
        "overall_assessment": "This solution offers the most comprehensive and efficient means of automating form processing. Community support indicates strong consensus on this being the best choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです：AWS Step FunctionsとAWS Lambdaを使用するアプリケーション層をシステムに拡張し、この層を構成してAmazon Textractを使用します。",
        "situation_analysis": "現在のシステムは、正確さと低い運用オーバーヘッドを確保しつつ、フォームの手動処理を自動化する必要があります。",
        "option_analysis": "選択肢Dは、AWS Step FunctionsとAWS Lambdaを用いたオーケストレーションとサーバーレス実行を活用し、Amazon Textractによるフォームデータ抽出を組み合わせているため、有効です。",
        "additional_knowledge": "選択肢A、B、およびCは、ドキュメント処理に特化した既存のAWSサービスを効果的に統合していません。",
        "key_terminology": "AWS Step Functions, AWS Lambda, Amazon Textract, 自動化, OCR",
        "overall_assessment": "このソリューションは、フォーム処理を自動化するための最も包括的かつ効率的な手段を提供します。コミュニティの支持も、この選択が最良であるという強い合意を示しています。"
      }
    ],
    "keywords": [
      "AWS Step Functions",
      "AWS Lambda",
      "Amazon Textract",
      "automation",
      "OCR"
    ]
  },
  {
    "No": "95",
    "question": "A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a\nfieet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the\norders. The company does not want to make any major changes to the application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業が自社のオンプレミスのオーダー処理プラットフォームをAWSクラウドにリファクタリングしています。このプラットフォームには、VMの群にホストされたWebフロントエンド、フロントエンドとバックエンドを接続するためのRabbitMQ、及びコンテナ化されたバックエンドシステムを稼働させて注文を処理するためのKubernetesクラスターが含まれています。企業はアプリケーションに大きな変更を加えたくありません。この要件を最も運用負荷が少なく満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up",
        "text_jp": "WebサーバーVMのAMIを作成します。AMIを使用するAmazon EC2 Auto Scalingグループとアプリケーションロードバランサーを作成します。設定します"
      },
      {
        "key": "B",
        "text": "Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end",
        "text_jp": "Webサーバー環境を模倣するカスタムAWS Lambdaランタイムを作成します。フロントエンドを置き換えるためのAmazon API Gateway APIを作成します"
      },
      {
        "key": "C",
        "text": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up",
        "text_jp": "WebサーバーVMのAMIを作成します。AMIを使用するAmazon EC2 Auto Scalingグループとアプリケーションロードバランサーを作成します。設定します"
      },
      {
        "key": "D",
        "text": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up",
        "text_jp": "WebサーバーVMのAMIを作成します。AMIを使用するAmazon EC2 Auto Scalingグループとアプリケーションロードバランサーを作成します。設定します"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This solution minimizes operational overhead while maintaining application functionality by using existing VMs and leveraging EC2 Auto Scaling.",
        "situation_analysis": "The company is looking to refactor its existing order-processing application in AWS with the least amount of changes.",
        "option_analysis": "Option A provides a seamless migration using AMIs and Auto Scaling while utilizing an Application Load Balancer for traffic distribution. Options B, C, and D may introduce complexity or do not properly utilize existing resources.",
        "additional_knowledge": "Choosing an AMI ensures that the existing environment can be replicated quickly and easily without substantial development effort.",
        "key_terminology": "Amazon EC2, Auto Scaling, AMI, Application Load Balancer",
        "overall_assessment": "Option A is validated to be the best approach by community votes and by aligning with AWS best practices for minimal change. The high community support suggests a strong consensus on its effectiveness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。このソリューションは、既存のVMを使用し、EC2オートスケーリングを活用することで、運用負荷を最小限に抑えながらアプリケーションの機能性を維持する。",
        "situation_analysis": "企業は、最小限の変更で既存のオーダー処理アプリケーションをAWSにリファクタリングしたいと考えている。",
        "option_analysis": "選択肢Aは、AMIとオートスケーリングを利用し、トラフィック分散のためにアプリケーションロードバランサーを活用することでシームレスな移行を提供する。選択肢B、C、およびDは、既存リソースを適切に活用していないか、複雑さをもたらす可能性がある。",
        "additional_knowledge": "AMIを選択することで、既存の環境を迅速かつ容易に再現でき、大幅な開発労力なしで済む。",
        "key_terminology": "Amazon EC2、Auto Scaling、AMI、アプリケーションロードバランサー",
        "overall_assessment": "選択肢Aはコミュニティ投票によって最良のアプローチとされ、AWSのベストプラクティスに沿った最小限の変更を提案している。コミュニティの支持が高いことから、その効果について強いコンセンサスがあることが示されている。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Auto Scaling",
      "AMI",
      "Application Load Balancer"
    ]
  },
  {
    "No": "96",
    "question": "A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The\nsolutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.\nThe solutions architect created the following IAM policy and attached it to an IAM role:\nDuring tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object\nresulted in an error message. The error message stated that the action was forbidden.\nWhich action must the solutions architect add to the IAM policy to meet all the requirements?",
    "question_jp": "ソリューションアーキテクトは、新しいAmazon S3バケットに格納されるオブジェクトのために、クライアント側の暗号化メカニズムを実装する必要があります。ソリューションアーキテクトは、この目的のためにAWS Key Management Service（AWS KMS）に保存されるCMK（カスタマーマスターキー）を作成しました。ソリューションアーキテクトは、以下のIAMポリシーを作成し、IAMロールにアタッチしました。テスト中、ソリューションアーキテクトはS3バケット内の既存のテストオブジェクトを正常に取得することができました。しかし、新しいオブジェクトをアップロードしようとしたところ、エラーメッセージが表示され、アクションが禁止されていることが示されました。ソリューションアーキテクトは、すべての要件を満たすためにIAMポリシーにどのアクションを追加する必要がありますか？",
    "choices": [
      {
        "key": "A",
        "text": "kms:GenerateDataKey",
        "text_jp": "kms:GenerateDataKey"
      },
      {
        "key": "B",
        "text": "kms:GetKeyPolicy",
        "text_jp": "kms:GetKeyPolicy"
      },
      {
        "key": "C",
        "text": "kms:GetPublicKey",
        "text_jp": "kms:GetPublicKey"
      },
      {
        "key": "D",
        "text": "kms:Sign",
        "text_jp": "kms:Sign"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [
      "image_55_0.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: kms:GenerateDataKey. This action allows the solution architect to generate a data encryption key that can be used to encrypt the object before it is uploaded to S3.",
        "situation_analysis": "The requirements indicate that client-side encryption is needed, where a key is generated to encrypt objects before they are uploaded. The existing permissions allowed for retrieving objects but did not include permissions for generating encryption keys.",
        "option_analysis": "Option A (kms:GenerateDataKey) is the correct answer as it provides the necessary permissions to generate a data key. Options B, C, and D are not relevant for uploading encrypted objects.",
        "additional_knowledge": "Selecting proper IAM policies is crucial to ensure secure access to AWS services and resources.",
        "key_terminology": "AWS KMS, client-side encryption, IAM policy, data encryption key, encryption keys.",
        "overall_assessment": "Given the community vote distribution showing 100% for option A, it is evident that it aligns with best practices for enabling client-side encryption for objects in S3. No discrepancies were noted."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はA: kms:GenerateDataKeyである。このアクションにより、ソリューションアーキテクトはオブジェクトをS3にアップロードする前に暗号化するために使用できるデータ暗号化キーを生成することができる。",
        "situation_analysis": "要求は、オブジェクトをアップロードする前に暗号化キーを生成する必要があるクライアント側の暗号化が必要であることを示している。既存の権限はオブジェクトの取得を許可しているが、暗号化キーの生成のための権限が含まれていなかった。",
        "option_analysis": "オプションA（kms:GenerateDataKey）は、データキーを生成するために必要な権限を提供するため、正しい回答である。オプションB、C、Dは、暗号化されたオブジェクトのアップロードには関連しない。",
        "additional_knowledge": "適切なIAMポリシーを選択することは、AWSサービスやリソースへの安全なアクセスを確保するために重要である。",
        "key_terminology": "AWS KMS, クライアント側暗号化, IAMポリシー, データ暗号化キー, 暗号化キー。",
        "overall_assessment": "コミュニティ投票の分布がオプションAに100%集中していることから、S3のオブジェクトに対するクライアント側の暗号化を有効にするためのベストプラクティスに一致していることが明らかである。矛盾点は見られなかった。"
      }
    ],
    "keywords": [
      "AWS KMS",
      "client-side encryption",
      "IAM policy",
      "data encryption key",
      "encryption keys"
    ]
  },
  {
    "No": "97",
    "question": "A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application\nLoad Balancer. The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not\nadversely affect legitimate trafic to the application.\nHow should a solutions architect configure the web ACLs to meet these requirements?",
    "question_jp": "ある企業がウェブアプリケーションを開発しました。企業は、アプリケーションをアプリケーションロードバランサーの背後にある一群のAmazon EC2インスタンスでホストしています。企業はアプリケーションのセキュリティ態勢を改善したいと考えており、AWS WAFウェブACLを使用する計画です。このソリューションは、アプリケーションへの正当なトラフィックに対して悪影響を及ぼすべきではありません。どのようにソリューションアーキテクトはWeb ACLを設定してこれらの要件に応えるべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid",
        "text_jp": "Web ACLルールのアクションをカウントに設定します。AWS WAFロギングを有効にします。誤検知についてリクエストを分析します。ルールを修正して回避します。"
      },
      {
        "key": "B",
        "text": "Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the",
        "text_jp": "Web ACLにレートベースのルールのみを使用し、スロットル制限をできるだけ高く設定します。制限を超えるすべてのリクエストを一時的にブロックします。"
      },
      {
        "key": "C",
        "text": "Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using",
        "text_jp": "Web ACLルールのアクションをブロックに設定します。Web ACLにAWS管理ルールグループのみを使用します。ルールグループを評価します。"
      },
      {
        "key": "D",
        "text": "Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false",
        "text_jp": "Web ACLにカスタムルールグループのみを使用し、アクションを許可に設定します。AWS WAFロギングを有効にします。誤検知についてリクエストを分析します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Setting the action of the web ACL rules to Count allows the architect to monitor traffic patterns without blocking legitimate users.",
        "situation_analysis": "The company wants to enhance security but must avoid disrupting legitimate traffic. Monitoring is crucial before applying any blocking rules.",
        "option_analysis": "Option A is beneficial as it helps identify false positives. Option B may lead to blocking legitimate traffic. Option C involves a complete block which is too strict. Option D does not analyze blocking effects before implementing.",
        "additional_knowledge": "Regularly reviewing the gathered data is key to refining rules and improving the security posture.",
        "key_terminology": "AWS WAF, web ACL, false positives, logging, traffic monitoring.",
        "overall_assessment": "Option A provides a balanced approach to enhancing security while ensuring legitimate users are not impacted. It has community support with 100% agreement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。Web ACLルールのアクションをカウントに設定することで、正当なユーザーをブロックせずにトラフィックパターンを監視できる。",
        "situation_analysis": "企業はセキュリティを強化したいが、正当なトラフィックを妨げることは避けなければならない。ブロッキングルールを適用する前に監視が重要である。",
        "option_analysis": "選択肢Aは、誤検知を特定するのに役立つため有益である。選択肢Bは、正当なトラフィックをブロックする可能性がある。選択肢Cは、完全にブロックするため厳しすぎる。選択肢Dは実装の前にブロッキング効果を解析しない。",
        "additional_knowledge": "収集したデータを定期的にレビューすることが、ルールを精緻化し、セキュリティ態勢を改善するための鍵となる。",
        "key_terminology": "AWS WAF, web ACL, 誤検知, ロギング, トラフィック監視。",
        "overall_assessment": "選択肢Aは、正当なユーザーに影響を与えずにセキュリティを強化するバランスの取れたアプローチを提供する。コミュニティのサポートがあり、100％の合意がある。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "web ACL",
      "false positives",
      "logging",
      "traffic monitoring"
    ]
  },
  {
    "No": "98",
    "question": "A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company\nmanages common security group rules for the AWS accounts in the organization.\nThe company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company's on-premises\nnetwork. Developers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own\nAWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.\nThe solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "question_jp": "企業にはAWS Organizations内に多くのAWSアカウントを持つ組織があります。ソリューションアーキテクトは、企業がこの組織内のAWSアカウントに対して共通のセキュリティグループルールを管理する方法を改善しなければなりません。企業は、社内ネットワークへのアクセスを許可するために、各AWSアカウントにおいて許可リストに共通のIP CIDR範囲のセットを持っています。各アカウント内の開発者は、自分たちのセキュリティグループに新しいIP CIDR範囲を追加する責任を持ちます。セキュリティチームは独自のAWSアカウントを持っており、現在、セキュリティチームは他のAWSアカウントの所有者に許可リストの変更が行われた際に通知しています。ソリューションアーキテクトは、すべてのアカウントに共通のCIDR範囲を配布するソリューションを設計しなければなりません。どのソリューションが最小限の運用負荷でこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS account. Deploy an AWS Lambda function in",
        "text_jp": "セキュリティチームのAWSアカウントにAmazon Simple Notification Service (Amazon SNS)トピックをセットアップします。AWS Lambda関数をデプロイします。"
      },
      {
        "key": "B",
        "text": "Create new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all",
        "text_jp": "組織内の各AWSアカウントに新しい顧客管理プレフィックスリストを作成します。各アカウントのプレフィックスリストにすべてを追加します。"
      },
      {
        "key": "C",
        "text": "Create a new customer-managed prefix list in the security team's AWS account. Populate the customer-managed prefix list with all internal",
        "text_jp": "セキュリティチームのAWSアカウントに新しい顧客管理プレフィックスリストを作成します。顧客管理プレフィックスリストにすべての内部を追加します。"
      },
      {
        "key": "D",
        "text": "Create an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in",
        "text_jp": "組織内の各アカウントにIAMロールを作成します。セキュリティグループを更新する権限を付与します。AWS Lambda関数をデプロイします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (84%) D (16%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, creating a new customer-managed prefix list in the security team's AWS account.",
        "situation_analysis": "The organization has multiple AWS accounts with a common set of security group rules that rely on CIDR ranges. The current management of IP address ranges is inefficient and requires better coordination.",
        "option_analysis": "Option C allows for centralized management of CIDR ranges, reducing operational overhead since updates will only need to be made in one account. Option A requires managing SNS notifications, which does not directly manage CIDR ranges. Option B requires updates in every account, increasing overhead. Option D implies an additional role management overhead without resolving the CIDR sharing problem.",
        "additional_knowledge": "Using prefix lists is linked to better governance of resources and compliance across multiple accounts.",
        "key_terminology": "AWS Organizations, Customer-managed Prefix List, CIDR Blocks, Security Groups, IAM Roles",
        "overall_assessment": "C is the most efficient option as it streamlines management and ensures consistency across accounts. The community vote supports this option due to its effectiveness and reduced operational needs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCであり、セキュリティチームのAWSアカウントに新しい顧客管理プレフィックスリストを作成することです。",
        "situation_analysis": "この組織には、CIDR範囲に依存する共通のセキュリティグループルールを持つ複数のAWSアカウントがあります。現在のIPアドレス範囲の管理は効率的でなく、より良い調整が必要です。",
        "option_analysis": "Cの選択肢はCIDR範囲の中央管理を可能にし、更新を1つのアカウントでのみ行えるため運用負荷が軽減されます。Aの選択肢はSNS通知の管理が必要であり、CIDR範囲の直接管理にはつながりません。Bの選択肢はすべてのアカウントでの更新を必要とし、負荷が増えます。Dの選択肢は国際的な役割管理の負荷を追加することになり、CIDR共有の問題を解決しません。",
        "additional_knowledge": "プレフィックスリストの使用は、複数アカウントにまたがるリソースのより良いガバナンスとコンプライアンスに結びついています。",
        "key_terminology": "AWS Organizations, 顧客管理プレフィックスリスト, CIDRブロック, セキュリティグループ, IAMロール",
        "overall_assessment": "Cが最も効率的な選択肢であり、管理を簡素化し、アカウント間の一貫性を確保します。コミュニティ投票もその効果と運用ニーズの削減からこの選択肢を支持しています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Customer-managed Prefix List",
      "CIDR Blocks",
      "Security Groups",
      "IAM Roles"
    ]
  },
  {
    "No": "99",
    "question": "A company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a VPN. The company is\nhosting internal applications with VPCs in multiple AWS accounts. Currently, the applications are accessible from the company's on-premises\nofice network through an AWS Site-to-Site VPN connection. The VPC in the company's main AWS account has peering connections established\nwith VPCs in other AWS accounts.\nA solutions architect must design a scalable AWS Client VPN solution for employees to use while they work from home.\nWhat is the MOST cost-effective solution that meets these requirements?",
    "question_jp": "ある企業は、従業員が自宅からリモートワークを行うことを可能にする新たなポリシーを導入しました。従業員はVPNを使用して接続する必要があります。この企業は複数のAWSアカウントにVPCを持つ内部アプリケーションをホストしています。現在、これらのアプリケーションはAWS Site-to-Site VPN接続を通じて、企業のオンプレミスオフィスネットワークからアクセス可能です。企業のメインAWSアカウントのVPCは、他のAWSアカウントのVPCとピアリング接続を確立しています。ソリューションアーキテクトは、従業員が自宅で作業する際に利用できるスケーラブルなAWS Client VPNソリューションを設計する必要があります。この要件を満たす最もコスト効果の高いソリューションは何でしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a Client VPN endpoint in each AWS account. Configure required routing that allows access to internal applications.",
        "text_jp": "各AWSアカウントにClient VPNエンドポイントを作成します。内部アプリケーションへのアクセスを許可するために必要なルーティングを設定します。"
      },
      {
        "key": "B",
        "text": "Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications.",
        "text_jp": "メインAWSアカウントにClient VPNエンドポイントを作成します。内部アプリケーションへのアクセスを許可するために必要なルーティングを設定します。"
      },
      {
        "key": "C",
        "text": "Create a Client VPN endpoint in the main AWS account. Provision a transit gateway that is connected to each AWS account. Configure",
        "text_jp": "メインAWSアカウントにClient VPNエンドポイントを作成します。各AWSアカウントに接続されたトランジットゲートウェイをプロビジョニングします。"
      },
      {
        "key": "D",
        "text": "Create a Client VPN endpoint in the main AWS account. Establish connectivity between the Client VPN endpoint and the AWS Site-to-Site",
        "text_jp": "メインAWSアカウントにClient VPNエンドポイントを作成します。Client VPNエンドポイントとAWS Site-to-Siteの間に接続性を確立します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (66%) C (34%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Create a Client VPN endpoint in the main AWS account and configure routing.",
        "situation_analysis": "The company uses multiple AWS accounts with VPCs that require secure access for remote employees. The main account already has peering connections established with other accounts, making it strategically placed for a centralized VPN solution.",
        "option_analysis": "Option B is the most cost-effective as it requires a single Client VPN endpoint in the main account, whereas Option A requires multiple endpoints, increasing costs. Option C adds unnecessary complexity with a transit gateway when a single endpoint suffices. Option D also adds unnecessary connections that are not needed.",
        "additional_knowledge": "Keeping routing configurations minimal simplifies future changes and scalability as more accounts may be added or accessed.",
        "key_terminology": "Client VPN, VPC Peering, Routing, AWS Site-to-Site VPN, Transit Gateway",
        "overall_assessment": "Answer B fits the requirements best. It minimizes costs and simplifies management while providing the necessary functionality. The community vote largely supports this choice, indicating it aligns with desired practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです：メインAWSアカウントにClient VPNエンドポイントを作成し、ルーティングを設定します。",
        "situation_analysis": "この企業は、リモート従業員向けの安全なアクセスを必要とする複数のAWSアカウントとVPCを使用しています。メインアカウントには他のアカウントとピアリング接続がすでに確立されており、中央集権的なVPNソリューションに戦略的に配置されています。",
        "option_analysis": "オプションBは、メインアカウントの単一のClient VPNエンドポイントが必要なため、最もコスト効果が高いです。オプションAは複数のエンドポイントを必要とし、コストが増加します。オプションCは、単一のエンドポイントで十分であるのに対し、不要なトランジットゲートウェイの複雑さを追加します。オプションDも不要な接続を追加しており、必要ありません。",
        "additional_knowledge": "ルーティング構成を最小限に抑えることで、将来的な変更や、より多くのアカウントが追加またはアクセスされる際のスケーラビリティが簡素化されます。",
        "key_terminology": "Client VPN、VPC Peering、ルーティング、AWS Site-to-Site VPN、トランジットゲートウェイ",
        "overall_assessment": "回答Bは最も要件を満たしています。コストを最小限に抑え、管理を簡素化しながら必要な機能を提供します。コミュニティの投票も主にこの選択肢を支持しており、望ましい実践と一致しています。"
      }
    ],
    "keywords": [
      "Client VPN",
      "VPC Peering",
      "Routing"
    ]
  },
  {
    "No": "100",
    "question": "A company is running an application in the AWS Cloud. Recent application metrics show inconsistent response times and a significant increase in\nerror rates. Calls to third-party services are causing the delays. Currently, the application calls third-party services synchronously by directly\ninvoking an AWS Lambda function.\nA solutions architect needs to decouple the third-party service calls and ensure that all the calls are eventually completed.\nWhich solution will meet these requirements?",
    "question_jp": "ある会社がAWSクラウドでアプリケーションを運営しています。最近のアプリケーションのメトリクスは、不規則な応答時間とエラー率の大幅な増加を示しています。サードパーティのサービスへの呼び出しが遅延を引き起こしています。現在、アプリケーションはサードパーティサービスを同期的に呼び出すために、AWS Lambda関数を直接起動しています。ソリューションアーキテクトは、サードパーティサービスの呼び出しを分離し、すべての呼び出しが最終的に完了することを確保する必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function.",
        "text_jp": "Amazon Simple Queue Service (Amazon SQS) キューを使用してイベントを保存し、Lambda関数を起動する。"
      },
      {
        "key": "B",
        "text": "Use an AWS Step Functions state machine to pass events to the Lambda function.",
        "text_jp": "AWS Step Functions ステートマシンを使用してイベントをLambda関数に渡す。"
      },
      {
        "key": "C",
        "text": "Use an Amazon EventBridge rule to pass events to the Lambda function.",
        "text_jp": "Amazon EventBridge ルールを使用してイベントをLambda関数に渡す。"
      },
      {
        "key": "D",
        "text": "Use an Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function.",
        "text_jp": "Amazon Simple Notification Service (Amazon SNS) トピックを使用してイベントを保存し、Lambda関数を起動する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function.",
        "situation_analysis": "The application is experiencing delays due to synchronous calls to third-party services, which negatively impacts response times and reliability.",
        "option_analysis": "Option A allows decoupling by storing requests in SQS, enabling asynchronous processing by the Lambda function. Option B (Step Functions) and Option C (EventBridge) are more complex and may not directly address the problem of synchronous calls effectively. Option D (SNS) is more suited for broadcasting messages rather than reliable queue-based processing.",
        "additional_knowledge": "SQS also features message retention and dead-letter queue options to handle failures.",
        "key_terminology": "Amazon SQS, AWS Lambda, Decoupling, Asynchronous Processing, Eventual Consistency.",
        "overall_assessment": "Overall, option A stands out as the most efficient solution for the specified problem of decoupling synchronous calls to third-party services."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA: Amazon Simple Queue Service (Amazon SQS) キューを使用してイベントを保存し、Lambda関数を起動することである。",
        "situation_analysis": "アプリケーションは、サードパーティサービスへの同期呼び出しによる遅延を経験しており、応答時間と信頼性に悪影響を与えている。",
        "option_analysis": "選択肢Aは、リクエストをSQSに保存することで非同期処理を可能にし、Lambda関数の起動を行うため、呼び出しの分離が可能である。選択肢B（Step Functions）や選択肢C（EventBridge）はより複雑で、不適切な解決策となる可能性がある。選択肢D（SNS）はメッセージのブロードキャストに適しているが、信頼性のあるキュー処理には向いていない。",
        "additional_knowledge": "SQSにはメッセージ保持およびデッドレターキューオプションがあり、失敗を管理する手段も提供する。",
        "key_terminology": "Amazon SQS、AWS Lambda、デカップリング、非同期処理、最終的な整合性。",
        "overall_assessment": "全体的に見て、選択肢Aはサードパーティサービスへの同期的呼び出しをデカップルするための最も効率的な解決策となる。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "AWS Lambda",
      "Decoupling",
      "Asynchronous Processing",
      "Eventual Consistency"
    ]
  },
  {
    "No": "101",
    "question": "A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS\naccounts in AWS Organizations.\nThe sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The\nmarketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key\nManagement Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in\nthe marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "企業は、複数のアカウント環境でAWS上にアプリケーションを運用しています。企業の営業チームとマーケティングチームは、AWS Organizations内で別々のAWSアカウントを使用しています。営業チームはペタバイトのデータをAmazon S3バケットに保存しています。マーケティングチームはデータの視覚化のためにAmazon QuickSightを使用しています。マーケティングチームは、営業チームがS3バケットに保存しているデータにアクセスする必要があります。企業はS3バケットをAWS Key Management Service（AWS KMS）キーで暗号化しています。マーケティングチームはすでにQuickSightへのアクセスを提供するIAMサービスロールをマーケティングAWSアカウントに作成しています。企業は、AWSアカウント間でS3バケット内のデータへの安全なアクセスを提供するソリューションが必要です。最も運用上のオーバーヘッドが少ない方法はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3",
        "text_jp": "マーケティングアカウントに新しいS3バケットを作成します。営業アカウントにS3レプリケーションルールを作成して、新しいS3にオブジェクトをコピーします。"
      },
      {
        "key": "B",
        "text": "Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the",
        "text_jp": "SCPを作成して、マーケティングアカウントにS3バケットへのアクセスを付与します。AWS Resource Access Manager（AWS RAM）を使用して共有します。"
      },
      {
        "key": "C",
        "text": "Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that",
        "text_jp": "マーケティングアカウントのS3バケットポリシーを更新して、QuickSightロールへのアクセスを許可します。暗号化キーのKMSグラントを作成します。"
      },
      {
        "key": "D",
        "text": "Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales",
        "text_jp": "営業アカウントにIAMロールを作成し、S3バケットへのアクセスを許可します。マーケティングアカウントから、営業アカウントのIAMロールを引き受けます。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (74%) C (15%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. By creating an IAM role in the sales account and allowing the marketing account to assume this role, the marketing team can securely access the S3 bucket while maintaining the original data ownership and permissions.",
        "situation_analysis": "The sales team has a large amount of data in S3, and the marketing team requires access for analysis purposes. The access must be done securely and efficiently without creating duplicate data.",
        "option_analysis": "Option A would create unnecessary duplicates of data and increase operational overhead. Option B does not cover cross-account access to KMS-encrypted data. Option C could work but doesn't provide as straightforward a mechanism for role assumption compared to Option D.",
        "additional_knowledge": "IAM roles are reusable and can be configured per environmental changes, supporting ongoing operational scalability.",
        "key_terminology": "IAM Role, S3 Bucket Policy, AWS KMS, Cross-Account Access, Permissions",
        "overall_assessment": "The combination of an IAM role and S3 bucket access is a best practice in AWS for multi-account scenarios, and this scenario illustrates a classic use case for leveraging IAM roles effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。営業アカウントにIAMロールを作成し、マーケティングアカウントにこのロールを引き受ける権限を与えることで、マーケティングチームはS3バケットへの安全なアクセスを実現できるが、元のデータ所有権やアクセス権は維持される。",
        "situation_analysis": "営業チームはS3に大量のデータを持っており、マーケティングチームは分析目的でアクセスを必要としている。このアクセスは安全かつ効率的に行う必要があり、データの重複を避けなければならない。",
        "option_analysis": "選択肢Aはデータの不必要な重複を生み出し、運用上のオーバーヘッドを増加させる。選択肢BはKMSで暗号化されたデータへのクロスアカウントアクセスをカバーしていない。選択肢Cは機能する可能性があるが、ロールの引き受けに関しては選択肢Dほど直感的ではない。",
        "additional_knowledge": "IAMロールは再利用可能で、環境の変化に応じて設定でき、持続的な運用の拡張性をサポートする。",
        "key_terminology": "IAMロール, S3バケットポリシー, AWS KMS, クロスアカウントアクセス, 権限",
        "overall_assessment": "IAMロールとS3バケットアクセスの組み合わせは、AWSの複数アカウントシナリオにおけるベストプラクティスであり、このシナリオはIAMロールを効果的に活用する古典的なユースケースを示している。"
      }
    ],
    "keywords": [
      "IAM Role",
      "S3 Bucket Policy",
      "AWS KMS",
      "Cross-Account Access",
      "Permissions"
    ]
  },
  {
    "No": "102",
    "question": "A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises\ninstallation of a Microsoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service. A solutions\narchitect must design a heterogeneous database migration on AWS.\nWhich solution will meet these requirements?",
    "question_jp": "企業は、ビジネスクリティカルなアプリケーションをオンプレミスのデータセンターからAWSに移行する計画を立てています。この企業は、オンプレミスのMicrosoft SQL Server Always Onクラスターを持っています。企業は、AWSの管理されたデータベースサービスに移行したいと考えています。ソリューションアーキテクトは、AWS上での異種データベース移行を設計する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities.",
        "text_jp": "バックアップと復元ユーティリティを使用して、SQL ServerデータベースをAmazon RDS for MySQLに移行します。"
      },
      {
        "key": "B",
        "text": "Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration",
        "text_jp": "AWS Snowball Edge Storage Optimizedデバイスを使用してデータをAmazon S3に転送します。Amazon RDS for MySQLを設定します。S3統合を使用します。"
      },
      {
        "key": "C",
        "text": "Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration",
        "text_jp": "AWSスキーマ変換ツールを使用して、データベーススキーマをAmazon RDS for MySQLに翻訳します。その後、AWSデータベース移行サービスを使用します。"
      },
      {
        "key": "D",
        "text": "Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3",
        "text_jp": "AWS DataSyncを使用して、オンプレミスのストレージとAmazon S3の間でデータをネットワーク経由で移行します。Amazon RDS for MySQLを設定します。S3を使用します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service to migrate the data. This solution effectively supports the migration of a Microsoft SQL Server Always On cluster to AWS managed databases.",
        "situation_analysis": "The company is migrating on-premises applications to AWS and specifically needs to migrate its Microsoft SQL Server databases, which require careful schema conversion to ensure compatibility with AWS services.",
        "option_analysis": "Option A is incorrect as it does not support the heterogeneous nature of the migration. Option B does not directly relate to migrating SQL Server databases to Amazon RDS for MySQL. Option D, while feasible, does not utilize the benefits of the AWS Schema Conversion Tool, which is crucial for this migration.",
        "additional_knowledge": "This migration process is critical for maintaining application availability and performance during the transition to cloud services.",
        "key_terminology": "AWS Schema Conversion Tool, AWS Database Migration Service, Amazon RDS for MySQL",
        "overall_assessment": "Overall, the impact of selecting the right migration strategy cannot be underestimated. Option C leverages AWS's capabilities to ensure a successful and efficient migration."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。AWSスキーマ変換ツールを使用して、データベーススキーマをAmazon RDS for MySQLに翻訳し、その後、AWSデータベース移行サービスを使用してデータを移行します。このソリューションは、Microsoft SQL Server Always OnクラスターをAWS管理データベースに移行するのに効果的です。",
        "situation_analysis": "企業はオンプレミスのアプリケーションをAWSに移行しており、特にMicrosoft SQL Serverのデータベースを移行する必要があり、AWSサービスとの互換性を確保するために慎重なスキーマ変換が必要です。",
        "option_analysis": "選択肢Aは、異種移行に対してサポートが不足しているため不正解です。選択肢Bは、SQL ServerデータベースをAmazon RDS for MySQLに移行することに直接関係していません。選択肢Dも実行可能ですが、AWSスキーマ変換ツールの利点を活用しておらず、この移行には重要です。",
        "additional_knowledge": "この移行プロセスは、クラウドサービスへの移行中にアプリケーションの可用性とパフォーマンスを維持するために重要です。",
        "key_terminology": "AWSスキーマ変換ツール、AWSデータベース移行サービス、Amazon RDS for MySQL",
        "overall_assessment": "適切な移行戦略を選択することの影響は過小評価できません。選択肢Cは、AWSの機能を活用して成功で効率的な移行を確保します。"
      }
    ],
    "keywords": [
      "AWS Schema Conversion Tool",
      "AWS Database Migration Service",
      "Amazon RDS for MySQL"
    ]
  },
  {
    "No": "103",
    "question": "A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the\nicons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account\nthat members of the design team can access.\nAfter the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the\nproduction account. A solutions architect must provide the design team with access to the production account without exposing other parts of the\nweb application to the risk of unwanted changes.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "出版社のデザインチームが、eコマースウェブアプリケーションで使用されるアイコンやその他の静的アセットを更新します。この会社は、アイコンやアセットを、会社のプロダクションアカウントにホストされているAmazon S3バケットから提供しています。また、会社は開発アカウントも使用しており、デザインチームのメンバーがアクセスできます。デザインチームが開発アカウントで静的アセットをテストした後、デザインチームはそれらのアセットをプロダクションアカウントのS3バケットにロードする必要があります。ソリューションアーキテクトは、他のウェブアプリケーションの部分が不要な変更のリスクにさらされることなく、デザインチームにプロダクションアカウントへのアクセスを提供しなければなりません。これらの要件を満たすための手順の組み合わせはどれですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "In the production account, create a new IAM policy that allows read and write access to the S3 bucket.",
        "text_jp": "プロダクションアカウントにおいて、S3バケットへの読み取りおよび書き込みアクセスを許可する新しいIAMポリシーを作成すること。"
      },
      {
        "key": "B",
        "text": "In the development account, create a new IAM policy that allows read and write access to the S3 bucket.",
        "text_jp": "開発アカウントにおいて、S3バケットへの読み取りおよび書き込みアクセスを許可する新しいIAMポリシーを作成すること。"
      },
      {
        "key": "C",
        "text": "In the production account, create a role Attach the new policy to the role. Define the development account as a trusted entity.",
        "text_jp": "プロダクションアカウントにおいて、ロールを作成し、新しいポリシーをそのロールにアタッチすること。開発アカウントを信頼されたエンティティとして定義すること。"
      },
      {
        "key": "D",
        "text": "In the development account, create a role. Attach the new policy to the role Define the production account as a trusted entity.",
        "text_jp": "開発アカウントにおいて、ロールを作成し、新しいポリシーをそのロールにアタッチすること。プロダクションアカウントを信頼されたエンティティとして定義すること。"
      },
      {
        "key": "E",
        "text": "In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to",
        "text_jp": "開発アカウントにおいて、デザインチームの全IAMユーザーを含むグループを作成し、そのグループに別のIAMポリシーをアタッチすること。"
      },
      {
        "key": "F",
        "text": "In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to",
        "text_jp": "開発アカウントにおいて、デザインチームの全IAMユーザーを含むグループを作成し、そのグループに別のIAMポリシーをアタッチすること。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACE (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are steps A, C, and A. These steps enable secure access to the S3 bucket while minimizing risk to the web application.",
        "situation_analysis": "The design team needs a secure way to update static assets in the production S3 bucket without introducing risk to the rest of the application.",
        "option_analysis": "Option A is correct as it establishes necessary IAM permissions. Option C correctly sets up a role to allow access from the development account. Options B and D incorrectly focus on read/write permissions in the wrong account. Options E and F are incomplete and do not relate to direct access to S3.",
        "additional_knowledge": "",
        "key_terminology": "IAM Policy, S3 Bucket, Trusted Entity, Least Privilege, Role.",
        "overall_assessment": "This question is well-designed for assessing knowledge of IAM roles and policies, particularly focusing on cross-account access and security best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はステップA、C、およびAである。これらのステップは、S3バケットへの安全なアクセスを可能にし、ウェブアプリケーションへのリスクを最小化する。",
        "situation_analysis": "デザインチームは、アプリケーションの他の部分にリスクを与えることなく、プロダクションS3バケット内の静的アセットを更新する安全な方法を必要としている。",
        "option_analysis": "オプションAは、必要なIAM権限を確立するため、正しい。オプションCは、開発アカウントからのアクセスを許可するためのロールを正しく設定する。オプションBとDは、間違ったアカウントで読み取り/書き込み権限に焦点を当てている。オプションEとFは不完全であり、S3への直接アクセスに関係しない。",
        "additional_knowledge": "",
        "key_terminology": "IAMポリシー, S3バケット, 信頼されたエンティティ, 最小特権, ロール。",
        "overall_assessment": "この質問は、IAMロールとポリシー、特にアカウント間アクセスとセキュリティベストプラクティスに対する知識を評価するためにうまく設計されている。"
      }
    ],
    "keywords": [
      "IAM Policy",
      "S3 Bucket",
      "Trusted Entity",
      "Least Privilege",
      "Role"
    ]
  },
  {
    "No": "104",
    "question": "A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's\ndevelopment team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU\nthan expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.\nA solutions architect must mitigate the performance issues before the company launches the application to production.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業は、AWS Elastic Beanstalk と Java を使用してパイロットアプリケーションを開発しました。コストを節約するために、同社の開発チームはアプリケーションをシングルインスタンス環境にデプロイしました。最近のテストで、アプリケーションは予想以上に CPU を消費していることが示されています。CPU 使用率は常に 85% を超えており、これがいくつかのパフォーマンスボトルネックを引き起こしています。ソリューションアーキテクトは、企業がアプリケーションを本番環境にローンチする前にパフォーマンスの問題を軽減する必要があります。運用オーバーヘッドを最小限に抑えつつ、どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that",
        "text_jp": "新しい Elastic Beanstalk アプリケーションを作成する。負荷分散環境タイプを選択する。すべてのアベイラビリティゾーンを選択する。スケールアウトルールを追加する。"
      },
      {
        "key": "B",
        "text": "Create a second Elastic Beanstalk environment. Apply the trafic-splitting deployment policy. Specify a percentage of incoming trafic to",
        "text_jp": "2 番目の Elastic Beanstalk 環境を作成する。トラフィックスプリッティングデプロイメントポリシーを適用する。受信トラフィックのパーセンテージを指定する。"
      },
      {
        "key": "C",
        "text": "Modify the existing environment's capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a",
        "text_jp": "既存の環境のキャパシティ構成を変更して、負荷分散環境タイプを使用する。すべてのアベイラビリティゾーンを選択する。スケールアウトルールを追加する。"
      },
      {
        "key": "D",
        "text": "Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the",
        "text_jp": "負荷分散オプションを選択して環境を再構築するアクションを選択する。アベイラビリティゾーンを選択する。CPU 使用率が特定のしきい値を超えた場合に起動するスケールアウトルールを追加する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, which suggests creating a new Elastic Beanstalk application with a load-balanced environment type to handle increased CPU utilization effectively.",
        "situation_analysis": "The application currently experiences high CPU utilization, indicating that it requires more resources or an improved architecture for scalability before going to production.",
        "option_analysis": "Option A is correct as it directly addresses the CPU issues by introducing load balancing. Option B shares traffic between two environments but does not resolve the high CPU issue upfront. Option C modifies an existing setup but likely involves more operational overhead. Option D also involves rebuilding which may require additional configurations and management.",
        "additional_knowledge": "Using Elastic Beanstalk's capabilities ensures minimal operation management and integrates tightly with AWS services.",
        "key_terminology": "Elastic Beanstalk, load balancing, CPU utilization, auto-scaling, environment type.",
        "overall_assessment": "Although community votes show C as a popular choice (93%), it perhaps stems from concern over operational simplicity. However, A is indeed the most efficient method to directly mitigate the high CPU issue with minimum change."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは A であり、負荷が増加する CPU 使用量に効果的に対処するために、負荷分散環境タイプの新しい Elastic Beanstalk アプリケーションを作成することを提案しています。",
        "situation_analysis": "現在、アプリケーションは高い CPU 使用率に苦しんでおり、本番環境に移行する前にリソースを増やすか、スケーラビリティの向上が必要です。",
        "option_analysis": "A の選択肢は、CPU の問題を直接解決するために負荷分散を導入するため、正しい選択です。B は 2 つの環境間でトラフィックを分割しますが、すぐに高い CPU の問題を解決するものではありません。C は既存のセットアップを変更しますが、恐らくより多くの運用オーバーヘッドが伴います。D は再構築を必要とし、追加の設定と管理が必要となるかもしれません。",
        "additional_knowledge": "Elastic Beanstalk の機能を利用することで、運用管理を最小限に抑え、AWS サービスと密接に統合することができます。",
        "key_terminology": "Elastic Beanstalk, 負荷分散, CPU 使用率, オートスケーリング, 環境タイプ。",
        "overall_assessment": "コミュニティ投票では C が人気の選択肢（93%）でしたが、運用の簡素性への懸念から生じたものでしょう。しかし、A は高い CPU 問題を最小限の変更で直接軽減する最も効果的な方法です。"
      }
    ],
    "keywords": [
      "Elastic Beanstalk",
      "load balancing",
      "CPU utilization",
      "auto-scaling",
      "environment type"
    ]
  },
  {
    "No": "105",
    "question": "A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed\nMySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of trafic during the month.\nHowever, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load\nBalancers and Auto Scaling within its infrastructure to meet the increased demand.\nWhich of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?",
    "question_jp": "ある金融会社が、最新世代のLinux EC2インスタンス上でビジネスクリティカルなアプリケーションを運用しています。このアプリケーションには、重いI/O操作を行う自己管理型のMySQLデータベースが含まれています。アプリケーションは、月内の中程度のトラフィックを処理するために正常に作動しています。しかしながら、月末の報告のために月の最終3日間に遅くなります。企業は、増加する需要に応じるためにElastic Load BalancersとAuto Scalingをインフラ内で使用しています。それにより、データベースが月末の負荷をパフォーマンスへの影響を最小限に抑えて処理できるようにするには、以下の選択肢のうちどれが最適でしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.",
        "text_jp": "Elastic Load Balancersの事前準備を行い、より大きなインスタンスタイプを使用し、すべてのAmazon EBSボリュームをGP2ボリュームに変更する。"
      },
      {
        "key": "B",
        "text": "Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load",
        "text_jp": "データベースクラスタをAmazon RDSに一度だけ移行し、負荷を処理するために追加の読み取りレプリカを複数作成する。"
      },
      {
        "key": "C",
        "text": "Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific",
        "text_jp": "特定の条件に基づいて、Amazon EBSボリュームのタイプ、サイズ、またはIOPSを変更するために、Amazon CloudWatchとAWS Lambdaを使用する。"
      },
      {
        "key": "D",
        "text": "Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by",
        "text_jp": "既存のすべてのAmazon EBSボリュームを、新しいPIOPSボリュームに置き換え、最大のストレージサイズおよびI/O毎秒を持つものにする。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Migrating to Amazon RDS and adding read replicas helps distribute the load effectively.",
        "situation_analysis": "The finance company experiences performance issues during month-end reporting, despite having scaling solutions in place.",
        "option_analysis": "Option B is the most effective as it uses RDS to handle database loads efficiently, while other options do not address the root cause of the I/O bottleneck effectively.",
        "additional_knowledge": "Choosing RDS can also provide benefits like automated backups and multi-AZ deployments for high availability.",
        "key_terminology": "Amazon RDS, read replicas, database migration, I/O performance, load distribution.",
        "overall_assessment": "Option B is the optimal choice for handling the increased load with minimal performance impact."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。Amazon RDSへの移行と読み取りレプリカの追加は、負荷を効果的に分散させる。",
        "situation_analysis": "この金融会社は月末の報告時にパフォーマンスの問題を経験しており、スケーリングソリューションが整っているにも関わらず問題が解決していない。",
        "option_analysis": "選択肢Bは最も効果的であり、RDSを使用してデータベースの負荷を効率的に処理することができる。他の選択肢はI/Oのボトルネック問題の根本原因に対処していない。",
        "additional_knowledge": "RDSを選択することで、自動バックアップや、可用性を高めるためのマルチAZ展開などの利点も得られる。",
        "key_terminology": "Amazon RDS, 読み取りレプリカ, データベース移行, I/Oパフォーマンス, 負荷分散。",
        "overall_assessment": "選択肢Bは、パフォーマンスへの影響を最小限に抑えつつ、負荷を処理するための最適な選択肢である。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "read replicas",
      "database migration",
      "I/O performance",
      "load distribution"
    ]
  },
  {
    "No": "106",
    "question": "A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable. but\nthe company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative\noverhead to maintain the servers.\nWhich solution will meet these requirements with the LEAST code changes?",
    "question_jp": "ある企業は、データセンターにある仮想マシンに複雑に依存するJavaアプリケーションを運営している。このアプリケーションは安定しているが、企業はテクノロジースタックをモダナイズしたいと考えている。企業はこのアプリケーションをAWSに移行し、サーバーを維持するための管理のオーバーヘッドを最小限に抑えたいと考えている。\nこの要件を最小限のコード変更で満たす解決策はどれか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container",
        "text_jp": "Amazon Elastic Container Service (Amazon ECS)をAWS Fargate上に移行し、AWS App2Containerを使用する。コンテナをストア"
      },
      {
        "key": "B",
        "text": "Migrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration.",
        "text_jp": "アプリケーションコードをAWS Lambdaで実行されるコンテナに移行する。Lambda統合を使用したAmazon API GatewayのREST APIを構築する。"
      },
      {
        "key": "C",
        "text": "Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container.",
        "text_jp": "Amazon Elastic Kubernetes Service (Amazon EKS)のEKS管理ノードグループ上にアプリケーションを移行する。AWS App2Containerを使用する。"
      },
      {
        "key": "D",
        "text": "Migrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use",
        "text_jp": "アプリケーションコードをAWS Lambdaで実行されるコンテナに移行する。Lambdaをアプリケーションロードバランサー(ALB)を使用するように設定する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Migrating the application code to a container that runs in AWS Lambda minimizes administrative overhead while allowing the application to operate in its existing architecture with minimal code changes.",
        "situation_analysis": "The company wants to migrate a stable Java application with complex dependencies while minimizing maintenance overhead.",
        "option_analysis": "Option B is correct because AWS Lambda manages the infrastructure, thus reducing the administrative burden. Other options involve more management complexity, such as container orchestration.",
        "additional_knowledge": "Containers in Lambda enhance portability and scalability of the application.",
        "key_terminology": "AWS Lambda, Amazon API Gateway, containerization, infrastructure as a service (IaaS), serverless architecture.",
        "overall_assessment": "Option B directly addresses the company's goals, while options A, C, and D may require more substantial changes to the application's architecture."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。アプリケーションコードをAWS Lambdaで実行されるコンテナに移行することで、管理のオーバーヘッドを最小限に抑えつつ、最小限のコード変更でアプリケーションを既存のアーキテクチャで運用できる。",
        "situation_analysis": "企業は、安定したJavaアプリケーションを複雑な依存関係を持ちながら移行し、メンテナンスのオーバーヘッドを最小限に抑えたいと考えている。",
        "option_analysis": "選択肢Bが正解である。AWS Lambdaはインフラストラクチャを管理するため、管理負担を軽減できる。他の選択肢は、コンテナオーケストレーションなど、より多くの管理の複雑さを伴う。",
        "additional_knowledge": "Lambda内のコンテナはアプリケーションの移植性とスケーラビリティを向上させる。",
        "key_terminology": "AWS Lambda、Amazon API Gateway、コンテナ化、インフラストラクチャ・アズ・ア・サービス(IaaS)、サーバーレスアーキテクチャ。",
        "overall_assessment": "選択肢Bは、企業の目標に直接応えている一方で、選択肢A、C、Dはアプリケーションのアーキテクチャにより多くの変更を要求する可能性がある。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon API Gateway",
      "containerization"
    ]
  },
  {
    "No": "107",
    "question": "A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes\nthe Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign\nthe application to support failover to another AWS Region.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業には、AWS Lambda 関数としてホストされた非同期 HTTP アプリケーションがあります。パブリック Amazon API Gateway エンドポイントが Lambda 関数を呼び出しています。Lambda 関数と API Gateway エンドポイントは、us-east-1 リージョンに存在します。ソリューションアーキテクトは、障害時に他の AWS リージョンへのフェイルオーバーをサポートするようにアプリケーションを再設計する必要があります。この要件を満たす解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an API Gateway endpoint in the us-west-2 Region to direct trafic to the Lambda function in us-east-1. Configure Amazon Route 53 to",
        "text_jp": "us-west-2 リージョンに API Gateway エンドポイントを作成し、トラフィックを us-east-1 の Lambda 関数へ誘導する。Amazon Route 53 を設定する。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct trafic to the SQS queue instead of to the",
        "text_jp": "Amazon Simple Queue Service (Amazon SQS) キューを作成する。API Gateway を構成して、トラフィックを Lambda 関数ではなく SQS キューに誘導する。"
      },
      {
        "key": "C",
        "text": "Deploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 10 direct trafic to the Lambda function",
        "text_jp": "Lambda 関数を us-west-2 リージョンにデプロイする。us-west-2 に API Gateway エンドポイントを作成し、トラフィックを Lambda 関数に誘導する。"
      },
      {
        "key": "D",
        "text": "Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing",
        "text_jp": "Lambda 関数と API Gateway エンドポイントを us-west-2 リージョンにデプロイする。Amazon Route 53 を設定して、フェイルオーバールーティングを使用する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. By creating an Amazon SQS queue, the application can decouple the API layer from the processing logic in the Lambda function. This allows the system to handle increased load and can be configured to failover to another region.",
        "situation_analysis": "The company needs a reliable failover mechanism to another AWS Region, which is crucial for high availability.",
        "option_analysis": "Option A does not support failover but merely routes traffic to the Lambda function in a single region. Option C and D propose running the function in another region, but they do not handle the decoupling of traffic management effectively.",
        "additional_knowledge": "It's important to also set up an appropriate monitoring and alerting mechanism to ensure that any issues in the main region can be swiftly detected and mitigated.",
        "key_terminology": "Amazon SQS, Amazon API Gateway, AWS Regions, High Availability, Failover.",
        "overall_assessment": "While the community strongly supports option D, option B is more aligned with AWS architectural best practices for asynchronous processing."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは B です。Amazon SQS キューを作成することで、アプリケーションは API 層と Lambda 関数内の処理ロジックを切り離すことができます。これにより、システムは負荷の増加に対応でき、他のリージョンへのフェイルオーバーを構成できます。",
        "situation_analysis": "企業は、もう一つの AWS リージョンへの信頼性の高いフェイルオーバー機構を必要としており、これは高可用性にとって重要です。",
        "option_analysis": "選択肢 A はフェイルオーバーをサポートすることなく、単にトラフィックを単一リージョンの Lambda 関数にルーティングします。選択肢 C と D は他のリージョンで関数を実行することを提案していますが、トラフィック管理の切り離しを効果的には行っていません。",
        "additional_knowledge": "メインリージョンでの問題を迅速に検出し、軽減できるように、適切な監視とアラート機構を設定することも重要です。",
        "key_terminology": "Amazon SQS, Amazon API Gateway, AWS リージョン, 高可用性, フェイルオーバー。",
        "overall_assessment": "コミュニティは選択肢 D を強く支持していますが、選択肢 B は非同期処理のための AWS のアーキテクチャベストプラクティスとより一致しています。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "Amazon API Gateway",
      "AWS Regions",
      "High Availability",
      "Failover"
    ]
  },
  {
    "No": "108",
    "question": "A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated\nbilling and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has\nmultiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and\nproduction.\nThe HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved\nInstances (RIs) in its production AWS account. The HR department will install the new application on this account. The HR department wants to\nmake sure that other departments cannot share the RI discounts.\nWhich solution will meet these requirements?",
    "question_jp": "小売会社は、AWS Organizationsの一部としてAWSアカウントを構成しています。この会社は、統合請求を設定し、以下のOUに部門をマッピングしました：財務、営業、人事（HR）、マーケティング、運用。各OUには、部門内の各環境に対する1つのAWSアカウントがあります。これらの環境は、開発、テスト、前生産、及び生産です。人事部門は、3ヶ月後に立ち上げる新しいシステムをリリースします。その準備として、人事部門は生産AWSアカウントでいくつかのリザーブドインスタンス（RIs）を購入しました。人事部門はこのアカウントに新しいアプリケーションをインストールします。他の部門がRIの割引を共有できないようにしたいと考えています。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "In the AWS Billing and Cost Management console for the HR department's production account turn off RI sharing.",
        "text_jp": "HR部門の生産アカウントのAWS請求とコスト管理コンソールでRI共有をオフにする。"
      },
      {
        "key": "B",
        "text": "Remove the HR department's production AWS account from the organization. Add the account 10 the consolidating billing configuration",
        "text_jp": "HR部門の生産AWSアカウントを組織から削除し、そのアカウントを統合請求設定に追加する。"
      },
      {
        "key": "C",
        "text": "In the AWS Billing and Cost Management console. use the organization's management account 10 turn off RI Sharing for the HR",
        "text_jp": "AWS請求とコスト管理コンソールで、組織の管理アカウントを使用してHRのRI共有をオフにする。"
      },
      {
        "key": "D",
        "text": "Create an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments.",
        "text_jp": "組織内にSCPを作成し、RIへのアクセスを制限する。他の部門のOUにSCPを適用する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (82%) D (18%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. By using the organization's management account to turn off RI Sharing for the HR department, you ensure that no other departments can share the Reserved Instance discounts.",
        "situation_analysis": "The HR department has placed a priority on ensuring that their Reserved Instances cannot be shared across departments, as they are preparing for a new application launch.",
        "option_analysis": "Option A could prevent sharing but only within the account itself, option B would remove the account from the organization's consolidated billing which is unnecessary, and option D does not directly disable the sharing of RIs.",
        "additional_knowledge": "Organizations can manage billing and access controls through the use of Service Control Policies (SCPs) and management accounts.",
        "key_terminology": "Reserved Instances, RI Sharing, AWS Organizations, Management Account, SCP",
        "overall_assessment": "The choice of C aligns with AWS best practices for managing cost sharing in a multi-account environment, ensuring compliance with the HR department's requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。組織の管理アカウントを使用してHR部門のRI共有をオフにすることで、他の部門がリザーブドインスタンスの割引を共有できないようにすることができる。",
        "situation_analysis": "人事部門は新しいアプリケーションの立ち上げに備えて、リザーブドインスタンスを他の部門と共有できないようにする高い優先順位を置いている。",
        "option_analysis": "Aはアカウント内での共有を防ぐことができるが、Bは組織からアカウントを削除することになるため、不必要である。そしてDはRIの共有を直接無効にするものではない。",
        "additional_knowledge": "組織はサービスコントロールポリシー（SCP）や管理アカウントを使用して請求やアクセス制御を管理できる。",
        "key_terminology": "リザーブドインスタンス、RI共有、AWS Organizations、管理アカウント、SCP",
        "overall_assessment": "Cの選択は、複数のアカウント環境におけるコスト共有を管理するためのAWSのベストプラクティスと一致し、人事部門の要件を満たすことを保証する。"
      }
    ],
    "keywords": [
      "Reserved Instances",
      "RI Sharing",
      "AWS Organizations",
      "Management Account",
      "SCP"
    ]
  },
  {
    "No": "109",
    "question": "A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a\nprivate subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager\nSession Manager is configured, and AWS Systems Manager Agent is running on all the EC2 instances.\nThe company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being\nterminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon\nCloudWatch logs that are collected from the application, but the logs are inconclusive.\nHow should the solutions architect gain access to an EC2 instance to troubleshoot the issue?",
    "question_jp": "大企業が人気のウェブアプリケーションを運営しています。このアプリケーションは、プライベートサブネット内のAuto Scalingグループ内の複数のAmazon EC2 Linuxインスタンス上で実行されています。アプリケーションロードバランサーは、プライベートサブネット内のAuto Scalingグループのインスタンスをターゲットにしています。AWS Systems Manager Session Managerが設定され、AWS Systems Manager AgentがすべてのEC2インスタンスで実行されています。最近、会社はアプリケーションの新しいバージョンをリリースしました。一部のEC2インスタンスが不健康としてマークされ、終了されています。その結果、アプリケーションは減少したキャパシティで実行されています。ソリューションアーキテクトは、アプリケーションから収集されたAmazon CloudWatchログを分析して根本原因を特定しようとしますが、ログは決定的ではありません。ソリューションアーキテクトは、問題をトラブルシューティングするためにEC2インスタンスにアクセスするにはどうすればよいですか？",
    "choices": [
      {
        "key": "A",
        "text": "Suspend the Auto Scaling group's HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy.",
        "text_jp": "Auto ScalingグループのHealthCheckスケーリングプロセスを一時停止します。Session Managerを使用して、不健康としてマークされたインスタンスにログインします。"
      },
      {
        "key": "B",
        "text": "Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy.",
        "text_jp": "EC2インスタンスの終了保護を有効にします。Session Managerを使用して、不健康としてマークされたインスタンスにログインします。"
      },
      {
        "key": "C",
        "text": "Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an",
        "text_jp": "Auto Scalingグループの終了ポリシーをOldestInstanceに設定します。Session Managerを使用して、不健康としてマークされたインスタンスにログインします。"
      },
      {
        "key": "D",
        "text": "Suspend the Auto Scaling group's Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy.",
        "text_jp": "Auto ScalingグループのTerminateプロセスを一時停止します。Session Managerを使用して、不健康としてマークされたインスタンスにログインします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (89%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Suspending the Auto Scaling group's Terminate process allows access to the unhealthy instance for troubleshooting.",
        "situation_analysis": "The company is experiencing issues with some EC2 instances being marked as unhealthy, necessitating access to these instances for investigation.",
        "option_analysis": "Option D is correct because suspending the Terminate process prevents the instance from being terminated, providing necessary access. Options A, B, and C are less effective because they don't guarantee access to unhealthy instances.",
        "additional_knowledge": "Given that instance health checks are failing, it is critical to understand why they are labeled unhealthy and Session Manager provides the best means of access.",
        "key_terminology": "Amazon EC2, Auto Scaling, Session Manager, HealthCheck, Terminate process",
        "overall_assessment": "The question tests knowledge of AWS Auto Scaling and access methods to EC2 instances. Option D is optimal for troubleshooting."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDである。Auto ScalingグループのTerminateプロセスを一時停止することで、トラブルシューティングのために不健康なインスタンスにアクセスできるようになる。",
        "situation_analysis": "会社はいくつかのEC2インスタンスが不健康としてマークされる問題を経験しており、調査のためにこれらのインスタンスにアクセスする必要がある。",
        "option_analysis": "選択肢Dは正しい。Terminateプロセスを一時停止することにより、インスタンスが終了されるのを防ぎ、必要なアクセスを提供する。他の選択肢A、B、Cは不健康なインスタンスへのアクセスを保証しないため、効果が低い。",
        "additional_knowledge": "インスタンスの健康チェックに失敗しているため、なぜそれらが不健康としてラベル付けされているのかを理解することが重要である。Session Managerが最良のアクセス手段を提供する。",
        "key_terminology": "Amazon EC2, Auto Scaling, Session Manager, HealthCheck, Terminateプロセス",
        "overall_assessment": "この質問はAWS Auto ScalingとEC2インスタンスへのアクセス方法に関する知識をテストしている。選択肢Dはトラブルシューティングに最適である。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Auto Scaling",
      "Session Manager",
      "HealthCheck",
      "Terminate process"
    ]
  },
  {
    "No": "110",
    "question": "A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under\ndifferent OUs in AWS Organizations.\nAdministrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the\nability to automatically update and remediate noncompliant AWS WAF rules in all accounts.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "question_jp": "ある企業が、複数のAWSアカウントにわたってAWS WAFルールを管理するAWS WAFソリューションを展開したいと考えています。アカウントはAWS Organizationsの異なるOUの下で管理されています。管理者は、必要に応じて管理されたAWS WAFルールセットからアカウントやOUを追加または削除できる必要があります。また、管理者は、すべてのアカウント内で不遵守のAWS WAFルールを自動的に更新および修正できる必要があります。どのソリューションが、最も運用負荷が少なくこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store",
        "text_jp": "AWS Firewall Managerを使用して、組織内のアカウントにまたがるAWS WAFルールを管理します。AWS Systems Manager Parameter Storeを使用します。"
      },
      {
        "key": "B",
        "text": "Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy",
        "text_jp": "選択したOU内のすべてのリソースがAWS WAFルールを関連付けることを要求する組織全体のAWS Configルールを展開します。"
      },
      {
        "key": "C",
        "text": "Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers",
        "text_jp": "組織の管理アカウントにAWS WAFルールを作成します。AWS Lambda環境変数を使用してアカウント番号を格納します。"
      },
      {
        "key": "D",
        "text": "Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to",
        "text_jp": "AWS Control Towerを使用して、組織内のアカウントにまたがるAWS WAFルールを管理します。AWS Key Management Service (AWS KMS)を使用します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using AWS Control Tower allows for centralized governance of AWS WAF rules across the organization with minimal overhead.",
        "situation_analysis": "The requirement is to manage AWS WAF rules across multiple AWS accounts under different Organizational Units (OUs) with the ability to update and remediate rules automatically.",
        "option_analysis": "Option D allows for the implementation of a centralized control system using AWS Control Tower. In contrast, Options A, B, and C do not provide the same level of operational efficiency.",
        "additional_knowledge": "AWS Firewall Manager can manage AWS WAF rules but does not provide the same management efficiency as AWS Control Tower.",
        "key_terminology": "AWS WAF, AWS Control Tower, AWS Organizations, operational overhead, centralized governance",
        "overall_assessment": "The community might support Option A due to its focus on AWS Firewall Manager, but D is superior in addressing the overall goal with the least operational burden."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。AWS Control Towerを使用することで、最小限の運用負荷で組織全体にわたってAWS WAFルールの集中管理が可能となる。",
        "situation_analysis": "複数のAWSアカウントにわたってAWS WAFルールを管理し、ルールを自動的に更新および修正できる能力が必要である。",
        "option_analysis": "選択肢Dでは、AWS Control Towerを使用して集中管理システムの実装が可能である。これに対して、選択肢A、B、Cは同じレベルの運用効率を提供しない。",
        "additional_knowledge": "AWS Firewall ManagerはAWS WAFルールを管理できるが、AWS Control Towerほどの管理効率は提供しない。",
        "key_terminology": "AWS WAF, AWS Control Tower, AWS Organizations, 運用負荷, 集中管理",
        "overall_assessment": "コミュニティはAWS Firewall Managerに焦点を当てているため選択肢Aを支持するかもしれないが、Dは運用負荷が最も少なく全体の目標を達成するために優れている。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "AWS Control Tower",
      "AWS Organizations",
      "operational overhead",
      "centralized governance"
    ]
  },
  {
    "No": "111",
    "question": "A solutions architect is auditing the security setup or an AWS Lambda function for a company. The Lambda function retrieves, the latest changes\nfrom an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the\ndatabase credentials to the Lambda function.\nThe Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with\nAWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised,\nthe company needs a solution that minimizes the impact of the compromise.\nWhat should the solutions architect recommend to meet these requirements?",
    "question_jp": "企業のためにAWS Lambda関数のセキュリティ設定を監査しているソリューションアーキテクトがいます。このLambda関数は、Amazon Auroraデータベースから最新の変更を取得します。Lambda関数とデータベースは同じVPC内で実行されています。Lambda環境変数は、データベースの認証情報をLambda関数に提供しています。Lambda関数はデータを集約し、データをAWS KMS管理の暗号化キー（SSE-KMS）でサーバー側暗号化が設定されたAmazon S3バケットに配置します。データはインターネットを経由して移動してはなりません。データベースの認証情報が侵害される場合、企業は侵害の影響を最小限に抑えるソリューションが必要です。これらの要件を満たすために、ソリューションアーキテクトは何を推奨するべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access",
        "text_jp": "Aurora DBクラスターでIAMデータベース認証を有効にします。Lambda関数がアクセスできるように、そのIAMロールを変更します。"
      },
      {
        "key": "B",
        "text": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access",
        "text_jp": "Aurora DBクラスターでIAMデータベース認証を有効にします。Lambda関数がアクセスできるように、そのIAMロールを変更します。"
      },
      {
        "key": "C",
        "text": "Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store.",
        "text_jp": "データベースの認証情報をAWS Systems Manager Parameter Storeに保存します。Parameter Storeで認証情報のパスワードローテーションを設定します。"
      },
      {
        "key": "D",
        "text": "Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM",
        "text_jp": "データベースの認証情報をAWS Secrets Managerに保存します。Secrets Managerで認証情報のパスワードローテーションを設定します。そのIAMロールを変更します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (84%) D (16%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Storing database credentials in AWS Secrets Manager allows for secure management and automatic rotation of credentials.",
        "situation_analysis": "The requirements state that the data must not travel across the Internet, and the company needs to minimize the impact of compromised credentials.",
        "option_analysis": "Option D addresses both security and management of database credentials. AWS Secrets Manager provides built-in support for rotating credentials automatically, which meets the needs outlined in the question.",
        "additional_knowledge": "Furthermore, using AWS Secrets Manager helps to maintain compliance with security best practices.",
        "key_terminology": "AWS Secrets Manager, IAM Roles, automatic credential rotation, VPC, Aurora.",
        "overall_assessment": "Despite the community vote distribution showing 84% support for option A, option D is the best practice for managing sensitive data like database credentials."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです。AWS Secrets Managerにデータベースの認証情報を保存することで、認証情報の安全な管理と自動ローテーションが可能になります。",
        "situation_analysis": "要件として、データはインターネットを経由して移動してはならず、企業は侵害された認証情報の影響を最小限に抑える必要があります。",
        "option_analysis": "選択肢Dは、データベースの認証情報のセキュリティと管理の両方に対応しています。AWS Secrets Managerは、認証情報の自動ローテーションをサポートしているため、質問で述べられているニーズを満たします。",
        "additional_knowledge": "さらに、AWS Secrets Managerを使用することで、セキュリティのベストプラクティスに準拠することができます。",
        "key_terminology": "AWS Secrets Manager、IAMロール、自動認証情報ローテーション、VPC、Aurora。",
        "overall_assessment": "コミュニティ投票分布では選択肢Aが84%支持されていますが、選択肢Dはデータベースの認証情報などのセンシティブなデータを管理するための最良のプラクティスです。"
      }
    ],
    "keywords": [
      "AWS Secrets Manager",
      "IAM Roles",
      "Automatic Credential Rotation",
      "VPC",
      "Aurora"
    ]
  },
  {
    "No": "112",
    "question": "A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is\nreviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected\nFramework.\nWhile reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several\nlarge instance types account for a high proportion of the costs. The solutions architect finds out that the company's developers are launching new\nAmazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types.\nThe solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.\nWhich solution will meet these requirements?",
    "question_jp": "大規模なモバイルゲーム会社は、そのすべてのオンプレミスインフラをAWSクラウドに正常に移行しました。ソリューションアーキテクトは、設計に基づいて構築されていることと、Well-Architected Frameworkに沿って実行されていることを確認するために環境をレビューしています。コストエクスプローラーで以前の月間コストを確認していると、いくつかの大規模インスタンスタイプの作成とその後の終了がコストの高い割合を占めていることに気付きました。ソリューションアーキテクトは、同社の開発者がテストの一環として新しいAmazon EC2インスタンスを起動しており、開発者が適切なインスタンスタイプを使用していないことを発見しました。ソリューションアーキテクトは、開発者が起動できるインスタンスタイプを制限するための制御メカニズムを実装する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to",
        "text_jp": "AWS Configで希望のインスタンスタイプ管理ルールを作成します。許可されたインスタンスタイプでルールを構成します。このルールを"
      },
      {
        "key": "B",
        "text": "In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the",
        "text_jp": "EC2コンソールで許可されたインスタンスタイプを指定した起動テンプレートを作成します。この起動テンプレートを"
      },
      {
        "key": "C",
        "text": "Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for",
        "text_jp": "新しいIAMポリシーを作成します。許可されたインスタンスタイプを指定します。このポリシーを開発者のIAMアカウントを含むIAMグループにアタッチします。"
      },
      {
        "key": "D",
        "text": "Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image.",
        "text_jp": "EC2 Image Builderを使用して開発者のためのイメージパイプラインを作成し、ゴールデンイメージの作成を支援します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating a new IAM policy that specifies allowed instance types will directly restrict the developers to only use those instance types.",
        "situation_analysis": "The company needs a control mechanism to limit the instance types that developers can launch, which will help reduce unnecessary costs associated with using large instance types for testing.",
        "option_analysis": "Option A involves AWS Config, which is useful for compliance, but it won't prevent developers from launching unwanted instance types. Option B focuses on launch templates, but it doesn't enforce restrictions effectively at the IAM level. Option D does not address the core issue of restricting instance types.",
        "additional_knowledge": "Implementing effective IAM policies can significantly reduce costs and improve governance.",
        "key_terminology": "IAM Policy, Instance Type, EC2, Cost Management",
        "overall_assessment": "This question effectively tests knowledge of IAM policies and their role in controlling resource usage within AWS, highlighting the importance of cost optimization in cloud environments."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。許可されたインスタンスタイプを指定した新しいIAMポリシーを作成することによって、開発者がそのインスタンスタイプのみを使用するように直接制限されます。",
        "situation_analysis": "会社は、開発者が起動できるインスタンスタイプを制限するための制御メカニズムが必要であり、これによりテストのために大規模なインスタンスタイプを使用することによる不要なコストを削減するのに役立ちます。",
        "option_analysis": "選択肢AはAWS Configを含み、コンプライアンスには有用ですが、開発者が望ましくないインスタンスタイプを起動するのを防ぐことはできません。選択肢Bは起動テンプレートに焦点を当てていますが、IAMレベルでの制限を効果的に施行することはできません。選択肢Dはインスタンスタイプを制限するという根本的な問題には対処していません。",
        "additional_knowledge": "効果的なIAMポリシーの実装は、コストを大幅に削減し、ガバナンスを改善する可能性があります。",
        "key_terminology": "IAMポリシー, インスタンスタイプ, EC2, コスト管理",
        "overall_assessment": "この質問は、AWS内でのリソース使用を制御する上でのIAMポリシーの知識を効果的にテストしており、クラウド環境におけるコスト最適化の重要性を強調しています。"
      }
    ],
    "keywords": [
      "IAM Policy",
      "Instance Type",
      "EC2",
      "Cost Management"
    ]
  },
  {
    "No": "113",
    "question": "A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the\nsame organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team\nresponsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.\nWhich actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.)",
    "question_jp": "ある企業がAWSクラウドでいくつかのプロジェクトを開発およびホスティングしています。プロジェクトは同じ組織のAWS Organizationsの複数のAWSアカウントで開発されています。企業は、クラウドインフラのコストを所有プロジェクトに割り当てる必要があります。すべてのAWSアカウントを担当するチームは、いくつかのAmazon EC2インスタンスにコスト割り当てに使用されるProjectタグが欠けていることを発見しました。この問題を解決し、将来同じことが起きないようにするためにソリューションアーキテクトが取るべき行動はどれか。（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Config rule in each account to find resources with missing tags.",
        "text_jp": "各アカウントにAWS Configルールを作成し、タグが欠けているリソースを見つける。"
      },
      {
        "key": "B",
        "text": "Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.",
        "text_jp": "プロジェクトタグが欠けている場合にec2:RunInstancesの拒否アクションを持つSCPを組織内に作成する。"
      },
      {
        "key": "C",
        "text": "Use Amazon Inspector in the organization to find resources with missing tags.",
        "text_jp": "組織内でAmazon Inspectorを使用して、タグが欠けているリソースを見つける。"
      },
      {
        "key": "D",
        "text": "Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing.",
        "text_jp": "各アカウントにIAMポリシーを作成し、プロジェクトタグが欠けている場合にec2:RunInstancesの拒否アクションを持たせる。"
      },
      {
        "key": "E",
        "text": "Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.",
        "text_jp": "組織用のAWS Configアグリゲーターを作成し、Projectタグが欠けているEC2インスタンスのリストを収集する。"
      },
      {
        "key": "F",
        "text": "Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag.",
        "text_jp": "AWS Security Hubを使用して、Projectタグが欠けているEC2インスタンスのリストを集約する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ABE (82%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using Amazon Inspector is beneficial for identifying resources without appropriate tags.",
        "situation_analysis": "The organization has multiple AWS accounts with projects, and proper cost allocation is essential for financial management.",
        "option_analysis": "Options A, B, D, and E can help manage tags, but only options C and F explicitly focus on identifying missing tags. Option F does not directly address the issue as it aggregates outputs rather than identifies missing tags.",
        "additional_knowledge": "Tags are essential for cost allocation, and identifying missing tags helps mitigate compliance issues.",
        "key_terminology": "AWS Config, tags, Amazon Inspector, cost allocation, IAM policy",
        "overall_assessment": "While options C and E provide solutions, C is the most effective for identifying issues proactively. The community shows strong support for A, B, and E, but the right focus remains on identifying issues."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。Amazon Inspectorを使用することで、適切なタグが欠けているリソースを特定するのに役立つ。",
        "situation_analysis": "この組織は複数のAWSアカウントを持ち、プロジェクトがあるため、適切なコスト割り当てが財務管理において重要である。",
        "option_analysis": "選択肢A、B、D、Eもタグ管理に役立つが、CとFの選択肢だけが明示的にタグが欠けているリソースを特定することに焦点を当てている。選択肢Fは出力を集約するものであり、欠けているタグを特定することには直接関与していない。",
        "additional_knowledge": "タグはコスト割り当てに不可欠であり、欠けているタグを特定することでコンプライアンスの問題を緩和する。",
        "key_terminology": "AWS Config、タグ、Amazon Inspector、コスト割り当て、IAMポリシー",
        "overall_assessment": "Cは問題を事前に特定するのに最も効果的であるため、選択肢CとEが解決策を提供しつつも、Cが最も優れている。コミュニティの支持はA、B、Eに強いが、問題の特定に焦点を当てることが重要である。"
      }
    ],
    "keywords": [
      "AWS Config",
      "tags",
      "Amazon Inspector",
      "cost allocation",
      "IAM policy"
    ]
  },
  {
    "No": "114",
    "question": "A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due\nto heavy ingestion and it frequently runs out of storage.\nThe company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should\ninclude the following attributes:\n• Managed AWS services to minimize operational complexity.\n• A buffer that automatically scales to match the throughput of data and requires no ongoing administration.\n• A visualization tool to create dashboards to observe events in near-real time.\n• Support for semi-structured JSON data and dynamic schemas.\nWhich combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)",
    "question_jp": "ある企業は、イベントの永続化のためにPostgreSQLデータベースを使用したオンプレミスの監視ソリューションを持っています。このデータベースは、濃厚なデータの取り込みによりスケールできず、頻繁にストレージが不足しています。この企業はハイブリッドソリューションを構築したいと考えており、すでにそのネットワークとAWS間にVPN接続を設定しています。このソリューションには以下の要件が含まれるべきです： • 運用の複雑さを最小限に抑えるための管理されたAWSサービス。 • データのスループットに応じて自動的にスケールするバッファで、継続的な管理が不要。 • 近いリアルタイムでイベントを観察するためのダッシュボードを作成する可視化ツール。 • 半構造化されたJSONデータと動的スキーマのサポート。 どのコンポーネントの組み合わせが、企業がこれらの要件を満たす監視ソリューションを作成することを可能にしますか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.",
        "text_jp": "Amazon Kinesis Data Firehoseを使用してイベントをバッファリングします。AWS Lambda関数を作成してイベントを処理及び変換します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.",
        "text_jp": "Amazon Kinesisデータストリームを作成してイベントをバッファリングします。AWS Lambda関数を作成してイベントを処理及び変換します。"
      },
      {
        "key": "C",
        "text": "Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-",
        "text_jp": "Amazon Aurora PostgreSQL DBクラスターを設定してイベントを受信します。Amazon QuickSightを使用してデータベースから読み取り、近くの"
      },
      {
        "key": "D",
        "text": "Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-",
        "text_jp": "Amazon Elasticsearch Service (Amazon ES)を設定してイベントを受信します。Amazon ESにデプロイされたKibanaエンドポイントを使用して近くの"
      },
      {
        "key": "E",
        "text": "Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time",
        "text_jp": "Amazon Neptune DBインスタンスを設定してイベントを受信します。Amazon QuickSightを使用してデータベースから読み取り、近いリアルタイムで"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and B. Both solutions utilize AWS services designed to handle streaming data, allowing for scalability and proper event handling.",
        "situation_analysis": "The company requires a solution to manage heavy event ingestion without operational overhead and that can handle semi-structured data.",
        "option_analysis": "Option A uses Kinesis Data Firehose, which automatically scales and requires no ongoing management. Option B uses Kinesis Data Streams, which also meets the buffering requirement but is more complex than Firehose.",
        "additional_knowledge": "In addition to Kinesis Firehose, other services like S3 can be used for storage after processing.",
        "key_terminology": "Amazon Kinesis Data Firehose, AWS Lambda, semi-structured data, JSON, real-time processing",
        "overall_assessment": "Analysis indicates that while both answers meet the requirements, A is preferred due to its simplicity and automatic management capabilities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとBである。両方のソリューションはストリーミングデータを処理するために設計されたAWSサービスを利用しており、スケーラビリティと適切なイベント処理を可能にする。",
        "situation_analysis": "この企業は、運用の負担をかけずに大量のイベントの取り込みを管理し、半構造化データを扱うことができるソリューションを必要としている。",
        "option_analysis": "選択肢Aは、Kinesis Data Firehoseを使用して自動的にスケールし、継続的な管理が不要なバッファリングを実現している。選択肢Bは、Kinesisデータストリームを使用しており、バッファリング要件を満たすがFirehoseよりも複雑である。",
        "additional_knowledge": "Kinesis Firehoseに加えて、処理後のストレージにはS3などの他のサービスも使用できる。",
        "key_terminology": "Amazon Kinesis Data Firehose, AWS Lambda, 半構造化データ, JSON, リアルタイム処理",
        "overall_assessment": "分析によれば、両方の回答が要件を満たすが、Aの方がシンプルさと自動管理機能が優れているため推奨される。"
      }
    ],
    "keywords": [
      "Amazon Kinesis Data Firehose",
      "AWS Lambda",
      "semi-structured data",
      "JSON",
      "real-time processing"
    ]
  },
  {
    "No": "115",
    "question": "A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private\nsubnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company's applications read from and write to\nAmazon Kinesis Data Streams. Most of the workloads run in private subnets.\nA solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications.\nThe solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that\nNatGateway-Bytes charges are increasing the cost in the EC2-Other category.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "チームは会社全体の行動データを収集し、ルーティングしています。この会社は、パブリックサブネット、プライベートサブネット、インターネットゲートウェイを持つMulti-AZ VPC環境を運用しています。各パブリックサブネットには、またNATゲートウェイも含まれています。会社のほとんどのアプリケーションは、Amazon Kinesis Data Streamsから読み取り、書き込みを行います。ほとんどのワークロードはプライベートサブネットで実行されています。ソリューションアーキテクトはインフラストラクチャをレビューする必要があります。ソリューションアーキテクトは、コストを削減し、アプリケーションの機能を維持する必要があります。ソリューションアーキテクトはCost Explorerを使用し、EC2-Otherカテゴリのコストが常に高いことに気付きました。さらにレビューすると、NatGateway-Bytesの料金がEC2-Otherカテゴリのコストを増加させていることが明らかになりました。ソリューションアーキテクトはこれらの要件を満たすために何をすべきか？",
    "choices": [
      {
        "key": "A",
        "text": "Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for trafic that can be removed. Ensure that security groups are blocking",
        "text_jp": "VPCフローログを有効にする。Amazon Athenaを使用して、削除可能なトラフィックのログを分析する。セキュリティグループがブロックしていることを確認する"
      },
      {
        "key": "B",
        "text": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the",
        "text_jp": "Kinesis Data Streams用のインターフェイスVPCエンドポイントをVPCに追加する。アプリケーションが正しいIAM権限を持っていることを確認する"
      },
      {
        "key": "C",
        "text": "Enable VPC Flow Logs and Amazon Detective. Review Detective findings for trafic that is not related to Kinesis Data Streams. Configure",
        "text_jp": "VPCフローログとAmazon Detectiveを有効にする。Kinesis Data Streamsに関連しないトラフィックの調査結果をレビューする"
      },
      {
        "key": "D",
        "text": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows trafic from the",
        "text_jp": "Kinesis Data Streams用のインターフェイスVPCエンドポイントをVPCに追加する。VPCエンドポイントポリシーがトラフィックを許可することを確認する"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Adding an interface VPC endpoint for Kinesis Data Streams will allow the applications running in private subnets to access Kinesis without incurring NAT Gateway charges. This reduces costs significantly.",
        "situation_analysis": "The company is experiencing high costs related to NatGateway-Bytes, which suggests that the applications access Kinesis Data Streams over the internet rather than through private links.",
        "option_analysis": "Option D directly addresses the cost issue by allowing private subnets to connect to Kinesis Data Streams without using the NAT Gateway, thus reducing costs. Option A does not directly address the NAT Gateway charges, while Option B and C focus on additional monitoring or IAM setups without reducing the cost source.",
        "additional_knowledge": "Both interface and gateway endpoints can be used in a VPC, depending on service requirements and cost factors.",
        "key_terminology": "VPC Endpoint, Amazon Kinesis Data Streams, NAT Gateway, Private Subnet, Cost Optimization",
        "overall_assessment": "Option D is the most effective solution to meet the requirements of reducing costs while maintaining application functionality. Community votes support this, indicating a strong consensus on the effectiveness of this solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDである。Kinesis Data Streams用のインターフェイスVPCエンドポイントを追加することで、プライベートサブネットで実行されているアプリケーションがNATゲートウェイの料金を発生させることなくKinesisにアクセスでき、コストを大幅に削減することができる。",
        "situation_analysis": "会社はNatGateway-Bytesに関連する高いコストを経験しており、これはアプリケーションがインターネットを介してKinesis Data Streamsにアクセスしていることを示唆している。",
        "option_analysis": "選択肢Dは、NATゲートウェイを使用することなくプライベートサブネットからKinesis Data Streamsに接続できるようにするので、コストの問題に直接対応している。一方、選択肢AはNATゲートウェイ料金には直接関与しない、選択肢BとCは追加の監視やIAMの設定に焦点をあてるがコスト発生源を減少させるものではない。",
        "additional_knowledge": "インターフェイスエンドポイントとゲートウェイエンドポイントは、サービスの要件やコスト要因に応じてVPC内で使用されることができる。",
        "key_terminology": "VPCエンドポイント、Amazon Kinesis Data Streams、NATゲートウェイ、プライベートサブネット、コスト最適化",
        "overall_assessment": "選択肢Dは、コスト削減とアプリケーションの機能維持の要件を満たすために最も効果的な解決策である。コミュニティの投票もこれを支持しており、この解決策の効果について強い合意があることを示している。"
      }
    ],
    "keywords": [
      "VPC Endpoint",
      "Amazon Kinesis Data Streams",
      "NAT Gateway",
      "Private Subnet",
      "Cost Optimization"
    ]
  },
  {
    "No": "116",
    "question": "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and\nus-east-1 Regions. The company wants to be able to route network trafic from its on-premises infrastructure into VPCs in either of those Regions.\nThe company also needs to support trafic that is routed directly between VPCs in those Regions. No single points of failure can exist on the\nnetwork.\nThe company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a\nseparate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a\nsingle AWS Transit Gateway that is configured to route all inter-VPC trafic within that Region.\nWhich solution will meet these requirements?",
    "question_jp": "小売会社はヨーロッパにオンプレミスのデータセンターを持っています。この会社は、eu-west-1およびus-east-1リージョンを含むマルチリージョンのAWSプレゼンスも持っています。会社は、オンプレミスのインフラストラクチャからこれらのリージョンのVPCにネットワークトラフィックをルーティングできるようにしたいと考えています。また、これらのリージョン内のVPC間で直接ルーティングされるトラフィックをサポートする必要があります。ネットワーク上には単一故障点が存在してはいけません。会社はすでにオンプレミスのデータセンターから2つの1 GbpsのAWS Direct Connect接続を作成しています。各接続は、高可用性のためにヨーロッパの異なるDirect Connectロケーションに接続されています。これら2つのロケーションは、それぞれDX-AおよびDX-Bと名付けられています。各リージョンには、そのリージョン内のすべてのVPC間トラフィックをルーティングするように設定された単一のAWSトランジットゲートウェイがあります。どのソリューションがこれらの要件を満たすことができますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same",
        "text_jp": "DX-A接続からのプライベートVIFをDirect Connectゲートウェイに作成します。同じ接続先へのDX-B接続からのプライベートVIFを作成します。"
      },
      {
        "key": "B",
        "text": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct",
        "text_jp": "DX-A接続からのトランジットVIFをDirect Connectゲートウェイに作成します。このDirectにeu-west-1トランジットゲートウェイを関連付けます。"
      },
      {
        "key": "C",
        "text": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same",
        "text_jp": "DX-A接続からのトランジットVIFをDirect Connectゲートウェイに作成します。DX-B接続からのトランジットVIFを同じ接続先に作成します。"
      },
      {
        "key": "D",
        "text": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same",
        "text_jp": "DX-A接続からのトランジットVIFをDirect Connectゲートウェイに作成します。DX-B接続からのトランジットVIFを同じ接続先に作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (93%) 3%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This solution allows for highly available connectivity from the on-premises environment to the AWS VPCs.",
        "situation_analysis": "The company requires reliable network connections without any single points of failure while allowing traffic routing between on-premises data centers and AWS VPCs across multiple regions.",
        "option_analysis": "Option A effectively utilizes the private virtual interface (VIF) from both Direct Connect locations to achieve redundancy and high availability. Other options do not sufficiently address high availability.",
        "additional_knowledge": "Using Direct Connect for private connectivity enhances security compared to public routes.",
        "key_terminology": "Direct Connect, VPC, Transit Gateway",
        "overall_assessment": "Considering community voting heavily favors option D, it is crucial to analyze whether this option meets the requirements effectively. Despite the majority vote, the technical accuracy supports option A as the correct solution, as it specifically addresses high availability needs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。このソリューションは、オンプレミス環境とAWSのVPC間での高可用性接続を実現可能である。",
        "situation_analysis": "会社は、オンプレミスのデータセンターと複数のリージョン内のAWS VPCとの間で、単一故障点なしに信頼できるネットワーク接続を必要としている。",
        "option_analysis": "選択肢Aは、冗長性と高可用性を達成するために、両方のDirect Connectロケーションからのプライベート仮想インターフェース（VIF）を効果的に利用している。他の選択肢は、高可用性を十分に考慮していない。",
        "additional_knowledge": "Direct Connectを使用することで、パブリックなルートに比べてセキュリティが向上するプライベート接続が可能となる。",
        "key_terminology": "Direct Connect, VPC, Transit Gateway",
        "overall_assessment": "コミュニティ投票は選択肢Dに大きく偏っているが、この選択肢が要件に効果的に合致しているかを分析することが重要である。多数票にもかかわらず、技術的な正確さは選択肢Aを正しい解答として支持しており、高可用性のニーズを特に考慮している。"
      }
    ],
    "keywords": [
      "Direct Connect",
      "VPC",
      "Transit Gateway"
    ]
  },
  {
    "No": "117",
    "question": "A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new\nIAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user.\nThe company has a multi-Region AWS CloudTrail trail in the AWS account.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ある企業はAWSクラウドでアプリケーションを実行しています。この企業のセキュリティチームは、新しいIAMユーザーの作成を承認しなければなりません。新しいIAMユーザーが作成されると、そのユーザーに対するすべてのアクセス権を自動的に削除する必要があります。その後、セキュリティチームはユーザーを承認するための通知を受け取る必要があります。この企業は、AWSアカウント内にマルチリージョンのAWS CloudTrailトレイルを持っています。どのステップの組み合わせがこれらの要件を満たしますか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via",
        "text_jp": "Amazon EventBridge (Amazon CloudWatch Events) ルールを作成します。パターンを定義し、詳細タイプ値を AWS API Call via に設定します"
      },
      {
        "key": "B",
        "text": "Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "text_jp": "CloudTrailを構成して、CreateUser イベントの通知をAmazon Simple Notification Service (Amazon SNS) トピックに送信します。"
      },
      {
        "key": "C",
        "text": "Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access.",
        "text_jp": "AWS Fargate技術を使用して、Amazon Elastic Container Service (Amazon ECS) で実行されるコンテナを呼び出してアクセスを削除します。"
      },
      {
        "key": "D",
        "text": "Invoke an AWS Step Functions state machine to remove access.",
        "text_jp": "AWS Step Functions ステートマシンを呼び出してアクセスを削除します。"
      },
      {
        "key": "E",
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.",
        "text_jp": "Amazon Simple Notification Service (Amazon SNS) を使用して、セキュリティチームに通知します。"
      },
      {
        "key": "F",
        "text": "Use Amazon Pinpoint to notify the security team.",
        "text_jp": "Amazon Pinpoint を使用して、セキュリティチームに通知します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ADE (88%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, B, and E. These steps establish a process to remove access and notify the security team.",
        "situation_analysis": "The requirements indicate that IAM user creation must be monitored, access automatically revoked, and security notified.",
        "option_analysis": "A sets up the event rule, B ensures CloudTrail notifications are sent for user creation, and E delivers the notification to the security team.",
        "additional_knowledge": "Consider AWS Lambda or Step Functions for more complex access removal automation when leveraging these notifications.",
        "key_terminology": "IAM, EventBridge, CloudTrail, SNS",
        "overall_assessment": "The community supports options A and E, indicating a strong consensus on the need for immediate notifications and automation."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA、B、およびEです。これらのステップは、アクセスを削除し、セキュリティチームに通知するプロセスを確立します。",
        "situation_analysis": "要求は、IAMユーザーの作成を監視し、アクセスを自動的に剥奪し、セキュリティに通知する必要があることを示しています。",
        "option_analysis": "Aはイベントルールを設定し、Bはユーザー作成の通知がCloudTrailから送信されることを確保し、Eはセキュリティチームへの通知を提供します。",
        "additional_knowledge": "これらの通知を活用する際には、アクセス削除の自動化をより複雑に行うためにAWS LambdaやStep Functionsを考慮してください。",
        "key_terminology": "IAM、EventBridge、CloudTrail、SNS",
        "overall_assessment": "コミュニティはAおよびEのオプションを支持しており、即時通知と自動化の必要性に関する強い合意を示しています。"
      }
    ],
    "keywords": [
      "IAM",
      "EventBridge",
      "CloudTrail",
      "SNS"
    ]
  },
  {
    "No": "118",
    "question": "A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and\napplications. The company also wants to keep the trafic on a private network. Multi-factor authentication (MFA) is required at login, and specific\nroles are assigned to user groups.\nThe company must create separate accounts for development. staging, production, and shared network. The production account and the shared\nnetwork account must have connectivity to all accounts. The development account and the staging account must have access only to each other.\nWhich combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)",
    "question_jp": "ある企業がAWSへの移行を希望しています。企業は、すべてのアカウントとアプリケーションへの中央管理されたアクセスを持つマルチアカウント構造を使用したいと考えています。また、トラフィックはプライベートネットワーク上に保持したいと考えています。ログイン時に多要素認証（MFA）が必要であり、ユーザーグループには特定の役割が割り当てられます。企業は開発、ステージング、プロダクション、共有ネットワーク用の別々のアカウントを作成する必要があります。プロダクションアカウントと共有ネットワークアカウントはすべてのアカウントに接続する必要があります。開発アカウントとステージングアカウントは互いにのみアクセス可能でなければなりません。これらの要件を満たすためにソリューションアーキテクトが取るべきステップの組み合わせはどれですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization",
        "text_jp": "AWS Control Towerを使用してランディングゾーン環境をデプロイします。アカウントを登録し、既存のアカウントを結果的な組織に招待します。"
      },
      {
        "key": "B",
        "text": "Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login.",
        "text_jp": "すべてのアカウントでAWS Security Hubを有効にしてクロスアカウントアクセスを管理します。AWS CloudTrailを通じて発見結果を収集し、MFAログインを強制します。"
      },
      {
        "key": "C",
        "text": "Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables.",
        "text_jp": "各アカウントにトランジットゲートウェイとトランジットゲートウェイVPCアタッチメントを作成します。適切なルートテーブルを構成します。"
      },
      {
        "key": "D",
        "text": "Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing",
        "text_jp": "AWS IAMアイデンティティセンター（AWS SSO）を設定して有効にします。既存のアカウントに必要なMFAを備えた適切なアクセス権セットを作成します。"
      },
      {
        "key": "E",
        "text": "Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA",
        "text_jp": "すべてのアカウントでAWS Control Towerを有効にしてアカウント間のルーティングを管理します。AWS CloudTrailを通じて発見結果を収集し、MFAを強制します。"
      },
      {
        "key": "F",
        "text": "Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognoto user pools and Identity pools to manage access to",
        "text_jp": "IAMユーザーとグループを作成します。すべてのユーザーにMFAを設定します。Amazon Cognitoユーザープールとアイデンティティプールを設定してアクセスを管理します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ACD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which emphasizes enabling AWS Security Hub for managing cross-account access with findings collected through AWS CloudTrail.",
        "situation_analysis": "The company needs a multi-account setup to securely manage development, staging, and production environments while enforcing MFA for access control.",
        "option_analysis": "Option B allows for centralized management of security findings and access, ensuring compliance with MFA requirements. Options A, C, D, and E have partial solutions but do not encapsulate the complete requirement for MFA enforcement and centralized management as effectively as option B.",
        "additional_knowledge": "Multi-factor authentication strengthens security by requiring two or more verification methods.",
        "key_terminology": "AWS Security Hub, multi-account structure, MFA, AWS CloudTrail, centralized management.",
        "overall_assessment": "Option B meets the company’s requirements effectively, although options A, C, D, and E may provide useful supplemental capabilities"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、クロスアカウントアクセスを管理するためにAWS Security Hubを有効にし、AWS CloudTrailを通じて発見結果を収集することを強調しています。",
        "situation_analysis": "企業は、開発、ステージング、プロダクション環境を安全に管理するためのマルチアカウント構造が必要であり、アクセス制御のためにMFAを施行する必要があります。",
        "option_analysis": "Bオプションは、セキュリティ発見とアクセスの中央管理を可能にし、MFA要件を確実に守ることができます。A、C、D、Eの各オプションは部分的な解決策です。完全なMFAの施行と中央管理の要件をBオプションほど効果的に含んでいません。",
        "additional_knowledge": "多要素認証は、2つ以上の検証方法を要求することによってセキュリティを強化します。",
        "key_terminology": "AWS Security Hub、マルチアカウント構造、MFA、AWS CloudTrail、中央管理。",
        "overall_assessment": "Bオプションは、企業の要件を効果的に満たしており、A、C、D、Eオプションは役立つ補助機能を提供するかもしれません。"
      }
    ],
    "keywords": [
      "AWS Security Hub",
      "MFA",
      "AWS CloudTrail",
      "multi-account structure",
      "centralized management"
    ]
  },
  {
    "No": "119",
    "question": "A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production.\nAll the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases.\nThe databases are between 500 GB and 800 GB in size.\nThe development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7\ndays a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or\nproduction as the key.\nWhat should a solutions architect do to reduce costs with the LEAST operational effort?",
    "question_jp": "会社は、eu-west-1 リージョンでアプリケーションを運用しており、開発、テスト、および本番環境ごとに 1 つのアカウントを持っています。すべての環境は、状態を保持する Amazon EC2 インスタンスおよび Amazon RDS for MySQL データベースを使用して、24 時間年中無休で稼働しています。データベースのサイズは 500 GB から 800 GB の間です。開発チームとテストチームは、営業日の日中に作業を行っていますが、本番環境は、24 時間年中無休で稼働しています。会社はコストを削減したいと考えています。すべてのリソースには、development、testing、または production のいずれかのキーが付いた環境タグが付けられています。運用上の手間を最小限に抑えながらコストを削減するために、ソリューションアーキテクトは何を行うべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or slops",
        "text_jp": "Amazon EventBridge ルールを作成し、それを毎日 1 回実行します。ルールを設定して、1 つの AWS Lambda 関数を呼び出して、起動または停止します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that",
        "text_jp": "Amazon EventBridge ルールを作成し、それを営業日の夕方に毎日実行します。ルールを設定して、AWS Lambda 関数を呼び出して、"
      },
      {
        "key": "C",
        "text": "Create an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that",
        "text_jp": "Amazon EventBridge ルールを作成し、それを営業日の夕方に毎日実行します。ルールを設定して、AWS Lambda 関数を呼び出して、"
      },
      {
        "key": "D",
        "text": "Create an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores",
        "text_jp": "Amazon EventBridge ルールを作成し、それを毎時間実行します。ルールを設定して、1 つの AWS Lambda 関数を呼び出して、終了または復元します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating a daily EventBridge rule with an AWS Lambda function optimally addresses the cost reduction requirements for non-production environments while reducing operational efforts.",
        "situation_analysis": "The application operates in multiple environments, with production running continuously and development/testing limited to business hours. This provides an opportunity to scale down non-production resources during off-hours to save costs.",
        "option_analysis": "Option A allows for daily resource management, which fits perfectly with the development and testing schedules. In contrast, options B, C, and D involve more complex schedules or higher frequency that may be unnecessary, particularly for environments that are not in use regularly.",
        "additional_knowledge": "Establishing cost-saving measures using automation should be a primary consideration in architecting cloud solutions.",
        "key_terminology": "Amazon EC2, Amazon RDS, AWS Lambda, EventBridge, Automation",
        "overall_assessment": "The question effectively assesses understanding of cost management strategies on AWS, particularly focusing on automation tools. The community vote suggesting option B reflects a potential misunderstanding of the task's requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは A である。AWS Lambda 関数を使用して毎日実行される EventBridge ルールを作成することで、非生産環境のコスト削減要件を最適に満たし、運用の手間を減らすことができる。",
        "situation_analysis": "このアプリケーションは複数の環境で運用されており、本番環境は常時稼働し、開発/テストは営業日のみ実施されている。このため、オフ時間に非生産的なリソースをスケールダウンすることでコスト削減の機会がある。",
        "option_analysis": "選択肢 A は、開発やテストのスケジュールにぴったり合った毎日のリソース管理を可能にする。対照的に、選択肢 B、C、D は、複雑なスケジュールや高頻度が必要となり、特に定期的に使用されない環境には必要ない可能性がある。",
        "additional_knowledge": "自動化を使用したコスト削減措置を確立することは、クラウドソリューションを設計する際の主要な考慮事項となるべきである。",
        "key_terminology": "Amazon EC2, Amazon RDS, AWS Lambda, EventBridge, 自動化",
        "overall_assessment": "この質問は、AWS のコスト管理戦略、特に自動化ツールに関する理解を評価するのに効果的である。コミュニティの投票が選択肢 B を推薦しているが、これはタスクの要件を誤解している可能性がある。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Amazon RDS",
      "AWS Lambda",
      "EventBridge",
      "Automation"
    ]
  },
  {
    "No": "120",
    "question": "A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed an Amazon API Gateway REST API with AWS\nLambda integration in multiple AWS Regions and in the same production account.\nThe company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of API calls per second. The\npremium tier offers up to 3,000 calls per second, and customers are identified by a unique API key. Several premium tier customers in various\nRegions report that they receive error responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logs indicate\nthat the Lambda function is never invoked.\nWhat could be the cause of the error messages for these customers?",
    "question_jp": "ある会社がAWS上にサービスとしてのソフトウェア（SaaS）ソリューションを構築しています。会社は、複数のAWSリージョンおよび同じプロダクションアカウントでAWS Lambda統合を持つAmazon API Gateway REST APIを展開しています。会社は、顧客が毎秒一定回数のAPI呼び出しを行うためのキャパシティに対して支払うことができる段階価格を提供しています。プレミアムティアは毎秒最大3,000回の呼び出しを提供し、顧客はユニークなAPIキーによって識別されます。複数のリージョンのプレミアムティア顧客が、ピーク使用時間中に複数のAPIメソッドから429 Too Many Requestsのエラーレスポンスを受け取るとの報告があります。ログは、Lambda関数が決して呼び出されていないことを示しています。これらの顧客に対するエラーメッセージの原因は何でしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "The Lambda function reached its concurrency limit.",
        "text_jp": "Lambda関数が同時実行制限に達しました。"
      },
      {
        "key": "B",
        "text": "The Lambda function its Region limit for concurrency.",
        "text_jp": "Lambda関数がリージョンごとの同時実行制限に達しました。"
      },
      {
        "key": "C",
        "text": "The company reached its API Gateway account limit for calls per second.",
        "text_jp": "会社がAPI Gatewayのアカウントごとの呼び出し制限に達しました。"
      },
      {
        "key": "D",
        "text": "The company reached its API Gateway default per-method limit for calls per second.",
        "text_jp": "会社がAPI Gatewayのメソッドごとのデフォルト呼び出し制限に達しました。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is 'C', indicating that the company has reached its API Gateway account limit for calls per second.",
        "situation_analysis": "The company offers a tiered pricing model, and premium customers are experiencing errors during peak hours, which suggests a capacity issue.",
        "option_analysis": "'A' and 'B' discuss limits related to Lambda, but they are not invoked based on logs. Therefore, they cannot be the cause. 'D' relates to method-specific limits but does not encompass the overall account limit, which affects all methods.",
        "additional_knowledge": "Monitoring limits can help avoid service disruptions during peak usage.",
        "key_terminology": "API Gateway, calls per second, account limit, 429 Too Many Requests, tiered pricing",
        "overall_assessment": "Given the community vote distribution, there's unanimous support for 'C', reinforcing the conclusion that account limits are likely reached."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは「C」であり、会社がAPI Gatewayのアカウントごとの呼び出し制限に達したことを示しています。",
        "situation_analysis": "会社は段階的な価格モデルを提供しており、プレミアム顧客はピーク時にエラーを経験しています。これにより、キャパシティの問題が示唆されます。",
        "option_analysis": "選択肢'A'と'B'はLambdaに関連する制限について述べていますが、ログによると呼び出されていないため、原因にはなりません。選択肢'D'はメソッド特有の制限に関連しますが、全体のアカウント制限を考慮しないため、全てのメソッドに影響を与えるアカウント制限が問題です。",
        "additional_knowledge": "制限を監視することで、ピーク使用時のサービス中断を避けるのに役立ちます。",
        "key_terminology": "API Gateway、呼び出し毎秒、アカウント制限、429 Too Many Requests、段階的価格設定",
        "overall_assessment": "コミュニティの投票分布を考慮すると、「C」に対して皆が支持しているため、アカウント制限に到達しているという結論を裏付けています。"
      }
    ],
    "keywords": [
      "API Gateway",
      "calls per second",
      "account limit",
      "429 Too Many Requests",
      "tiered pricing"
    ]
  },
  {
    "No": "121",
    "question": "A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor\nthe inbound trafic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available\nfrom its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology.\nThe company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a\ndedicated VPC. The company needs to use the security tool to inspect all packets that come in and out of the VPC. This inspection must occur in\nreal time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available\nwithin an AWS Region.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "金融業界の会社が、オンプレミスからAWSへのウェブアプリケーションの移行を計画しています。会社は、アプリケーションへの受信トラフィックを監視するためにサードパーティのセキュリティツールを使用しています。このセキュリティツールは過去15年間使用されており、ベンダーからクラウドソリューションは提供されていません。会社のセキュリティチームは、AWSテクノロジーとの統合方法を懸念しています。会社は、アプリケーションの移行をAWS上でAmazon EC2インスタンスにデプロイする予定であり、EC2インスタンスは専用VPC内のオートスケーリンググループで実行されます。会社は、VPCに出入りするすべてのパケットをインスペクトするためにセキュリティツールを使用する必要があります。このインスペクションはリアルタイムで行う必要があり、アプリケーションのパフォーマンスに影響を与えてはなりません。ソリューションアーキテクトは、AWS内で非常に高い可用性を持つターゲットアーキテクチャを設計する必要があります。これらの要件を満たすために、ソリューションアーキテクトはどのステップの組み合わせを取るべきですか？（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC",
        "text_jp": "EC2インスタンスにセキュリティツールをデプロイし、既存のVPC内の新しいオートスケーリンググループで実行する"
      },
      {
        "key": "B",
        "text": "Deploy the web application behind a Network Load Balancer",
        "text_jp": "ウェブアプリケーションをネットワークロードバランサーの背後にデプロイする"
      },
      {
        "key": "C",
        "text": "Deploy an Application Load Balancer in front of the security tool instances",
        "text_jp": "セキュリティツールインスタンスの前にアプリケーションロードバランサーをデプロイする"
      },
      {
        "key": "D",
        "text": "Provision a Gateway Load Balancer for each Availability Zone to redirect the trafic to the security tool",
        "text_jp": "各アベイラビリティゾーンのためにゲートウェイロードバランサーをプロビジョニングし、トラフィックをセキュリティツールにリダイレクトする"
      },
      {
        "key": "E",
        "text": "Provision a transit gateway to facilitate communication between VPCs.",
        "text_jp": "VPC間の通信を促進するためにトランジットゲートウェイをプロビジョニングする"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (54%) DE (40%) 4%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Deploying the security tool on EC2 instances in a new Auto Scaling group allows real-time packet inspection without affecting performance.",
        "situation_analysis": "The company has an existing on-premises application using a third-party security tool. They seek to migrate this application to AWS but need to keep using the same security tool, which does not support cloud solutions. The solution must maintain application performance while ensuring real-time security checks.",
        "option_analysis": "Option A is correct because it allows the integration of the existing security tool in a cloud environment using EC2 and Auto Scaling. Option B does not directly solve the inspection requirement; Option C does not address how to incorporate the existing security tool; Option D introduces unnecessary complexity; Option E does not fulfill the security inspection requirement.",
        "additional_knowledge": "Understanding how to scale security solutions in the cloud is essential for maintaining application integrity.",
        "key_terminology": "EC2, Auto Scaling, Security Tool, Load Balancer, VPC",
        "overall_assessment": "This question is well-constructed because it covers the integration of existing on-premises systems into AWS and the critical consideration of maintaining performance during security inspections."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。EC2インスタンスにセキュリティツールをデプロイし、新しいオートスケーリンググループで実行することで、パフォーマンスに影響を与えることなくリアルタイムのパケットインスペクションを実現できます。",
        "situation_analysis": "会社は、サードパーティのセキュリティツールを使用した既存のオンプレミスアプリケーションを持っています。このアプリケーションをAWSに移行したいが、同じセキュリティツールを引き続き使用する必要があります。このツールはクラウドソリューションをサポートしていません。ソリューションは、アプリケーションのパフォーマンスを維持しつつ、リアルタイムのセキュリティチェックを保証する必要があります。",
        "option_analysis": "選択肢Aは、既存のセキュリティツールをクラウド環境でEC2とオートスケーリングを使用して統合できるため、正しいです。選択肢Bはインスペクション要件に直接的な解決策を提供しません。選択肢Cは、既存のセキュリティツールの統合方法を示していません。選択肢Dは不必要な複雑さをもたらします。選択肢Eは、セキュリティインスペクション要件を満たしていません。",
        "additional_knowledge": "クラウドにおけるセキュリティソリューションのスケーリング方法を理解することは、アプリケーションの整合性を維持するために不可欠です。",
        "key_terminology": "EC2, オートスケーリング, セキュリティツール, ロードバランサー, VPC",
        "overall_assessment": "この問題は、既存のオンプレミスシステムをAWSに統合すること、及びセキュリティインスペクション中のパフォーマンス維持の重要な考慮事項を含んでいるため、よく構成されています。"
      }
    ],
    "keywords": [
      "EC2",
      "Auto Scaling",
      "Security Tool",
      "Load Balancer",
      "VPC"
    ]
  },
  {
    "No": "122",
    "question": "A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the\nvendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique\nformat. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis.\nThe company needs to design a new data analysis solution that can deliver faster and optimize costs.\nWhich solution will meet these requirements?",
    "question_jp": "企業は異なるベンダーからデバイスを購入しました。これらのデバイスにはすべてIoTセンサーが搭載されています。センサーは、ベンダー独自の形式で状態情報を送信し、レガシーアプリケーションがその情報をJSON形式に解析します。解析は簡単ですが、各ベンダーには独自の形式があります。アプリケーションは1日1回、すべてのJSONレコードを解析し、分析用にリレーショナルデータベースに記録を保存します。企業は、より高速でコストを最適化できる新しいデータ分析ソリューションを設計する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to",
        "text_jp": "IoTセンサーをAWS IoT Coreに接続します。情報を解析するためにAWS Lambda関数を呼び出すルールを設定し、.csvファイルを保存します。"
      },
      {
        "key": "B",
        "text": "Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a",
        "text_jp": "アプリケーションサーバーをAWS Fargateに移行し、IoTセンサーから情報を受け取り、情報を解析します。"
      },
      {
        "key": "C",
        "text": "Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use",
        "text_jp": "AWS Transfer for SFTPサーバーを作成します。IoTセンサーのコードを更新して、情報を.csvファイルとしてSFTPを介してサーバーに送信します。使用します。"
      },
      {
        "key": "D",
        "text": "Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon",
        "text_jp": "AWS Snowball Edgeを使用して、IoTセンサーからデータを直接収集し、ローカル分析を行います。定期的にデータをAmazonに収集します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This option optimizes data transmission and parsing.",
        "situation_analysis": "The company needs to improve data analysis speed and reduce costs.",
        "option_analysis": "Option C allows direct data transmission in a well-known format (.csv), simplifying the process and reducing latency compared to proprietary formats.",
        "additional_knowledge": "Using standardized formats facilitates easier integration with analytics tools.",
        "key_terminology": "IoT, SFTP, AWS Transfer for SFTP, JSON, cost optimization",
        "overall_assessment": "Overall, option C is the best solution aligning with the company's needs for speed and cost efficiency."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。この選択肢はデータの送信と解析を最適化する。",
        "situation_analysis": "企業はデータ分析の速度を向上させ、コストを削減する必要がある。",
        "option_analysis": "選択肢Cは、広く知られている形式（.csv）でのデータ送信を可能にし、プロプライエタリ形式と比較してプロセスを簡素化し、レイテンシを削減する。",
        "additional_knowledge": "標準化された形式を使用することで、分析ツールとの統合が容易になる。",
        "key_terminology": "IoT、SFTP、AWS Transfer for SFTP、JSON、コスト最適化",
        "overall_assessment": "全体として、選択肢Cが企業の求める速度とコスト効率に最も適した解決策である。"
      }
    ],
    "keywords": [
      "IoT",
      "SFTP",
      "AWS Transfer for SFTP",
      "JSON",
      "cost optimization"
    ]
  },
  {
    "No": "123",
    "question": "A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes\nnetworking and security strategies. The company has set up an AWS Direct Connect connection in a central network account.\nThe company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the\nresources on AWS seamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to\nthe internet through its on-premises data center.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ある会社がいくつかのアプリケーションをAWSに移行しています。この会社は、ネットワークおよびセキュリティ戦略が確定した後、アプリケーションを迅速に移行およびモダナイズしたいと考えています。会社は中央ネットワークアカウントにAWS Direct Connect接続を設定しました。将来的には数百のAWSアカウントとVPCを持つことを期待しています。企業のネットワークは、AWS上のリソースにシームレスにアクセスでき、すべてのVPCと通信できる必要があります。また、会社はクラウドリソースをオンプレミスのデータセンターを通じてインターネットにルーティングすることも望んでいます。これらの要件を満たすために必要な手順の組み合わせはどれですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a Direct Connect gateway in the central account. In each of the accounts, create an association proposal by using the Direct",
        "text_jp": "中央アカウントにDirect Connectゲートウェイを作成します。各アカウントで、Directを使用してアソシエーション提案を作成します。"
      },
      {
        "key": "B",
        "text": "Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect",
        "text_jp": "中央ネットワークアカウントにDirect Connectゲートウェイとトランジットゲートウェイを作成します。トランジットゲートウェイをDirect Connectに接続します。"
      },
      {
        "key": "C",
        "text": "Provision an internet gateway. Attach the internet gateway to subnets. Allow internet trafic through the gateway.",
        "text_jp": "インターネットゲートウェイをプロビジョニングします。サブネットにインターネットゲートウェイを接続します。ゲートウェイを通じてインターネットトラフィックを許可します。"
      },
      {
        "key": "D",
        "text": "Share the transit gateway with other accounts. Attach VPCs to the transit gateway.",
        "text_jp": "トランジットゲートウェイを他のアカウントと共有します。VPCをトランジットゲートウェイに接続します。"
      },
      {
        "key": "E",
        "text": "Provision VPC peering as necessary.",
        "text_jp": "必要に応じてVPCピアリングをプロビジョニングします。"
      },
      {
        "key": "F",
        "text": "Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet trafic",
        "text_jp": "プライベートサブネットのみをプロビジョニングします。トランジットゲートウェイとカスタマーゲートウェイ上で必要なルートを開いてアウトバウンドインターネットトラフィックを許可します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BDF (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves creating both a Direct Connect gateway and a transit gateway in the central network account, and then attaching the transit gateway to the Direct Connect connection. This setup allows for seamless integration of multiple VPCs and ensures secure communication through the AWS infrastructure.",
        "situation_analysis": "The company requires a solution that not only connects many AWS accounts and VPCs but also allows for direct routing of traffic through their on-premise data center. The use of both gateways aligns with these needs.",
        "option_analysis": "Option B is correct because it effectively leverages Direct Connect and transit gateways to create a scalable architecture. The other options do not adequately address the requirement for seamless connectivity across multiple VPCs or do not provide a robust method for routing cloud resources to the internet.",
        "additional_knowledge": "Direct Connect can significantly reduce latency and increase bandwidth for on-premises applications interacting with AWS services.",
        "key_terminology": "Direct Connect, transit gateway, VPC, internet gateway, subnet, on-premises data center",
        "overall_assessment": "This question emphasizes key AWS networking principles and the use of essential services like Direct Connect and transit gateways to solve complex networking scenarios. The community supports this answer due to its alignment with best practices in network design."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。これは、中央ネットワークアカウントでDirect Connectゲートウェイとトランジットゲートウェイの両方を作成し、トランジットゲートウェイをDirect Connect接続に接続することを含みます。このセットアップにより、複数のVPCのシームレスな統合が可能になり、AWSインフラストラクチャを介した安全な通信が保証されます。",
        "situation_analysis": "この会社は、複数のAWSアカウントとVPCを接続するだけでなく、オンプレミスのデータセンターを通じてトラフィックを直接ルーティングできるソリューションを必要としています。両方のゲートウェイの使用は、これらのニーズと一致しています。",
        "option_analysis": "Bの選択肢は、Direct Connectとトランジットゲートウェイを活用してスケーラブルなアーキテクチャを作成するため、正しいです。他の選択肢は、複数のVPC間のシームレスな接続に対する要件を十分に満たしていないか、クラウドリソースをインターネットにルーティングするための強固な方法を提供していません。",
        "additional_knowledge": "Direct Connectは、AWSサービスとやり取りするオンプレミスアプリケーションのレイテンシを大幅に低下させ、帯域幅を増加させることができます。",
        "key_terminology": "Direct Connect, トランジットゲートウェイ, VPC, インターネットゲートウェイ, サブネット, オンプレミスデータセンター",
        "overall_assessment": "この問題は、主要なAWSネットワークの原則と、Direct Connectおよびトランジットゲートウェイなどの重要なサービスを使用して複雑なネットワーキングシナリオを解決することを強調しています。コミュニティは、ネットワーク設計のベストプラクティスに沿った回答であるため、この回答を支持しています。"
      }
    ],
    "keywords": [
      "Direct Connect",
      "transit gateway",
      "VPC"
    ]
  },
  {
    "No": "124",
    "question": "A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved\nInstances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved\nInstances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances\nin their own respective AWS accounts autonomously.\nA solutions architect needs to enforce the new process in the most secure way possible.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "企業は数百のAWSアカウントを持っています。最近、企業は新しいリザーブドインスタンスの購入と既存のリザーブドインスタンスの変更のための集中化された内部プロセスを実装しました。このプロセスでは、リザーブドインスタンスを購入または変更したいすべてのビジネスユニットが、調達のために専用チームにリクエストを提出する必要があります。以前は、ビジネスユニットは独自のAWSアカウント内で自主的にリザーブドインスタンスを購入または変更していました。ソリューションアーキテクトは、できるだけ安全に新しいプロセスを強制する必要があります。この要件を満たすためにソリューションアーキテクトが取るべき手順の組み合わせはどれですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.",
        "text_jp": "すべてのAWSアカウントがAWS Organizationsの組織の一部であり、すべての機能が有効になっていることを確認します。"
      },
      {
        "key": "B",
        "text": "Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and",
        "text_jp": "AWS Configを使用して、ec2:PurchaseReservedInstancesOfferingアクションへのアクセスを拒否するIAMポリシーの添付を報告します。"
      },
      {
        "key": "C",
        "text": "In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the",
        "text_jp": "各AWSアカウントに、ec2:PurchaseReservedInstancesOfferingアクションを拒否するIAMポリシーを作成します。"
      },
      {
        "key": "D",
        "text": "Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the",
        "text_jp": "ec2:PurchaseReservedInstancesOfferingアクションとec2:ModifyReservedInstancesアクションを拒否するSCPを作成し、それを\n（依存先に付けます）。"
      },
      {
        "key": "E",
        "text": "Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature.",
        "text_jp": "すべてのAWSアカウントが統合請求機能を使用するAWS Organizationsの組織の一部であることを確認します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D. Implementing an organization with all features enabled allows centralized management.",
        "situation_analysis": "The company needs a centralized process for managing Reserved Instances across multiple accounts for security and efficiency.",
        "option_analysis": "Option A allows organization-wide policies to be enforced; Option D enhances security by denying specific actions across all accounts.",
        "additional_knowledge": "Organizations can simplify billing and security measures with consolidated billing.",
        "key_terminology": "AWS Organizations, Service Control Policies (SCP), IAM policy, centralized management, security.",
        "overall_assessment": "The proposed solution encompasses the required security measures by using AWS Organizations to enable central oversight."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとDである。すべての機能が有効な組織を実装することで、中央管理が可能になる。",
        "situation_analysis": "企業は、セキュリティと効率のために、複数のアカウントにわたるリザーブドインスタンスの管理のための集中化されたプロセスを必要としている。",
        "option_analysis": "Aの選択肢は組織全体でのポリシーの強制を可能にし、Dの選択肢はすべてのアカウントで特定のアクションを拒否することでセキュリティを高める。",
        "additional_knowledge": "組織は、統合請求を用いることで、請求とセキュリティ対策を簡素化できる。",
        "key_terminology": "AWS Organizations、サービス制御ポリシー(SCP)、IAMポリシー、中央管理、セキュリティ。",
        "overall_assessment": "提案されたソリューションは、AWS Organizationsを利用して中央の監視を可能にし、必要なセキュリティ対策を包含している。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Service Control Policies",
      "IAM Policy",
      "Reserved Instances",
      "Centralized Management"
    ]
  },
  {
    "No": "125",
    "question": "A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-\nAZ mode.\nA recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the\noutage time to less than 20 seconds.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "ある企業が、Amazon RDS for MySQL データベースを使用してデータを保存する重要なアプリケーションを運用しています。RDS DB インスタンスは、Multi-AZ モードでデプロイされています。最近の RDS データベースフェイルオーバーテストでは、アプリケーションに 40 秒の停電が発生しました。ソリューションアーキテクトは、停電時間を 20 秒未満に短縮するためのソリューションを設計する必要があります。要件を満たすために、ソリューションアーキテクトが取るべき手順の組み合わせはどれですか?(3つ選択してください)",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon ElastiCache for Memcached in front of the database",
        "text_jp": "データベースの前に Amazon ElastiCache for Memcached を使用する"
      },
      {
        "key": "B",
        "text": "Use Amazon ElastiCache for Redis in front of the database",
        "text_jp": "データベースの前に Amazon ElastiCache for Redis を使用する"
      },
      {
        "key": "C",
        "text": "Use RDS Proxy in front of the database.",
        "text_jp": "データベースの前に RDS Proxy を使用する。"
      },
      {
        "key": "D",
        "text": "Migrate the database to Amazon Aurora MySQL.",
        "text_jp": "データベースを Amazon Aurora MySQL に移行する。"
      },
      {
        "key": "E",
        "text": "Create an Amazon Aurora Replica.",
        "text_jp": "Amazon Aurora レプリカを作成する。"
      },
      {
        "key": "F",
        "text": "Create an RDS for MySQL read replica",
        "text_jp": "RDS for MySQL リードレプリカを作成する"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CDE (89%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is to use Amazon ElastiCache for Redis in front of the database, along with RDS Proxy and creating an Amazon Aurora Replica.",
        "situation_analysis": "The application experiences significant downtime due to database failover, and the goal is to reduce this outage to less than 20 seconds. Multi-AZ deployments are designed for failover but may not always meet stringent uptime requirements.",
        "option_analysis": "Option B (Amazon ElastiCache for Redis) can cache frequently requested data, therefore reducing load on the database during failover. RDS Proxy (Option C) provides a connection pool that can speed up failover times. Creating an Amazon Aurora Replica (Option E) can enhance the performance as it allows for faster recovery from failover events due to the underlying architecture of Aurora.",
        "additional_knowledge": "It's critical to evaluate the specific use case and existing architecture before implementing the solutions.",
        "key_terminology": "Amazon RDS, Multi-AZ, ElastiCache, Redis, RDS Proxy, Aurora, failover",
        "overall_assessment": "The majority of the community seems to suggest using RDS Proxy and Aurora solutions as well as caching strategies, indicating a collective understanding of best practices for high availability and disaster recovery."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答は、データベースの前に Amazon ElastiCache for Redis を使用し、RDS Proxy を利用し、Amazon Aurora レプリカを作成することである。",
        "situation_analysis": "アプリケーションはデータベースのフェイルオーバーにより重要なダウンタイムが発生し、目標はこの停電を 20 秒未満に短縮することである。Multi-AZ デプロイメントはフェイルオーバーのために設計されているが、常に厳しい稼働要件を満たすわけではない。",
        "option_analysis": "選択肢 B (Amazon ElastiCache for Redis) は、頻繁に要求されるデータをキャッシュすることで、フェイルオーバー中にデータベースへの負荷を軽減することができる。RDS Proxy (選択肢 C) は接続プールを提供し、フェイルオーバーの時間を短縮する。Amazon Aurora レプリカを作成すること (選択肢 E) は、Aurora の基盤となるアーキテクチャにより、フェイルオーバーイベントからの回復を早めるためパフォーマンスを強化する。",
        "additional_knowledge": "ソリューションを実施する前に、特定のユースケースと既存のアーキテクチャを評価することが重要である。",
        "key_terminology": "Amazon RDS、Multi-AZ、ElastiCache、Redis、RDS Proxy、Aurora、フェイルオーバー",
        "overall_assessment": "コミュニティの大半は RDS Proxy と Aurora ソリューション、ならびにキャッシング戦略の使用を支持しており、高可用性と災害復旧のベストプラクティスに対する理解が共有されていることを示している。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "Multi-AZ",
      "ElastiCache",
      "Redis",
      "RDS Proxy",
      "Aurora",
      "failover"
    ]
  },
  {
    "No": "126",
    "question": "An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company\nto have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least\nprivilege security access using an API or command line tool to the customer account.\nWhat is the MOST secure way to allow org1 to access resources in org2?",
    "question_jp": "AWSパートナー企業は、org1という名前の組織を使用してAWS Organizations内でサービスを構築しています。このサービスは、パートナー企業が別の組織であるorg2の顧客アカウント内のAWSリソースにアクセスすることを必要としています。企業は、APIまたはコマンドラインツールを使用して顧客アカウントへの最小限の特権セキュリティアクセスを確立する必要があります。org1がorg2内のリソースにアクセスするための最も安全な方法は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.",
        "text_jp": "顧客はパートナー企業にAWSアカウントのアクセスキーを提供し、ログインして必要なタスクを実行させるべきである。"
      },
      {
        "key": "B",
        "text": "The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the",
        "text_jp": "顧客はIAMユーザーを作成し、そのユーザーに必要な権限を付与するべきである。顧客はその後、"
      },
      {
        "key": "C",
        "text": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM",
        "text_jp": "顧客はIAMロールを作成し、そのロールに必要な権限を付与するべきである。パートナー企業はその後、IAM"
      },
      {
        "key": "D",
        "text": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM",
        "text_jp": "顧客はIAMロールを作成し、そのロールに必要な権限を付与するべきである。パートナー企業はその後、IAM"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. The customer should create an IAM role and assign the required permissions. This method allows org1 to assume the role temporarily, maintaining least privilege access.",
        "situation_analysis": "The partner company requires access to AWS resources in the customer account (org2). Security and least privilege are critical.",
        "option_analysis": "Option A is insecure as it shares access keys. Option B is not ideal since an IAM user does not support cross-account access efficiently. Option C is partially correct but does not specify proper role assumption by the partner.",
        "additional_knowledge": "Using roles enhances security as it avoids long-term access credential sharing.",
        "key_terminology": "AWS Organizations, IAM roles, least privilege, cross-account access, access keys.",
        "overall_assessment": "The question effectively assesses knowledge of secure access practices in AWS. The community's support for option D indicates strong alignment with AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。顧客はIAMロールを作成し、そのロールに必要な権限を付与すべきである。この方法は、org1がロールを一時的に引き受けることを可能にし、最小限の特権アクセスを維持する。",
        "situation_analysis": "パートナー企業は顧客アカウント（org2）のAWSリソースにアクセスする必要がある。セキュリティと最小限の特権が重要である。",
        "option_analysis": "選択肢Aはアクセスキーを共有するため、安全ではない。選択肢BはIAMユーザーではクロスアカウントアクセスを効率的にサポートしないため、理想的ではない。選択肢Cは部分的に正しいが、パートナーによる適切なロール引き受けを指定していない。",
        "additional_knowledge": "ロールを使用すると長期的なアクセス資格情報の共有を避けることができ、セキュリティが強化される。",
        "key_terminology": "AWS Organizations、IAMロール、最小限の特権、クロスアカウントアクセス、アクセスキー。",
        "overall_assessment": "この質問はAWSにおける安全なアクセス手法に関する知識を効果的に評価している。選択肢Dに対するコミュニティの支持は、AWSのベストプラクティスと強く一致していることを示している。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "IAM Roles",
      "Least Privilege",
      "Cross-Account Access",
      "Access Keys"
    ]
  },
  {
    "No": "127",
    "question": "A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a\npublic registry. The image can run in as many containers as required to generate the route map.\nThe company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the\nhubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom\nconfiguration that processes orders only in the section's area.\nThe company needs the ability to allocate resources cost-effectively based on the number of running containers.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "配送会社は、サードパーティのルート計画アプリケーションをAWSに移行する必要があります。サードパーティは、公開レジストリからサポートされているDockerイメージを提供しています。このイメージは、必要なだけ多くのコンテナで実行でき、ルートマップを生成します。会社は、配達地域をセクションに分けて供給ハブを設け、配達ドライバーがハブから顧客までできるだけ短い距離を移動できるようにしています。ルートマップの生成に必要な時間を短縮するために、各セクションは独自のDockerコンテナのセットを使用し、そのセクションのエリアの注文のみを処理するカスタム設定をしています。会社は、実行中のコンテナの数に基づいて、コスト効果的にリソースを割り当てる能力が必要です。どのソリューションが最小の運用負荷でこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning",
        "text_jp": "Amazon EC2上にAmazon Elastic Kubernetes Service（Amazon EKS）クラスターを作成し、Amazon EKS CLIを使用して計画を起動します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning",
        "text_jp": "AWS Fargate上にAmazon Elastic Kubernetes Service（Amazon EKS）クラスターを作成し、Amazon EKS CLIを使用して計画を起動します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch",
        "text_jp": "Amazon EC2上にAmazon Elastic Container Service（Amazon ECS）クラスターを作成し、run-tasksをtrueに設定してAWS CLIを使用して起動します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set",
        "text_jp": "AWS Fargate上にAmazon Elastic Container Service（Amazon ECS）クラスターを作成し、AWS CLIのrun-taskコマンドを使用して起動します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (79%) B (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, which suggests creating an Amazon Elastic Container Service (ECS) cluster on AWS Fargate. This solution allows for serverless management of containerized applications, significantly reducing operational overhead.",
        "situation_analysis": "The delivery company requires a solution to efficiently run multiple instances of a Docker image without the need to manage the underlying EC2 infrastructure, as their primary goal is to optimize resource allocation based on running containers.",
        "option_analysis": "Option A and B involve EKS, which, while powerful, may introduce unnecessary complexity for a straightforward use case like this one. Option C requires management of EC2 instances, which increases operational overhead. Option D, however, with AWS Fargate, abstracts away the server management.",
        "additional_knowledge": "Fargate's compatibility with ECS means services can be developed and deployed more rapidly, suitable for dynamic workloads like those presented in this scenario.",
        "key_terminology": "Amazon ECS, AWS Fargate, serverless containers, resource allocation, operational overhead",
        "overall_assessment": "D is the most efficient option for handling container orchestration without the complexity of managing servers, which aligns with best practices for cloud-native applications."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDであり、AWS Fargate上にAmazon Elastic Container Service（ECS）クラスターを作成することを提案しています。このソリューションにより、コンテナ化されたアプリケーションのサーバーレス管理が可能となり、運用負荷が大幅に軽減されます。",
        "situation_analysis": "配送会社は、基盤となるEC2インフラストラクチャを管理する必要なく、Dockerイメージの複数のインスタンスを効率的に実行するソリューションを必要としています。彼らの主な目標は、実行中のコンテナに基づいてリソースを最適に割り当てることです。",
        "option_analysis": "選択肢AとBはEKSを含んでいますが、強力であるものの、このような単純なユースケースにとっては不要な複雑さを導入する可能性があります。選択肢CはEC2インスタンスの管理を必要とし、運用負荷を増加させることになります。しかし選択肢Dは、AWS Fargateを使用することでサーバー管理を抽象化しています。",
        "additional_knowledge": "Fargateとの互換性があるECSを利用することで、サービスを迅速に開発・デプロイでき、今回のシナリオのような動的なワークロードに適しています。",
        "key_terminology": "Amazon ECS, AWS Fargate, サーバーレスコンテナ, リソース割当, 運用負荷",
        "overall_assessment": "Dは、サーバー管理の複雑さを排除しつつコンテナオーケストレーションを扱う最も効率的な選択肢であり、クラウドネイティブアプリケーションのベストプラクティスにも沿っています。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "AWS Fargate",
      "serverless containers",
      "resource allocation",
      "operational overhead"
    ]
  },
  {
    "No": "128",
    "question": "A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of\nAmazon EC2 instances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account,\na shared services VPC is located in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS\nCloudFormation to attempt to peer the application VPC with the shared services VPC, an error message indicates a peering failure.\nWhich factors could cause this error? (Choose two.)",
    "question_jp": "ソフトウェア会社が、複数のAWSアカウントとリージョンにリソースを持つアプリケーションをAWS上にホストしています。アプリケーションは、us-east-1リージョンにあるアプリケーションVPC内の複数のAmazon EC2インスタンスで実行されており、IPv4 CIDRブロックは10.10.0.0/16です。別のAWSアカウントには、us-east-2リージョンにある共有サービスVPCがあり、IPv4 CIDRブロックは10.10.10.0/24です。クラウドエンジニアがAWS CloudFormationを使用してアプリケーションVPCを共有サービスVPCとピア接続しようとすると、エラーメッセージが表示され、ピアリングに失敗します。このエラーを引き起こす可能性のある要因は何ですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "The IPv4 CIDR ranges of the two VPCs overlap",
        "text_jp": "2つのVPCのIPv4 CIDR範囲が重なっている"
      },
      {
        "key": "B",
        "text": "The VPCs are not in the same Region",
        "text_jp": "VPCが同じリージョンにない"
      },
      {
        "key": "C",
        "text": "One or both accounts do not have access to an Internet gateway",
        "text_jp": "1つまたは両方のアカウントがインターネットゲートウェイへのアクセスを持っていない"
      },
      {
        "key": "D",
        "text": "One of the VPCs was not shared through AWS Resource Access Manager",
        "text_jp": "VPCの1つがAWSリソースアクセスマネージャーを通じて共有されていない"
      },
      {
        "key": "E",
        "text": "The IAM role in the peer accepter account does not have the correct permissions",
        "text_jp": "ピアの受け入れアカウントのIAMロールに正しい権限がない"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AE (84%) BE (16%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and B. The IPv4 CIDR ranges of the two VPCs overlap causing a peering failure, as there cannot be any overlapping CIDR blocks in VPC peering relationships.",
        "situation_analysis": "In this scenario, we have two VPCs located in different regions and accounts, which are attempting to create a peering connection.",
        "option_analysis": "Option A is correct because the CIDR range of the application VPC overlaps with the shared services VPC. Option B is incorrect, as AWS allows VPC peering connections between VPCs across different regions, provided the CIDR blocks do not overlap. Options C, D, and E are related to connectivity and permissions but do not directly cause a peering failure.",
        "additional_knowledge": "Overlap in CIDR blocks is a fundamental aspect that needs to be accounted for when designing networks in AWS.",
        "key_terminology": "VPC Peering, CIDR Block, Internet Gateway, AWS Resource Access Manager, IAM Roles",
        "overall_assessment": "This question accurately tests understanding of VPC peering and the importance of CIDR block configuration. Community vote suggests a misunderstanding about the necessity of the same region for VPC peering."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとBである。2つのVPCのIPv4 CIDR範囲が重なっているためピアリングに失敗しており、VPCピアリング関係には重複するCIDRブロックが存在できない。",
        "situation_analysis": "このシナリオでは、異なるリージョンとアカウントに存在する2つのVPCがピア接続を作成しようとしている。",
        "option_analysis": "選択肢Aが正しい。アプリケーションVPCのCIDR範囲が共有サービスVPCと重なっているためである。選択肢Bは間違っている。AWSでは、CIDRブロックが重複しない限り、異なるリージョンのVPC間でのピアリング接続が可能である。選択肢C、D、およびEは接続性や権限に関連するが、ピアリング失敗の直接的な要因ではない。",
        "additional_knowledge": "CIDRブロックの重複は、AWSにおけるネットワーク設計の際に考慮しなければならない基本的な側面である。",
        "key_terminology": "VPCピアリング, CIDRブロック, インターネットゲートウェイ, AWSリソースアクセスマネージャー, IAMロール",
        "overall_assessment": "この質問はVPCピアリングの理解とCIDRブロックの設定の重要性を正しくテストしている。コミュニティの投票は、VPCピアリングに同じリージョンが必要であるという誤解を示唆している。"
      }
    ],
    "keywords": [
      "VPC Peering",
      "CIDR Block",
      "Internet Gateway",
      "AWS Resource Access Manager",
      "IAM Roles"
    ]
  },
  {
    "No": "129",
    "question": "An external audit of a company's serverless application reveals IAM policies that grant too many permissions. These policies are attached to the\ncompany's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to\nAmazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function\nneeds to complete its task.\nA solutions architect must determine which permissions each Lambda function needs.\nWhat should the solutions architect do to meet this requirement with the LEAST amount of effort?",
    "question_jp": "外部監査によって、企業のサーバーレスアプリケーションにおいて、IAMポリシーが過剰な権限を付与していることが明らかになりました。これらのポリシーは企業のAWS Lambda実行ロールに関連付けられています。企業の何百ものLambda関数は、Amazon S3バケットやAmazon DynamoDBテーブルへの完全なアクセス権など、広範なアクセス権限を持っています。企業は各関数に、そのタスクを完了するために必要な最小限の権限だけを持たせたいと考えています。ソリューションアーキテクトは、各Lambda関数にどの権限が必要かを特定する必要があります。最も少ない労力でこの要件を満たすためには、ソリューションアーキテクトは何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and",
        "text_jp": "Amazon CodeGuruを設定してLambda関数をプロファイルし、AWS APIコールを検索します。必要なAPIコールのインベントリを作成します。"
      },
      {
        "key": "B",
        "text": "Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access",
        "text_jp": "AWSアカウントのAWS CloudTrailロギングをオンにします。AWS Identity and Access Management Access Analyzerを使用してIAMアクセスを生成します。"
      },
      {
        "key": "C",
        "text": "Turn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda",
        "text_jp": "AWSアカウントのAWS CloudTrailロギングをオンにします。CloudTrailログを解析するスクリプトを作成し、LambdaによるAWS APIコールを検索します。"
      },
      {
        "key": "D",
        "text": "Turn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail",
        "text_jp": "AWSアカウントのAWS CloudTrailロギングをオンにします。CloudTrailログをAmazon S3にエクスポートします。Amazon EMRを使用してCloudTrailを処理します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This approach streamlines the process of identifying permissions for Lambda functions by enabling CloudTrail logging and utilizing IAM Access Analyzer.",
        "situation_analysis": "The company needs to refine its IAM policies to enforce least privilege access for its Lambda functions while minimizing workload.",
        "option_analysis": "Option A requires additional work to set up CodeGuru and may be less efficient. Option C involves complex scripting, and Option D requires data processing, which is more work than necessary.",
        "additional_knowledge": "By using AWS services effectively, access rights can be monitored and adjusted, ensuring better security compliance.",
        "key_terminology": "AWS Lambda, IAM policies, CloudTrail, Access Analyzer, least privilege",
        "overall_assessment": "This question emphasizes the importance of least privilege in IAM configurations and the use of CloudTrail and Access Analyzer to achieve this efficiently."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。このアプローチは、CloudTrailロギングを有効にし、IAM Access Analyzerを利用することで、Lambda関数のための権限を特定するプロセスを効率化します。",
        "situation_analysis": "企業は、労力を最小限に抑えつつ、Lambda関数のIAMポリシーを改良し、最小限の権限アクセスを実施する必要があります。",
        "option_analysis": "選択肢AはCodeGuruの設定に追加の作業が必要で、効率的ではない可能性があります。選択肢Cは複雑なスクリプトの作成を含み、選択肢Dはデータ処理を必要とするため、必要以上の労力がかかります。",
        "additional_knowledge": "AWSサービスを効果的に使用することで、アクセス権を監視し、調整することで、より良いセキュリティのコンプライアンスを保証できます。",
        "key_terminology": "AWS Lambda、IAMポリシー、CloudTrail、Access Analyzer、最小権限",
        "overall_assessment": "この質問は、IAM構成における最小権限の重要性と、効率的にこれを達成するためのCloudTrailとAccess Analyzerの使用を強調しています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "IAM policies",
      "CloudTrail",
      "Access Analyzer",
      "least privilege"
    ]
  },
  {
    "No": "130",
    "question": "A solutions architect must analyze a company's Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine\nwhether the company is using resources eficiently. The company is running several large, high-memory EC2 instances to host database clusters\nthat are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and\nthe company has not identified a pattern.\nThe solutions architect must analyze the environment and take action based on the findings.\nWhich solution meets these requirements MOST cost-effectively?",
    "question_jp": "ソリューションアーキテクトは、会社のAmazon EC2インスタンスおよびAmazon Elastic Block Store (Amazon EBS) ボリュームを分析し、会社がリソースを効率的に使用しているかどうかを確認する必要があります。会社は、アクティブ/パッシブ構成でデプロイされたデータベースクラスターをホストするために、いくつかの大規模な高メモリEC2インスタンスを実行しています。これらのEC2インスタンスの利用率はデータベースを使用するアプリケーションによって異なり、会社はパターンを特定していません。ソリューションアーキテクトは、環境を分析し、その結果に基づいてアクションを取る必要があります。どのソリューションが、これらの要件を最もコスト効率よく満たすことができますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are",
        "text_jp": "AWS Systems Manager OpsCenterを使用してダッシュボードを作成します。可視化をAmazon CloudWatchメトリクスに構成します。"
      },
      {
        "key": "B",
        "text": "Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based",
        "text_jp": "EC2インスタンスおよびそのEBSボリュームに対してAmazon CloudWatchの詳細監視を有効にします。作成し、レビューするダッシュボードを基にします。"
      },
      {
        "key": "C",
        "text": "Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours.",
        "text_jp": "Amazon CloudWatchエージェントを各EC2インスタンスにインストールします。AWS Compute Optimizerを有効にし、少なくとも12時間実行させます。"
      },
      {
        "key": "D",
        "text": "Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted",
        "text_jp": "AWSエンタープライズサポートプランにサインアップします。AWS Trusted Advisorを有効にします。12時間待ちます。Trustedからの推奨事項をレビューします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. By installing the CloudWatch agent and running AWS Compute Optimizer, the architect can gather detailed utilization metrics, which helps in analyzing resource efficiencies effectively.",
        "situation_analysis": "The company is struggling with inefficient use of resources and cannot identify usage patterns of their high-memory EC2 instances.",
        "option_analysis": "Option A does not provide direct resource utilization metrics; Option B offers monitoring but lacks detailed insights; Option D requires a support plan that is costly, whereas Option C provides a direct solution with analytical insights.",
        "additional_knowledge": "The CloudWatch agent collects additional metrics that are not collected by default, enabling a deeper analysis.",
        "key_terminology": "AWS CloudWatch, AWS Compute Optimizer, EC2 instances, resource optimization",
        "overall_assessment": "Based on the requirements, Option C offers the best cost-effective approach with actionable insights for optimizing resource usage, aligning well with the company’s needs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。CloudWatchエージェントをインストールし、AWS Compute Optimizerを実行することで、アーキテクトは詳細な利用状況メトリクスを収集でき、リソースの効率性を効果的に分析することができる。",
        "situation_analysis": "会社はリソースを非効率的に使用しており、高メモリEC2インスタンスの使用パターンを特定できていない。",
        "option_analysis": "選択肢Aは直接的なリソース利用状況メトリクスを提供しない; 選択肢Bは監視を提供するが、詳細な洞察には欠ける; 選択肢Dはコストがかかるサポートプランを必要とし、一方で選択肢Cは分析的な洞察を提供する直接的な解決策を提供する。",
        "additional_knowledge": "CloudWatchエージェントは、デフォルトで収集されない追加メトリクスを収集し、より深い分析を可能にする。",
        "key_terminology": "AWS CloudWatch、AWS Compute Optimizer、EC2インスタンス、リソース最適化",
        "overall_assessment": "要件に基づくと、選択肢Cはリソース使用を最適化するための実用的な洞察を提供するコスト効率の良いアプローチを提供し、会社のニーズに適合する。"
      }
    ],
    "keywords": [
      "AWS CloudWatch",
      "AWS Compute Optimizer",
      "EC2 instances",
      "resource optimization"
    ]
  },
  {
    "No": "131",
    "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses\nAWS Transit Gateway for VPC connectivity across accounts.\nIn an AWS application account, the company's application team has deployed a web application that uses AWS Lambda and Amazon RDS. The\ncompany's database administrators have a separate DBA account and use the account to centrally manage all the databases across the\norganization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is\ndeployed m the application account.\nThe application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is\nmanually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in\nthe application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and\neliminates the need to manually share the secrets.\nWhich solution will meet these requirements?",
    "question_jp": "企業はAWS Organizationsを使用してAWSクラウド内でマルチアカウントのセットアップを行っています。企業はガバナンスのためにAWS Control Towerを使用し、アカウント間のVPC接続にはAWS Transit Gatewayを使用しています。AWSアプリケーションアカウントでは、企業のアプリケーションチームがAWS LambdaとAmazon RDSを使用したWebアプリケーションをデプロイしています。企業のデータベース管理者は別のDBAアカウントを持ち、そのアカウントを使用して組織全体のすべてのデータベースを中央管理しています。データベース管理者はDBAアカウントにデプロイされたAmazon EC2インスタンスを使用して、アプリケーションアカウントにデプロイされたRDSデータベースにアクセスしています。アプリケーションチームはAWS Secrets Managerにデータベース認証情報を秘密情報としてアプリケーションアカウント内に保存しています。アプリケーションチームは手動でデータベース管理者と秘密情報を共有しています。秘密情報はアプリケーションアカウント内のSecrets ManagerのデフォルトのAWSマネージドキーによって暗号化されています。ソリューションアーキテクトは、データベース管理者にデータベースへアクセスを提供し、秘密情報の手動共有をなくすソリューションを実装する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA",
        "text_jp": "AWS Resource Access Manager (AWS RAM)を使用して、アプリケーションアカウントからDBAアカウントに秘密情報を共有します。DBAでは"
      },
      {
        "key": "B",
        "text": "In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In",
        "text_jp": "アプリケーションアカウントにDBA-Secretという名前のIAMロールを作成します。そのロールに秘密情報にアクセスするための必要な権限を付与します。DBAでは"
      },
      {
        "key": "C",
        "text": "In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the",
        "text_jp": "DBAアカウントにDBA-Adminという名前のIAMロールを作成します。そのロールに秘密情報とACCESS権限を付与します。DBAでは"
      },
      {
        "key": "D",
        "text": "In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the",
        "text_jp": "DBAアカウントにDBA-Adminという名前のIAMロールを作成します。そのロールにアプリケーションアカウント内の秘密情報にアクセスするための必要な権限を付与します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (78%) 8% 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using AWS Resource Access Manager (AWS RAM) allows sharing of secrets between accounts securely and efficiently.",
        "situation_analysis": "The application team needs to share secrets securely without manual intervention. The central management of databases across accounts is critical.",
        "option_analysis": "Option A allows for streamlined management of secrets across accounts, which is ideal. Other options involve creating IAM roles but do not utilize RAM for resource-sharing.",
        "additional_knowledge": "AWS Secrets Manager also automates the rotation of secrets, enhancing security practices.",
        "key_terminology": "AWS Resource Access Manager, IAM roles, AWS Secrets Manager",
        "overall_assessment": "Option A is the most efficient solution for secure access to secrets, while other options add complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。AWS Resource Access Manager (AWS RAM)を使用することにより、アカウント間で秘密情報を安全かつ効率的に共有することができます。",
        "situation_analysis": "アプリケーションチームは手動介入なしで秘密情報を安全に共有する必要があります。複数のアカウントにわたるデータベースの中央管理は重要です。",
        "option_analysis": "選択肢Aは、アカウント間での秘密情報の管理を合理化するための方法を提供します。他の選択肢はIAMロールの作成を伴いますが、リソース共有のためにRAMを利用していません。",
        "additional_knowledge": "AWS Secrets Managerは、秘密の回転を自動化するサービスであり、セキュリティプラクティス向上に寄与します。",
        "key_terminology": "AWS Resource Access Manager, IAMロール, AWS Secrets Manager",
        "overall_assessment": "選択肢Aは、秘密情報の安全なアクセスに最も効率的な解決策であり、他の選択肢は複雑さを増加させます。"
      }
    ],
    "keywords": [
      "AWS Resource Access Manager",
      "IAM roles",
      "AWS Secrets Manager"
    ]
  },
  {
    "No": "132",
    "question": "A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs: Research and DataOps.\nBecause of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region.\nAdditionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types.\nA solutions architect must implement a solution that applies these restrictions. The solution must maximize operational eficiency and must\nminimize ongoing maintenance.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "question_jp": "ある企業は、AWS Organizationsを使用して複数のAWSアカウントを管理しています。ルートOUの下に、企業は研究およびDataOpsの2つのOUを持っています。\n規制要件により、企業が組織内でデプロイするすべてのリソースは、ap-northeast-1リージョン内に存在しなければなりません。\nさらに、企業がDataOps OUにデプロイするEC2インスタンスは、事前定義されたインスタンスタイプのリストを使用する必要があります。\nソリューションアーキテクトは、これらの制限を適用するソリューションを実装しなければなりません。このソリューションは、運用効率を最大化し、継続的なメンテナンスを最小限に抑える必要があります。\nこれらの要件を満たす手順の組み合わせはどれですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict",
        "text_jp": "DataOps OUの下の1つのアカウントにIAMロールを作成します。ロールのインラインポリシーでec2:InstanceType条件キーを使用して制限します"
      },
      {
        "key": "B",
        "text": "Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict",
        "text_jp": "ルートOUの下のすべてのアカウントにIAMユーザーを作成します。各ユーザーのインラインポリシーでaws:RequestedRegion条件キーを使用して制限します"
      },
      {
        "key": "C",
        "text": "Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to",
        "text_jp": "SCPを作成します。aws:RequestedRegion条件キーを使用して、ap-northeast-1以外のすべてのAWSリージョンへのアクセスを制限します。このSCPを適用します"
      },
      {
        "key": "D",
        "text": "Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU,",
        "text_jp": "SCPを作成します。ec2:Region条件キーを使用して、ap-northeast-1以外のすべてのAWSリージョンへのアクセスを制限します。このSCPをルートOUに適用します"
      },
      {
        "key": "E",
        "text": "Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.",
        "text_jp": "SCPを作成します。ec2:InstanceType条件キーを使用して特定のインスタンスタイプへのアクセスを制限します。このSCPをDataOps OUに適用します"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "CE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are C and E. Option C correctly restricts access to all AWS Regions except ap-northeast-1 using a Service Control Policy (SCP).",
        "situation_analysis": "The organization has strict regulatory requirements and needs to ensure that all resources are deployed in the ap-northeast-1 region and specific EC2 instance types are used.",
        "option_analysis": "Option C's SCP using the aws:RequestedRegion condition key is appropriate for ensuring resources are only in ap-northeast-1, while option E restricts EC2 instances based on approved types.",
        "additional_knowledge": "By using SCPs, the organization can maintain a higher level of security and governance across its accounts.",
        "key_terminology": "Service Control Policies (SCPs), AWS Organizations, aws:RequestedRegion, ec2:InstanceType",
        "overall_assessment": "Community support for options C and E indicates a strong alignment with AWS best practices for compliance and resource management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCとEである。選択肢CはSCP（サービスコントロールポリシー）を使用して、ap-northeast-1以外のすべてのAWSリージョンへのアクセスを正しく制限する。",
        "situation_analysis": "組織は厳しい規制要件を持ち、すべてのリソースがap-northeast-1リージョンにデプロイされ、特定のEC2インスタンスタイプが使用されることを保証する必要がある。",
        "option_analysis": "選択肢CのSCPは、aws:RequestedRegion条件キーを使用してap-northeast-1のみでリソースが確保されることを保証し、選択肢Eは承認されたタイプに基づいてEC2インスタンスを制限する。",
        "additional_knowledge": "SCPを使用することで、組織はアカウント全体で高いレベルのセキュリティとガバナンスを維持できる。",
        "key_terminology": "サービスコントロールポリシー（SCP）、AWS Organizations、aws:RequestedRegion、ec2:InstanceType",
        "overall_assessment": "選択肢CとEに対するコミュニティの支持は、コンプライアンスやリソース管理におけるAWSのベストプラクティスと強く一致することを示している。"
      }
    ],
    "keywords": [
      "Service Control Policies",
      "AWS Organizations",
      "aws:RequestedRegion",
      "ec2:InstanceType"
    ]
  },
  {
    "No": "133",
    "question": "A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites.\nThe company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon\nSQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an\nAmazon S3 bucket.\nThe company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the\nexisting Region. Results must be written to the existing S3 bucket in the current Region.\nWhich combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)",
    "question_jp": "ある企業が単一のAWSリージョンでサーバーレスアプリケーションを運用している。このアプリケーションは外部URLにアクセスし、これらのサイトからメタデータを抽出する。企業はAmazon Simple Notification Service（Amazon SNS）トピックを使用して、URLをAmazon Simple Queue Service（Amazon SQS）キューに公開している。AWS Lambda関数はキューをイベントソースとして使用し、キューからURLを処理する。結果はAmazon S3バケットに保存される。企業は、各URLを他のリージョンで処理して、サイトのローカリゼーションの違いを比較したいと考えている。URLは既存のリージョンから公開する必要がある。結果は現在のリージョンの既存のS3バケットに書き込む必要がある。この要件を満たす複数リージョンデプロイメントを実現するために、どの組み合わせの変更が必要か？（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the SQS queue with the Lambda function to other Regions.",
        "text_jp": "SQSキューとLambda関数を他のリージョンにデプロイする。"
      },
      {
        "key": "B",
        "text": "Subscribe the SNS topic in each Region to the SQS queue.",
        "text_jp": "各リージョンでSQSキューにSNSトピックをサブスクライブする。"
      },
      {
        "key": "C",
        "text": "Subscribe the SQS queue in each Region to the SNS topic.",
        "text_jp": "各リージョンでSNSトピックにSQSキューをサブスクライブする。"
      },
      {
        "key": "D",
        "text": "Configure the SQS queue to publish URLs to SNS topics in each Region.",
        "text_jp": "SQSキューを構成して、各リージョンのSNSトピックにURLを公開する。"
      },
      {
        "key": "E",
        "text": "Deploy the SNS topic and the Lambda function to other Regions.",
        "text_jp": "SNSトピックとLambda関数を他のリージョンにデプロイする。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are 'A' and 'D'. Deploying the SQS queue and the Lambda function in other Regions allows them to process the URLs published from the existing Region.",
        "situation_analysis": "The application is serverless and relies on AWS services like SNS and SQS. Processing URLs in other Regions for localization comparisons is required, while maintaining results in the current S3 bucket.",
        "option_analysis": "Option 'A' is necessary to enable Lambda functions in other Regions, while option 'D' allows SQS to publish URLs to SNS topics elsewhere for distribution. Options 'B' and 'C' do not directly support the requirement of having results in the original S3 bucket.",
        "additional_knowledge": "Consider the implications of latency and data transfer costs when operating across multiple AWS Regions.",
        "key_terminology": "AWS Lambda, Amazon SQS, Amazon SNS, multi-Region architecture, serverless applications",
        "overall_assessment": "This question effectively tests knowledge of multi-Region deployments and serverless architecture, with the community uniformly supporting the chosen answer."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答は「A」と「D」である。他のリージョンにSQSキューとLambda関数をデプロイすることで、既存のリージョンから公開されたURLを処理できる。",
        "situation_analysis": "このアプリケーションはサーバーレスであり、SNSやSQSなどのAWSサービスに依存している。他のリージョンでローカリゼーションの比較のためにURLを処理する必要がありつつ、結果は現在のS3バケットに保存する必要がある。",
        "option_analysis": "選択肢「A」は、他のリージョンでLambda関数を有効にするために必要であり、選択肢「D」はSQSが他のリージョンのSNSトピックにURLを公開できるようにする。選択肢「B」と「C」は、元のS3バケットで結果を保持する要件を直接サポートしていない。",
        "additional_knowledge": "複数のAWSリージョンで運用する際のレイテンシーやデータ転送料金の影響を考慮することが重要である。",
        "key_terminology": "AWS Lambda、Amazon SQS、Amazon SNS、マルチリージョンアーキテクチャ、サーバーレスアプリケーション",
        "overall_assessment": "この質問は、マルチリージョンデプロイメントとサーバーレスアーキテクチャに関する知識を効果的にテストしており、コミュニティは選択された回答を一貫して支持している。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon SQS",
      "Amazon SNS",
      "multi-Region architecture",
      "serverless applications"
    ]
  },
  {
    "No": "134",
    "question": "A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code\ncannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4\nhours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.\nWhich strategy should the solutions architect use?",
    "question_jp": "企業は、Amazon EC2 Linux インスタンスで独自のステートレス ETL アプリケーションを実行しています。このアプリケーションは Linux バイナリであり、ソースコードを変更することはできません。アプリケーションは単一スレッドであり、2 GB の RAM を使用し、CPU に高負荷がかかります。アプリケーションは 4 時間ごとに実行され、最大 20 分間実行されます。ソリューション アーキテクトは、ソリューションのアーキテクチャを見直したいと考えています。ソリューション アーキテクトはどの戦略を使用すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.",
        "text_jp": "AWS Lambda を使用してアプリケーションを実行します。Amazon CloudWatch Logs を使用して、4 時間ごとに Lambda 関数を呼び出します。"
      },
      {
        "key": "B",
        "text": "Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.",
        "text_jp": "AWS Batch を使用してアプリケーションを実行します。AWS Step Functions ステート マシンを使用して、4 時間ごとに AWS Batch ジョブを呼び出します。"
      },
      {
        "key": "C",
        "text": "Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.",
        "text_jp": "AWS Fargate を使用してアプリケーションを実行します。Amazon EventBridge (Amazon CloudWatch Events) を使用して、4 時間ごとに Fargate タスクを呼び出します。"
      },
      {
        "key": "D",
        "text": "Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours.",
        "text_jp": "Amazon EC2 スポットインスタンスを使用してアプリケーションを実行します。AWS CodeDeploy を使用して、4 時間ごとにアプリケーションをデプロイおよび実行します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which suggests using AWS Fargate to run the application and invoking the Fargate task through Amazon EventBridge every 4 hours.",
        "situation_analysis": "The application is stateless and cannot be modified because it's a binary. It runs for short bursts every 4 hours, making it suitable for serverless computing where instances are not running constantly.",
        "option_analysis": "Option C is correct because AWS Fargate allows running containerized applications without managing the underlying infrastructure. Other options (A, B, D) either do not fit the requirement of a stateless app or require more management overhead.",
        "additional_knowledge": "Since the application is single-threaded and CPU-intensive, using Fargate allows optimizing resource allocation effectively.",
        "key_terminology": "AWS Fargate, EventBridge, stateless applications, containerization, serverless architecture",
        "overall_assessment": "C is supported by 93% of the community votes, indicating strong consensus. Other options do not leverage serverless capabilities efficiently."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCであり、AWS Fargateを使用してアプリケーションを実行し、Amazon EventBridgeを介して4時間ごとにFargateタスクを呼び出すことを提案しています。",
        "situation_analysis": "アプリケーションはステートレスであり、バイナリ形式のため変更できません。4時間ごとに短時間実行されるため、常時稼働しないサーバーレスコンピューティングに適しています。",
        "option_analysis": "Cの選択肢が正しいのは、AWS Fargateが基盤となるインフラストラクチャを管理することなくコンテナ化されたアプリケーションを実行できるからです。他の選択肢（A、B、D）は、ステートレスアプリケーションの要件に合わないか、管理の手間が増えます。",
        "additional_knowledge": "アプリケーションは単一スレッドでCPU集約型であるため、Fargateを使用することでリソースの最適な配分が可能になります。",
        "key_terminology": "AWS Fargate、EventBridge、ステートレスアプリケーション、コンテナ化、サーバーレスアーキテクチャ",
        "overall_assessment": "Cはコミュニティの票の93%に支持されており、強い合意を示しています。他の選択肢は効率的にサーバーレス機能を活用していません。"
      }
    ],
    "keywords": [
      "AWS Fargate",
      "EventBridge",
      "stateless applications",
      "containerization",
      "serverless architecture"
    ]
  },
  {
    "No": "135",
    "question": "A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week\nafter launch. Currently, the game consists of the following components deployed in a single AWS Region:\n• Amazon S3 bucket that stores game assets\n• Amazon DynamoDB table that stores player scores\nA solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "ある企業が人気オンラインゲームの続編を作成しています。ゲームのリリース後の最初の週に、世界中から多くのユーザーがゲームをプレイします。現在、ゲームは以下のコンポーネントで構成されており、単一のAWSリージョンにデプロイされています：\n• ゲームアセットを格納するAmazon S3バケット\n• プレイヤースコアを格納するAmazon DynamoDBテーブル\nソリューションアーキテクトは、レイテンシを削減し、信頼性を向上させ、実装の手間を最小限に抑えるマルチリージョンソリューションを設計する必要があります。\nこれらの要件を満たすために、ソリューションアーキテクトは何をするべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、S3バケットからアセットを提供します。S3クロスリージョンレプリケーションを設定します。新しい"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、S3バケットからアセットを提供します。S3同一リージョンレプリケーションを設定します。新しい"
      },
      {
        "key": "C",
        "text": "Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront",
        "text_jp": "新しいリージョンに別のS3バケットを作成し、バケット間でS3クロスリージョンレプリケーションを設定します。Amazon CloudFront"
      },
      {
        "key": "D",
        "text": "Create another S3 bucket in the sine Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront",
        "text_jp": "同じリージョンに別のS3バケットを作成し、バケット間でS3同一リージョンレプリケーションを設定します。Amazon CloudFront"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (88%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This option provides a multi-region solution to reduce latency and improve reliability.",
        "situation_analysis": "The scenario involves a global online game that will experience significant traffic in the first week of launch. The solution must handle high availability and low latency.",
        "option_analysis": "Option C creates an additional S3 bucket in a new region and sets up cross-region replication. This ensures faster access to game assets for users in different locations. Other options either involve single-region setups or do not optimize for latency sufficiently.",
        "additional_knowledge": "Implementing CloudFront enhances performance and user experience during peak loads.",
        "key_terminology": "Amazon S3, Amazon CloudFront, Cross-Region Replication, DynamoDB, latency reduction",
        "overall_assessment": "Option C is the most recommended approach as it meets the requirements effectively. The community vote strongly supports this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。この選択肢は、レイテンシを削減し、信頼性を向上させるマルチリージョンソリューションを提供する。",
        "situation_analysis": "シナリオは、リリースの最初の週に大きなトラフィックを経験するグローバルオンラインゲームに関するものである。このソリューションは、高可用性と低レイテンシを処理する必要がある。",
        "option_analysis": "選択肢Cは、新しいリージョンに追加のS3バケットを作成し、バケット間でクロスリージョンレプリケーションを設定する。このことにより、異なる場所のユーザーにとってゲームアセットへのアクセスが早くなる。他の選択肢は、単一リージョンの設定を含むか、レイテンシを十分に最適化していない。 ",
        "additional_knowledge": "CloudFrontを導入することで、ピーク時の負荷に対してパフォーマンスとユーザーエクスペリエンスが向上する。",
        "key_terminology": "Amazon S3, Amazon CloudFront, クロスリージョンレプリケーション, DynamoDB, レイテンシ削減",
        "overall_assessment": "選択肢Cは、要件を効果的に満たすため、最も推奨されるアプローチである。コミュニティの投票もこの選択肢を強く支持している。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Amazon CloudFront",
      "Cross-Region Replication",
      "DynamoDB",
      "latency reduction"
    ]
  },
  {
    "No": "136",
    "question": "A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java\nbackend and a NoSQL MongoDB database to store subscriber data.\nThe company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and\nthe company cannot make changes to the application.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、潜在的な賃貸者や購入者のための不動産情報を提供するオンプレミスのウェブサイトアプリケーションを持っています。このウェブサイトはJavaバックエンドとNoSQLのMongoDBデータベースを使用して購読者データを保存しています。企業は、アプリケーション全体をAWSに移行する必要がありますが、構造は同様でなければなりません。アプリケーションは高可用性のためにデプロイされなければならず、企業はアプリケーションを変更することができません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across",
        "text_jp": "Amazon Aurora DBクラスターを購読者データのデータベースとして使用します。Auto ScalingグループにAmazon EC2インスタンスをデプロイします。"
      },
      {
        "key": "B",
        "text": "Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single",
        "text_jp": "Amazon EC2インスタンス上にMongoDBを用いて購読者データのデータベースを構成します。単一のAuto ScalingグループにEC2インスタンスを展開します。"
      },
      {
        "key": "C",
        "text": "Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the",
        "text_jp": "Amazon DocumentDB（MongoDB互換）を適切なサイズのインスタンスで複数のアベイラビリティゾーンに設定します。"
      },
      {
        "key": "D",
        "text": "Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database",
        "text_jp": "Amazon DocumentDB（MongoDB互換）をオンデマンドキャパシティモードで複数のアベイラビリティゾーンにデプロイします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (86%) 14%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, which involves configuring Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode across multiple Availability Zones. This solution meets the high availability requirement and does not require changes to the application.",
        "situation_analysis": "The company needs a seamless migration to AWS with high availability while maintaining the existing application structure and not making changes.",
        "option_analysis": "Option D provides the needed MongoDB compatibility with DocumentDB while ensuring high availability due to its deployment across multiple Availability Zones. Other options either involve unwanted alterations to the database structure or do not guarantee high availability.",
        "additional_knowledge": "Implementing Amazon DocumentDB allows the company to leverage fully managed service capabilities, including automated backups and scaling without downtime.",
        "key_terminology": "High Availability, Amazon DocumentDB, MongoDB compatibility, On-Demand Capacity, Availability Zones",
        "overall_assessment": "Despite community voting suggesting option C, the key requirements indicate that D is the best fit due to its specific high availability and operational requirements. Community votes might be influenced by other use cases or interpretations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDであり、Amazon DocumentDB（MongoDB互換）を複数のアベイラビリティゾーンでオンデマンドキャパシティモードに設定することを含みます。このソリューションは高可用性の要件を満たし、アプリケーションの変更を必要としません。",
        "situation_analysis": "企業は、高可用性を維持しながら既存のアプリケーション構造を保ったままAWSへのシームレスな移行を必要としています。",
        "option_analysis": "オプションDは、複数のアベイラビリティゾーンでデプロイすることにより高可用性を保証しつつ、DocumentDBによりMongoDB互換性を提供します。他のオプションは、データベース構造に不要な変更を伴うか、高可用性を保証しません。",
        "additional_knowledge": "Amazon DocumentDBを実装することで、企業は自動バックアップやダウンタイムなしのスケーリングを含むフルマネージドサービスの機能を活用できます。",
        "key_terminology": "高可用性、Amazon DocumentDB、MongoDB互換、オンデマンドキャパシティ、アベイラビリティゾーン",
        "overall_assessment": "コミュニティ投票ではオプションCが示唆されているが、主要な要件はDが最適な選択であることを示しています。コミュニティの投票は他のユースケースや解釈に影響される可能性があります。"
      }
    ],
    "keywords": [
      "High Availability",
      "Amazon DocumentDB",
      "MongoDB Compatibility",
      "On-Demand Capacity",
      "Availability Zones"
    ]
  },
  {
    "No": "137",
    "question": "A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS\naccount to securely store images and media files that are used as content for the company's marketing campaigns. The creative team wants to\nshare the S3 bucket with the strategy team so that the strategy team can view the objects.\nA solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a\ncustom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when\nusers from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.\nThe solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only\nthe minimum permissions that they need.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "デジタルマーケティング会社には、さまざまなチームに属する複数のAWSアカウントがあります。クリエイティブチームは、会社のマーケティングキャンペーンのコンテンツとして使用される画像やメディアファイルを安全に保存するために、AWSアカウント内のAmazon S3バケットを使用しています。クリエイティブチームは、戦略チームがオブジェクトを表示できるようにS3バケットを共有したいと考えています。ソリューションアーキテクトは、戦略アカウントにstrategy_reviewerという名前のIAMロールを作成しました。また、ソリューションアーキテクトは、クリエイティブアカウント内にカスタムAWSキーマネジメントサービス（AWS KMS）キーを設定し、そのキーをS3バケットに関連付けました。しかし、戦略アカウントのユーザーがIAMロールを引き受けてS3バケット内のオブジェクトにアクセスしようとすると、アクセス拒否エラーが表示されます。ソリューションアーキテクトは、戦略アカウントのユーザーがS3バケットにアクセスできることを確認する必要があります。ソリューションは、これらのユーザーに必要な最小限の権限のみを提供する必要があります。これらの要件を満たすために、ソリューションアーキテクトはどのような手順の組み合わせを取るべきですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the",
        "text_jp": "S3バケットに対して読み取り権限を含むバケットポリシーを作成します。バケットポリシーのプリンシパルをアカウントIDに設定します。"
      },
      {
        "key": "B",
        "text": "Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.",
        "text_jp": "strategy_reviewer IAMロールを更新して、S3バケットへのフルアクセス権を付与し、カスタムKMSキーへの復号権限を付与します。"
      },
      {
        "key": "C",
        "text": "Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.",
        "text_jp": "クリエイティブアカウント内のカスタムKMSキーのポリシーを更新して、strategy_reviewer IAMロールに復号権限を付与します。"
      },
      {
        "key": "D",
        "text": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user.",
        "text_jp": "S3バケットに対して読み取り権限を含むバケットポリシーを作成します。バケットポリシーのプリンシパルを匿名ユーザーに設定します。"
      },
      {
        "key": "E",
        "text": "Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role.",
        "text_jp": "クリエイティブアカウント内のカスタムKMSキーのポリシーを更新して、strategy_reviewer IAMロールに暗号化権限を付与します。"
      },
      {
        "key": "F",
        "text": "Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS",
        "text_jp": "strategy_reviewer IAMロールを更新して、S3バケットへの読み取り権限とカスタムKMSキーへの復号権限を付与します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ACF (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Updating the strategy_reviewer IAM role to grant full permissions for the S3 bucket and decrypt permissions for the KMS key is essential for ensuring access.",
        "situation_analysis": "The creative team needs to securely share the S3 bucket with the strategy team while adhering to AWS security best practices. This requires precise permissions on both the S3 bucket and the KMS key.",
        "option_analysis": "Option A does not fully address permissions needed for the IAM role. Option C is necessary but incomplete on its own. Option D incorrectly allows anonymous access. Option E grants unnecessary permissions. Option F partially solves the issue but does not cover all access needs.",
        "additional_knowledge": "Cross-account access with limited permissions is a common challenge in multi-account AWS architectures.",
        "key_terminology": "IAM roles, bucket policies, KMS, principle of least privilege, S3",
        "overall_assessment": "The question tests understanding of permissions management across AWS services, particularly with IAM and S3 access control. It highlights the importance of precise policy adjustments."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。strategy_reviewer IAMロールを更新して、S3バケットへのフルアクセス権及びKMSキーへの復号権限を付与することが、アクセス確保に不可欠である。",
        "situation_analysis": "クリエイティブチームはAWSのセキュリティのベストプラクティスに従いながら、戦略チームとS3バケットを安全に共有する必要がある。これには、S3バケットとKMSキーの両方に正確な権限が必要である。",
        "option_analysis": "オプションAはIAMロールに必要な権限を完全に解決していない。オプションCは必要だが単独では不完全である。オプションDは匿名アクセスを誤って許可する。オプションEは不必要な権限を付与する。オプションFは問題の一部を解決するが、すべてのアクセスニーズをカバーしていない。",
        "additional_knowledge": "限られた権限でのアカウント間アクセスは、マルチアカウントAWSアーキテクチャでよく見られる課題である。",
        "key_terminology": "IAMロール、バケットポリシー、KMS、最小権限の原則、S3",
        "overall_assessment": "この問題は、特にIAMとS3のアクセス制御に関するAWSサービス間の権限管理の理解を試験するものである。ポリシーの調整の重要性を強調している。"
      }
    ],
    "keywords": [
      "IAM Roles",
      "S3 Bucket Policies",
      "KMS",
      "Least Privilege Principle"
    ]
  },
  {
    "No": "138",
    "question": "A life sciences company is using a combination of open source tools to manage data analysis workfiows and Docker containers running on\nservers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN),\nand then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their\ngenomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days.\nThe company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual\njobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is\nexpecting 10-15 job requests each day.\nWhich solution meets these requirements?",
    "question_jp": "ライフサイエンス企業は、オープンソースツールの組み合わせを使用してデータ分析ワークフローを管理し、オンプレミスのデータセンターでDockerコンテナを実行してゲノムデータを処理しています。シーケンシングデータはローカルストレージエリアネットワーク（SAN）に生成・保存され、その後データが処理されます。研究開発チームは、キャパシティの問題に直面し、作業負荷の要求に基づいてスケールできるように、ゲノム解析プラットフォームをAWS上で再アーキテクチャすることを決定しました。会社には高速なAWS Direct Connect接続があります。シーケンサーは各ゲノムごとに約200 GBのデータを生成し、個々のジョブは理想的なコンピュートキャパシティでデータを処理するのに数時間かかる場合があります。最終結果はAmazon S3に保存されます。会社は毎日10-15のジョブリクエストを期待しています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge",
        "text_jp": "AWS Snowball Edgeデバイスを定期的にスケジュールして、シーケンシングデータをAWSに転送します。AWSがSnowball Edgeを受信したとき"
      },
      {
        "key": "B",
        "text": "Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to",
        "text_jp": "AWS Data Pipelineを使用してシーケンシングデータをAmazon S3に転送します。S3イベントを使用してAmazon EC2オートスケーリンググループをトリガーします"
      },
      {
        "key": "C",
        "text": "Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS",
        "text_jp": "AWS DataSyncを使用してシーケンシングデータをAmazon S3に転送します。S3イベントを使用してAWS Lambda関数をトリガーし、AWSを開始します"
      },
      {
        "key": "D",
        "text": "Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that",
        "text_jp": "AWS Storage Gatewayファイルゲートウェイを使用してシーケンシングデータをAmazon S3に転送します。S3イベントを使用してAWS Batchジョブをトリガーします"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (70%) D (30%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Use AWS DataSync to transfer the sequencing data to Amazon S3.",
        "situation_analysis": "The company needs to efficiently transfer large amounts of genomic data to AWS while maintaining a scalable architecture. Given the volume of data (200 GB per genome) and the expected job requests (10-15 per day), a solution that automates data transfer and triggers processing is required.",
        "option_analysis": "Option C is the most suitable as AWS DataSync provides a fast and efficient way to transfer data to S3 and integrates with AWS Lambda for processing the data. Options A and B may not handle the data volume efficiently, and D relies on a Storage Gateway, which is less optimal for the given use case.",
        "additional_knowledge": "AWS DataSync is designed for large data transfers with minimal operational effort. It simplifies and speeds up copying large amounts of data to AWS, ensuring that the process aligns with AWS best practices.",
        "key_terminology": "AWS DataSync, AWS Lambda, Amazon S3, data transfer, automated processing.",
        "overall_assessment": "The choice of C aligns well with the company's requirements for performance and scalability. The community supports this choice significantly, indicating confidence in this solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです：AWS DataSyncを使用してシーケンシングデータをAmazon S3に転送します。",
        "situation_analysis": "会社は大規模なゲノムデータをAWSに効率的に転送し、スケーラブルなアーキテクチャを維持する必要があります。データの量（ゲノムごとに200 GB）と予想されるジョブリクエスト（毎日10-15件）を考えると、自動化されたデータ転送と処理のトリガーが必要です。",
        "option_analysis": "選択肢Cが最も適しています。AWS DataSyncは、S3へのデータ転送を迅速かつ効率的に行い、データ処理のためにAWS Lambdaと統合されています。選択肢AとBはデータ量を効率的に処理できない可能性があり、Dはストレージゲートウェイに依存しており、与えられたユースケースには最適ではありません。",
        "additional_knowledge": "AWS DataSyncは、大量データ転送のために設計されており、最小限の運用労力でデータをAWSにコピーするプロセスを簡素化し、高速化します。プロセスはAWSのベストプラクティスと整合性があります。",
        "key_terminology": "AWS DataSync、AWS Lambda、Amazon S3、データ転送、自動化処理。",
        "overall_assessment": "選択Cは、パフォーマンスとスケーラビリティという会社の要件に非常に適しています。コミュニティはこの選択肢を大きく支持しており、この解決策に自信を示しています。"
      }
    ],
    "keywords": [
      "AWS DataSync",
      "AWS Lambda",
      "Amazon S3",
      "data transfer",
      "automated processing"
    ]
  },
  {
    "No": "139",
    "question": "A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application\nreads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device.\nThe company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2\ninstances across multiple Availability Zones.\nA solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also\nmust implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running\ninstances at any given point in time.\nWhich solution will meet these requirements with the LEAST management overhead?",
    "question_jp": "企業は、開発環境で単一のWindows Amazon EC2インスタンス上でコンテンツ管理アプリケーションを実行しています。このアプリケーションは、インスタンスのルートデバイスとして接続された2TBのAmazon Elastic Block Store（Amazon EBS）ボリュームに対して静的コンテンツを読み書きします。企業は、このアプリケーションを、本番環境で高可用性かつ耐障害性のソリューションとして、複数のアベイラビリティゾーンにまたがる少なくとも3つのEC2インスタンスで実行することを計画しています。ソリューションアーキテクトは、アプリケーションを実行しているすべてのインスタンスをActive Directoryドメインに参加させるソリューションを設計する必要があります。このソリューションは、ファイルコンテンツへのアクセスを制御するためにWindows ACLも実装しなければなりません。アプリケーションは常に、すべての実行中のインスタンスで、ある瞬間において全く同じコンテンツを維持しなければなりません。この要件を最も管理オーバーヘッドが少なく満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones",
        "text_jp": "Amazon Elastic File System（Amazon EFS）ファイル共有を作成し、3つのアベイラビリティゾーンにまたがるAuto Scalingグループを作成します"
      },
      {
        "key": "B",
        "text": "Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group",
        "text_jp": "現在実行中のEC2インスタンスから新しいAMIを作成します。Amazon FSx for Lustreファイルシステムを作成します。そして、Auto Scalingグループを作成します"
      },
      {
        "key": "C",
        "text": "Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and",
        "text_jp": "Amazon FSx for Windows File Serverファイルシステムを作成します。3つのアベイラビリティゾーンにまたがるAuto Scalingグループを作成します"
      },
      {
        "key": "D",
        "text": "Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an",
        "text_jp": "現在実行中のEC2インスタンスから新しいAMIを作成します。Amazon Elastic File System（Amazon EFS）ファイルシステムを作成します。そして"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating a new AMI from the current EC2 instance and using Amazon FSx for Lustre allows for a high performance, scalable file system suitable for the needs of the application.",
        "situation_analysis": "The application must maintain the same content across multiple EC2 instances, requiring shared storage and high availability across different Availability Zones.",
        "option_analysis": "Option B meets the requirements well by providing a centralized storage system with Amazon FSx for Lustre that integrates seamlessly with Windows environments. Other options either do not facilitate the necessary storage solution or do not focus on the High Availability aspect effectively.",
        "additional_knowledge": "It's crucial to consider application performance requirements when determining the best storage solution.",
        "key_terminology": "Amazon FSx for Lustre, Auto Scaling, High Availability, Windows ACLs, Active Directory",
        "overall_assessment": "The community vote largely supports option C, which indicates a preference for the file server but fails to acknowledge the specific needs for Lustre's performance in this scenario. Therefore, while community votes may differ, option B remains technically correct based on the application's requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。現在のEC2インスタンスから新しいAMIを作成し、Amazon FSx for Lustreを使用することで、アプリケーションのニーズに適した高性能でスケーラブルなファイルシステムを実現できる。",
        "situation_analysis": "アプリケーションは、複数のEC2インスタンスで同じコンテンツを維持する必要があり、共有ストレージと異なるアベイラビリティゾーンにおける高可用性が求められる。",
        "option_analysis": "選択肢Bは、Windows環境とシームレスに統合されたAmazon FSx for Lustreを使用することで、必要なストレージソリューションを提供するため、要件を十分に満たしている。他の選択肢は必要なストレージソリューションを提供しないか、高可用性の側面に十分に焦点を当てていない。",
        "additional_knowledge": "アプリケーションのパフォーマンス要件を考慮することが、最良のストレージソリューションを決定する際に重要である。",
        "key_terminology": "Amazon FSx for Lustre, Auto Scaling, 高可用性, Windows ACLs, Active Directory",
        "overall_assessment": "コミュニティの投票は主に選択肢Cを支持しており、ファイルサーバーの好ましさを示しているが、シナリオにおけるLustreのパフォーマンスの特定のニーズを考慮していない。そのため、コミュニティの投票が異なっても、アプリケーションの要求に基づくと、選択肢Bが技術的に正しいままである。"
      }
    ],
    "keywords": [
      "Amazon FSx for Lustre",
      "Auto Scaling",
      "High Availability",
      "Windows ACLs",
      "Active Directory"
    ]
  },
  {
    "No": "140",
    "question": "A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a\nstandalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email\ntemplate for acknowledgement email messages that populate customer data before the application sends the email message to the customer.\nThe company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "あるソフトウェア・アズ・ア・サービス（SaaS）ベースの会社が顧客にケース管理ソリューションを提供しており、そのソリューションの一部として顧客に提供されるメールの応答が含まれています。会社はアプリケーションからメールメッセージを送信するためにスタンドアロンのシンプルメール転送プロトコル（SMTP）サーバを使用しています。また、アプリケーションは、お客様のデータを埋め込むためのメールテンプレートを保存し、アプリケーションがそのメールメッセージを顧客に送信する前に内容を補充します。会社はこのメッセージング機能をAWSクラウドに移行する計画で、運用オーバーヘッドを最小限に抑える必要があります。どのソリューションが最もコスト効果的にこれらの要求を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3",
        "text_jp": "AWS MarketplaceからのAMIを使用してAmazon EC2インスタンス上にSMTPサーバを設定する。メールテンプレートをAmazon S3に保存する。"
      },
      {
        "key": "B",
        "text": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an",
        "text_jp": "Amazon Simple Email Service（Amazon SES）を設定してメールメッセージを送信する。メールテンプレートをAmazon S3バケットに保存する。"
      },
      {
        "key": "C",
        "text": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple",
        "text_jp": "AWS MarketplaceからのAMIを使用してAmazon EC2インスタンス上にSMTPサーバを設定する。メールテンプレートをAmazon Simpleに保存する。"
      },
      {
        "key": "D",
        "text": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for",
        "text_jp": "Amazon Simple Email Service（Amazon SES）を設定してメールメッセージを送信する。メールテンプレートをAmazon SESに保存し、パラメータを設定する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Setting up Amazon Simple Email Service (Amazon SES) to send email messages is the most effective and cost-efficient solution for this case.",
        "situation_analysis": "The company aims to migrate email functionalities to AWS Cloud while minimizing operational overhead.",
        "option_analysis": "Options A and C require maintaining EC2 instances which involves more operational overhead and costs. Option D, while using SES, is less efficient than B because it implies more complexity in managing templates directly in SES.",
        "additional_knowledge": "Integrating SES with S3 ensures easy updates to email templates without needing to modify code.",
        "key_terminology": "Amazon SES, EC2, S3, email templates, operational overhead",
        "overall_assessment": "Given the requirements, option B offers an optimal balance of functionality, ease of use, and cost-effectiveness. The community overwhelmingly supports option D which might stem from misunderstanding the operational benefits of using SES with S3."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBです：メールメッセージを送信するためにAmazon Simple Email Service（Amazon SES）を設定することが、このケースにおいて最も効果的でコスト効率の良いソリューションです。",
        "situation_analysis": "この会社は、運用オーバーヘッドを最小限に抑えながら、メール機能をAWSクラウドに移行することを目指しています。",
        "option_analysis": "選択肢AとCは、EC2インスタンスを維持する必要があるため、運用オーバーヘッドとコストが増加します。選択肢DはSESを使用していますが、テンプレートを直接SESで管理することになるため、Bよりも効率が低くなります。",
        "additional_knowledge": "SESとS3を統合することで、コードを変更することなくメールテンプレートを簡単に更新できます。",
        "key_terminology": "Amazon SES、EC2、S3、メールテンプレート、運用オーバーヘッド",
        "overall_assessment": "要件を考慮すると、選択肢Bは機能性、使いやすさ、コスト効率の最適なバランスを提供します。コミュニティは選択肢Dを圧倒的に支持していますが、これはSESをS3で使用する運用上の利点を誤解している可能性があります。"
      }
    ],
    "keywords": [
      "Amazon SES",
      "Amazon S3",
      "EC2",
      "email templates",
      "operational overhead"
    ]
  },
  {
    "No": "141",
    "question": "A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a\nvideo Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.\nThe company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The\ncompany has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the\ndevelopment team when there are messages in the dead-letter queue.\nSeveral times during the day. the development team receives notification that messages are in the dead-letter queue and that videos have not\nbeen processed property. An investigation finds no errors m the application logs.\nHow can the company solve this problem?",
    "question_jp": "会社は、AWSクラウド上でAmazon EC2インスタンスを使用して動画を処理しています。動画の処理には30分かかります。いくつかのEC2インスタンスは、Amazon Simple Queue Service（Amazon SQS）キュー内の動画の数に応じてスケールインおよびスケールアウトします。会社は、ターゲットデッドレターキューとmaxReceiveCountを1に指定したリドライブポリシーを設定したSQSキューを構成しています。また、SQSキューの可視性タイムアウトを1時間に設定しています。会社は、デッドレターキューにメッセージがあるときに開発チームに通知するようにAmazon CloudWatchアラームを設定しています。日中何度か、開発チームはデッドレターキューにメッセージがあるとの通知を受け取り、動画が正常に処理されていないことを確認しました。調査の結果、アプリケーションログにエラーは見つかりませんでした。この問題をどのように解決できますか？",
    "choices": [
      {
        "key": "A",
        "text": "Turn on termination protection tor the EC2 Instances",
        "text_jp": "EC2インスタンスの終了保護を有効にする"
      },
      {
        "key": "B",
        "text": "Update the visibility timeout for the SQS queue to 3 hours",
        "text_jp": "SQSキューの可視性タイムアウトを3時間に更新する"
      },
      {
        "key": "C",
        "text": "Configure scale-in protection for the instances during processing",
        "text_jp": "処理中のインスタンスに対してスケールイン保護を構成する"
      },
      {
        "key": "D",
        "text": "Update the redrive policy and set maxReceiveCount to 0.",
        "text_jp": "リドライブポリシーを更新し、maxReceiveCountを0に設定する"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (76%) D (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. By updating the redrive policy and setting maxReceiveCount to 0, the messages are immediately sent to the dead-letter queue when they are not processed correctly without being retried.",
        "situation_analysis": "The primary requirement is to ensure videos are processed correctly and eliminate messages going to the dead-letter queue unnecessarily.",
        "option_analysis": "Choice A does not address the processing of videos directly. Choice B may delay the reprocessing of messages, which does not solve the underlying issue. Choice C does not ensure messages are not attempted again after a failure. Only choice D effectively prevents failed messages from being retried, immediately moving them to the dead-letter queue.",
        "additional_knowledge": "Setting maxReceiveCount to zero means that messages will not be retried; however, it's essential to ensure proper monitoring and alerting mechanisms are in place.",
        "key_terminology": "Amazon EC2, Amazon SQS, dead-letter queue, maxReceiveCount, Amazon CloudWatch",
        "overall_assessment": "The community vote did not significantly align with the correct answer due to misunderstanding the purpose of maxReceiveCount. Understanding how dead-letter queues operate is key in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。リドライブポリシーを更新し、maxReceiveCountを0に設定することで、メッセージは正しく処理されなかった場合に即座にデッドレターキューに送信され、再試行されることはない。",
        "situation_analysis": "主な要件は、動画が正しく処理されることを保証し、不要にデッドレターキューにメッセージが送信されるのを排除することである。",
        "option_analysis": "選択肢Aは、動画の処理自体には直接関係しない。選択肢Bは、メッセージの再処理を遅らせる可能性があり、根本的な問題を解決しない。選択肢Cは、障害後にメッセージが再試行されないことを保障しない。選択肢Dのみが、失敗したメッセージが再試行される前に即座にデッドレターキューに移動することを効果的に防ぐ。",
        "additional_knowledge": "maxReceiveCountをゼロに設定すると、メッセージは再試行されないが、適切な監視およびアラート機構を確保することが重要である。",
        "key_terminology": "Amazon EC2, Amazon SQS, デッドレターキュー, maxReceiveCount, Amazon CloudWatch",
        "overall_assessment": "コミュニティの投票は正しい答えと大きく一致しなかったが、maxReceiveCountの目的を誤解していたためである。このシナリオでは、デッドレターキューの動作を理解することが重要である。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Amazon SQS",
      "dead-letter queue",
      "maxReceiveCount",
      "Amazon CloudWatch"
    ]
  },
  {
    "No": "142",
    "question": "A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API\nGateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.\nThe solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an\nauthenticated user\nWhich solution will meet these requirements with the LEAST amount of effort?",
    "question_jp": "企業は、Amazon API Gatewayを使用してRegionalエンドポイントを持つAPIを開発しました。APIはAPI Gatewayの認証メカニズムを使用するAWS Lambda関数を呼び出します。設計レビューの結果、ソリューションアーキテクトは、公開アクセスを必要としないAPIのセットを特定しました。ソリューションアーキテクトは、APIのセットをVPCからのみアクセス可能にするソリューションを設計する必要があります。すべてのAPIは認証されたユーザーで呼び出す必要があります。どのソリューションが最も少ない労力でこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to",
        "text_jp": "内部アプリケーションロードバランサー（ALB）を作成します。ターゲットグループを作成します。呼び出すLambda関数を選択します。ALBのDNS名を使用します。"
      },
      {
        "key": "B",
        "text": "Remove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in",
        "text_jp": "API Gatewayに関連付けられたDNSエントリを削除します。Amazon Route 53でホスティングゾーンを作成します。CNAMEレコードを作成します。"
      },
      {
        "key": "C",
        "text": "Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPCreate a resource policy, and",
        "text_jp": "API GatewayでAPIエンドポイントをRegionalからプライベートに更新します。VPCにインターフェースVPCエンドポイントを作成します。リソースポリシーを作成します。"
      },
      {
        "key": "D",
        "text": "Deploy the Lambda functions inside the VPC Provision an EC2 instance, and install an Apache server. From the Apache server, call the",
        "text_jp": "Lambda関数をVPC内にデプロイします。EC2インスタンスをプロビジョニングし、Apacheサーバーをインストールします。Apacheサーバーから呼び出します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Deploying the Lambda functions inside the VPC allows them to be accessible only from within that VPC without exposing them to the public internet.",
        "situation_analysis": "The company requires their APIs to be accessible only from within a VPC. This is crucial for controlling access and ensuring enhanced security.",
        "option_analysis": "Option A involves using an ALB, which might unnecessarily complicate the architecture. Option B is not valid because it removes public accessibility entirely and does not provide access from a VPC. Option C suggests modifying the API endpoint, which is a viable option but does not include the deployment of Lambda functions within the VPC. Option D is straightforward for limiting access to a VPC environment and includes necessary infrastructure.",
        "additional_knowledge": "In a private VPC environment, services can interact more securely, reducing the risk of exposure to potential threats from the public internet.",
        "key_terminology": "VPC, AWS Lambda, API Gateway, security, public access",
        "overall_assessment": "While option C might seem viable, the full requirement of ensuring API calls are authenticated users and contained within a VPC is best met by option D, which allows for both."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。Lambda関数をVPC内にデプロイすることで、インターネットに公開することなく、そのVPC内からのみアクセス可能にします。",
        "situation_analysis": "企業は、APIをVPC内からのみアクセス可能にしたいと考えています。これは、アクセスを制御し、セキュリティを強化するために重要です。",
        "option_analysis": "選択肢AはALBを使用することを含みますが、アーキテクチャを不必要に複雑にする可能性があります。選択肢Bは、公共のアクセスを完全に削除するため、VPCからのアクセスを提供しません。選択肢CはAPIエンドポイントを変更することを提案していますが、Lambda関数をVPC内にデプロイするものではありません。選択肢Dは、アクセスをVPC環境に制限し、必要なインフラストラクチャを含むため、単純明快です。",
        "additional_knowledge": "プライベートVPC環境では、サービス間での安全な相互作用が可能となり、インターネットからの潜在的な脅威への曝露が減少します。",
        "key_terminology": "VPC、AWS Lambda、API Gateway、セキュリティ、公開アクセス",
        "overall_assessment": "選択肢Cも妥当であるように見えますが、API呼び出しが認証されたユーザーによって、かつVPC内に制限されるという完全な要件を満たすのは、選択肢Dです。"
      }
    ],
    "keywords": [
      "VPC",
      "AWS Lambda",
      "API Gateway",
      "security",
      "public access"
    ]
  },
  {
    "No": "143",
    "question": "A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are\nupdated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.\nThe company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is\nslow from time to time.\nWhich combination of steps will resolve the us-east-1 performance issues? (Choose two.)",
    "question_jp": "天気サービスは、eu-west-1リージョンにホストされたウェブアプリケーションから高解像度の天気マップを提供しています。天気マップは頻繁に更新され、静的なHTMLコンテンツと共にAmazon S3に保存されています。ウェブアプリケーションはAmazon CloudFrontによってフロントエンドされています。最近、同社はus-east-1リージョンのユーザーにサービスを拡大しましたが、これらの新しいユーザーは時々天気マップの表示が遅いと報告しています。us-east-1のパフォーマンス問題を解決するための手順の組み合わせはどれですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-",
        "text_jp": "AWS Global Acceleratorエンドポイントをeu-west-1のS3バケットに設定します。us-east-1でTCPポート80および443のエンドポイントグループを設定します。"
      },
      {
        "key": "B",
        "text": "Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.",
        "text_jp": "us-east-1に新しいS3バケットを作成します。eu-west-1のS3バケットから同期するためにS3クロスリージョンレプリケーションを設定します。"
      },
      {
        "key": "C",
        "text": "Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.",
        "text_jp": "Lambda@Edgeを使用して、北アメリカからのリクエストをus-east-1のS3転送アクセラレーションエンドポイントを使用するように変更します。"
      },
      {
        "key": "D",
        "text": "Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.",
        "text_jp": "Lambda@Edgeを使用して、北アメリカからのリクエストをus-east-1のS3バケットを使用するように変更します。"
      },
      {
        "key": "E",
        "text": "Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify",
        "text_jp": "AWS Global Acceleratorエンドポイントをus-east-1をCloudFrontディストリビューションのオリジンとして設定します。Lambda@Edgeを使用してリクエストを変更します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BD (96%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are B and E. Creating a new S3 bucket in us-east-1 and configuring cross-Region replication improves data access performance for users in that Region.",
        "situation_analysis": "Users in the us-east-1 region experience slow access to frequently updated weather maps due to geographical distance and latency from the origin in eu-west-1.",
        "option_analysis": "Option B (creating an S3 bucket and configuring replication) directly addresses the performance issue by placing the data closer to the users, while Option E (using Global Accelerator) can further improve access speeds by routing traffic optimally. Options A, C, and D do not appropriately resolve the performance bottleneck for the users in us-east-1.",
        "additional_knowledge": "It's important to monitor performance metrics continuously and adjust services based on user feedback.",
        "key_terminology": "S3, cross-Region replication, Global Accelerator, CloudFront, latency",
        "overall_assessment": "This question targets essential knowledge of optimizing AWS services for performance. The community votes show strong support for options B and D, suggesting they align with common practices for managing region-based performance issues."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBおよびEである。us-east-1に新しいS3バケットを作成し、クロスリージョンレプリケーションを設定することで、そのリージョンのユーザーに対するデータアクセスパフォーマンスが向上する。",
        "situation_analysis": "us-east-1リージョンのユーザーは、eu-west-1のオリジンからの地理的距離とレイテンシにより、頻繁に更新される天気マップへのアクセスが遅いと感じている。",
        "option_analysis": "選択肢B（S3バケットを作成しレプリケーションを設定すること）は、データをユーザーに近づけることでパフォーマンスの問題を直接解決する。一方、選択肢E（Global Acceleratorを使用すること）は、トラフィックを最適にルーティングすることでアクセスポイントを改善する。選択肢A、C、Dはus-east-1のユーザーに対するパフォーマンスのボトルネックを適切に解決していない。",
        "additional_knowledge": "パフォーマンスメトリクスを継続的に監視し、ユーザーのフィードバックに基づいてサービスを調整することが重要である。",
        "key_terminology": "S3, クロスリージョンレプリケーション, Global Accelerator, CloudFront, レイテンシ",
        "overall_assessment": "この質問は、パフォーマンス最適化のためのAWSサービスの重要な知識をターゲットにしている。コミュニティの投票はBおよびDの選択肢に強く支持されており、リージョンに基づくパフォーマンス問題を管理するための一般的な手法に合致していることを示している。"
      }
    ],
    "keywords": [
      "S3",
      "cross-Region replication",
      "Global Accelerator",
      "CloudFront",
      "latency"
    ]
  },
  {
    "No": "144",
    "question": "A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis\nindicates that the issue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as\nthe profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.\nThe solutions architect discovers that the file system has reached Its maximum capacity. The solutions architect must ensure that users can\nregain access. The solution also must prevent the problem from occurring again.\nWhich solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、企業がAmazon Workspacesで新しいセッションを確立できない問題を調査しています。初期の分析によれば、この問題はユーザープロファイルに関係しています。Amazon Workspaces環境は、プロファイル共有ストレージとしてAmazon FSx for Windows File Serverを使用するように構成されています。FSx for Windows File Serverファイルシステムは10 TBのストレージで構成されています。ソリューションアーキテクトは、ファイルシステムが最大容量に達していることを発見しました。ソリューションアーキテクトは、ユーザーが再びアクセスできるようにしなければなりません。また、問題が再発することも防がなければなりません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system.",
        "text_jp": "古いユーザープロファイルを削除してスペースを作成し、ユーザープロファイルをAmazon FSx for Lustreファイルシステムに移行する。"
      },
      {
        "key": "B",
        "text": "Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use",
        "text_jp": "update-file-systemコマンドを使用して容量を増やし、空き容量を監視するAmazon CloudWatchメトリックを実装する。"
      },
      {
        "key": "C",
        "text": "Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity",
        "text_jp": "Amazon CloudWatchのFreeStorageCapacityメトリックを使用してファイルシステムを監視し、AWS Step Functionsを使用して容量を増やす。"
      },
      {
        "key": "D",
        "text": "Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection",
        "text_jp": "古いユーザープロファイルを削除してスペースを作成し、追加のFSx for Windows File Serverファイルシステムを作成し、ユーザープロファイルのリダイレクションを更新する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (91%) 4%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It suggests monitoring the FreeStorageCapacity metric to ensure that the file system's health and performance can be maintained, while allowing for better capacity management through AWS Step Functions.",
        "situation_analysis": "The company is facing issues with new sessions in Amazon Workspaces due to the FSx for Windows File Server reaching its maximum capacity. This situation requires an immediate solution to restore user access and a plan to prevent future occurrences.",
        "option_analysis": "Option A involves migrating to FSx for Lustre, which is unnecessary when the current FSx for Windows File Server can be managed. Option B suggests increasing capacity but does not address ongoing monitoring, and option D would add complexity without ensuring a long-term solution.",
        "additional_knowledge": "Monitoring and proactive management are vital in cloud systems to ensure continued performance and to avoid user disruption.",
        "key_terminology": "Amazon Workspaces, FSx for Windows File Server, CloudWatch, FreeStorageCapacity, AWS Step Functions",
        "overall_assessment": "Option C not only addresses the immediate issue of capacity but also implements monitoring, preventing a recurrence of the problem. The community vote suggests a significant misunderstanding of the available options, with B receiving high support yet lacking long-term preventative measures."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。これは、ファイルシステムの健康とパフォーマンスを維持し、AWS Step Functionsを使用して容量管理をより良く行えるように、FreeStorageCapacityメトリックを監視することを提案している。",
        "situation_analysis": "企業は、FSx for Windows File Serverが最大容量に達しているため、Amazon Workspacesでの新しいセッションに問題を抱えている。この状況では、ユーザーのアクセスを回復するための即時の解決策と、今後の再発を防ぐための計画が必要である。",
        "option_analysis": "選択肢Aは、FSx for Lustreへの移行を含むが、現在のFSx for Windows File Serverの管理が可能なため、不必要である。選択肢Bは容量を増やすことを提案するが、継続的な監視に対処しておらず、選択肢Dは複雑さを加えつつ、長期的な解決策を保証しない。",
        "additional_knowledge": "監視と積極的な管理は、クラウドシステムにおいて、パフォーマンスを継続し、ユーザーの中断を避けるために重要である。",
        "key_terminology": "Amazon Workspaces, FSx for Windows File Server, CloudWatch, FreeStorageCapacity, AWS Step Functions",
        "overall_assessment": "選択肢Cは即時の容量の問題に対処するだけでなく、監視を実装し、問題の再発を防止する。コミュニティ投票は、選択肢Bが高い支持を受けているにもかかわらず、利用可能な選択肢に対する理解が大きく不足していることを示唆している。"
      }
    ],
    "keywords": [
      "Amazon Workspaces",
      "FSx for Windows File Server",
      "CloudWatch",
      "FreeStorageCapacity",
      "AWS Step Functions"
    ]
  },
  {
    "No": "145",
    "question": "An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery.\nConfirmation includes the recipient's signature or a photo of the package with the recipient. The driver's handheld device uploads signatures and\nphotos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file\nname matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information.\nThe file is then placed in Amazon S3 for archiving.\nAs the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped\nconnections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30\nminutes. The billing team reports that files are not always in the archive and that the central system is not always updated.\nA solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems\nare always updated. The handheld devices cannot be modified, so the company cannot deploy a new application.\nWhich solution will meet these requirements?",
    "question_jp": "国際配送会社はAWS上に配送管理システムをホストしています。ドライバーはこのシステムを使用して配送確認をアップロードします。確認には受取人のサインまたは受取人と共にあるパッケージの写真が含まれます。ドライバーのハンドヘルドデバイスは、FTPを通じて単一のAmazon EC2インスタンスにサインと写真をアップロードします。各ハンドヘルドデバイスは、サインインしたユーザーに基づいてディレクトリ内にファイルを保存し、ファイル名は配送番号と一致します。その後、EC2インスタンスは中央データベースに問い合わせて配送情報を取得し、ファイルにメタデータを追加します。その後、ファイルはアーカイブのためにAmazon S3に配置されます。会社が拡大するにつれて、ドライバーはシステムが接続を拒否していると報告しています。FTPサーバーは接続のドロップとメモリの問題により問題を抱えており、これに応じてシステムエンジニアはEC2インスタンスを30分ごとに再起動するcronタスクをスケジュールしました。請求チームはファイルが常にアーカイブにないことや、中央システムが常に更新されていないことを報告しています。ソリューションアーキテクトは、アーカイブが常にファイルを受け取り、システムが常に更新されるようにスケーラビリティを最大化するソリューションを設計する必要があります。ハンドヘルドデバイスは修正できないので、会社は新しいアプリケーションを展開できません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure",
        "text_jp": "既存のEC2インスタンスのAMIを作成します。EC2インスタンスのAuto Scalingグループを作成し、Application Load Balancerの後ろに配置します。"
      },
      {
        "key": "B",
        "text": "Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume",
        "text_jp": "AWS Transfer Familyを使用してFTPサーバーを作成し、ファイルをAmazon Elastic File System (Amazon EFS) に配置します。EFSボリュームをマウントします。"
      },
      {
        "key": "C",
        "text": "Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple",
        "text_jp": "AWS Transfer Familyを使用してFTPサーバーを作成し、ファイルをAmazon S3に配置します。Amazon SimpleのS3イベント通知を使用します。"
      },
      {
        "key": "D",
        "text": "Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service",
        "text_jp": "ハンドヘルドデバイスを更新してファイルを直接Amazon S3に配置します。Amazon Simple Queue Serviceを介してS3イベント通知を使用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Using AWS Transfer Family to create an FTP server that places files in Amazon Elastic File System (EFS) addresses the scalability issue. EFS is a managed file storage that scales automatically as you add or remove files.",
        "situation_analysis": "The company needs a robust solution that scales with increasing delivery volume. The current FTP server is failing and requiring frequent reboots. Files are inconsistently archived due to connection issues.",
        "option_analysis": "Option A suggests creating an Auto Scaling group, but it does not solve the core problem with the FTP server's limitations. Option C uses AWS Transfer Family with S3, but EFS is a better choice for file handling when the application doesn’t change. Option D is not feasible as the devices can't be modified.",
        "additional_knowledge": "It's crucial to ensure that EFS is configured correctly to avoid performance bottlenecks that could arise from high concurrent access.",
        "key_terminology": "AWS Transfer Family, FTP, Amazon EFS, Amazon S3, scalability.",
        "overall_assessment": "The solution chosen maximizes scalability and reliability. Community votes suggest a preference for a solution that provides real-time updates, aligning closely with EFS while addressing the delivery system’s constraints."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。AWS Transfer Familyを使用してFTPサーバーを作成し、ファイルをAmazon Elastic File System (EFS) に配置することは、スケーラビリティの問題に対処している。EFSは、ファイルの追加または削除に応じて自動的にスケールするマネージドファイルストレージである。",
        "situation_analysis": "会社は、増加する配送ボリュームに伴ってスケールする堅牢なソリューションが必要である。現在のFTPサーバーは故障し、頻繁に再起動を必要としている。接続の問題により、ファイルが不一致にアーカイブされている。",
        "option_analysis": "選択肢AはAuto Scalingグループの作成を提案しているが、FTPサーバーの限界という核心的な問題を解決していない。選択肢CはAWS Transfer FamilyをS3と使用しているが、アプリケーションが変更されない場合はEFSがファイル処理により適している。選択肢Dはデバイスが修正できないため実行可能ではない。",
        "additional_knowledge": "EFSが高負荷の同時アクセスによるパフォーマンスボトルネックを回避するために正しく設定されていることを確認することが重要である。",
        "key_terminology": "AWS Transfer Family、FTP、Amazon EFS、Amazon S3、スケーラビリティ。",
        "overall_assessment": "選ばれたソリューションはスケーラビリティと信頼性を最大化する。コミュニティの投票は、リアルタイムの更新を提供するソリューションの好みを示し、EFSと密接に連携しながら配送システムの制約に対処している。"
      }
    ],
    "keywords": [
      "AWS Transfer Family",
      "FTP",
      "Amazon EFS",
      "Amazon S3",
      "scalability"
    ]
  },
  {
    "No": "146",
    "question": "A company is running an application in the AWS Cloud. The application runs on containers m an Amazon Elastic Container Service (Amazon ECS)\ncluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory\nrequirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data\ncan be lost.\nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "question_jp": "ある企業がAWSクラウド上でアプリケーションを運用しています。このアプリケーションは、Amazon Elastic Container Service (Amazon ECS) クラスター上でコンテナとして稼働しています。ECSタスクはFargate起動タイプを使用しています。アプリケーションのデータはリレーショナルデータで、Amazon Aurora MySQLに保存されています。規制要件を満たすために、アプリケーションは障害が発生した場合に別のAWSリージョンに復元できる必要があります。障害が発生した場合、データは失われてはなりません。この要件を最小限の運用負荷で満たす解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Provision an Aurora Replica in a different Region.",
        "text_jp": "別のリージョンにAuroraレプリカをProvisionする。"
      },
      {
        "key": "B",
        "text": "Set up AWS DataSync for continuous replication of the data to a different Region.",
        "text_jp": "AWS DataSyncを設定して、データを別のリージョンに継続的にレプリケーションする。"
      },
      {
        "key": "C",
        "text": "Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.",
        "text_jp": "AWS Database Migration Service (AWS DMS)を使用して、データを別のリージョンに継続的にレプリケーションする。"
      },
      {
        "key": "D",
        "text": "Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes.",
        "text_jp": "Amazon Data Lifecycle Manager (Amazon DLM)を使用して、5分ごとにスナップショットをスケジュールする。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Provision an Aurora Replica in a different Region. This provides an effective disaster recovery solution with minimal operational overhead, as Aurora Replicas can automatically replicate data from the primary instance to a replica in another region asynchronously.",
        "situation_analysis": "The application requires a robust disaster recovery process due to regulatory compliance, and it should ensure no data loss in case of failure.",
        "option_analysis": "Option A aligns well with the requirements, allowing for automatic replication with minimal operational tasks after setup. Options B and C introduce additional complexity and operational overhead with continual monitoring and maintenance. Option D does not provide continuous data protection and requires manual intervention to restore data.",
        "additional_knowledge": "The efficiency of using built-in AWS features for replication can significantly reduce operational complexity.",
        "key_terminology": "Amazon Aurora, replication, disaster recovery, AWS Regions, operational overhead.",
        "overall_assessment": "The question effectively tests knowledge of AWS’s disaster recovery solutions and operational overhead. Community support for A indicates strong consensus on the best practice for achieving the objectives outlined."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はA：別のリージョンにAuroraレプリカをProvisionすることである。この方法は、最小限の運用負荷で効果的な災害復旧ソリューションを提供し、Auroraレプリカは主インスタンスから別のリージョン内のレプリカへのデータを非同期的に自動的にレプリケーションすることができる。",
        "situation_analysis": "アプリケーションには、規制要件により堅牢な災害復旧プロセスが必要であり、障害が発生した場合にデータ損失がないことを保証する必要がある。",
        "option_analysis": "選択肢Aは、要件にうまく合致し、自動的なレプリケーションを提供し、設定後は運用タスクが最小限で済む。選択肢BおよびCは、継続的な監視とメンテナンスを必要とし、追加の複雑さと運用負荷を導入する。選択肢Dは継続的なデータ保護を提供せず、データの復元には手動による介入が必要である。",
        "additional_knowledge": "AWSの組み込み機能を使用したレプリケーションの効率は、運用の複雑さを大幅に軽減する可能性がある。",
        "key_terminology": "Amazon Aurora、レプリケーション、災害復旧、AWSリージョン、運用負荷。",
        "overall_assessment": "この質問は、AWSの災害復旧ソリューションと運用負荷に関する知識を効果的にテストしている。選択肢Aに対するコミュニティの支持は、概説された目的を達成するための最良の実践についての強い合意を示している。"
      }
    ],
    "keywords": [
      "Amazon Aurora",
      "disaster recovery",
      "replication",
      "AWS Regions",
      "operational overhead"
    ]
  },
  {
    "No": "147",
    "question": "A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15\nminutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card\nprimary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for\nadditional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format.\nAdditionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.\nWhich solutions will meet these requirements?",
    "question_jp": "金融サービス会社は、クレジットカードサービスパートナーから定期的にデータフィードを受信しています。約5,000件のレコードが15分ごとに平文で送信され、HTTPSを介して直接Amazon S3バケットにサーバー側の暗号化された状態で配信されます。このフィードには、機密性の高いクレジットカードの主アカウント番号（PAN）データが含まれています。会社は、データを別のS3バケットに送信する前に自動的にPANをマスクする必要があります。また、特定のフィールドを削除・統合し、レコードをJSON形式に変換する必要があります。さらに、将来的には追加のフィードが追加される可能性があるため、設計は簡単に拡張できる必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda",
        "text_jp": "ファイル配信時にAWS Lambda関数を呼び出して、各レコードを抽出し、Amazon SQSキューに書き込みます。別のLambdaを呼び出します"
      },
      {
        "key": "B",
        "text": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate",
        "text_jp": "ファイル配信時にAWS Lambda関数を呼び出して、各レコードを抽出し、Amazon SQSキューに書き込みます。AWS Fargateを設定します"
      },
      {
        "key": "C",
        "text": "Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS",
        "text_jp": "データフィード形式に基づいてAWS Glueクローラーおよびカスタムクラシファイアを作成し、一致するテーブル定義を構築します。AWSを呼び出します"
      },
      {
        "key": "D",
        "text": "Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an",
        "text_jp": "データフィード形式に基づいてAWS Glueクローラーおよびカスタムクラシファイアを作成し、一致するテーブル定義を構築します。実行します"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which involves creating an AWS Glue crawler and custom classifier to process and transform the data feed before storing it.",
        "situation_analysis": "The company needs a scalable solution that can handle sensitive credit card data while allowing for future data feed integrations.",
        "option_analysis": "Option C effectively meets the requirements of transforming data into JSON and masking sensitive information. Other options lack the scalable integration needed.",
        "additional_knowledge": "Creating custom classifiers allows for flexibility in accommodating various input formats and transforming them into standardized structures.",
        "key_terminology": "AWS Glue, ETL, crawler, custom classifier, S3, JSON.",
        "overall_assessment": "Choosing option C aligns well with AWS best practices for handling sensitive data and creating scalable architectures. The community vote confirms this as the preferred solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCであり、AWS Glueクローラーおよびカスタムクラシファイアを作成して、データフィードを処理・変換してストレージに保存することが含まれます。",
        "situation_analysis": "会社は、敏感なクレジットカードデータを処理できる拡張可能なソリューションを必要としており、将来的なデータフィードの統合にも対応できる必要があります。",
        "option_analysis": "選択肢Cは、データをJSON形式に変換し、敏感な情報をマスクする要件に効果的に適合します。他の選択肢は、必要な拡張性の統合が不足しています。",
        "additional_knowledge": "カスタムクラシファイアを作成することで、さまざまな入力形式に対応し、標準化された構造へ変換する柔軟性が得られます。",
        "key_terminology": "AWS Glue、ETL、クローラー、カスタムクラシファイア、S3、JSON。",
        "overall_assessment": "選択肢Cの選択は、敏感なデータを扱い、拡張可能なアーキテクチャを作成する際のAWSのベストプラクティスに適合しています。コミュニティの投票もこのソリューションが好まれることを確認しています。"
      }
    ],
    "keywords": [
      "AWS Glue",
      "ETL",
      "crawler",
      "custom classifier",
      "S3",
      "JSON"
    ]
  },
  {
    "No": "148",
    "question": "A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application\nruns on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL\ndatabase as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.\nWhich solution will achieve the company's goal with the LEAST operational overhead?",
    "question_jp": "企業は、主なオンプレミスアプリケーションが失敗した場合に備えてビジネス継続ソリューションをAWSを使用して作成したいと考えています。このアプリケーションは、他のアプリケーションも稼働している物理サーバー上で実行されています。企業が移行を計画しているオンプレミスアプリケーションは、データストアとしてMySQLデータベースを使用しています。企業のすべてのオンプレミスアプリケーションは、Amazon EC2と互換性のあるオペレーティングシステムを使用しています。運用オーバーヘッドが最も少ない方法で企業の目標を達成するには、どのソリューションを選択すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test",
        "text_jp": "ソースサーバー（MySQLサーバーを含む）にAWSレプリケーションエージェントをインストールします。すべてのサーバーのレプリケーションを設定します。テストを開始します"
      },
      {
        "key": "B",
        "text": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target",
        "text_jp": "ソースサーバー（MySQLサーバーを含む）にAWSレプリケーションエージェントをインストールします。ターゲットでAWS Elastic Disaster Recoveryを初期化します"
      },
      {
        "key": "C",
        "text": "Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the",
        "text_jp": "AWSデータベース移行サービス（AWS DMS）レプリケーションサーバーとターゲットのAmazon Aurora MySQL DBクラスターを作成します"
      },
      {
        "key": "D",
        "text": "Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the",
        "text_jp": "AWSストレージゲートウェイボリュームゲートウェイをオンプレミスに展開します。すべてのオンプレミスサーバーにボリュームをマウントします。アプリケーションとデータをインストールします"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (79%) C (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating AWS DMS replication servers and a target Amazon Aurora MySQL DB cluster allows for minimal operational overhead and effective migration.",
        "situation_analysis": "The company requires a solution for business continuity that can efficiently replicate its MySQL database with little operational burden.",
        "option_analysis": "Option C directly addresses the need for database replication with a managed service that minimizes operational complexity. Option A would require manual setup and monitoring. Option B introduces overhead with disaster recovery initialization. Option D involves storage management that may not directly support operational continuity.",
        "additional_knowledge": "AWS DMS offers continuous data replication with low administrative effort, making it a highly suitable choice.",
        "key_terminology": "AWS DMS, Aurora MySQL, replication, operational overhead, business continuity.",
        "overall_assessment": "Even though many community votes lean towards Option B, Option C is better aligned with the requirement for minimal operational overhead due to AWS DMS capabilities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。AWS DMSレプリケーションサーバーとターゲットのAmazon Aurora MySQL DBクラスターを作成することで、運用オーバーヘッドを最小限に抑え、効果的な移行が可能である。",
        "situation_analysis": "企業は、運用的な負担が最小限である効率的なMySQLデータベースのレプリケーションのためのビジネス継続ソリューションを必要としている。",
        "option_analysis": "選択肢Cは、管理されたサービスを使用してデータベースのレプリケーション要件に直接対応しており、運用の複雑さを最小化する。また、選択肢Aは手動での設定と監視が必要である。選択肢Bは、災害復旧の初期化によるオーバーヘッドをもたらす。選択肢Dは、運用の継続性を直接サポートしない可能性のあるストレージ管理を含む。",
        "additional_knowledge": "AWS DMSは、低い管理努力での継続的なデータレプリケーションを提供し、非常に適した選択肢となる。",
        "key_terminology": "AWS DMS, Aurora MySQL, レプリケーション, 運用オーバーヘッド, ビジネス継続性。",
        "overall_assessment": "多くのコミュニティ票が選択肢Bに傾いているが、選択肢CはAWS DMSの機能により、運用オーバーヘッドが最小限に保たれる要件とより整合している。"
      }
    ],
    "keywords": [
      "AWS DMS",
      "Aurora MySQL",
      "replication",
      "operational overhead",
      "business continuity"
    ]
  },
  {
    "No": "149",
    "question": "A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the\ncompany's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The\nsolution must comply with AWS security best practices.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、財務情報に対する規制監査の対象となっています。外部監査人は単一のAWSアカウントを使用し、企業のAWSアカウントへのアクセスが必要です。ソリューションアーキテクトは、監査人に企業のAWSアカウントへの安全な読み取り専用アクセスを提供する必要があります。このソリューションはAWSのセキュリティベストプラクティスに準拠する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "In the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS account.",
        "text_jp": "企業のAWSアカウント内で、すべてのリソースに対するリソースポリシーを作成し、監査人のAWSアカウントにアクセスを許可する。"
      },
      {
        "key": "B",
        "text": "In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required",
        "text_jp": "企業のAWSアカウント内で、監査人のAWSアカウントを信頼するIAMロールを作成する。必要な権限を持つIAMポリシーを作成する。"
      },
      {
        "key": "C",
        "text": "In the company's AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM",
        "text_jp": "企業のAWSアカウント内でIAMユーザーを作成する。必要なIAMポリシーをIAMユーザーにアタッチする。IAMユーザーのためにAPIアクセスキーを作成する。"
      },
      {
        "key": "D",
        "text": "In the company's AWS account, create an IAM group that has the required permissions. Create an IAM user in the company's account for",
        "text_jp": "企業のAWSアカウント内で必要な権限を持つIAMグループを作成する。企業のアカウント内でIAMユーザーを作成する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating an IAM role that trusts the auditors' AWS account and allows read-only access meets security best practices while granting appropriate permissions.",
        "situation_analysis": "External auditors need secure, read-only access to the company's AWS account for compliance and audits. It is necessary to limit their access to only what is needed.",
        "option_analysis": "Option A does not follow best practices, as resource policies can be more complex and less secure than using IAM roles. Option C involves creating a user, but this leads to managing API keys, which can be less secure. Option D implies excessive user management instead of leveraging roles.",
        "additional_knowledge": "Utilizing IAM roles is recommended in scenarios where cross-account access is required without creating and managing permanent access credentials.",
        "key_terminology": "IAM, Role, Policy, Trust Relationship, Permissions",
        "overall_assessment": "This question emphasizes secure access management within AWS. B is clearly favored in community voting, showing alignment with industry best practices for granting temporary access."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。監査人のAWSアカウントを信頼するIAMロールを作成し、読み取り専用アクセスを許可することで、適切な権限を与えつつセキュリティベストプラクティスを満たす。",
        "situation_analysis": "外部監査人は、企業のAWSアカウントへの安全な読み取り専用アクセスを必要としており、コンプライアンスと監査のために必要なアクセスを制限する必要がある。",
        "option_analysis": "選択肢Aは、リソースポリシーがIAMロールを使用するよりも複雑でセキュリティが低下する可能性があるため、ベストプラクティスに従っていない。選択肢Cはユーザーを作成するが、APIキーの管理が必要になり、セキュリティが低下する可能性がある。選択肢Dは過剰なユーザー管理を示唆しており、ロールを活用する方が適している。",
        "additional_knowledge": "IAMロールを利用することで、永続的なアクセス資格情報を管理することなく、クロスアカウントアクセスが必要なシナリオでのアプローチが推奨される。",
        "key_terminology": "IAM, ロール, ポリシー, 信頼関係, 権限",
        "overall_assessment": "この質問はAWS内のセキュアなアクセス管理を強調している。Bはコミュニティの投票でも明確に支持されており、業界のベストプラクティスに沿っている。"
      }
    ],
    "keywords": [
      "IAM",
      "Role",
      "Policy",
      "Trust Relationship",
      "Permissions"
    ]
  },
  {
    "No": "150",
    "question": "A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB\ntable to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The\nnew solution must ensure high availability for the trading platform.\nWhich solution will meet these requirements with the LEAST latency?",
    "question_jp": "ある企業は、Amazon DynamoDBをストレージバックエンドとして使用するレイテンシーに敏感な取引プラットフォームを運営しています。企業はDynamoDBテーブルをオンデマンドキャパシティモードで構成しました。ソリューションアーキテクトは、取引プラットフォームのパフォーマンスを向上させるソリューションを設計する必要があります。新しいソリューションは、取引プラットフォームの高可用性を確保する必要があります。どのソリューションが最も低いレイテンシーでこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.",
        "text_jp": "2ノードのDynamoDB Accelerator (DAX) クラスターを作成します。アプリケーションを構成して、DAXを使用してデータを読み書きします。"
      },
      {
        "key": "B",
        "text": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to",
        "text_jp": "3ノードのDynamoDB Accelerator (DAX) クラスターを作成します。アプリケーションを構成して、DAXを使用してデータを読み、直接書き込みます。"
      },
      {
        "key": "C",
        "text": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to",
        "text_jp": "3ノードのDynamoDB Accelerator (DAX) クラスターを作成します。アプリケーションを構成して、DynamoDBテーブルから直接データを読み、書き込みます。"
      },
      {
        "key": "D",
        "text": "Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to",
        "text_jp": "1ノードのDynamoDB Accelerator (DAX) クラスターを作成します。アプリケーションを構成して、DAXを使用してデータを読み、直接書き込みます。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create a two-node DynamoDB Accelerator (DAX) cluster. This design will help reduce latency for read and write operations, thus improving overall performance while ensuring high availability.",
        "situation_analysis": "The trading platform is latency-sensitive, which means that response times are crucial. High availability is also necessary to avoid downtime during trading activities.",
        "option_analysis": "Option A provides a DAX cluster that will cache data, significantly reducing read latencies. Options B and C introduce higher configurations but do not necessarily guarantee lower latencies. Option D has a single-node configuration which fails to provide high availability.",
        "additional_knowledge": "AWS recommends using DAX for applications that require high-speed access to DynamoDB, especially under peak load.",
        "key_terminology": "DynamoDB, DAX, caching, high availability, latency-sensitive applications.",
        "overall_assessment": "Answer A aligns with AWS best practices for improving performance in latency-sensitive applications. The community voting leaning towards B raises a note of consideration, but A remains the most appropriate choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はA: 2ノードのDynamoDB Accelerator (DAX) クラスターを作成することです。この設計は、読み取りおよび書き込み操作のレイテンシーを削減し、全体的なパフォーマンスを向上させ、高可用性を確保するのに役立ちます。",
        "situation_analysis": "取引プラットフォームはレイテンシーに敏感であり、応答時間が重要です。高可用性を維持することも、取引活動中のダウンタイムを避けるために必要です。",
        "option_analysis": "選択肢AはDAXクラスターを提供し、データをキャッシュすることで、レイテンシーを大幅に削減します。選択肢BとCはより高い構成を導入しますが、必ずしも低いレイテンシーを保証するわけではありません。選択肢Dはシングルノード構成であり、高可用性を提供しないため不適切です。",
        "additional_knowledge": "AWSでは、DynamoDBへの高速アクセスを必要とするアプリケーション、特にピーク負荷時にDAXの使用を推奨しています。",
        "key_terminology": "DynamoDB, DAX, キャッシング, 高可用性, レイテンシーに敏感なアプリケーション。",
        "overall_assessment": "選択肢Aは、レイテンシーに敏感なアプリケーションのパフォーマンスを向上させるためのAWSのベストプラクティスと整合しています。コミュニティの投票がBを支持していることは考慮対象ですが、Aが最も適切な選択です。"
      }
    ],
    "keywords": [
      "DynamoDB",
      "DAX",
      "caching",
      "high availability",
      "latency-sensitive applications"
    ]
  },
  {
    "No": "151",
    "question": "A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2\ninstances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind\nanother ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak\nusage of the application.\nThe application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives\nminimal trafic during the rest of the day.\nA solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "question_jp": "ある企業が、オンプレミスからAWSにアプリケーションを移行しました。アプリケーションのフロントエンドは、2つのAmazon EC2インスタンスの背後にあるアプリケーションロードバランサー（ALB）で動作する静的ウェブサイトです。アプリケーションのバックエンドは、3つのEC2インスタンスの背後にある別のALBで動作するPythonアプリケーションです。EC2インスタンスは、大型で汎用的なオンデマンドインスタンスで、アプリケーションのピーク使用に対するオンプレミスの仕様を満たすためにサイズされています。アプリケーションは毎月数十万のリクエストを受けています。しかし、アプリケーションは主に昼食時に使用され、他の時間帯には最小限のトラフィックを受けています。ソリューションアーキテクトは、アプリケーションの可用性に悪影響を与えずにインフラコストを最適化する必要があります。どの組み合わせの手順がこれらの要件を満たすでしょうか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances.",
        "text_jp": "すべてのEC2インスタンスを、既存のEC2インスタンスと同じコア数を持つコンピュート最適化インスタンスに変更します。"
      },
      {
        "key": "B",
        "text": "Move the application frontend to a static website that is hosted on Amazon S3.",
        "text_jp": "アプリケーションのフロントエンドを、Amazon S3でホストされる静的ウェブサイトに移行します。"
      },
      {
        "key": "C",
        "text": "Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes.",
        "text_jp": "アプリケーションのフロントエンドをAWS Elastic Beanstalkを使用してデプロイします。ノードには同じインスタンスタイプを使用します。"
      },
      {
        "key": "D",
        "text": "Change all the backend EC2 instances to Spot Instances.",
        "text_jp": "すべてのバックエンドEC2インスタンスをスポットインスタンスに変更します。"
      },
      {
        "key": "E",
        "text": "Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2",
        "text_jp": "バックエンドのPythonアプリケーションを、既存のEC2インスタンスと同じコア数を持つ汎用バースト可能なEC2インスタンスにデプロイします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BE (88%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Moving the application frontend to Amazon S3 significantly reduces costs while still serving the static website effectively. Additionally, a backend optimization would also be required, which can likely be achieved through option D.",
        "situation_analysis": "The application experiences high traffic during lunch hours but is generally underutilized during other times, indicating the potential for cost optimization by using services that can scale based on demand.",
        "option_analysis": "Option A does not address the unnecessary capacity of existing instances during off-peak times. Option C may increase complexity without optimizing costs. Option D (changing to Spot Instances) could be beneficial for the backend but may not match the requirement of immediate availability. Option E doesn't provide cost optimization since it maintains the same instance type.",
        "additional_knowledge": "Understanding the use of cost-effective solutions like Amazon S3 aligns with AWS best practices for minimizing operational expenses.",
        "key_terminology": "Amazon S3, EC2, Application Load Balancer, Spot Instances, cost optimization, availability, static website",
        "overall_assessment": "While the community leans toward B overwhelmingly, careful consideration of backend cost optimization strategies such as D should also be recognized."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBです。アプリケーションのフロントエンドをAmazon S3に移行することにより、静的ウェブサイトを効果的に提供しつつ、コストを大幅に削減できます。また、バックエンドの最適化もDのオプションを通じて達成可能です。",
        "situation_analysis": "アプリケーションは昼食時間帯に高トラフィックを経験していますが、他の時間帯は一般的に利用度が低いため、需要に基づいてスケールできるサービスを使用することでコストを最適化する可能性があります。",
        "option_analysis": "オプションAは、オフピーク時に既存のインスタンスの過剰容量に対処していません。オプションCは、コストを最適化せずに複雑さを増す可能性があります。オプションD（スポットインスタンスへの変更）は、バックエンドに有益ですが、即時の可用性の要件に合致しないかもしれません。オプションEは、同じインスタンスタイプを維持するため、コスト最適化には寄与しません。",
        "additional_knowledge": "Amazon S3のようなコスト効果の高いソリューションの使用は、運用コストを最小化するためのAWSのベストプラクティスに沿っています。",
        "key_terminology": "Amazon S3, EC2, アプリケーションロードバランサー, スポットインスタンス, コスト最適化, 可用性, 静的ウェブサイト",
        "overall_assessment": "コミュニティはBに圧倒的に傾いていますが、Dのようなバックエンドのコスト最適化戦略も検討する必要があります。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "EC2",
      "Application Load Balancer",
      "Spot Instances",
      "cost optimization",
      "availability",
      "static website"
    ]
  },
  {
    "No": "152",
    "question": "A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on\nAmazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is\ndeveloping new application features to run on Amazon EKS with AWS Fargate.\nThe platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.\nWhich solution will provide the MOST cost-effective setup for the platform?",
    "question_jp": "ある企業がAWS上でイベントチケットプラットフォームを運営しており、プラットフォームのコスト効率を最適化したいと考えています。このプラットフォームは、Amazon Elastic Kubernetes Service（Amazon EKS）を使用してAmazon EC2上に展開されており、Amazon RDS for MySQL DBインスタンスにサポートされています。企業は、新しいアプリケーション機能をAmazon EKS上でAWS Fargateを使用して実行するために開発しています。このプラットフォームは、需要の高いピークが稀に発生します。この需要の急増は、イベントの日付によって異なります。どのソリューションがプラットフォームにとって最もコスト効率の良い構成を提供しますか？",
    "choices": [
      {
        "key": "A",
        "text": "Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot",
        "text_jp": "EKSクラスタが基盤負荷で使用するEC2インスタンスのためにスタンダードリザーブドインスタンスを購入し、クラスタをスポットインスタンスでスケールする。"
      },
      {
        "key": "B",
        "text": "Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity",
        "text_jp": "EKSクラスタの予測される中程度の負荷のためにコンピュートセービングスプランを購入し、オンデマンドキャパシティでクラスタをスケールする。"
      },
      {
        "key": "C",
        "text": "Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks.",
        "text_jp": "EKSクラスタの予測される基本負荷のためにEC2インスタンスセービングスプランを購入し、ピークに対処するためにスポットインスタンスでクラスタをスケールする。"
      },
      {
        "key": "D",
        "text": "Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks.",
        "text_jp": "EKSクラスタの予測される基本負荷のためにコンピュートセービングスプランを購入し、ピークに対処するためにスポットインスタンスでクラスタをスケールする。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (80%) 9% 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The most cost-effective solution is to purchase Compute Savings Plans for the predicted medium load of the EKS cluster, and scale with On-Demand Capacity to accommodate peak demands.",
        "situation_analysis": "The platform has infrequent high peaks in demand, which indicates that while baseline usage can be predicted, the demand spikes are less predictable and may require different scalability methods.",
        "option_analysis": "Option B aligns with AWS best practices by utilizing Savings Plans to handle expected moderate usage while leveraging on-demand capacity for spikes. Options A and C also suggest savings plans but do not address the unpredictable demand as effectively as B. Option D suggests computing savings plans but does not match load predictions adequately.",
        "additional_knowledge": "AWS services such as Amazon EC2 and EKS should be managed in consideration of their workloads to maintain cost efficiency.",
        "key_terminology": "Compute Savings Plans, On-Demand Capacity, Amazon EKS, Amazon EC2, cost-effectiveness",
        "overall_assessment": "Option B is favored by community voting and matches the best practices for cost management by predicting load and addressing peak demand. There is strong community support behind option B, making it a well-recognized choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "最もコスト効率の良いソリューションは、EKSクラスタの予測される中程度の負荷のためにコンピュートセービングスプランを購入し、ピーク需要に対応するためにオンデマンドキャパシティでスケールすることです。",
        "situation_analysis": "このプラットフォームは、需要の高いピークが稀に発生しており、これはベースラインの使用量を予測できる一方で、需要の急増は予測が難しいことを意味しています。",
        "option_analysis": "選択肢Bは、予測される中程度の使用量に対処するためにセービングスプランを利用し、急増に対応するためにオンデマンドキャパシティを活用するというAWSのベストプラクティスに合致しています。選択肢AやCもセービングスプランを提案していますが、Bほど柔軟に需要の変動に対処できません。選択肢Dはコンピュートセービングスプランを提案していますが、負荷予測と十分に一致しません。",
        "additional_knowledge": "AWSサービス、特にAmazon EC2およびEKSは、そのワークロードを考慮に入れて管理することで、コスト効率を維持すべきです。",
        "key_terminology": "コンピュートセービングスプラン、オンデマンドキャパシティ、Amazon EKS、Amazon EC2、コスト効率",
        "overall_assessment": "選択肢Bはコミュニティの投票によって支持されており、負荷の予測とピーク需要への対処におけるコスト管理のベストプラクティスと一致しています。また、コミュニティからの強い支持があります。"
      }
    ],
    "keywords": [
      "Compute Savings Plans",
      "On-Demand Capacity",
      "Amazon EKS",
      "Amazon EC2",
      "cost-effectiveness"
    ]
  },
  {
    "No": "153",
    "question": "A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon\nCloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured\nwith an alternate domain name that visitors use when they access the application.\nEach week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the\ncompany wants visitors to receive an informational message instead of a CloudFront error message.\nA solutions architect creates an Amazon S3 bucket as the first step in the process.\nWhich combination of steps should the solutions architect take next to meet the requirements? (Choose three.)",
    "question_jp": "ある企業がAWS Elastic Beanstalkにアプリケーションをデプロイしています。このアプリケーションはAmazon Auroraをデータベースレイヤーとして使用しています。Amazon CloudFrontディストリビューションがWebリクエストを処理し、Elastic Beanstalkのドメイン名をオリジンサーバーとして含んでいます。ディストリビューションは、訪問者がアプリケーションにアクセスする際に使用する代替ドメイン名で構成されています。毎週、企業は定期メンテナンスのためにアプリケーションをサービスから取り外します。アプリケーションが利用できないものの、企業は訪問者にCloudFrontのエラーメッセージではなく、情報メッセージを受け取ってもらいたいと考えています。ソリューションアーキテクトは、プロセスの最初のステップとしてAmazon S3バケットを作成します。その後、要件を満たすためにソリューションアーキテクトはどのようなステップの組み合わせを取るべきですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Upload static informational content to the S3 bucket.",
        "text_jp": "S3バケットに静的な情報コンテンツをアップロードする。"
      },
      {
        "key": "B",
        "text": "Create a new CloudFront distribution. Set the S3 bucket as the origin.",
        "text_jp": "新しいCloudFrontディストリビューションを作成する。S3バケットをオリジンとして設定する。"
      },
      {
        "key": "C",
        "text": "Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin",
        "text_jp": "S3バケットを元のCloudFrontディストリビューションのセカンドオリジンとして設定する。ディストリビューションとS3バケットがオリジンを使用するように構成する。"
      },
      {
        "key": "D",
        "text": "During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete.",
        "text_jp": "週次メンテナンス中にデフォルトキャッシュの動作を編集してS3オリジンを使用させる。メンテナンスが完了したら、その変更を元に戻す。"
      },
      {
        "key": "E",
        "text": "During the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \\ Set the",
        "text_jp": "週次メンテナンス中に、新しいディストリビューションでS3オリジンのキャッシュ動作を作成する。パスパターンを設定する。"
      },
      {
        "key": "F",
        "text": "During the weekly maintenance, configure Elastic Beanstalk to serve trafic from the S3 bucket.",
        "text_jp": "週次メンテナンス中にElastic BeanstalkをS3バケットからトラフィックを提供するように構成する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Uploading static informational content to the S3 bucket ensures that visitors receive relevant information during the application's downtime.",
        "situation_analysis": "The company needs to handle visitor requests during maintenance downtime without displaying CloudFront error messages.",
        "option_analysis": "Option A is correct as it directly addresses the requirement to provide static information. Option B is unnecessary as the existing distribution can manage the S3 origin. Option C introduces complexity without significant benefit. Option D suggests temporary changes that can be avoided. Option E is similar to B and is not needed. Option F is impractical as Elastic Beanstalk should not serve from S3 directly.",
        "additional_knowledge": "Consider setting cache invalidation rules if the information on the S3 bucket needs updating frequently.",
        "key_terminology": "Amazon S3, CloudFront, static content, downtimes, origin.",
        "overall_assessment": "The problem is clearly defined and has a straightforward solution. Community voting emphasizes the clarity of the option A as a necessary initial step."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。S3バケットに静的な情報コンテンツをアップロードすることで、アプリケーションのダウンタイム中に訪問者が関連情報を受け取ることが保証される。",
        "situation_analysis": "企業はメンテナンスダウンタイム中に訪問者リクエストを処理し、CloudFrontのエラーメッセージを表示しないようにする必要がある。",
        "option_analysis": "オプションAは静的情報を提供する必要性に直接応えているため正しい。オプションBは不要であり、既存のディストリビューションでS3オリジンを管理できる。オプションCは特に利益がない複雑さをもたらす。オプションDは避けられる一時的な変更を提案している。オプションEはBと似ており必要ではない。オプションFはElastic Beanstalkが直接S3から提供するのが実用的ではないため、実行不可能である。",
        "additional_knowledge": "S3バケット内の情報を頻繁に更新する場合、キャッシュ無効化ルールの設定を検討すべきである。",
        "key_terminology": "Amazon S3、CloudFront、静的コンテンツ、ダウンタイム、オリジン。",
        "overall_assessment": "問題は明確に定義されており、簡単な解決策がある。コミュニティによる投票は、オプションAが必要な初期ステップであることを強調している。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "CloudFront",
      "static content",
      "downtimes",
      "origin"
    ]
  },
  {
    "No": "154",
    "question": "A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that\nprocesses and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN.\nThe Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment\nvariables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new\nfunction version with the updated environment variables after validating results. This update process also requires frequent changes to the\ncustom application to invoke the new function version ARN. These changes cause interruptions for users.\nA solutions architect needs to simplify this process to minimize disruption to users.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある会社がユーザーにカスタムアプリケーションから画像をアップロードする機能を提供している。このアップロードプロセスは、AWS Lambda関数を呼び出し、その関数が画像を処理してAmazon S3バケットに保存する。アプリケーションは特定の関数バージョンARNを使用してLambda関数を呼び出す。Lambda関数は環境変数を使用して画像処理のパラメータを受け取る。会社は、最適な画像処理出力を達成するために、Lambda関数の環境変数を頻繁に調整する。会社は異なるパラメータをテストし、結果を検証した後、更新された環境変数を持つ新しい関数バージョンを公開する。この更新プロセスには、アプリケーションを頻繁に変更して新しい関数バージョンARNを呼び出すことも必要であり、これらの変更はユーザーに中断を引き起こす。ソリューションアーキテクトは、ユーザーへの中断を最小限に抑えるためにこのプロセスを簡素化する必要がある。どのソリューションが最も低い運用負担でこれらの要件を満たすことができるか？",
    "choices": [
      {
        "key": "A",
        "text": "Directly modify the environment variables of the published Lambda function version. Use the SLATEST version to test image processing",
        "text_jp": "公開されたLambda関数バージョンの環境変数を直接変更する。SLATESTバージョンを使用して画像処理をテストする"
      },
      {
        "key": "B",
        "text": "Create an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image",
        "text_jp": "画像処理パラメータを格納するAmazon DynamoDBテーブルを作成する。Lambda関数を変更して画像を取得する"
      },
      {
        "key": "C",
        "text": "Directly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function",
        "text_jp": "画像処理パラメータをLambda関数内に直接コーディングし、環境変数を削除する。新しい関数を公開する"
      },
      {
        "key": "D",
        "text": "Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new",
        "text_jp": "Lambda関数エイリアスを作成する。クライアントアプリケーションを変更して関数エイリアスARNを使用する。Lambdaエイリアスを新しいものに再構成する"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, creating a Lambda function alias. This allows for easy management of function versions without requiring changes to the client application.",
        "situation_analysis": "The company frequently adjusts its Lambda environment variables, leading to interruptions for users when deploying new function versions.",
        "option_analysis": "Option D allows the application to point to a stable alias, minimizing the need for the application to change with each version. Options A, B, and C introduce more operational overhead or user disruption.",
        "additional_knowledge": "Lambda function aliases help maintain stability in production while allowing for experimentation in the background.",
        "key_terminology": "AWS Lambda, function alias, environment variable, Amazon S3, operational overhead.",
        "overall_assessment": "Option D is the best choice as it simplifies the process and reduces disruptions for end-users, aligning with AWS best practices in version management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDであり、Lambda関数エイリアスを作成することです。これにより、クライアントアプリケーションに変更を加えることなく、関数バージョンの簡易管理ができます。",
        "situation_analysis": "会社はLambdaの環境変数を頻繁に調整しており、新しい関数バージョンを展開するとユーザーに中断が発生します。",
        "option_analysis": "オプションDは、アプリケーションが安定したエイリアスを指すことを可能にし、各バージョンごとにアプリケーションが変更される必要を最小限に抑えます。オプションA、B、Cは、運用上の負担やユーザーの中断を増加させます。",
        "additional_knowledge": "Lambda関数エイリアスは、バックグラウンドで実験を行える一方で、本番環境の安定性を維持する助けになります。",
        "key_terminology": "AWS Lambda、関数エイリアス、環境変数、Amazon S3、運用負担。",
        "overall_assessment": "オプションDはプロセスを簡素化し、エンドユーザーへの中断を減らす最良の選択であり、AWSのバージョン管理におけるベストプラクティスに沿っています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "function alias",
      "environment variable",
      "Amazon S3",
      "operational overhead"
    ]
  },
  {
    "No": "155",
    "question": "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to\nkeep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application\nLoad Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex\ndomain.\nWhich solution will meet these requirements with the LEAST effort?",
    "question_jp": "グローバルメディア会社が、アプリケーションのマルチリージョン展開を計画している。Amazon DynamoDBグローバルテーブルによって、2つの大陸に集まるユーザー間で一貫したユーザーエクスペリエンスを維持する。その展開では、パブリックアプリケーションロードバランサー（ALB）を使用する。この会社はパブリックDNSを内部で管理している。会社は、エイペックスドメインを通じてアプリケーションを利用できるようにしたい。どのソリューションが最も少ない手間でこれらの要件を満たすことができるか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy",
        "text_jp": "パブリックDNSをAmazon Route 53に移行する。エイペックスドメインからALBを指し示すCNAMEレコードを作成する。地理的位置ルーティングポリシーを使用する"
      },
      {
        "key": "B",
        "text": "Place a Network Load Balancer (NLB) in front of the ALMigrate public DNS to Amazon Route 53. Create a CNAME record for the apex",
        "text_jp": "Network Load Balancer（NLB）をALBの前に配置し、パブリックDNSをAmazon Route 53に移行する。エイペックスドメイン用にCNAMEレコードを作成する"
      },
      {
        "key": "C",
        "text": "Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the",
        "text_jp": "AWS Global Acceleratorを作成し、適切なAWSリージョンのエンドポイントをターゲットとする複数のエンドポイントグループを設定する。"
      },
      {
        "key": "D",
        "text": "Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route trafic",
        "text_jp": "AWS LambdaにバックされたAmazon API Gateway APIをAWSリージョンの1つで作成する。トラフィックをルーティングするLambda関数を構成する"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. This solution minimizes effort as it allows for easy routing of traffic across regions and provides a single entry point.",
        "situation_analysis": "The application demands a global reach with low latency, and there is a need for consistent user experience on multiple continents. AWS Global Accelerator provides a straightforward solution to manage traffic effectively.",
        "option_analysis": "Option A introduces additional complexity with DNS management and routing policies. Option B involves unnecessary infrastructure (NLB) that does not add significant value. Option D overcomplicates the solution by adding an API gateway and Lambda, which are not required for simple routing.",
        "additional_knowledge": "Global application architectures often benefit from services like Global Accelerator, which simplifies the management of global traffic routing while enhancing performance.",
        "key_terminology": "AWS Global Accelerator, Elastic Load Balancer, multi-region deployment, traffic management, DNS routing",
        "overall_assessment": "The question is clear, and option C aligns best with the requirements. Since the community heavily favored option C, it validates the solution's effectiveness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はC：AWS Global Acceleratorを作成し、適切なAWSリージョンのエンドポイントをターゲットとする複数のエンドポイントグループを設定する。このソリューションは、地域間でのトラフィックの容易なルーティングを可能にし、単一のエントリーポイントを提供するため、手間を最小限に抑えることができる。",
        "situation_analysis": "アプリケーションは、低遅延のグローバルリーチと一貫したユーザーエクスペリエンスを必要としている。AWS Global Acceleratorは、トラフィックを効果的に管理するための簡単なソリューションを提供する。",
        "option_analysis": "選択肢Aは、DNS管理とルーティングポリシーで追加の複雑さを導入している。選択肢Bは、価値を付加しない余計なインフラストラクチャ（NLB）を含んでいる。選択肢Dは、APIゲートウェイとLambdaの追加により過度に複雑化されているが、シンプルなルーティングには不要である。",
        "additional_knowledge": "グローバルアプリケーションアーキテクチャは、Global Acceleratorのようなサービスの恩恵を受けることが多く、グローバルトラフィックルーティングの管理を簡素化しながらパフォーマンスを向上させる。",
        "key_terminology": "AWS Global Accelerator、Elastic Load Balancer、マルチリージョン展開、トラフィック管理、DNSルーティング",
        "overall_assessment": "質問は明確であり、選択肢Cが要件に最もよく合致している。コミュニティも選択肢Cを強く支持しているため、ソリューションの有効性が裏付けられている。"
      }
    ],
    "keywords": [
      "AWS Global Accelerator",
      "Elastic Load Balancer",
      "multi-region deployment",
      "traffic management",
      "DNS routing"
    ]
  },
  {
    "No": "156",
    "question": "A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions\nwith API Gateway to use several shared libraries and custom classes.\nA solutions architect needs to simplify the deployment of the solution and optimize for code reuse.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、Amazon API GatewayとAWS Lambdaを使用して新しいサーバーレスAPIを開発しています。企業は、API Gatewayと統合したLambda関数を使用して、いくつかの共有ライブラリとカスタムクラスを使用しています。\nソリューションアーキテクトは、ソリューションのデプロイを簡素化し、コードの再利用を最適化する必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the",
        "text_jp": "共有ライブラリとカスタムクラスをDockerイメージにデプロイします。そのイメージをS3バケットに保存します。Lambdaレイヤーを作成して使用します。"
      },
      {
        "key": "B",
        "text": "Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR).",
        "text_jp": "共有ライブラリとカスタムクラスをDockerイメージにデプロイします。そのイメージをAmazon Elastic Container Registry（Amazon ECR）にアップロードします。"
      },
      {
        "key": "C",
        "text": "Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS",
        "text_jp": "共有ライブラリとカスタムクラスをAWSを使用してAmazon Elastic Container Service（Amazon ECS）のDockerコンテナにデプロイします。"
      },
      {
        "key": "D",
        "text": "Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon",
        "text_jp": "共有ライブラリ、カスタムクラス、APIのLambda関数のコードをDockerイメージにデプロイします。そのイメージをAmazonにアップロードします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (65%) B (35%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Deploying the shared libraries and custom classes to a Docker image and uploading it to Amazon Elastic Container Registry (Amazon ECR) simplifies the management of dependencies and promotes code reuse.",
        "situation_analysis": "The company is looking for a solution that allows for easier deployment and reusability of code components in their serverless architecture.",
        "option_analysis": "Option A suggests using S3 for Docker images, which is not a standard practice for container deployment. Option C involves Amazon ECS, which might introduce unnecessary complexity for a serverless API. Option D lacks clarity about the upload target and does not emphasize using a dedicated container registry, making Option B the best choice.",
        "additional_knowledge": "Amazon ECR integrates seamlessly with AWS Lambda, allowing for streamlined deployments.",
        "key_terminology": "AWS Lambda, Amazon API Gateway, Docker, Amazon ECR, container management.",
        "overall_assessment": "The question accurately tests knowledge of container deployment strategies in serverless environments. While community votes favored D, B is technically the most correct solution in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBである。共有ライブラリとカスタムクラスをDockerイメージにデプロイし、それをAmazon Elastic Container Registry（Amazon ECR）にアップロードすることで、依存関係の管理が簡素化され、コードの再利用が促進される。",
        "situation_analysis": "企業は、サーバーレスアーキテクチャにおけるコードコンポーネントのデプロイの容易さと再利用性の向上を求めている。",
        "option_analysis": "選択肢AはDockerイメージの保存にS3を提案しており、これはコンテナデプロイメントにおける標準的な手法ではない。選択肢CはAmazon ECSを利用し、サーバーレスAPIに不必要な複雑さを持ち込む可能性がある。選択肢Dはアップロード先が不明瞭で、専用のコンテナレジストリの使用を強調しておらず、Bが最も良い選択肢となる。",
        "additional_knowledge": "Amazon ECRはAWS Lambdaとの統合がシームレスで、デプロイメントを効率的に行うことができる。",
        "key_terminology": "AWS Lambda、Amazon API Gateway、Docker、Amazon ECR、コンテナ管理。",
        "overall_assessment": "この質問は、サーバーレス環境におけるコンテナデプロイ戦略の知識を正確にテストしている。コミュニティ投票ではDが支持されたが、このシナリオにおいてBが技術的に最も正しい解決策である。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon API Gateway",
      "Docker",
      "Amazon ECR",
      "container management"
    ]
  },
  {
    "No": "157",
    "question": "A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The\ncompany has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images.\nThe company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback\neven if the factory's internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the\nworkers.\nHow should the company deploy the ML model to meet these requirements?",
    "question_jp": "製造会社は工場の検査ソリューションを構築しています。会社は各組立ラインの端にIPカメラを設置しています。この会社はAmazon SageMakerを使用して、静止画像から一般的な欠陥を識別する機械学習（ML）モデルを訓練しました。会社は欠陥が検出されたときに工場作業員にローカルフィードバックを提供したいと考えています。会社は工場のインターネット接続がダウンしていてもこのフィードバックを提供できる必要があります。会社には作業員にローカルフィードバックを提供するAPIをホストするローカルLinuxサーバーがあります。この要件を満たすために、会社はMLモデルをどのように展開すべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams.",
        "text_jp": "各IPカメラからAWSへのAmazon Kinesisビデオストリームをセットアップします。Amazon EC2インスタンスを使用してストリームの静止画像を取得します。"
      },
      {
        "key": "B",
        "text": "Deploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still",
        "text_jp": "ローカルサーバーにAWS IoT Greengrassを展開します。GreengrassサーバーにMLモデルを展開します。静止"
      },
      {
        "key": "C",
        "text": "Order an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still",
        "text_jp": "AWS Snowballデバイスを注文します。Snowballデバイス上にSageMakerエンドポイントとMLモデルを展開し、Amazon EC2インスタンスを展開します。"
      },
      {
        "key": "D",
        "text": "Deploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the",
        "text_jp": "各IPカメラにAmazon Monitronデバイスを展開します。オンプレミスにAmazon Monitronゲートウェイを展開します。MLモデルを展開します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "正解はDです。Amazon Monitronデバイスはリアルタイムでの欠陥検出とローカルフィードバックの提供を可能にします。",
        "situation_analysis": "会社は工場のインターネット接続がダウンしている状況でも欠陥についてフィードバックを提供する必要があります。これには、ローカルでのデータ処理が可能なソリューションが求められます。",
        "option_analysis": "AはAWS上での処理を前提としており、インターネット接続が必要です。Bはローカルデプロイを提案していますが、欠陥検出デバイスとしてのMonitronの効果には劣ります。Cもインターネット接続が必要であり、Snowballデバイスではローカルフィードバックが間に合わない恐れがあります。",
        "additional_knowledge": "生産ラインの効率を向上させるためには、リアルタイムでのデータ処理が重要であり、各組立行の監視強化に役立ちます。",
        "key_terminology": "Amazon Monitron, AWS IoT Greengrass, AWS Snowball, Amazon Kinesis, EC2",
        "overall_assessment": "Dは要件を完全に満たす解決策であり、コミュニティ投票の結果からも支持されています。特に、ローカルでのデータ処理ができる点が強調されるべきです。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。Amazon Monitronデバイスは、リアルタイムでの欠陥検出とローカルフィードバックの提供を可能にする。",
        "situation_analysis": "会社は工場のインターネット接続がダウンしている場合でも欠陥についてフィードバックを提供する必要がある。これには、ローカルでデータ処理を行えるソリューションが求められる。",
        "option_analysis": "AはAWS上での処理を前提としており、インターネット接続が必要である。Bはローカルデプロイを提案しているが、欠陥検出デバイスとしてのMonitronの効果には劣る。Cもインターネット接続が必要であり、Snowballデバイスではローカルフィードバックが間に合わないおそれがある。",
        "additional_knowledge": "生産ラインの効率を向上させるためには、リアルタイムのデータ処理が重要であり、各組立行の監視強化に役立つ。",
        "key_terminology": "Amazon Monitron, AWS IoT Greengrass, AWS Snowball, Amazon Kinesis, EC2",
        "overall_assessment": "Dは要件を完全に満たす解決策であり、コミュニティ投票結果からも支持されている。特にローカルでのデータ処理ができる点が強調されるべきである。"
      }
    ],
    "keywords": [
      "Amazon Monitron",
      "AWS IoT Greengrass",
      "AWS Snowball",
      "Amazon Kinesis",
      "EC2"
    ]
  },
  {
    "No": "158",
    "question": "A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect\nwill use a configuration management database (CMDB) export of all the company's servers to create the case.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ソリューションアーキテクトは、企業のオンプレミスデータセンターをAWSクラウドに移行するためのビジネスケースを作成しなければなりません。ソリューションアーキテクトは、企業のすべてのサーバーの構成管理データベース（CMDB）のエクスポートを使用してケースを作成します。どのソリューションが要求を最もコスト効率的に満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations.",
        "text_jp": "AWS Well-Architected Toolを使用してCMDBデータをインポートし、分析を実施して推奨事項を生成する。"
      },
      {
        "key": "B",
        "text": "Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.",
        "text_jp": "Migration Evaluatorを使用して分析を実施します。データインポートテンプレートを使用してCMDBエクスポートからデータをアップロードします。"
      },
      {
        "key": "C",
        "text": "Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in",
        "text_jp": "リソースマッチングルールを実装します。CMDBエクスポートとAWS Price List Bulk APIを使用してCMDBデータをAWSサービスに対して照会します。"
      },
      {
        "key": "D",
        "text": "Use AWS Application Discovery Service to import the CMDB data to perform an analysis.",
        "text_jp": "AWS Application Discovery Serviceを使用してCMDBデータをインポートし、分析を実施します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (85%) D (15%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. AWS Application Discovery Service is designed specifically for gathering information about on-premises resources, especially during migration planning.",
        "situation_analysis": "The requirement is to create a business case for migrating on-premises data to the AWS Cloud using a CMDB export.",
        "option_analysis": "Option D is optimal as it offers a dedicated service for discovery and analysis of on-premises resources. Option A and B do not focus on CMDB import capabilities. Option C lacks the integration specifically aimed at simplifying migration assessments.",
        "additional_knowledge": "Understanding the distinction between general analysis tools and those tailored for migration is crucial.",
        "key_terminology": "Application Discovery Service, CMDB, Migration Planning, Resource Discovery, Cost-Effectiveness",
        "overall_assessment": "While there is a significant community vote for option B, the specific use-case for migration planning justifies D as the most cost-effective solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。AWS Application Discovery Serviceは、特に移行計画時にオンプレミスリソースに関する情報を収集するために設計されています。",
        "situation_analysis": "企業のオンプレミスデータをAWSクラウドに移行するためのビジネスケースを作成する必要があります。CMDBのエクスポートを使用します。",
        "option_analysis": "オプションDは、オンプレミスリソースの発見と分析のための専用サービスを提供するため、最適です。オプションAおよびBはCMDBインポート機能に焦点を当てていません。オプションCは、移行評価を簡素化するために特化した統合が不足しています。",
        "additional_knowledge": "一般的な分析ツールと移行のために特化したツールの違いを理解することが重要です。",
        "key_terminology": "Application Discovery Service、CMDB、移行計画、リソース発見、コスト効率",
        "overall_assessment": "コミュニティの投票ではオプションBに多くの支持がありますが、移行計画の具体的な用途に基づき、Dが最もコスト効率的な解決策と正当化されます。"
      }
    ],
    "keywords": [
      "AWS Application Discovery Service",
      "CMDB",
      "Migration Planning",
      "Resource Discovery",
      "Cost-Effectiveness"
    ]
  },
  {
    "No": "159",
    "question": "A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling\ngroup. The ALB is associated with an AWS WAF web ACL.\nThe website often encounters attacks in the application layer. The attacks produce sudden and significant increases in trafic on the application\nserver. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to\nmitigate these attacks.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業には、アプリケーションロードバランサー (ALB) の背後で動作するウェブサイトがあります。このインスタンスはオートスケーリンググループに属しています。ALB は AWS WAF ウェブ ACL に関連付けられています。ウェブサイトはアプリケーション層での攻撃にしばしば直面します。攻撃はアプリケーションサーバーへのトラフィックの突然かつ大幅な増加を引き起こします。アクセスログには、各攻撃が異なる IP アドレスから発生していることが示されています。ソリューションアーキテクトは、これらの攻撃を緩和するためのソリューションを実装する必要があります。どのソリューションが最も運用上のオーバーヘッドを軽減しながらこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm",
        "text_jp": "サーバーアクセスを監視する Amazon CloudWatch アラームを作成します。IP アドレスによるアクセスに基づいて閾値を設定します。アラームを設定します"
      },
      {
        "key": "B",
        "text": "Deploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource.",
        "text_jp": "AWS WAF に加えて AWS Shield Advanced を展開します。ALB を保護されたリソースとして追加します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm",
        "text_jp": "ユーザーの IP アドレスを監視する Amazon CloudWatch アラームを作成します。IP アドレスによるアクセスに基づいて閾値を設定します。アラームを設定します"
      },
      {
        "key": "D",
        "text": "Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny",
        "text_jp": "アクセスログを検査して、攻撃を行った IP アドレスのパターンを見つけます。Amazon Route 53 の地理的位置ルーティングポリシーを使用して拒否します"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating a CloudWatch alarm that monitors user IP addresses provides a proactive approach to identify potential attacks based on sudden traffic spikes from unique IPs.",
        "situation_analysis": "The website is experiencing application layer attacks that generate unexpected traffic from diverse IP addresses, which requires a solution with minimal overhead.",
        "option_analysis": "Option A focuses on server access but not specifically on IP address monitoring; Option B involves additional costs with AWS Shield Advanced; Option D is reactive and requires manual log analysis.",
        "additional_knowledge": "Evolving DDoS mitigation strategies, incorporating automatic scaling, and monitoring can enhance application resilience.",
        "key_terminology": "Amazon CloudWatch, AWS WAF, AWS Shield, Application Load Balancer, Auto Scaling",
        "overall_assessment": "The community voting distribution indicates a preference for option B, suggesting a misunderstanding of operational overhead vs. protection mechanisms. Option C remains optimal for the required scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは C です。ユーザーの IP アドレスを監視する CloudWatch アラームを作成することで、特異な IP からのトラフィックの急増を基に潜在的攻撃を特定するプロアクティブなアプローチが可能となります。",
        "situation_analysis": "ウェブサイトでは、さまざまな IP アドレスからの突然のトラフィックを生成するアプリケーション層の攻撃が発生しており、運用負担を最小限に抑える解決策が必要です。",
        "option_analysis": "オプション A はサーバーアクセスに焦点を当てていますが、IP アドレス監視には特に触れていません。オプション B は AWS Shield Advanced の追加コストがかかります。オプション D は反応的で、手動のログ分析が必要です。",
        "additional_knowledge": "DDoS 緩和戦略の進化、自動スケーリングの取り入れ、監視を組み合わせることで、アプリケーションの耐障害性を高めることができます。",
        "key_terminology": "Amazon CloudWatch, AWS WAF, AWS Shield, アプリケーションロードバランサー, オートスケーリング",
        "overall_assessment": "コミュニティの投票分布は、選択肢 B に対する好みを示していますが、運用オーバーヘッドと保護メカニズムの誤解があると思われます。選択肢 C は必要なシナリオには最適です。"
      }
    ],
    "keywords": [
      "Amazon CloudWatch",
      "AWS WAF",
      "AWS Shield",
      "Application Load Balancer",
      "Auto Scaling"
    ]
  },
  {
    "No": "160",
    "question": "A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and\nan Amazon Aurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already\ndeployed in two Regions.\nCompany policy states that critical applications must have application tier components and data tier components deployed across two Regions.\nThe RTO and RPO must be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant\nwith company policy.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "question_jp": "ある企業が、データ層が単一のAWSリージョンに展開されている重要なアプリケーションを持っています。データ層は、Amazon DynamoDBテーブルとAmazon Aurora MySQL DBクラスタを使用しています。現在のAurora MySQLエンジンバージョンはグローバルデータベースをサポートしています。アプリケーション層は、すでに2つのリージョンに展開されています。企業の方針では、重要なアプリケーションはアプリケーション層のコンポーネントとデータ層のコンポーネントを2つのリージョンに展開する必要があります。RTOとRPOは、それぞれ数分を超えてはなりません。ソリューションアーキテクトは、データ層を企業の方針に準拠させるためのソリューションを推奨する必要があります。要件を満たすために、どの組み合わせのステップを選べばよいですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Add another Region to the Aurora MySQL DB cluster",
        "text_jp": "Aurora MySQL DBクラスタに別のリージョンを追加する"
      },
      {
        "key": "B",
        "text": "Add another Region to each table in the Aurora MySQL DB cluster",
        "text_jp": "Aurora MySQL DBクラスタ内の各テーブルに別のリージョンを追加する"
      },
      {
        "key": "C",
        "text": "Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster",
        "text_jp": "DynamoDBテーブルとAurora MySQL DBクラスタのためにスケジュールされたクロスリージョンバックアップを設定する"
      },
      {
        "key": "D",
        "text": "Convert the existing DynamoDB table to a global table by adding another Region to its configuration",
        "text_jp": "既存のDynamoDBテーブルを、その構成に別のリージョンを追加することでグローバルテーブルに変換する"
      },
      {
        "key": "E",
        "text": "Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region",
        "text_jp": "Amazon Route 53 Application Recovery Controllerを使用して、データベースのバックアップと回復を二次リージョンに自動化する"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "AD (83%) AC (17%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are 'B' and 'D'. Adding another Region to each table in the Aurora MySQL DB cluster is essential for compliance with the company policy, as it will allow the database to work across multiple regions. Additionally, converting the existing DynamoDB table to a global table, which involves adding another region to its configuration, ensures that the data can be accessed and replicated across regions effectively.",
        "situation_analysis": "The application requires a highly available configuration across two regions. The current setup with a single region violates company policy, and having both data and application tier components across two regions is mandatory for business continuity.",
        "option_analysis": "'A' is incorrect because adding another region to the DB cluster alone does not ensure compliance for all tables. 'C' improves data durability but does not address the requirement for immediate availability and compliance. 'E' does not directly contribute to achieving the two regions requirement for both components.",
        "additional_knowledge": "When working with multi-region architectures, consider using AWS services like Amazon Route 53 for DNS management and traffic routing.",
        "key_terminology": "Amazon DynamoDB, Amazon Aurora, global database, RTO, RPO",
        "overall_assessment": "This question effectively tests knowledge of AWS architectures and best practices in designing resilient systems. While community votes heavily favor 'A' and 'D', the specific requirements of having multiple regions for each table must be prioritized."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は「B」と「D」である。Aurora MySQL DBクラスタ内の各テーブルに別のリージョンを追加することは、企業の方針に準拠するために必須であり、これによりデータベースが複数のリージョンで機能することが可能になる。また、既存のDynamoDBテーブルをグローバルテーブルに変換すること、すなわちその構成に別のリージョンを追加することで、データが効率的にリージョン間でアクセスされ、複製されることが保証される。",
        "situation_analysis": "アプリケーションは、2つのリージョン間での高可用性構成が必要である。現在の単一リージョンでの設定は企業の方針に違反しており、データ層とアプリケーション層のコンポーネントの両方を2つのリージョンに展開することが営業継続性のために義務付けられている。",
        "option_analysis": "'A'は誤りであり、DBクラスタに別のリージョンを追加するだけではすべてのテーブルに対するコンプライアンスが保証されない。'C'はデータの耐久性を向上させるが、即時の可用性とコンプライアンス要件に対応していない。'E'は、両方のコンポーネントのために2つのリージョン要件を達成するのに直接貢献しない。",
        "additional_knowledge": "マルチリージョンアーキテクチャを扱う際には、Amazon Route 53などのAWSサービスを使用してDNS管理やトラフィックルーティングを考慮することが重要である。",
        "key_terminology": "Amazon DynamoDB, Amazon Aurora, グローバルデータベース, RTO, RPO",
        "overall_assessment": "この質問は、AWSアーキテクチャや耐障害性システムの設計に関するベストプラクティスの知識を効果的にテストしている。コミュニティの投票は「A」と「D」に大きく偏っているが、各テーブルに対して複数のリージョンを持つという具体的な要件が優先されるべきである。"
      }
    ],
    "keywords": [
      "Amazon DynamoDB",
      "Amazon Aurora",
      "global database"
    ]
  },
  {
    "No": "161",
    "question": "A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the\ncompany's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones\nbehind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS\nterminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path.\nThe company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must\ndevelop a solution to allow trafic fiow to AWS from the on-premises network so that the clients can continue to access the application.\nWhich solution will meet these requirements?",
    "question_jp": "ある通信会社がAWS上でアプリケーションを運用している。この会社は、オンプレミスのデータセンターとAWSの間にAWS Direct Connect接続を設定している。会社は、複数のアベイラビリティゾーンに分散させたAmazon EC2インスタンス上にアプリケーションをデプロイし、内部のアプリケーションロードバランサー（ALB）の背後で実行している。会社のクライアントはHTTPSを使用してオンプレミスネットワークから接続しており、TLSはALBで終端している。会社は複数のターゲットグループを持ち、URLパスに基づいてリクエストを転送するためにパスベースのルーティングを使用している。会社は、IPアドレスに基づく許可リストを持つオンプレミスのファイアウォールアプライアンスをデプロイする予定である。ソリューションアーキテクトは、クライアントがアプリケーションに引き続きアクセスできるように、オンプレミスネットワークからAWSへのトラフィックフローを許可するソリューションを開発する必要がある。どのソリューションがこれらの要件を満たすか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP",
        "text_jp": "既存のALBを静的IPアドレスを使用するように構成する。ALBに複数のアベイラビリティゾーンのIPアドレスを割り当てる。ALB IPを追加する"
      },
      {
        "key": "B",
        "text": "Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type",
        "text_jp": "Network Load Balancer（NLB）を作成する。NLBを複数のアベイラビリティゾーンにある静的IPアドレスに関連付ける。ALBタイプのを作成する"
      },
      {
        "key": "C",
        "text": "Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing",
        "text_jp": "Network Load Balancer（NLB）を作成する。LNBを複数のアベイラビリティゾーンにある静的IPアドレスに関連付ける。既存のを追加する"
      },
      {
        "key": "D",
        "text": "Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target",
        "text_jp": "Gateway Load Balancer（GWLB）を作成する。GWLBに複数のアベイラビリティゾーンにおける静的IPアドレスを割り当てる。ALBタイプのターゲットを作成する"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (93%) 3%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Configuring the existing ALB to use static IP addresses allows traffic from the on-premises network to be routed correctly.",
        "situation_analysis": "The telecommunications company needs to allow access to its application hosted on AWS through its on-premises firewall, which is based on an allow list of IP addresses.",
        "option_analysis": "Option A allows for the ALB to retain a static IP, making it easier for the firewall to allow traffic. Option B, C, and D do not fulfill the requirement to use the existing ALB.",
        "additional_knowledge": "ALB with static IPs helps maintain consistent access for clients while ensuring compliance with firewall policies.",
        "key_terminology": "Application Load Balancer, static IP, Direct Connect, firewall, allow list",
        "overall_assessment": "Although community vote distribution is significant for option B, it does not satisfy the requirement for the existing ALB. Therefore, A is the best choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。既存のALBを静的IPアドレスを使用するように構成することにより、オンプレミスネットワークからのトラフィックが正しくルーティングされる。",
        "situation_analysis": "通信会社は、オンプレミスのファイアウォール、IPアドレスの許可リストに基づいて、AWS上にホストされているアプリケーションへのアクセスを許可する必要がある。",
        "option_analysis": "選択肢Aは、ALBが静的IPを保持でき、ファイアウォールによるトラフィック許可が容易になる。選択肢B、C、およびDは、既存のALBを使用するという要件を満たしていない。",
        "additional_knowledge": "静的IPを持つALBは、クライアントのアクセスの一貫性を維持しつつファイアウォールポリシーの遵守を確保するのに役立つ。",
        "key_terminology": "アプリケーションロードバランサー, 静的IP, Direct Connect, ファイアウォール, 許可リスト",
        "overall_assessment": "コミュニティの投票配分は選択肢Bに対して大きいが、既存のALBの要件を満たさない。したがって、Aが最良の選択肢である。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "static IP",
      "Direct Connect",
      "firewall",
      "allow list"
    ]
  },
  {
    "No": "162",
    "question": "A company runs an application on a fieet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer\n(ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is\nassociated with the CloudFront distribution.\nThe company needs a solution that will prevent internet trafic from directly accessing the ALB.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業は、インターネット向けのアプリケーションロードバランサー（ALB）の背後にあるプライベートサブネット上の一連のAmazon EC2インスタンスでアプリケーションを実行しています。ALBは、Amazon CloudFrontディストリビューションのオリジンです。さまざまなAWS管理ルールを含むAWS WAFウェブACLがCloudFrontディストリビューションに関連付けられています。企業は、インターネットトラフィックがALBに直接アクセスするのを防ぐソリューションが必要です。どのソリューションが最小の運用負荷でこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB.",
        "text_jp": "新しいウェブACLを作成し、既存のウェブACLが含まれているのと同じルールを含めます。新しいウェブACLをALBに関連付けます。"
      },
      {
        "key": "B",
        "text": "Associate the existing web ACL with the ALB.",
        "text_jp": "既存のウェブACLをALBに関連付けます。"
      },
      {
        "key": "C",
        "text": "Add a security group rule to the ALB to allow trafic from the AWS managed prefix list for CloudFront only.",
        "text_jp": "CloudFront専用のAWS管理プレフィックスリストからのトラフィックを許可するようにALBにセキュリティグループルールを追加します。"
      },
      {
        "key": "D",
        "text": "Add a security group rule to the ALB to allow only the various CloudFront IP address ranges.",
        "text_jp": "さまざまなCloudFront IPアドレス範囲のみを許可するようにALBにセキュリティグループルールを追加します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Add a security group rule to the ALB to allow only the various CloudFront IP address ranges.",
        "situation_analysis": "The requirement is to prevent direct internet traffic from accessing the ALB while ensuring that traffic coming from CloudFront is allowed.",
        "option_analysis": "Option D effectively limits access to the ALB by only allowing IPs associated with CloudFront, thus fulfilling the requirement without adding complexity. Option A requires additional management of a new web ACL, Option B lacks implementation since the existing ACL does not serve the ALB directly, and Option C does not cover all necessary ranges as efficiently as D.",
        "additional_knowledge": "Managing security at the ALB level using security groups rather than ACLs simplifies configuration and adheres to AWS recommendations.",
        "key_terminology": "Application Load Balancer, Amazon CloudFront, AWS WAF, Security Group, IP Address Ranges.",
        "overall_assessment": "Option D is the optimal solution with the least operational overhead, while community votes suggest a preference for option C, likely due to misunderstanding of direct IP management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです：さまざまなCloudFront IPアドレス範囲のみを許可するようにALBにセキュリティグループルールを追加します。",
        "situation_analysis": "インターネットからのトラフィックがALBに直接アクセスするのを防ぎつつ、CloudFrontからのトラフィックは許可する必要があります。",
        "option_analysis": "選択肢Dは、CloudFrontに関連するIPのみをALBにアクセス許可することで、要件を満たしつつ複雑さを追加しません。選択肢Aは新しいウェブACLの管理を追加で必要とし、選択肢Bは既存のACLがALBに直接役立たないため実装が不足しています。選択肢Cは、Dほど効率的にすべての必要な範囲をカバーしていません。",
        "additional_knowledge": "ALBレベルでのセキュリティ管理には、ACLよりもセキュリティグループを使用することで、設定が簡素化され、AWSの推奨に従った形になります。",
        "key_terminology": "アプリケーションロードバランサー、Amazon CloudFront、AWS WAF、セキュリティグループ、IPアドレス範囲。",
        "overall_assessment": "選択肢Dは最小限の運用負荷で最適なソリューションであり、コミュニティ投票は選択肢Cを支持していますが、実際のIP管理の誤解によるものと考えられます。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "Amazon CloudFront",
      "AWS WAF",
      "Security Group",
      "IP Address Ranges"
    ]
  },
  {
    "No": "163",
    "question": "A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that\nthe company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit.\nAdditionally, users can access the cache without authentication.\nA solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が Amazon ElastiCache for Redis クラスターをキャッシングレイヤーとして使用しているアプリケーションを運用しています。最近のセキュリティ監査により、同社は ElastiCache に対して暗号化された保存設定を行っていることが明らかになりました。しかし、同社は ElastiCache を使用してトランジットでの暗号化を設定していませんでした。さらに、ユーザーは認証なしでキャッシュにアクセスできます。ソリューションアーキテクトは、ユーザー認証を必要とする変更を行い、企業がエンドツーエンドの暗号化を使用していることを確認する必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with",
        "text_jp": "AUTH トークンを作成します。トークンを AWS Systems Manager Parameter Store に暗号化されたパラメータとして保存します。新しいクラスターを作成し"
      },
      {
        "key": "B",
        "text": "Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure",
        "text_jp": "AUTH トークンを作成します。トークンを AWS Secrets Manager に保存します。既存のクラスターを AUTH トークンを使用するように構成し、"
      },
      {
        "key": "C",
        "text": "Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the",
        "text_jp": "SSL 証明書を作成します。証明書を AWS Secrets Manager に保存します。新しいクラスターを作成し、トランジットでの暗号化を構成します。"
      },
      {
        "key": "D",
        "text": "Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the",
        "text_jp": "SSL 証明書を作成します。証明書を AWS Systems Manager Parameter Store に暗号化された高度なパラメータとして保存します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It involves creating an SSL certificate to ensure data is encrypted in transit during communication with the Redis cluster.",
        "situation_analysis": "The company needs to secure the ElastiCache Redis cluster by requiring user authentication and ensuring encryption of data in transit.",
        "option_analysis": "Option C stands out because it explicitly mentions creating an SSL certificate, which is necessary for enabling encryption in transit. Options A and B focus on AUTH tokens without addressing encryption, while D does not specify a method for ensuring encryption in transit.",
        "additional_knowledge": "Implementing SSL certificates enhances security by encrypting data during transmission, reducing vulnerabilities.",
        "key_terminology": "ElastiCache, SSL, encryption in transit, AUTH token, Secrets Manager",
        "overall_assessment": "While community voting strongly supports option B, option C remains the technically correct choice for meeting the encryption in transit requirement. It's important to validate the options with expert knowledge."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは C です。Redis クラスターとの通信中にデータがトランジットで暗号化されることを保証するために SSL 証明書を作成することが含まれています。",
        "situation_analysis": "この企業は、ユーザー認証を要求し、データがトランジット中に暗号化されることを確保することにより、ElastiCache Redis クラスターを保護する必要があります。",
        "option_analysis": "選択肢 C は、トランジットでの暗号化を有効にするために必要な SSL 証明書を作成することに言及しているため、際立っています。選択肢 A と B は認証トークンに焦点を当てており、暗号化に対応していません。選択肢 D は、トランジットでの暗号化を保証する方法を明示していません。",
        "additional_knowledge": "SSL証明書を実装することで、データ転送中に暗号化され、安全性が向上し、脆弱性が減少します。",
        "key_terminology": "ElastiCache, SSL, トランジットでの暗号化, AUTH トークン, Secrets Manager",
        "overall_assessment": "コミュニティ投票が選択肢 B を強く支持している一方で、選択肢 C はトランジットでの暗号化要件を満たすための技術的に正しい選択肢として残ります。オプションを専門知識と照らし合わせて確認することが重要です。"
      }
    ],
    "keywords": [
      "ElastiCache",
      "SSL",
      "encryption in transit",
      "AUTH token",
      "Secrets Manager"
    ]
  },
  {
    "No": "164",
    "question": "A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two\nplacement groups and a single instance type.\nRecently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The\ncompany needs to improve the overall reliability of the workload.\nWhich solution will meet this requirement?",
    "question_jp": "ある企業が、Amazon EC2 Spotインスタンスを使用してコンピュートワークロードを実行しており、それはオートスケーリンググループに属している。起動テンプレートは2つの配置グループと単一のインスタンスタイプを使用している。最近、監視システムがオートスケーリングインスタンスの起動失敗を報告し、これがシステムユーザーの待機時間の長期化と相関していることが確認された。企業はワークロードの全体的な信頼性を向上させる必要がある。どの解決策がこの要件を満たすことができるか？",
    "choices": [
      {
        "key": "A",
        "text": "Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.",
        "text_jp": "起動テンプレートを起動構成に置き換えて、属性ベースのインスタンスタイプ選択を使用するオートスケーリンググループを利用する。"
      },
      {
        "key": "B",
        "text": "Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new",
        "text_jp": "属性ベースのインスタンスタイプ選択を使用する新しい起動テンプレートのバージョンを作成する。オートスケーリンググループを新しいものに設定する。"
      },
      {
        "key": "C",
        "text": "Update the launch template Auto Scaling group to increase the number of placement groups.",
        "text_jp": "オートスケーリンググループの起動テンプレートを更新して、配置グループの数を増やす。"
      },
      {
        "key": "D",
        "text": "Update the launch template to use a larger instance type.",
        "text_jp": "より大きなインスタンスタイプを使用するように起動テンプレートを更新する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. By increasing the number of placement groups, the workload can take better advantage of the distributed resources, improving reliability during Auto Scaling operations.",
        "situation_analysis": "The requirement is to enhance the reliability of EC2 Spot Instances within an Auto Scaling group amid reported instance launch failures.",
        "option_analysis": "Option C addresses the need by allowing instances to spread across multiple placement groups, which can reduce the chance of resource contention and improve the overall launch success rate. The other options either do not increase reliability or change configurations that might not directly address the reported issues.",
        "additional_knowledge": "If users are experiencing longer wait times, they may also benefit from reviewing their scaling policies and instance types being used.",
        "key_terminology": "Amazon EC2, Auto Scaling, Spot Instances, placement groups, instance launch failures",
        "overall_assessment": "Option C is the most suitable solution for improving the reliability of the workload as it directly addresses the issue by optimizing instance placement. While community vote favors option B, it does not enhance placement group utilization."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。配置グループの数を増やすことで、ワークロードは分散資源をより良く利用でき、オートスケーリング操作中の信頼性が向上する。",
        "situation_analysis": "要件は、報告されたインスタンス起動失敗の中で、オートスケーリンググループ内のEC2 Spotインスタンスの信頼性を高めることである。",
        "option_analysis": "選択肢Cは、インスタンスを複数の配置グループに分散させることにより、リソース競合の可能性を減らし、全体的な起動成功率を向上させる必要に応えています。他の選択肢は、信頼性を向上させるものではなかったり、報告された問題に直接対処しない構成の変更を行ったりします。",
        "additional_knowledge": "ユーザーが待機時間の延長を経験している場合、スケーリングポリシーや使用しているインスタンスタイプの見直しも役立つかもしれない。",
        "key_terminology": "Amazon EC2、オートスケーリング、Spotインスタンス、配置グループ、インスタンス起動失敗",
        "overall_assessment": "選択肢Cは、インスタンス配置の最適化によって直接問題に対処するため、ワークロードの信頼性を向上させる最も適切な解決策である。コミュニティの投票は選択肢Bを支持しているが、配置グループの利用向上には寄与しない。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Auto Scaling",
      "Spot Instances",
      "placement groups",
      "instance launch failures"
    ]
  },
  {
    "No": "165",
    "question": "A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3\nAPI to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the\ndocument processing is finished, customers can download the documents directly from Amazon S3.\nDuring the migration, the company discovered that it could not immediately update the processing server that generates many documents to\nsupport the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server\nfinishes processing, the files must be available to the public for download within 30 minutes.\nWhich solution will meet these requirements with the LEAST amount of effort?",
    "question_jp": "ある企業がドキュメント処理ワークロードをAWSに移行しています。企業は、多くのアプリケーションを更新して、処理サーバーが生成するドキュメントを保存、取得、および変更するためにAmazon S3 APIをネイティブに使用しています。処理サーバーは、約5秒ごとに5つのドキュメントを生成します。ドキュメント処理が完了した後、顧客はドキュメントをAmazon S3から直接ダウンロードできます。移行中、企業は、多くのドキュメントを生成する処理サーバーをS3 APIに対応させることが直ちにできないことを発見しました。このサーバーはLinux上で動作しており、生成・変更するファイルに対して高速なローカルアクセスを必要とします。サーバーが処理を終了した際には、ファイルは30分以内に公開され、ダウンロード可能でなければなりません。この要件を最小限の労力で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company",
        "text_jp": "アプリケーションをAWS Lambda関数に移行します。AWS SDK for Javaを使用して、企業が生成、変更、およびアクセスするファイルを処理します。"
      },
      {
        "key": "B",
        "text": "Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2",
        "text_jp": "Amazon S3 File Gatewayを設定し、ドキュメントストアにリンクされたファイル共有を構成します。ファイル共有をAmazon EC2にマウントします。"
      },
      {
        "key": "C",
        "text": "Configure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and",
        "text_jp": "Amazon FSx for Lustreをインポートおよびエクスポートポリシーで構成します。新しいファイルシステムをS3バケットにリンクさせます。Lustreクライアントをインストールします。"
      },
      {
        "key": "D",
        "text": "Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon",
        "text_jp": "AWS DataSyncを設定してAmazon EC2インスタンスに接続します。生成されたファイルをAmazon S3と同期するタスクを構成します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (57%) C (38%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Configuring Amazon FSx for Lustre with an import and export policy allows for fast access to files and integration with Amazon S3.",
        "situation_analysis": "The company requires fast local access to generated files and needs files to be publicly downloadable within 30 minutes after processing.",
        "option_analysis": "Option C meets the requirements by providing a high-performance file system that can connect to S3. Other options may introduce latency or complexity that does not directly meet the needs.",
        "additional_knowledge": "Using FSx for Lustre ensures optimal performance as it supports high I/O throughput.",
        "key_terminology": "Amazon FSx for Lustre, S3, high-throughput workloads, file sharing, latency",
        "overall_assessment": "Despite community preference for option B, option C is the best choice considering the performance requirement and integration capabilities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。Amazon FSx for Lustreをインポートおよびエクスポートポリシーで構成することで、ファイルへの高速アクセスとAmazon S3との統合が可能になります。",
        "situation_analysis": "企業は生成されたファイルに対して高速なローカルアクセスを必要としており、処理後30分以内にファイルを一般利用可能にする必要があります。",
        "option_analysis": "選択肢Cは、S3に接続可能な高性能ファイルシステムを提供するため、要件を満たします。他の選択肢は、遅延や複雑さを導入する可能性があり、ニーズを直接満たすことはできません。",
        "additional_knowledge": "FSx for Lustreを利用することで、高I/Oスループットに最適なパフォーマンスが保証されます。",
        "key_terminology": "Amazon FSx for Lustre, S3, 高スループットワークロード, ファイル共有, 遅延",
        "overall_assessment": "コミュニティの選好が選択肢Bに傾いているものの、パフォーマンス要件と統合機能を考慮すると、選択肢Cが最適な選択肢です。"
      }
    ],
    "keywords": [
      "Amazon FSx for Lustre",
      "S3",
      "high-throughput workloads",
      "file sharing",
      "latency"
    ]
  },
  {
    "No": "166",
    "question": "A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase\ndetails. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of\nthe other microservices store a copy of parts of the sensitive data in different storage services.\nThe company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other\nmicroservice must also delete its copy of the data immediately.\nWhich solution will meet these requirements?",
    "question_jp": "配送会社は、AWSクラウドでサーバーレスソリューションを運用しています。このソリューションは、ユーザーデータ、配信情報、および過去の購入詳細を管理しています。このソリューションは、いくつかのマイクロサービスで構成されています。中央のユーザーサービスは、機密データをAmazon DynamoDBテーブルに格納しています。他のいくつかのマイクロサービスは、異なるストレージサービスに機密データのコピーを格納しています。会社には、リクエストに応じてユーザー情報を削除する機能が必要です。中央のユーザーサービスがユーザーを削除したら、他のすべてのマイクロサービスも直ちにデータのコピーを削除しなければなりません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about",
        "text_jp": "DynamoDBテーブルでDynamoDB Streamsを有効にします。DynamoDBストリームのAWS Lambdaトリガーを作成し、イベントを投稿します。"
      },
      {
        "key": "B",
        "text": "Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a",
        "text_jp": "DynamoDBテーブルでDynamoDBイベント通知を設定します。Amazon Simple Notification Service (Amazon SNS)トピックを作成します。"
      },
      {
        "key": "C",
        "text": "Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an",
        "text_jp": "会社がユーザーを削除する際に、中央ユーザーサービスを構成してカスタムAmazon EventBridgeイベントバスにイベントを投稿するようにします。"
      },
      {
        "key": "D",
        "text": "Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes",
        "text_jp": "会社がユーザーを削除する際に、中央ユーザーサービスを構成してAmazon Simple Queue Service (Amazon SQS)キューにメッセージを投稿します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (71%) A (24%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Configuring the central user service to post a message to an Amazon SQS queue allows other microservices to effectively listen for delete requests and take immediate action.",
        "situation_analysis": "The company needs an efficient way to propagate delete requests across multiple microservices. They must ensure that when user data is deleted by the central service, all copies of this data across other services are deleted simultaneously.",
        "option_analysis": "Option D is effective because SQS provides a robust messaging system that other services can poll to receive delete notifications. Option C, while plausible, might introduce delays due to event processing, and A and B are not as effective for immediate deletion needs.",
        "additional_knowledge": "In comparing community support for option C, the high vote indicates a belief in event-driven designs, but for the explicit requirement of immediate deletion, SQS remains the most effective choice.",
        "key_terminology": "Amazon SQS, Microservices Architecture, Asynchronous Processing",
        "overall_assessment": "Option D directly addresses the requirement for immediate action across multiple services upon user deletion. It is a well-structured solution for inter-service communication in a serverless architecture."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。中央ユーザーサービスがAmazon SQSキューにメッセージを投稿するように構成すると、他のマイクロサービスは削除要求を効果的に監視し、即座にアクションを起こすことができます。",
        "situation_analysis": "会社は、複数のマイクロサービス間で削除要求を伝播させる効率的な方法を必要としています。ユーザーデータが中央サービスによって削除されると、他のサービス間のデータのコピーも同時に削除されることを確実にしなければなりません。",
        "option_analysis": "選択肢Dは、SQSが他のサービスが削除通知を受け取るためにポーリングできる強力なメッセージングシステムを提供するため、効果的です。選択肢Cは考えられますが、イベント処理による遅延が発生する可能性があるため効果的ではなく、AおよびBは即時削除ニーズにはそれほど効果的ではありません。",
        "additional_knowledge": "コミュニティがCを支持している場合、イベント駆動の設計に対する信念を示していますが、明示的に即時削除を必要とする要件に対しては、SQSが最も効果的な選択肢であることがわかります。",
        "key_terminology": "Amazon SQS, マイクロサービスアーキテクチャ, 非同期処理",
        "overall_assessment": "選択肢Dは、ユーザー削除時に即座にアクションを起こすという要件に直接対応しています。サーバーレスアーキテクチャにおけるサービス間通信のための良好に構築されたソリューションです。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "Microservices Architecture",
      "Asynchronous Processing"
    ]
  },
  {
    "No": "167",
    "question": "A company is running a web application in a VPC. The web application runs on a group of Amazon EC2 instances behind an Application Load\nBalancer (ALB). The ALB is using AWS WAF.\nAn external customer needs to connect to the web application. The company must provide IP addresses to all external customers.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "会社はVPC内でウェブアプリケーションを実行しています。ウェブアプリケーションは、アプリケーションロードバランサー（ALB）の背後で動作している複数のAmazon EC2インスタンスのグループ上で動作しています。ALBはAWS WAFを使用しています。外部の顧客がウェブアプリケーションに接続する必要があります。会社はすべての外部顧客にIPアドレスを提供しなければなりません。運用の負荷が最も少ない方法はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Replace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB.",
        "text_jp": "ALBをネットワークロードバランサー（NLB）に置き換え、NLBにエラスティックIPアドレスを割り当てます。"
      },
      {
        "key": "B",
        "text": "Allocate an Elastic IP address. Assign the Elastic IP address to the ALProvide the Elastic IP address to the customer.",
        "text_jp": "エラスティックIPアドレスを割り当て、そのエラスティックIPアドレスをALBに割り当て、顧客にエラスティックIPアドレスを提供します。"
      },
      {
        "key": "C",
        "text": "Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP",
        "text_jp": "AWS Global Acceleratorの標準アクセラレーターを作成し、ALBをアクセラレーターのエンドポイントとして指定します。アクセラレーターのIPを提供します。"
      },
      {
        "key": "D",
        "text": "Configure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution's DNS name to determine the distribution's",
        "text_jp": "Amazon CloudFrontディストリビューションを構成し、ALBをオリジンとして設定します。ディストリビューションのDNS名をpingして、ディストリビューションのIPアドレスを確認します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (87%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Allocating an Elastic IP and associating it with the Application Load Balancer allows the organization to provide a static IP address to external customers, which is essential for communication.",
        "situation_analysis": "The company is required to enable external customers to connect to its web application through a well-defined IP address, which is crucial for consistency in access and security.",
        "option_analysis": "Option A introduces unnecessary complexity by replacing ALB with NLB, which increases operational overhead. Option C involves a Global Accelerator, adding cost and management overhead. Option D brings complexities of distribution setup without directly meeting IP address needs.",
        "additional_knowledge": "Understanding the functioning of Elastic IPs and the differences between load balancer types (ALB vs. NLB) is crucial.",
        "key_terminology": "Elastic IP Address, Application Load Balancer, AWS WAF, Amazon EC2, IP Addressing",
        "overall_assessment": "The community voting heavily favors option C at 87%, possibly due to misinterpretation of the question requirements or certain perceived benefits of Global Accelerator. However, given the need for the least operational overhead while fulfilling the requirement for providing IP addresses, option B remains the ideal choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。エラスティックIPを割り当て、それをアプリケーションロードバランサーに関連付けることで、外部顧客に静的IPアドレスを提供でき、この通信の一貫性を確保することができる。",
        "situation_analysis": "会社は外部顧客がウェブアプリケーションにアクセスできるように、明確に定義されたIPアドレスが必要であり、これはアクセスの一貫性とセキュリティにとって重要である。",
        "option_analysis": "オプションAはALBをNLBに置き換えることで不必要な複雑さを導入し、運用の負担を増加させる。オプションCはGlobal Acceleratorを導入するため、コストと管理の負担を増やす。オプションDはディストリビューションの設定の複雑さをもたらしながら、IPアドレスのニーズには直接応えない。",
        "additional_knowledge": "エラスティックIPの機能とロードバランサータイプ（ALB対NLB）の違いを理解することが重要である。",
        "key_terminology": "エラスティックIPアドレス、アプリケーションロードバランサー、AWS WAF、Amazon EC2、IPアドレス指定",
        "overall_assessment": "コミュニティ投票ではオプションCが87%と大きく支持されているが、これは質問の要件を誤解したためか、Global Acceleratorの特定の利点を考慮したものであると考えられる。しかし、運用の負荷が最も少ない方法でIPアドレスを提供するという要件を考慮すれば、オプションBが理想的な選択である。"
      }
    ],
    "keywords": [
      "Elastic IP Address",
      "Application Load Balancer",
      "AWS WAF",
      "Amazon EC2",
      "IP Addressing"
    ]
  },
  {
    "No": "168",
    "question": "A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce\nAmazon Elastic Block Store (Amazon EBS) encryption at rest current production accounts and future production accounts only. The company\nneeds a solution that includes built-in blueprints and guardrails.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ある企業は、開発のためにいくつかのAWSアカウントを持っており、運用アプリケーションをAWSに移行したいと考えています。企業は、現在の運用アカウントおよび今後の運用アカウントに対してのみ、Amazon Elastic Block Store (Amazon EBS) の暗号化を強制する必要があります。企業は、組み込みの青写真とガードレールを含むソリューションが必要です。これらの要件を満たすためのステップの組み合わせはどれですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts.",
        "text_jp": "AWS CloudFormation StackSetsを使用して運用アカウントにAWS Configルールをデプロイする。"
      },
      {
        "key": "B",
        "text": "Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development",
        "text_jp": "既存の開発者アカウントに新しいAWS Control Towerランディングゾーンを作成する。アカウントのOUを作成する。運用と開発を追加する。"
      },
      {
        "key": "C",
        "text": "Create a new AWS Control Tower landing zone in the company's management account. Add production and development accounts to",
        "text_jp": "企業の管理アカウントに新しいAWS Control Towerランディングゾーンを作成する。運用と開発アカウントを追加する。"
      },
      {
        "key": "D",
        "text": "Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.",
        "text_jp": "既存のアカウントをAWS Organizationsに招待する。遵守を確保するためにSCPを作成する。"
      },
      {
        "key": "E",
        "text": "Create a guardrail from the management account to detect EBS encryption.",
        "text_jp": "管理アカウントからEBS暗号化を検出するためのガードレールを作成する。"
      },
      {
        "key": "F",
        "text": "Create a guardrail for the production OU to detect EBS encryption.",
        "text_jp": "運用OUのEBS暗号化を検出するためのガードレールを作成する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CDF (71%) 13% Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is 'B'. Creating a new AWS Control Tower landing zone allows for structured account management and compliance enforcement across production and development accounts.",
        "situation_analysis": "The company has various AWS accounts and wants to enforce EBS encryption in its production environments. Control Tower helps manage accounts and provides built-in governance.",
        "option_analysis": "Option B is the best choice as it establishes both a landing zone and organizational units, directly leading to effective policy application. Other options, while beneficial individually, do not meet the combined need for built-in guardrails.",
        "additional_knowledge": "Control Tower provides a framework for deploying guardrails and account structures consistent with AWS best practices.",
        "key_terminology": "AWS Control Tower, EBS encryption, Service Control Policies, Account Management, Compliance, Governance.",
        "overall_assessment": "Option B aligns closely with the requirements of building a compliant and governed environment for both current and future production applications. The community vote indicates strong support for options C and D, but these do not provide the same level of governance as Control Tower."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは「B」です。新しいAWS Control Towerランディングゾーンを作成することで、運用および開発アカウント全体の構造化されたアカウント管理とコンプライアンスの強制が可能になります。",
        "situation_analysis": "企業はさまざまなAWSアカウントを持ち、運用環境でEBS暗号化を強制したいと考えています。Control Towerはアカウントを管理し、組み込みのガバナンスを提供します。",
        "option_analysis": "選択肢Bが最適です。これは、ランディングゾーンと組織単位を確立し、政策適用を効果的に導きます。他の選択肢は、個々には有益ですが、組み込みのガードレールの必要性を満たしていません。",
        "additional_knowledge": "Control Towerは、AWSのベストプラクティスに沿ったガードレールとアカウント構造の展開のためのフレームワークを提供します。",
        "key_terminology": "AWS Control Tower、EBS暗号化、サービスコントロールポリシー、アカウント管理、コンプライアンス、ガバナンス。",
        "overall_assessment": "選択肢Bは、現在および今後の運用アプリケーションのために、コンプライアントでガバナンスされた環境を構築する必要性に緊密に対応しています。コミュニティの投票は、選択肢CとDを支持する傾向が強いですが、これらはControl Towerほどのガバナンスを提供しません。"
      }
    ],
    "keywords": [
      "AWS Control Tower",
      "EBS encryption",
      "Service Control Policies",
      "Account Management",
      "Governance"
    ]
  },
  {
    "No": "169",
    "question": "A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an\nAmazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must\nrecommend a solution to improve the resiliency of the application.\nThe solution must meet the following objectives:\n• Application tier: RPO of 2 minutes. RTO of 30 minutes\n• Database tier: RPO of 5 minutes. RTO of 30 minutes\nThe company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after\na failover.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、アプリケーションロードバランサー (ALB) の背後にある2台のLinux Amazon EC2インスタンス上で重要なステートフルWebアプリケーションを実行しています。この企業は、Amazon RDS for MySQLデータベースを持っています。企業は、アプリケーションのDNSレコードをAmazon Route 53でホスティングしています。ソリューションアーキテクトは、アプリケーションの回復力を向上させるためのソリューションを推奨しなければなりません。ソリューションは次の目標を満たさなければなりません：アプリケーション層：RPOは2分、RTOは30分。データベース層：RPOは5分、RTOは30分。企業は既存のアプリケーションアーキテクチャに大きな変更を加えたくありません。企業は、フェイルオーバー後の最適なレイテンシを確保する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an",
        "text_jp": "EC2インスタンスをAWS Elastic Disaster Recoveryを使用するように構成する。RDS DBインスタンスのクロスリージョンリードレプリカを作成する。"
      },
      {
        "key": "B",
        "text": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS",
        "text_jp": "EC2インスタンスをAmazon Data Lifecycle Manager (Amazon DLM)を使用してEBSボリュームのスナップショットを取得するように構成する。RDSを設定する。"
      },
      {
        "key": "C",
        "text": "Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region.",
        "text_jp": "EC2インスタンスとRDS DBインスタンスのためにAWS Backupでバックアッププランを作成する。バックアップの複製を第二のAWSリージョンに設定する。"
      },
      {
        "key": "D",
        "text": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross-",
        "text_jp": "EC2インスタンスをAmazon Data Lifecycle Manager (Amazon DLM)を使用してEBSボリュームのスナップショットを取得するように構成する。クロスリージョンリードレプリカを作成する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. It suggests using Amazon Data Lifecycle Manager to manage EBS snapshots, which supports quick recovery.",
        "situation_analysis": "The application requires RPOs and RTOs that allow minimal data loss and quick recovery, emphasizing the need for managed backups.",
        "option_analysis": "Option A discusses cross-Region read replicas, which might complicate the existing architecture. Option C, while valid, doesn't directly address the rapid recovery needs for both tiers. Option D suggests similar DLM usage, which is relevant but incomplete.",
        "additional_knowledge": "The effective use of EBS snapshots can significantly improve application recoverability.",
        "key_terminology": "Amazon EC2, Amazon DLM, EBS Volumes, RPO, RTO",
        "overall_assessment": "Despite community voting showing high support for option A, option B provides a direct and practical solution to meet the company's requirements without significant architectural changes."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBです。これは、EBSスナップショットの管理にAmazon Data Lifecycle Managerを使用することを提案し、迅速な回復をサポートします。",
        "situation_analysis": "アプリケーションは、最小限のデータ損失と迅速な回復を許可するRPOおよびRTOを必要としており、管理されたバックアップの必要性を強調しています。",
        "option_analysis": "選択肢Aはクロスリージョンリードレプリカについて言及していますが、既存のアーキテクチャを複雑にする可能性があります。選択肢Cは有効ではありますが、両方の層の迅速な回復ニーズに直接対応していません。選択肢Dは類似のDLMの使用を示唆していますが、不完全です。",
        "additional_knowledge": "EBSスナップショットの効果的な使用は、アプリケーションの回復性を大幅に向上させることができます。",
        "key_terminology": "Amazon EC2, Amazon DLM, EBSボリューム, RPO, RTO",
        "overall_assessment": "コミュニティ投票では選択肢Aへの高い支持が示されているが、選択肢Bは企業の要件を満たすための直接的かつ実践的なソリューションを提供します。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Amazon DLM",
      "EBS Volumes",
      "RPO",
      "RTO"
    ]
  },
  {
    "No": "170",
    "question": "A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants\nto ensure that the instances are optimized based on CPU, memory, and network metrics.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "ソリューションアーキテクトは、コスト最適化を図り、単一のAWSアカウント内でAmazon EC2インスタンスの適切なサイズを決定したいと考えています。ソリューションアーキテクトは、CPU、メモリ、ネットワークのメトリクスに基づいてインスタンスが最適化されることを確認したいと考えています。これらの要件を満たすために、ソリューションアーキテクトはどの組み合わせの手順を踏むべきですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Purchase AWS Business Support or AWS Enterprise Support for the account.",
        "text_jp": "AWSビジネスサポートまたはAWSエンタープライズサポートをアカウントに購入する。"
      },
      {
        "key": "B",
        "text": "Turn on AWS Trusted Advisor and review any “Low Utilization Amazon EC2 Instances” recommendations.",
        "text_jp": "AWSトラステッドアドバイザーをオンにし、「低利用率のAmazon EC2インスタンス」推奨事項を確認する。"
      },
      {
        "key": "C",
        "text": "Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.",
        "text_jp": "Amazon CloudWatchエージェントをインストールし、EC2インスタンスのメモリメトリクス収集を設定する。"
      },
      {
        "key": "D",
        "text": "Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.",
        "text_jp": "AWSアカウントでAWSコンピュートオプティマイザーを設定し、所見と最適化推奨事項を受け取る。"
      },
      {
        "key": "E",
        "text": "Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest.",
        "text_jp": "関心のあるAWSリージョン、インスタンスファミリー、オペレーティングシステムのためにEC2インスタンスセービングスプランを作成する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CD (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct answers are B and D. Option B focuses on using AWS Trusted Advisor to find underutilized EC2 instances, while option D utilizes AWS Compute Optimizer to receive recommendations for instance optimization.",
        "situation_analysis": "The architect needs to ensure that the EC2 instances are sized appropriately based on their usage metrics, including CPU, memory, and network.",
        "option_analysis": "Option B provides direct information about underutilized instances. Option D complements this by giving specific optimization recommendations. Other options (A, C, and E) do not directly address the requirement to optimize based on the specified metrics.",
        "additional_knowledge": "Understanding how to leverage AWS tools to monitor and optimize resources can significantly lower AWS operational costs.",
        "key_terminology": "AWS Trusted Advisor, AWS Compute Optimizer, cost optimization, EC2 instance sizing.",
        "overall_assessment": "The question is well-structured in evaluating a solution architect's ability to optimize AWS resources effectively. The community vote indicating support for options C and D may reflect a misunderstanding of the specific requirement focused on direct metrics evaluation."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBとDである。BはAWSトラステッドアドバイザーを利用して低利用率のEC2インスタンスを見つけることに焦点を当て、DはAWSコンピュートオプティマイザーを使用してインスタンスの最適化に関する推奨事項を受け取る。",
        "situation_analysis": "アーキテクトは、EC2インスタンスがCPU、メモリ、およびネットワークの使用メトリクスに基づいて適切にサイズ設定されるようにする必要がある。",
        "option_analysis": "Bは低利用率のインスタンスに関する直接的な情報を提供する。Dはこれを補完し、特定の最適化推奨を提供する。A、C、Eの他の選択肢は、指定されたメトリクスに基づいて最適化する要件に直接対処していない。",
        "additional_knowledge": "AWSリソースを監視し最適化するためのツールの活用を理解することは、AWSの運用コストを大幅に削減することができる。",
        "key_terminology": "AWSトラステッドアドバイザー、AWSコンピュートオプティマイザー、コスト最適化、EC2インスタンスサイズ設定。",
        "overall_assessment": "この質問は、AWSリソースを効果的に最適化する能力を評価するために良く構成されている。コミュニティ投票の結果、CとDが支持されたことは、特定のメトリクス評価に焦点を当てた要件の誤解を反映している可能性がある。"
      }
    ],
    "keywords": [
      "AWS Trusted Advisor",
      "AWS Compute Optimizer",
      "cost optimization",
      "EC2 instance sizing"
    ]
  },
  {
    "No": "171",
    "question": "A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS\nRegion.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が AWS CodeCommit リポジトリを使用しています。企業は、リポジトリにあるデータのバックアップを 2 つ目の AWS リージョンに保存する必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region.",
        "text_jp": "AWS Elastic Disaster Recovery を設定して、CodeCommit リポジトリのデータを 2 つ目のリージョンにレプリケートする。"
      },
      {
        "key": "B",
        "text": "Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.",
        "text_jp": "AWS Backup を使用して、CodeCommit リポジトリを毎時バックアップする。2 つ目のリージョンでクロスリージョンコピーを作成する。"
      },
      {
        "key": "C",
        "text": "Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the",
        "text_jp": "企業がリポジトリにコードをプッシュする際に、AWS CodeBuild を呼び出す Amazon EventBridge ルールを作成する。CodeBuild を使用してクローンを作成する。"
      },
      {
        "key": "D",
        "text": "Create an AWS Step Functions workfiow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workfiow to",
        "text_jp": "毎時スケジュールされた AWS Step Functions ワークフローを作成し、CodeCommit リポジトリのスナップショットを作成する。ワークフローを"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It involves using Amazon EventBridge to trigger CodeBuild, which can then clone the repository data to another region.",
        "situation_analysis": "The company needs to have a backup copy of its CodeCommit repository data in another AWS region, indicating the need for a reliable method to replicate or clone the data.",
        "option_analysis": "Option A suggests using AWS Elastic Disaster Recovery, which is not applicable to CodeCommit repositories. Option B mentions AWS Backup; however, CodeCommit does not support AWS Backup for cross-region backups directly. Option D suggests using AWS Step Functions, but it lacks clarity on how snapshots would be stored in a cross-region manner. Hence, they do not fulfill the requirement sufficiently.",
        "additional_knowledge": "AWS CodeCommit is a managed source control service, and to achieve cross-region data availability, AWS services like CodeBuild and EventBridge provide a robust framework.",
        "key_terminology": "AWS CodeCommit, Amazon EventBridge, AWS CodeBuild, cross-region backup",
        "overall_assessment": "Given the community support vote of 93% for option C, it aligns well with best practices for backup solutions leveraging AWS services. This solution is well thought out for replicating data reliably across regions."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は C です。Amazon EventBridge を使用して CodeBuild をトリガーし、その後リポジトリデータを別のリージョンにクローンすることが含まれています。",
        "situation_analysis": "企業は、他の AWS リージョンに CodeCommit リポジトリのデータのバックアップコピーを持つ必要があり、これはデータを複製またはクローンする信頼性のある方法を必要としていることを示しています。",
        "option_analysis": "選択肢 A は AWS Elastic Disaster Recovery の使用を提案していますが、これは CodeCommit リポジトリには適用されません。選択肢 B は AWS Backup に言及していますが、CodeCommit はクロスリージョンバックアップを直接サポートしていません。選択肢 D は AWS Step Functions の使用を提案していますが、この方法でスナップショットがどのようにクロスリージョンで保存されるかが不明確です。したがって、彼らは要件を十分に満たしていません。",
        "additional_knowledge": "AWS CodeCommit は管理されたソース管理サービスであり、クロスリージョンのデータ可用性を達成するために、CodeBuild や EventBridge などの AWS サービスは強力なフレームワークを提供します。",
        "key_terminology": "AWS CodeCommit, Amazon EventBridge, AWS CodeBuild, クロスリージョンバックアップ",
        "overall_assessment": "選択肢 C に対するコミュニティの支持率が 93% であることから、AWS サービスを活用したバックアップソリューションのベストプラクティスと一致しています。このソリューションは、リージョン間でデータを信頼できる方法で複製するためによく考えられています。"
      }
    ],
    "keywords": [
      "AWS CodeCommit",
      "Amazon EventBridge",
      "AWS CodeBuild",
      "cross-region backup"
    ]
  },
  {
    "No": "172",
    "question": "A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several\nVPCs that have CIDR ranges that overlap. The company's marketing team has created a new internal application and wants to make the\napplication accessible to all the other business units. The solution must use private IP addresses only.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "企業は、各ビジネスユニットが AWS の別々のアカウントを持つ複数のビジネスユニットを持っています。各ビジネスユニットは、重複する CIDR 範囲を持つ複数の VPC を持つ独自のネットワークを管理しています。企業のマーケティングチームは、新しい内部アプリケーションを作成し、そのアプリケーションを他のすべてのビジネスユニットにアクセス可能にしたいと考えています。このソリューションは、プライベート IP アドレスのみを使用する必要があります。どのソリューションが最も運用オーバーヘッドを最小限に抑えて要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Instruct each business unit to add a unique secondary CIDR range to the business unit's VPC. Peer the VPCs and use a private NAT gateway",
        "text_jp": "各ビジネスユニットに独自のセカンダリ CIDR 範囲をビジネスユニットの VPC に追加するよう指示します。VPC をピア接続し、プライベート NAT ゲートウェイを使用します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPC. Create an AWS Site-to-Site VPN connection",
        "text_jp": "マーケティングアカウントの VPC に仮想アプライアンスとして機能する Amazon EC2 インスタンスを作成します。AWS Site-to-Site VPN 接続を作成します。"
      },
      {
        "key": "C",
        "text": "Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to",
        "text_jp": "マーケティングアプリケーションを共有するための AWS PrivateLink エンドポイントサービスを作成します。特定の AWS アカウントに接続する権限を付与します。"
      },
      {
        "key": "D",
        "text": "Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the",
        "text_jp": "マーケティングアプリケーションの前にプライベートサブネット内にネットワークロードバランサー（NLB）を作成します。API Gateway APIを作成します。使用します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Creating a Network Load Balancer (NLB) in front of the marketing application provides a scalable way to expose the application while keeping it private.",
        "situation_analysis": "The situation involves multiple business units with overlapping CIDR ranges that need to access a marketing application using only private IP addresses.",
        "option_analysis": "Option A requires changes to existing VPC configurations, which may increase operational overhead. Option B introduces a virtual appliance, which complicates the setup. Option C is an effective solution but may involve more overhead in managing permissions. Option D simplifies access without altering existing configurations and minimizes operational overhead.",
        "additional_knowledge": "Utilizing a Network Load Balancer allows for efficient routing of requests to the marketing application while maintaining security.",
        "key_terminology": "Network Load Balancer (NLB), AWS PrivateLink, API Gateway, VPC Peering, CIDR Block",
        "overall_assessment": "Overall, option D provides the least operational overhead while fulfilling the application's access requirements. While community support leans towards option C, option D is technically the most efficient in this context, as it leverages existing load balancing techniques without extensive reconfiguration."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです。マーケティングアプリケーションの前にネットワークロードバランサー（NLB）を作成することで、プライベートを保ちながらアプリケーションを公開するためのスケーラブルな方法を提供します。",
        "situation_analysis": "状況は、重複するCIDR範囲を持つ複数のビジネスユニットがあり、プライベートIPアドレスのみを使用してマーケティングアプリケーションにアクセスする必要があるというものです。",
        "option_analysis": "選択肢Aは、既存のVPC設定に変更が必要であり、それが運用オーバーヘッドを増加させる可能性があります。選択肢Bは、仮想アプライアンスを導入し、設定を複雑にします。選択肢Cは効果的なソリューションですが、権限管理を行う上でさらにオーバーヘッドが発生する可能性があります。選択肢Dは、既存の設定を変更することなくアクセスを単純にし、運用オーバーヘッドを最小限に抑えます。",
        "additional_knowledge": "ネットワークロードバランサーを活用することで、マーケティングアプリケーションへのリクエストルーティングが効率的に行され、セキュリティが維持されます。",
        "key_terminology": "ネットワークロードバランサー（NLB）、AWS PrivateLink、API Gateway、VPCピアリング、CIDRブロック",
        "overall_assessment": "全体として、選択肢Dは最も運用オーバーヘッドが少なく、アプリケーションへのアクセス要件を満たしています。コミュニティの支持は選択肢Cに傾いていますが、選択肢Dはこの文脈において最も効率的な技術です。これは、広範な再構成を行わずに既存の負荷分散技術を活用しています。"
      }
    ],
    "keywords": [
      "Network Load Balancer",
      "AWS PrivateLink",
      "API Gateway",
      "VPC Peering",
      "CIDR Block"
    ]
  },
  {
    "No": "173",
    "question": "A company needs to audit the security posture of a newly acquired AWS account. The company's data security team requires a notification only\nwhen an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon\nSNS) topic that has the data security team's email address subscribed.\nWhich solution will meet these requirements?",
    "question_jp": "企業は新しく取得したAWSアカウントのセキュリティポスチャーを監査する必要があります。企業のデータセキュリティチームは、Amazon S3バケットが公開されたときのみ通知を受け取ることを要求しています。企業はすでにデータセキュリティチームのメールアドレスが購読されたAmazon Simple Notification Service（Amazon SNS）トピックを設立しています。この要件を満たすための解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications.",
        "text_jp": "すべてのS3バケットに対してisPublicイベントのS3イベント通知を作成します。イベント通知のターゲットとしてSNSトピックを選択します。"
      },
      {
        "key": "B",
        "text": "Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type",
        "text_jp": "AWS Identity and Access Management Access Analyzerでアナライザーを作成します。イベントタイプのためにAmazon EventBridgeルールを作成します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon EventBridge rule for the event type “Bucket-Level API Call via CloudTrail” with a filter for “PutBucketPolicy.” Select the",
        "text_jp": "「CloudTrailを介したバケットレベルAPIコール」イベントタイプ用のAmazon EventBridgeルールを作成し、「PutBucketPolicy」のフィルターを設定します。"
      },
      {
        "key": "D",
        "text": "Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type “Config Rules",
        "text_jp": "AWS Configを有効にし、cloudtrail-s3-dataevents-enabledルールを追加します。「Config Rules」のイベントタイプ用のAmazon EventBridgeルールを作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. The solution suggests creating an S3 event notification that captures any changes in the public exposure status of S3 buckets and directs them to an SNS topic designated for notifications. This directly aligns with the requirement to notify the security team only when an S3 bucket becomes publicly exposed.",
        "situation_analysis": "The requirement is to only receive notifications when an S3 bucket's access changes to public. The company already has an SNS topic to which the security team's email is subscribed, simplifying the notification process.",
        "option_analysis": "Option A correctly implements an event-based notification system using S3 event notifications, which is the desired approach. Options B, C, and D do not provide a direct means of notification specifically for public exposure.",
        "additional_knowledge": "Given that public exposure requires immediate action, AWS best practices suggest using event notifications where possible to reduce the manual overhead of checking conditions.",
        "key_terminology": "Amazon S3, Amazon SNS, S3 event notification, public access, AWS security posture.",
        "overall_assessment": "This question is crafted in a way that tests knowledge on AWS services' interactions, specifically focusing on the notification mechanisms around S3. The community vote shows strong preference for B despite A being the correct choice, possibly indicating confusion around event-driven notifications versus access management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。この解決策は、S3バケットの公開状態の変更をキャプチャし、通知用に指定されたSNSトピックに送信するS3イベント通知を作成することを提案している。これにより、S3バケットが公開された時のみセキュリティチームに通知されるという要件に直接対応している。",
        "situation_analysis": "要件は、S3バケットのアクセスが公開に変更された時のみ通知を受け取ることです。企業はすでにセキュリティチームのメールが購読されているSNSトピックを持っており、通知プロセスが簡素化されている。",
        "option_analysis": "選択肢AはS3イベント通知を使用したイベント駆動型の通知システムを適切に実装しており、これは望ましいアプローチである。選択肢B、C、Dは公開アクセス専用の通知手段を提供していない。",
        "additional_knowledge": "公開アクセスが即座の対応を必要とするため、可能な限りイベント通知を使用することがAWSのベストプラクティスとして推奨されている。",
        "key_terminology": "Amazon S3、Amazon SNS、S3イベント通知、公開アクセス、AWSセキュリティポスチャー。",
        "overall_assessment": "この質問は、AWSサービス間の相互作用に関する知識をテストするように作成されており、特にS3に関連した通知メカニズムに焦点を当てている。コミュニティの投票はAが正しい選択肢のにもかかわらずBを強く支持していることから、イベント駆動型通知とアクセス管理に関する混乱を示しているかもしれない。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Amazon SNS",
      "S3 event notification",
      "public access",
      "AWS security posture"
    ]
  },
  {
    "No": "174",
    "question": "A solutions architect needs to assess a newly acquired company's portfolio of applications and databases. The solutions architect must create a\nbusiness case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is\nnot well documented. The solutions architect cannot immediately determine how many applications and databases exist. Trafic for the\napplications is variable. Some applications are batch processes that run at the end of each month.\nThe solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.\nWhich solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、新たに取得した企業のアプリケーションとデータベースのポートフォリオを評価する必要があります。ソリューションアーキテクトは、ポートフォリオをAWSに移行するためのビジネスケースを作成しなければなりません。新たに取得した企業は、オンプレミスのデータセンターでアプリケーションを稼働していますが、そのデータセンターは十分に文書化されていません。ソリューションアーキテクトは、アプリケーションとデータベースがいくつ存在するかを直ちに把握することができません。アプリケーションのトラフィックは変動します。また、一部のアプリケーションは、月末に実行されるバッチ処理です。ソリューションアーキテクトは、AWSへの移行を開始する前にポートフォリオをよりよく理解する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service",
        "text_jp": "AWS Server Migration Service (AWS SMS) と AWS Database Migration Service (AWS DMS) を使用して移行を評価します。AWSサービスを使用します。"
      },
      {
        "key": "B",
        "text": "Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub.",
        "text_jp": "AWS Application Migration Service を使用します。オンプレミスのインフラストラクチャにエージェントを実行します。エージェントは AWS Migration Hub を使用して管理します。"
      },
      {
        "key": "C",
        "text": "Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use",
        "text_jp": "Migration Evaluator を使用してサーバーのリストを生成します。ビジネスケースのためのレポートを作成します。AWS Migration Hub を使用してポートフォリオを表示します。"
      },
      {
        "key": "D",
        "text": "Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to",
        "text_jp": "AWS Control Tower を使用して目的のアカウントにアプリケーションポートフォリオを生成します。AWS Server Migration Service (AWS SMS) を使用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "正しい回答はBです。AWS Application Migration Serviceを使用することで、オンプレミスのアプリケーションをAWSに移行するための詳細な分析が可能です。エージェントをインストールして実行することで、アプリケーションの依存関係やパフォーマンスを把握できます。",
        "situation_analysis": "新たに取得した企業のアプリケーションポートフォリオの理解が求められ、移行の準備をする必要があります。アプリケーションは変動トラフィックを抱えており、バッチ処理も存在するため、詳細な調査が不可欠です。",
        "option_analysis": "オプションAでは、AWS SMSとAWS DMSの使用が提案されていますが、これらは移行後のソリューションです。オプションCはMigration Evaluatorを使用する提案ですが、詳細なエージェント管理が提供されないため十分ではありません。オプションDはControl Towerですが、これもポートフォリオを生成するための最適な方法ではありません。",
        "additional_knowledge": "AWSのサービスが提供する機能は、特定の状況において幅広いアプローチが可能であるため、それぞれの要件に適する方法を見極めることが重要です。",
        "key_terminology": "AWS Application Migration Service, AWS Migration Hub, エージェント, アプリケーション移行",
        "overall_assessment": "Bが選ばれるべきだが、コミュニティの投票はCが圧倒的に多いため、コミュニティの認識と公式なガイダンスの間で不一致が見られます。したがって、Cの高い支持率にも注意が必要です。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBです。AWS Application Migration Serviceを使用することで、オンプレミスのアプリケーションをAWSに移行するための詳細な分析が可能です。エージェントをインストールして実行することで、アプリケーションの依存関係やパフォーマンスを把握できます。",
        "situation_analysis": "新たに取得した企業のアプリケーションポートフォリオの理解が求められ、移行の準備をする必要があります。アプリケーションは変動トラフィックを抱えており、バッチ処理も存在するため、詳細な調査が不可欠です。",
        "option_analysis": "オプションAでは、AWS SMSとAWS DMSの使用が提案されていますが、これらは移行後のソリューションです。オプションCはMigration Evaluatorを使用する提案ですが、詳細なエージェント管理が提供されないため十分ではありません。オプションDはControl Towerですが、これもポートフォリオを生成するための最適な方法ではありません。",
        "additional_knowledge": "AWSのサービスが提供する機能は、特定の状況において幅広いアプローチが可能であるため、それぞれの要件に適する方法を見極めることが重要です。",
        "key_terminology": "AWS Application Migration Service, AWS Migration Hub, エージェント, アプリケーション移行",
        "overall_assessment": "Bが選ばれるべきだが、コミュニティの投票はCが圧倒的に多いため、コミュニティの認識と公式なガイダンスの間で不一致が見られます。したがって、Cの高い支持率にも注意が必要です。"
      }
    ],
    "keywords": [
      "AWS Application Migration Service",
      "AWS Migration Hub",
      "Agent",
      "Application Migration"
    ]
  },
  {
    "No": "175",
    "question": "A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS\ncluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances\nof the application. The company needs to back up the files and retain the backups for 1 year.\nWhich solution will meet these requirements while providing the FASTEST storage performance?",
    "question_jp": "ある企業が、Amazon Elastic Kubernetes Service（Amazon EKS）クラスター内の複数のポッドのReplicaSetとして実行されるアプリケーションを持っています。EKSクラスターには複数のアベイラビリティーゾーンにノードがあります。このアプリケーションは、実行中のすべてのインスタンスからアクセス可能でなければならない多くの小さなファイルを生成します。企業は、これらのファイルをバックアップし、バックアップを1年間保持する必要があります。\nこれらの要件を満たしながら、最も速いストレージパフォーマンスを提供するソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster.",
        "text_jp": "Amazon Elastic File System（Amazon EFS）ファイルシステムを作成し、EKSクラスター内のノードを含む各サブネットにマウントターゲットを作成します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the",
        "text_jp": "Amazon Elastic Block Store（Amazon EBS）ボリュームを作成します。EBS Multi-Attach機能を有効にします。ReplicaSetを構成し、マウントします。"
      },
      {
        "key": "C",
        "text": "Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket.",
        "text_jp": "Amazon S3バケットを作成します。ReplicaSetを構成し、S3バケットをマウントします。アプリケーションを指示してファイルをS3バケットに保存します。"
      },
      {
        "key": "D",
        "text": "Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool",
        "text_jp": "ReplicaSetを構成し、実行中のアプリケーションポッドにあるストレージを使用してファイルをローカルに保存します。サードパーティツールを使用します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster.",
        "situation_analysis": "The company needs a solution that allows multiple instances of an application to access and store many small files reliably across multiple Availability Zones, while ensuring data durability for 1 year.",
        "option_analysis": "Option A provides a managed, scalable solution that integrates well with EKS, allowing for low-latency access to the files across all pods. Option B, while allowing multiple pods to access EBS, does not fundamentally offer the same level of integration or scalability as EFS. Option C requires additional configuration and may involve performance overhead and possible complexity. Option D lacks durability and centralized access across multiple pods.",
        "additional_knowledge": "EFS also provides lifecycle policies to manage backups and data retention.",
        "key_terminology": "Amazon EFS, Amazon EKS, Availability Zones, scalable storage, file system.",
        "overall_assessment": "In this case, Option A (Amazon EFS) is the best choice due to its speed, scalability, and compatibility with the requirements outlined."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はA: Amazon Elastic File System（Amazon EFS）ファイルシステムを作成し、EKSクラスター内のノードを含む各サブネットにマウントターゲットを作成することです。",
        "situation_analysis": "企業は、アプリケーションの複数のインスタンスが、複数のアベイラビリティーゾーンにわたり信頼性を持って多くの小さなファイルにアクセスし保存できるソリューションが必要であり、データの耐久性を1年間確保する必要があります。",
        "option_analysis": "選択肢Aは、EKSとの統合が良好な管理されたスケーラブルなソリューションを提供し、すべてのポッドにおけるファイルへの低レイテンシアクセスを可能にします。選択肢Bは、複数のポッドがEBSにアクセスできるものの、EFSのようなレベルの統合やスケーラビリティを根本的に提供していません。選択肢Cは、追加の構成が必要で、パフォーマンスオーバーヘッドや複雑さが発生する可能性があります。選択肢Dは耐久性を欠き、複数のポッド間での集中アクセスができません。",
        "additional_knowledge": "EFSは、バックアップやデータ保持を管理するためのライフサイクルポリシーも提供しています。",
        "key_terminology": "Amazon EFS、Amazon EKS、アベイラビリティーゾーン、スケーラブルストレージ、ファイルシステム。",
        "overall_assessment": "この場合、選択肢A（Amazon EFS）は、速度、スケーラビリティ、要求された条件との互換性のため、最良の選択となります。"
      }
    ],
    "keywords": [
      "Amazon EFS",
      "Amazon EKS",
      "Availability Zones",
      "scalable storage",
      "file system"
    ]
  },
  {
    "No": "176",
    "question": "A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience\nsurvey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises\ndata center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to\nmigrate the system to AWS to improve reliability.\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?",
    "question_jp": "ある企業が顧客サービスセンターを運営しており、電話を受け付け、顧客に対して管理された対話型の双方向体験調査を自動的にテキストメッセージで送信しています。この顧客サービスセンターをサポートするアプリケーションは、企業がオンプレミスのデータセンターでホストしているマシン上で動作しています。企業が使用しているハードウェアは古く、システムにダウンタイムが発生しています。企業は信頼性を向上させるために、システムをAWSに移行したいと考えています。どのソリューションが、最も少ない運用オーバーヘッドでこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.",
        "text_jp": "Amazon Connectを使用して古いコールセンターハードウェアを置き換えます。Amazon Pinpointを使用して顧客にテキストメッセージ調査を送信します。"
      },
      {
        "key": "B",
        "text": "Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message",
        "text_jp": "Amazon Connectを使用して古いコールセンターハードウェアを置き換えます。Amazon Simple Notification Service (Amazon SNS)を使用してテキストメッセージを送信します。"
      },
      {
        "key": "C",
        "text": "Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message",
        "text_jp": "コールセンターソフトウェアを自動スケーリンググループにあるAmazon EC2インスタンスに移行します。EC2インスタンスを使用してテキストメッセージを送信します。"
      },
      {
        "key": "D",
        "text": "Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers.",
        "text_jp": "Amazon Pinpointを使用して古いコールセンターハードウェアを置き換え、顧客にテキストメッセージ調査を送信します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Amazon Connect is a cloud-based contact center solution that enables companies to replace old hardware with a more reliable system. Using Amazon Pinpoint allows the company to send two-way interactive surveys efficiently.",
        "situation_analysis": "The company seeks to migrate from outdated hardware to a cloud solution to ensure a reliable customer service experience. Reducing operational overhead is a key objective.",
        "option_analysis": "Option A employs both Amazon Connect for call center functionality and Amazon Pinpoint for messaging, optimizing reliability and reducing maintenance tasks. Option B lacks the interactive capabilities provided by Pinpoint while using SNS, which may require more management. Option C, while viable, introduces additional operational complexity as it involves managing EC2 instances directly. Option D incorrectly suggests replacing the call center hardware solely with Pinpoint without integrating a call center solution.",
        "additional_knowledge": "Utilizing AWS services allows the company to scale its customer service capabilities efficiently.",
        "key_terminology": "Amazon Connect, Amazon Pinpoint, SaaS, Contact Center, EC2",
        "overall_assessment": "The community overwhelmingly supports A as it successfully addresses all the requirements with minimal overhead. Options B and C, while partially correct, do not offer the same level of seamless integration as A."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。Amazon Connectは、企業が古いハードウェアをより信頼性の高いシステムに置き換えることができるクラウドベースのコンタクトセンターソリューションである。Amazon Pinpointを使用することで、企業は双方向の対話型調査を効率的に送信できる。",
        "situation_analysis": "企業は、古いハードウェアからクラウドソリューションへ移行し、信頼性の高い顧客サービス体験を確保することを求めている。運用オーバーヘッドを削減することが重要な目的である。",
        "option_analysis": "選択肢Aは、コールセンター機能にAmazon Connectを使用し、メッセージングにAmazon Pinpointを使用することで信頼性を最適化し、メンテナンス作業を削減する。選択肢Bは、SNSを利用してインタラクティブな機能を欠いているため、管理が増える可能性がある。選択肢Cは実行可能であるが、EC2インスタンスを直接管理する必要があるため、運用の複雑さが増す。選択肢Dは、コールセンター機能を統合せずにPinpointでハードウェアを置き換えようとしているため不正解である。",
        "additional_knowledge": "AWSサービスを活用することで、企業は顧客サービス機能を効率的にスケールアップできる。",
        "key_terminology": "Amazon Connect, Amazon Pinpoint, SaaS, コンタクトセンター, EC2",
        "overall_assessment": "コミュニティは、Aを圧倒的に支持しており、最小限のオーバーヘッドで全ての要件を満たすことができる。選択肢BとCは部分的に正しいが、Aと同じレベルのシームレスな統合を提供しない。"
      }
    ],
    "keywords": [
      "Amazon Connect",
      "Amazon Pinpoint",
      "SaaS",
      "Contact Center",
      "EC2"
    ]
  },
  {
    "No": "177",
    "question": "A company is building a call center by using Amazon Connect. The company's operations team is defining a disaster recovery (DR) strategy across\nAWS Regions. The contact center has dozens of contact fiows, hundreds of users, and dozens of claimed phone numbers.\nWhich solution will provide DR with the LOWEST RTO?",
    "question_jp": "ある企業がAmazon Connectを使用してコールセンターを構築しています。企業の運用チームはAWSリージョン全体での災害復旧（DR）戦略を定義しています。コールセンターには何十ものコンタクトフロー、何百ものユーザー、何十もの請求された電話番号があります。最も低いRTOでDRを提供するソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team",
        "text_jp": "AWS Lambda関数を作成して、Amazon Connectインスタンスの可用性をチェックし、運用チームに通知を送信する。"
      },
      {
        "key": "B",
        "text": "Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the",
        "text_jp": "別のリージョンに既存のすべてのユーザーを持つ新しいAmazon Connectインスタンスをプロビジョニングする。AWS Lambda関数を作成して、"
      },
      {
        "key": "C",
        "text": "Provision a new Amazon Connect instance with all existing contact fiows and claimed phone numbers in a second Region. Create an",
        "text_jp": "別のリージョンに既存のすべてのコンタクトフローと請求された電話番号を持つ新しいAmazon Connectインスタンスをプロビジョニングする。AWS Lambda関数を作成する。"
      },
      {
        "key": "D",
        "text": "Provision a new Amazon Connect instance with all existing users and contact fiows in a second Region. Create an Amazon Route 53 health",
        "text_jp": "別のリージョンに既存のすべてのユーザーとコンタクトフローを持つ新しいAmazon Connectインスタンスをプロビジョニングする。Amazon Route 53ヘルスを作成する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (82%) B (18%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Provision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check.",
        "situation_analysis": "The company's goal is to implement a disaster recovery strategy minimizing the Recovery Time Objective (RTO). This requires ensuring that the call center can quickly resume operations in another region.",
        "option_analysis": "Option D includes setting up a complete Amazon Connect instance in a second region with all necessary user configurations and contact flows. This allows for immediate failover. Other options do not provide full infrastructure replication, leading to longer recovery times.",
        "additional_knowledge": "Option B and C would require manual intervention and may result in longer RTO due to incomplete configurations.",
        "key_terminology": "Amazon Connect, disaster recovery (DR), Recovery Time Objective (RTO), Amazon Route 53, health check.",
        "overall_assessment": "Considering the community vote, which largely supports option D (82%), it aligns with AWS best practices for disaster recovery involving quick restoration of services in another region."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです：別のリージョンに既存のすべてのユーザーとコンタクトフローを持つ新しいAmazon Connectインスタンスをプロビジョニングし、Amazon Route 53ヘルスチェックを作成します。",
        "situation_analysis": "企業の目標は、復旧時間目標（RTO）を最小化する災害復旧戦略を実装することです。これにより、コールセンターが別のリージョンで迅速に業務を再開できるようにする必要があります。",
        "option_analysis": "選択肢Dには、別のリージョンにすべての必要なユーザー設定とコンタクトフローで構成された完全なAmazon Connectインスタンスをセットアップすることが含まれています。これにより、すぐにフェイルオーバーが可能です。他の選択肢は完全なインフラストラクチャの複製を提供せず、復旧に時間がかかります。",
        "additional_knowledge": "選択肢BおよびCは手動介入を必要とし、不完全な設定のためにRTOが長くなる可能性があります。",
        "key_terminology": "Amazon Connect、災害復旧（DR）、復旧時間目標（RTO）、Amazon Route 53、ヘルスチェック。",
        "overall_assessment": "コミュニティ投票を考慮すると、主に選択肢D（82％）を支持しており、これは他のリージョンでのサービスの迅速な復元に関するAWSのベストプラクティスに合致しています。"
      }
    ],
    "keywords": [
      "Amazon Connect",
      "disaster recovery",
      "Recovery Time Objective",
      "Amazon Route 53",
      "health check"
    ]
  },
  {
    "No": "178",
    "question": "A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to\nperform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift\ntables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits\nthe files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has\nbecome dificult.\nThe company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants\nto confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the\ncompany publishes the data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がAWS上でアプリケーションを運営しています。企業は複数の異なるソースからデータをキュレーションしています。企業は独自のアルゴリズムを使用してデータの変換と集約を行います。企業がETLプロセスを実行した後、結果をAmazon Redshiftテーブルに保存しています。企業はこのデータを他の企業に販売しています。企業はAmazon Redshiftテーブルからデータをファイルとしてダウンロードし、FTPを使用して複数のデータ顧客にファイルを送信しています。データ顧客の数が大幅に増加しました。データ顧客の管理が困難になっています。\n企業はAWS Data Exchangeを使用して、顧客とデータを共有するためのデータ商品を作成する予定です。企業はデータを共有する前に顧客の身元を確認したいと考えています。顧客は、企業がデータを公開した際に最新のデータにアクセスする必要があります。\nどのソリューションが最も運用上のオーバーヘッドが少なく、これらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company",
        "text_jp": "AWS Data ExchangeのAPIを使用してデータを顧客と共有する。サブスクリプションの確認を設定する。企業のAWSアカウント内で"
      },
      {
        "key": "B",
        "text": "In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to",
        "text_jp": "データを生成する企業のAWSアカウントで、AWS Data Exchangeを接続してAWS Data Exchangeデータ共有を作成する"
      },
      {
        "key": "C",
        "text": "Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data",
        "text_jp": "定期的にAmazon RedshiftテーブルからAmazon S3バケットにデータをダウンロードする。AWS Data Exchange for S3を使用してデータを共有する"
      },
      {
        "key": "D",
        "text": "Publish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS",
        "text_jp": "Amazon RedshiftデータをAWS Data Exchangeのオープンデータに公開する。顧客にAWSでデータ商品にサブスクライブすることを求める"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (83%) C (17%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This option offers a streamlined approach for data sharing and identifies customers.",
        "situation_analysis": "The company needs to manage customer verification and provide recent data efficiently as customer demand has increased.",
        "option_analysis": "Choice B directly integrates AWS Data Exchange, allowing for secure and seamless data sharing while verifying the identities of customers. Other options either do not focus on identity verification or introduce more complexity.",
        "additional_knowledge": "Leveraging AWS services effectively can greatly enhance operational efficiency.",
        "key_terminology": "AWS Data Exchange, ETL, Amazon Redshift, data product, customer verification",
        "overall_assessment": "The chosen solution minimizes overhead and supports smooth operation as opposed to other alternatives that add complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBである。この選択肢はデータ共有を効率的に行い、顧客を特定するための合理的なアプローチを提供する。",
        "situation_analysis": "この企業は顧客の確認を管理し、顧客の需要が増加しているため、最近のデータを効率的に提供する必要がある。",
        "option_analysis": "選択肢BはAWS Data Exchangeを直接統合し、顧客の身元を確認しながら安全かつ円滑にデータを共有できる。他の選択肢は身元確認に焦点を当てていなかったり、より複雑さを追加したりする。",
        "additional_knowledge": "AWSサービスを効果的に活用することは、運用の効率を大いに向上させることができる。",
        "key_terminology": "AWS Data Exchange、ETL、Amazon Redshift、データ商品、顧客確認",
        "overall_assessment": "選択されたソリューションはオーバーヘッドを最小限に抑え、他の代替策よりもスムーズな運用を支援する。"
      }
    ],
    "keywords": [
      "AWS Data Exchange",
      "ETL",
      "Amazon Redshift",
      "data product",
      "customer verification"
    ]
  },
  {
    "No": "179",
    "question": "A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of\nevents that the solution receives. If a processing error occurs, the event must move into a separate queue for review.\nWhich solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトがイベントを処理するためのソリューションを設計しています。このソリューションは、受信するイベントの数に基づいてスケールインおよびスケールアウトする能力を持たなければなりません。処理中にエラーが発生した場合、イベントはレビューのために別のキューに移動する必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Send event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to",
        "text_jp": "イベントの詳細をAmazon Simple Notification Service（Amazon SNS）トピックに送信します。AWS Lambda関数をサブスクライバーとして設定する"
      },
      {
        "key": "B",
        "text": "Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto",
        "text_jp": "イベントをAmazon Simple Queue Service（Amazon SQS）キューに公開します。Amazon EC2 Auto Scalingグループを作成します。Auto"
      },
      {
        "key": "C",
        "text": "Write events to an Amazon DynamoDB table. Configure a DynamoDB stream for the table. Configure the stream to invoke an AWS Lambda",
        "text_jp": "イベントをAmazon DynamoDBテーブルに書き込みます。テーブルのDynamoDBストリームを設定します。ストリームがAWS Lambdaを呼び出すように設定"
      },
      {
        "key": "D",
        "text": "Publish events to an Amazon EventBndge event bus. Create and run an application on an Amazon EC2 instance with an Auto Scaling group",
        "text_jp": "イベントをAmazon EventBridgeイベントバスに公開します。Auto Scalingグループを持つAmazon EC2インスタンス上でアプリケーションを作成して実行"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (53%) A (47%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "実際の解答はBであり、AWS SQSを使用することで、受信イベントに対するスケーラブルな処理が実現可能である。",
        "situation_analysis": "要求されているのは、受信イベント数に応じてスケールし、エラー発生時に特定のキューへ移動できる仕組みである。",
        "option_analysis": "Bは、SQSを使用してイベントを管理し、EC2 Auto Scalingグループを使用してスケーラビリティを確保します。他の選択肢は、スケーラビリティやエラーハンドリングの要件を満たしません。",
        "additional_knowledge": "他の選択肢は、特定の条件でのエラー処理やスケーラビリティの要件を最適に満たすわけではない。",
        "key_terminology": "Amazon SQS, Auto Scaling, event processing",
        "overall_assessment": "選択肢Bは、求めている要件を最も効果的に満たしており、スケールやエラー処理に関するAWSのベストプラクティスに合致する。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい解答はBであり、AWS SQSを使用することで、受信イベントに対するスケーラブルな処理が実現可能である。",
        "situation_analysis": "要求されているのは、受信イベント数に応じてスケールし、エラー発生時に特定のキューへ移動できる仕組みである。",
        "option_analysis": "Bは、SQSを使用してイベントを管理し、EC2 Auto Scalingグループを使用してスケーラビリティを確保します。他の選択肢は、スケーラビリティやエラーハンドリングの要件を満たしません。",
        "additional_knowledge": "他の選択肢は、特定の条件でのエラー処理やスケーラビリティの要件を最適に満たすわけではない。",
        "key_terminology": "Amazon SQS, Auto Scaling, event processing",
        "overall_assessment": "選択肢Bは、求めている要件を最も効果的に満たしており、スケールやエラー処理に関するAWSのベストプラクティスに合致する。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "Auto Scaling",
      "event processing"
    ]
  },
  {
    "No": "180",
    "question": "A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a\nsustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the\nprocessing engine through a RESTful API.\nThe API experiences unpredictable bursts of trafic. The company must implement a solution to process all data that the devices send to the\nprocessing engine. Data loss is unacceptable.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWSクラウドで処理エンジンを運用しています。このエンジンは、物流センターからの環境データを処理して持続可能性指数を計算します。企業はヨーロッパに広がる数百万のデバイスを物流センターに持っており、これらのデバイスはRESTful APIを介して処理エンジンに情報を送信します。このAPIは予測不可能なトラフィックバーストを経験します。企業はデバイスが処理エンジンに送信する全てのデータを処理するためのソリューションを実装しなければなりません。データ損失は許されません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a",
        "text_jp": "RESTful APIのためにアプリケーションロードバランサー（ALB）を作成します。Amazon Simple Queue Service（Amazon SQS）キューを作成します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue.",
        "text_jp": "RESTful APIを実装するAmazon API Gateway HTTP APIを作成します。Amazon Simple Queue Service（Amazon SQS）キューを作成します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon API Gateway REST API that implements the RESTful API. Create a fieet of Amazon EC2 instances in an Auto Scaling",
        "text_jp": "RESTful APIを実装するAmazon API Gateway REST APIを作成します。オートスケーリングでのAmazon EC2インスタンスのフリートを作成します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream",
        "text_jp": "RESTful APIのためにAmazon CloudFrontディストリビューションを作成します。Amazon Kinesis Data Streamsにデータストリームを作成します。データストリームを設定します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (84%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. An Amazon API Gateway HTTP API can effectively manage unpredictable bursts of traffic and scale automatically, making it suitable for the company's needs. Coupling this with an Amazon Simple Queue Service (SQS) queue ensures that there is no data loss even during high bursts of traffic. This combination meets the required reliability and scalability.",
        "situation_analysis": "The company requires a highly available and resilient solution to handle millions of incoming requests from devices. Given the unpredictable traffic, the solution needs to absorb these bursts without losing data.",
        "option_analysis": "Option B is preferred as the Amazon API Gateway provides built-in capabilities for handling high traffic volumes efficiently while also integrating seamlessly with SQS for backend processing. Option A lacks the direct handling of traffic spikes, while Options C and D do not adequately address data loss concerns under burst conditions.",
        "additional_knowledge": "Implementing such a design helps maintain service availability and enhances user experience during traffic fluctuations.",
        "key_terminology": "Amazon API Gateway, Amazon SQS, RESTful API, scalability, message queuing",
        "overall_assessment": "This question tests understanding of architecture design patterns that prioritize reliability and scalability, especially when dealing with variable traffic loads. Options A, C, and D either introduce complexity or fail to ensure data integrity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。Amazon API Gateway HTTP APIは予測不可能なトラフィックバーストを効果的に管理し、自動的にスケールしますので、企業のニーズに適しています。これをAmazon Simple Queue Service（SQS）キューと組み合わせることで、高いトラフィックバースト時でもデータ損失を防ぎます。この組み合わせは、要件である信頼性とスケーラビリティを満たします。",
        "situation_analysis": "企業は、数百万からのデバイスからのリクエストを処理するため、高可用性で回復力のあるソリューションを必要としています。予測不可能なトラフィックを考慮すると、データを失うことなくバーストを吸収できるソリューションが必要です。",
        "option_analysis": "Bオプションが好ましい理由は、Amazon API Gatewayが高トラフィックボリュームを効率的に処理するための内蔵機能を提供し、SQSとのバックエンド処理にシームレスに統合できるためです。Aオプションは、トラフィックスパイクの直接処理が不足しており、CおよびDオプションはバースト条件下でのデータ損失問題に適切に対処していません。",
        "additional_knowledge": "このような設計を実装することで、サービスの可用性を維持し、トラフィックの変動時にユーザーエクスペリエンスを向上させることができます。",
        "key_terminology": "Amazon API Gateway、Amazon SQS、RESTful API、スケーラビリティ、メッセージキューイング",
        "overall_assessment": "この質問は、可変トラフィック負荷に対して信頼性とスケーラビリティを優先するアーキテクチャ設計パターンの理解をテストします。A、C、Dオプションは、複雑さを引き入れるかデータ整合性を保証できません。"
      }
    ],
    "keywords": [
      "Amazon API Gateway",
      "Amazon SQS",
      "RESTful API",
      "scalability",
      "message queuing"
    ]
  },
  {
    "No": "181",
    "question": "A company is designing its network configuration in the AWS Cloud. The company uses AWS Organizations to manage a multi-account setup. The\ncompany has three OUs. Each OU contains more than 100 AWS accounts. Each account has a single VPC, and all the VPCs in each OU are in the\nsame AWS Region.\nThe CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution in which VPCs in the same OU can\ncommunicate with each other but cannot communicate with VPCs in other OUs.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がAWSクラウドにおけるネットワーク構成を設計しています。この企業はAWS Organizationsを使用してマルチアカウントセットアップを管理しています。企業は3つのOUを持ち、各OUには100を超えるAWSアカウントがあります。各アカウントには単一のVPCがあり、各OU内のすべてのVPCは同じAWSリージョンに存在します。すべてのAWSアカウントのCIDR範囲は重複していません。企業は、同じOU内のVPCが相互に通信できるが、他のOUのVPCとは通信できないソリューションを実装する必要があります。どのソリューションが最も運用負荷を軽減しつつ、これらの要件を満たすでしょうか。",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS CloudFormation stack set that establishes VPC peering between accounts in each OU. Provision the stack set in each OU.",
        "text_jp": "AWS CloudFormationスタックセットを作成し、各OUのアカウント間でVPCピアリングを確立します。各OUにスタックセットをプロビジョニングします。"
      },
      {
        "key": "B",
        "text": "In each OU, create a dedicated networking account that has a single VPC. Share this VPC with all the other accounts in the OU by using",
        "text_jp": "各OUに専用のネットワーキングアカウントを作成し、単一のVPCを設けます。このVPCをOU内のすべてのアカウントと共有します。"
      },
      {
        "key": "C",
        "text": "Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access",
        "text_jp": "各OUのアカウントにトランジットゲートウェイをプロビジョニングします。AWSリソースアクセスマネジメントを使用して、組織全体でトランジットゲートウェイを共有します。"
      },
      {
        "key": "D",
        "text": "In each OU, create a dedicated networking account that has a single VPC. Establish a VPN connection between the networking account and",
        "text_jp": "各OUに専用のネットワーキングアカウントを作成し、単一のVPCを設けます。ネットワーキングアカウントとVPN接続を確立します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (70%) B (17%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: In each OU, create a dedicated networking account that has a single VPC and establish a VPN connection between the networking account and other accounts in the OU. This minimizes operational overhead while allowing VPC communication within the same OU.",
        "situation_analysis": "The company needs to maintain VPC communication within the same OU and isolate communication between OUs. By creating a dedicated networking account in each OU with a single VPC, it becomes easier to manage inter-account connectivity.",
        "option_analysis": "Option A involves complex setup and management of VPC peering, which can be labor-intensive. Option B does not provide the necessary isolation from other OUs. Option C involves managing a transit gateway, which adds additional complexity and costs. Option D offers a simple and effective way to connect VPCs within the same OU with minimal overhead.",
        "additional_knowledge": "Centralized management of VPCs can lead to better governance and compliance.",
        "key_terminology": "VPC, AWS Organizations, VPN, network isolation, transit gateway",
        "overall_assessment": "The community vote shows a higher preference for option C, which may reflect a misunderstanding of operational overhead versus practical implementation of the requirements. However, option D is the best fit given the specific conditions outlined in the question."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです：各OUに専用のネットワーキングアカウントを作成し、単一のVPCを設け、ネットワーキングアカウントとOU内の他のアカウントとの間にVPN接続を確立します。これにより、運用負荷を最小限に抑えつつ、同じOU内のVPC間の通信を可能にします。",
        "situation_analysis": "企業は同じOU内でVPC通信を維持し、OU間の通信を隔離する必要があります。各OUに単一のVPCを持つ専用のネットワーキングアカウントを作成することで、アカウント間の接続管理が容易になります。",
        "option_analysis": "オプションAはVPCピアリングの設定と管理に関して複雑で時間がかかる可能性があります。オプションBは他のOUと必要な隔離を提供しません。オプションCはトランジットゲートウェイの管理を含み、追加の複雑さとコストを加えます。オプションDは、運用負荷を最小限に抑えつつ、同じOU内のVPCを接続するシンプルで効果的な方法を提供します。",
        "additional_knowledge": "VPCの集中管理は、より良いガバナンスとコンプライアンスを促進します。",
        "key_terminology": "VPC、AWS Organizations、VPN、ネットワーク隔離、トランジットゲートウェイ",
        "overall_assessment": "コミュニティの投票ではオプションCへの支持が高いですが、これは運用負荷と実際の要件に対する実装の理解に誤解があることを反映している可能性があります。しかし、オプションDは問題で示された特定の条件に最も適しています。"
      }
    ],
    "keywords": [
      "VPC",
      "AWS Organizations",
      "VPN",
      "network isolation",
      "transit gateway"
    ]
  },
  {
    "No": "182",
    "question": "A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company\nneeds to store large important documents within the application with the following requirements:",
    "question_jp": "ある企業がアプリケーションをAWSに移行しています。移行中は可能な限りフルマネージドサービスを使用したいと考えています。この企業は、以下の要件を持つアプリケーション内に重要な大容量ドキュメントを保存する必要があります。",
    "choices": [
      {
        "key": "1",
        "text": "The data must be highly durable and available",
        "text_jp": "データは非常に耐久性が高く、可用性がある必要がある"
      },
      {
        "key": "2",
        "text": "The data must always be encrypted at rest and in transit",
        "text_jp": "データは常に静止時および転送時に暗号化されている必要がある"
      },
      {
        "key": "3",
        "text": "The encryption key must be managed by the company and rotated periodically",
        "text_jp": "暗号化キーは企業が管理し、定期的にローテーションされなければならない"
      },
      {
        "key": "A",
        "text": "Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the",
        "text_jp": "ファイルゲートウェイモードでストレージゲートウェイをAWSにデプロイします。AWS KMSキーを使用してAmazon EBSボリュームの暗号化を行います。"
      },
      {
        "key": "B",
        "text": "Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS",
        "text_jp": "バケットポリシーを使用してバケットへの接続にHTTPSを強制し、サーバー側の暗号化とAWS KMSを強制するためにAmazon S3を使用します。"
      },
      {
        "key": "C",
        "text": "Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.",
        "text_jp": "Amazon DynamoDBにSSLを使用して接続し、AWS KMSキーを使用してDynamoDBオブジェクトを静止時に暗号化します。"
      },
      {
        "key": "D",
        "text": "Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the",
        "text_jp": "データを保存するためにAmazon EBSボリュームをアタッチしたインスタンスをデプロイします。AWS KMSキーを使用してEBSボリュームの暗号化を行います。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which utilizes Amazon S3 with appropriate encryption and security measures to meet all requirements.",
        "situation_analysis": "The situation requires a storage solution that offers durability, availability, and encryption both in transit and at rest. This implies high reliability and security for document storage.",
        "option_analysis": "Option B effectively meets all specified requirements. Option A focuses on EBS, which although it can be secure, does not align with full-managed services. Option C does not fulfill the requirement for managing encryption keys by the company. Option D uses EC2 which is not a fully managed service compared to S3.",
        "additional_knowledge": "Utilizing Amazon S3 allows for scaling operations without the overhead of managing the infrastructure.",
        "key_terminology": "Amazon S3, AWS KMS, HTTPS, encryption, server-side encryption",
        "overall_assessment": "Based on the requirements stated, option B is the most suitable choice, garnering full community support."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、これはAmazon S3を使用して適切な暗号化とセキュリティ対策を講じることで、すべての要件を満たします。",
        "situation_analysis": "状況は耐久性、可用性、そしてデータを転送中および静止時に暗号化することを提供するストレージソリューションが必要です。これは文書ストレージの高い信頼性とセキュリティを意味します。",
        "option_analysis": "オプションBはすべての指定された要件を効果的に満たします。オプションAはEBSに焦点を当てていますが、安全である一方で、完全に管理されたサービスにはなりません。オプションCは企業が暗号化キーを管理するという要件を満たしていません。オプションDはEC2を使用しており、S3に比べて完全に管理されたサービスとは言えません。",
        "additional_knowledge": "Amazon S3を利用することにより、インフラストラクチャの管理に悩まされることなく、運用をスケーリングすることが可能です。",
        "key_terminology": "Amazon S3, AWS KMS, HTTPS, 暗号化, サーバー側暗号化",
        "overall_assessment": "提示された要件に基づいて、Bオプションは最も適した選択であり、コミュニティからの支持を全て得ています。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "AWS KMS",
      "HTTPS",
      "encryption",
      "server-side encryption"
    ]
  },
  {
    "No": "183",
    "question": "A company's public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application\nLoad Balancer (ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for\nseveral months.\nRecently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection\nattacks had occurred against the API and that the API service had scaled to its maximum amount.\nA solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must\nallow legitimate trafic through and must maximize operational eficiency.\nWhich solution meets these requirements?",
    "question_jp": "ある企業の公開APIは、Amazon Elastic Container Service (Amazon ECS)上でタスクとして実行されています。タスクはAWS Fargate上で実行され、Application Load Balancer (ALB)の背後に配置されており、CPU利用率に基づいてタスクのサービスオートスケーリングが設定されています。このサービスは数ヶ月間順調に稼働していました。しかし最近、APIのパフォーマンスが低下し、アプリケーションが使えなくなりました。企業は、APIに対して大量のSQLインジェクション攻撃が発生し、APIサービスが最大限にスケールされていることを発見しました。ソリューションアーキテクトは、SQLインジェクション攻撃がECS APIサービスに到達するのを防ぐソリューションを実装する必要があります。このソリューションは、正当なトラフィックを通過させる必要があり、運用効率を最大化する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS",
        "text_jp": "ALBの前にあるHTTPリクエストとHTTPSリクエストを監視するために、新しいAWS WAF Web ACLを作成する"
      },
      {
        "key": "B",
        "text": "Create a new AWS WAF Bot Control implementation. Add a rule in the AWS WAF Bot Control managed rule group to monitor trafic and allow",
        "text_jp": "新しいAWS WAF Bot Control実装を作成する。トラフィックを監視し、許可するためのルールをAWS WAF Bot Control管理ルールグループに追加する"
      },
      {
        "key": "C",
        "text": "Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all",
        "text_jp": "新しいAWS WAF Web ACLを作成する。SQLデータベースルールグループに一致するリクエストをブロックする新しいルールを追加する。Web ACLをすべてのリクエストを許可するように設定する"
      },
      {
        "key": "D",
        "text": "Create a new AWS WAF web ACL. Create a new empty IP set in AWS WAF. Add a new rule to the web ACL to block requests that originate",
        "text_jp": "新しいAWS WAF Web ACLを作成する。AWS WAFで新しい空のIPセットを作成する。リクエストをブロックするための新しいルールをWeb ACLに追加する"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which specifies creating an AWS WAF web ACL and adding a rule that blocks SQL injection attempts while allowing legitimate requests.",
        "situation_analysis": "The API has been under SQL injection attacks resulting in performance issues. A solution is required to prevent these attacks without affecting legitimate traffic.",
        "option_analysis": "Option C correctly identifies the need for an AWS WAF and specifically targets SQL injection risks. Option A lacks specificity about SQL, whereas B focuses on bot control, which may not directly block SQL injections. Option D discusses an IP set without focusing on SQL injection.",
        "additional_knowledge": "",
        "key_terminology": "AWS WAF, Web ACL, SQL Injection, managed rule groups",
        "overall_assessment": "Answer C is the best approach that directly addresses the SQL injection threat and allows legitimate traffic, aligning with AWS security best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCであり、AWS WAF Web ACLを作成し、SQLインジェクションの試行をブロックし、正当なリクエストを許可するルールを追加することを示しています。",
        "situation_analysis": "APIがSQLインジェクション攻撃を受けており、パフォーマンスの問題が発生しています。これらの攻撃を防ぎつつ、正当なトラフィックには影響を与えない解決策が必要です。",
        "option_analysis": "選択肢CはAWS WAFの必要性を正しく特定し、SQLインジェクションのリスクに特化しています。選択肢AはSQLに関する特異性に欠け、選択肢Bはボット制御に焦点を当てており、SQLインジェクションを直接ブロックするものではありません。選択肢DはIPセットについて述べており、SQLインジェクションに焦点を当てていません。",
        "additional_knowledge": "",
        "key_terminology": "AWS WAF, Web ACL, SQLインジェクション, 管理ルールグループ",
        "overall_assessment": "選択肢Cは、SQLインジェクションの脅威に直接対処し、正当なトラフィックを許可する最良のアプローチであり、AWSのセキュリティベストプラクティスにも一致します。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "Web ACL",
      "SQL Injection",
      "Managed Rule Groups"
    ]
  },
  {
    "No": "184",
    "question": "An environmental company is deploying sensors in major cities throughout a country to measure air quality. The sensors connect to AWS IoT Core\nto ingest timeseries data readings. The company stores the data in Amazon DynamoDB.\nFor business continuity, the company must have the ability to ingest and store data in two AWS Regions.\nWhich solution will meet these requirements?",
    "question_jp": "環境保護企業が、全国の主要都市にセンサーを設置して空気品質を測定しています。センサーは、時系列データの読取を取り込むためにAWS IoT Coreに接続されます。企業はデータをAmazon DynamoDBに保存します。業務継続性のために、企業は2つのAWSリージョンでデータを取り込んで保存する能力を持たなければなりません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core data endpoints in both Regions Migrate data to",
        "text_jp": "Amazon Route 53のエイリアスフェイルオーバールーティングポリシーを作成し、両方のリージョンのAWS IoT Coreデータエンドポイントの値を設定する。データを移行する。"
      },
      {
        "key": "B",
        "text": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT",
        "text_jp": "各リージョンに対してAWS IoT Coreのドメイン設定を作成する。Amazon Route 53のレイテンシーベースのルーティングポリシーを作成する。AWS IoTを使用する。"
      },
      {
        "key": "C",
        "text": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain",
        "text_jp": "各リージョンに対してAWS IoT Coreのドメイン設定を作成する。ドメインを評価するAmazon Route 53のヘルスチェックを作成する。"
      },
      {
        "key": "D",
        "text": "Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Configure",
        "text_jp": "Amazon Route 53のレイテンシーベースのルーティングポリシーを作成する。両方のリージョンのAWS IoT Coreデータエンドポイントを値として使用する。設定する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. The solution includes creating a domain configuration for AWS IoT Core in each Region and implementing an Amazon Route 53 health check.",
        "situation_analysis": "The environmental company's requirements specify that data ingestion and storage must occur in two AWS Regions for business continuity.",
        "option_analysis": "Option C is correct as it directly addresses the need for health checks, which ensures that if one region fails, the other can continue operations. Other options either do not address both regions adequately or do not implement health checks.",
        "additional_knowledge": "It is important to understand the various routing policies of Amazon Route 53 and their specific use cases.",
        "key_terminology": "Amazon Route 53, AWS IoT Core, health check, latency-based routing policy, DynamoDB",
        "overall_assessment": "This question effectively tests knowledge of AWS services for ensuring data availability and business continuity across multiple regions. The community largely supports option C."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。このソリューションには、各リージョンに対してAWS IoT Coreのドメイン設定を作成し、Amazon Route 53のヘルスチェックを実装することが含まれている。",
        "situation_analysis": "環境保護企業の要件は、業務継続性のためにデータ取り込みと保存が2つのAWSリージョンで発生する必要があることを指定している。",
        "option_analysis": "選択肢Cは、ヘルスチェックの必要性に直接対応しているため正しい。これにより、いずれかのリージョンが失敗した場合に他のリージョンが稼働を続けることができる。他の選択肢は、両方のリージョンを適切に扱っていないか、ヘルスチェックを実施していない。",
        "additional_knowledge": "Amazon Route 53のさまざまなルーティングポリシーとその特定のユースケースを理解することが重要である。",
        "key_terminology": "Amazon Route 53, AWS IoT Core, ヘルスチェック, レイテンシーベースのルーティングポリシー, DynamoDB",
        "overall_assessment": "この質問は、複数のリージョン間でのデータの可用性と業務継続性を確保するためのAWSサービスに関する知識を効果的にテストしている。コミュニティは主に選択肢Cを支持している。"
      }
    ],
    "keywords": [
      "Amazon Route 53",
      "AWS IoT Core",
      "health check",
      "latency-based routing policy",
      "DynamoDB"
    ]
  },
  {
    "No": "185",
    "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application\nthat uses AWS Lambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table.\nThe DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table.\nThe finance team and the marketing team have separate AWS accounts.\nWhat should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?",
    "question_jp": "会社はAWS Organizationsを使用してAWSクラウド内のマルチアカウント設定を構成しています。会社の財務チームは、AWS LambdaとAmazon DynamoDBを使用したデータ処理アプリケーションを運用しています。会社のマーケティングチームは、DynamoDBテーブルに格納されているデータにアクセスしたいと考えています。このDynamoDBテーブルには機密データが含まれており、マーケティングチームはDynamoDBテーブル内の特定の属性のデータにのみアクセスできます。財務チームとマーケティングチームは別々のAWSアカウントを持っています。ソリューションアーキテクトは、マーケティングチームにDynamoDBテーブルへの適切なアクセスを提供するために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an SCP to grant the marketing team's AWS account access to the specific attributes of the DynamoDB table. Attach the SCP to the",
        "text_jp": "マーケティングチームのAWSアカウントにDynamoDBテーブルの特定の属性へのアクセスを付与するSCPを作成する。SCPを"
      },
      {
        "key": "B",
        "text": "Create an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access",
        "text_jp": "特定のDynamoDB属性に対してIAMポリシー条件を用いて財務チームのアカウントにIAMロールを作成する（きめ細やかなアクセス制御）"
      },
      {
        "key": "C",
        "text": "Create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). Attach the",
        "text_jp": "特定のDynamoDB属性の条件を含むリソースベースのIAMポリシーを作成する（きめ細やかなアクセス制御）。これを加える"
      },
      {
        "key": "D",
        "text": "Create an IAM role in the finance team's account to access the DynamoDB table. Use an IAM permissions boundary to limit the access to",
        "text_jp": "DynamoDBテーブルへのアクセスのために財務チームのアカウントにIAMロールを作成する。IAM権限境界を使用してアクセスを制限する"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating an IAM role in the finance team's account with IAM policy conditions allows for fine-grained access control to specific DynamoDB attributes.",
        "situation_analysis": "The marketing team requires selective access to confidential data stored in the DynamoDB table while ensuring compliance with security protocols. The finance team manages the relevant data and has an AWS account separate from the marketing team.",
        "option_analysis": "Option A is incorrect because Service Control Policies (SCPs) do not provide attribute-level access control. Option C offers a resource-based approach but may not apply cleanly between accounts without cross-account role assumption. Option D fails to address the requirement of specific attribute-level access effectively.",
        "additional_knowledge": "IAM roles and policies are essential for managing access in complex AWS environments.",
        "key_terminology": "IAM role, policy conditions, fine-grained access, cross-account access, security compliance",
        "overall_assessment": "Answer B is strongly supported by AWS best practices for security and access management, aligning with organizational requirements to safeguard confidential data while enabling necessary access. The community's strong support for this option aligns with its appropriateness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。財務チームのアカウントに特定のDynamoDB属性に対するIAMポリシー条件を用いてIAMロールを作成することで、きめ細やかなアクセス制御が可能となる。",
        "situation_analysis": "マーケティングチームはDynamoDBテーブルに格納されている機密データへの選択的アクセスを必要としており、セキュリティプロトコルの遵守も求められている。財務チームは関連データを管理し、マーケティングチームとは別のAWSアカウントを持っている。",
        "option_analysis": "選択肢Aは、サービスコントロールポリシー（SCP）が属性レベルのアクセス制御を提供しないため不正解である。選択肢Cはリソースベースのアプローチを提供するが、アカウント間でのロールの引き受けがないと適切に機能しない可能性がある。選択肢Dは、特定の属性レベルのアクセス要件に効果的に対応していない。",
        "additional_knowledge": "IAMロールやポリシーは、複雑なAWS環境でのアクセス管理において不可欠である。",
        "key_terminology": "IAMロール、ポリシー条件、きめ細やかなアクセス、クロスアカウントアクセス、セキュリティコンプライアンス",
        "overall_assessment": "選択肢Bは、AWSのセキュリティとアクセス管理に関するベストプラクティスを強く支持されており、機密データを保護しつつ必要なアクセスを有効にするという組織の要件と一致している。コミュニティによる強力な支持は、この選択肢が適切であることを示している。"
      }
    ],
    "keywords": [
      "IAM role",
      "policy conditions",
      "fine-grained access",
      "cross-account access",
      "security compliance"
    ]
  },
  {
    "No": "186",
    "question": "A solutions architect is creating an application that stores objects in an Amazon S3 bucket. The solutions architect must deploy the application in\ntwo AWS Regions that will be used simultaneously. The objects in the two S3 buckets must remain synchronized with each other.\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "question_jp": "ソリューションアーキテクトは、Amazon S3バケットにオブジェクトを保存するアプリケーションを作成しています。ソリューションアーキテクトは、同時に使用される2つのAWSリージョンにアプリケーションを展開する必要があります。2つのS3バケット内のオブジェクトは、互いに同期されたままでなければなりません。この要件を運用上の負荷を最小限にする方法で満たすための手順の組み合わせはどれですか？（3つ選んでください）",
    "choices": [
      {
        "key": "A",
        "text": "Create an S3 Multi-Region Access Point Change the application to refer to the Multi-Region Access Point",
        "text_jp": "S3マルチリージョンアクセスポイントを作成し、アプリケーションをマルチリージョンアクセスポイントを参照するように変更します"
      },
      {
        "key": "B",
        "text": "Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets",
        "text_jp": "2つのS3バケット間で双方向のS3クロスリージョンレプリケーション（CRR）を構成します"
      },
      {
        "key": "C",
        "text": "Modify the application to store objects in each S3 bucket",
        "text_jp": "アプリケーションを変更して、各S3バケットにオブジェクトを保存します"
      },
      {
        "key": "D",
        "text": "Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket",
        "text_jp": "各S3バケットにライフサイクルルールを作成して、1つのS3バケットから別のS3バケットにオブジェクトをコピーします"
      },
      {
        "key": "E",
        "text": "Enable S3 Versioning for each S3 bucket",
        "text_jp": "各S3バケットでS3バージョニングを有効にします"
      },
      {
        "key": "F",
        "text": "Configure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3",
        "text_jp": "各S3バケットにイベント通知を設定して、AWS Lambda関数を呼び出してオブジェクトを1つのS3バケットから別のS3バケットにコピーします"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ABE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating an S3 Multi-Region Access Point significantly simplifies the process of accessing S3 data across multiple regions with minimal operational overhead.",
        "situation_analysis": "The requirement is to maintain synchronization of objects in S3 buckets across two AWS Regions with minimal operational overhead.",
        "option_analysis": "Option A allows for a single entry point for accessing objects in S3 across regions. Option B introduces complexity with bi-directional replication and increased costs. Option C requires manual management for storage in both buckets. Options D and F also introduce operational burdens that could complicate synchronization.",
        "additional_knowledge": "In practice, utilizing an S3 Multi-Region Access Point leverages AWS's global infrastructure efficiently.",
        "key_terminology": "Multi-Region Access Point, S3 Cross-Region Replication, S3 Versioning",
        "overall_assessment": "The question effectively tests knowledge about AWS services and the best practices for minimizing operational overhead in multi-region configurations. Community votes support this correct answer, aligning with intended AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。S3マルチリージョンアクセスポイントを作成することで、運用上の負荷を最小限に抑えながら、複数のリージョン間でS3データにアクセスするプロセスが大幅に簡素化されます。",
        "situation_analysis": "要件は、運用上の負荷を最小限にして、2つのAWSリージョンにわたってS3バケットのオブジェクトを同期させることです。",
        "option_analysis": "オプションAは、リージョンをまたいだS3オブジェクトへのアクセスのための単一の入口を提供します。オプションBは双方向レプリケーションを採用し、複雑さとコストが増加します。オプションCは、両方のバケットにおけるストレージ管理を手動で行う必要があります。オプションDとFも、同期を複雑にする運用上の負担をもたらします。",
        "additional_knowledge": "実際には、S3マルチリージョンアクセスポイントを利用することで、AWSのグローバルインフラストラクチャを効率的に活用できます。",
        "key_terminology": "マルチリージョンアクセスポイント、S3クロスリージョンレプリケーション、S3バージョニング",
        "overall_assessment": "この質問はAWSサービスに関する知識およびマルチリージョン構成における運用上の負荷を最小限に抑えるためのベストプラクティスを試す良質なものです。コミュニティ投票はこの正しい答えを支持しており、意図されたAWSのベストプラクティスと一致しています。"
      }
    ],
    "keywords": [
      "Multi-Region Access Point",
      "Cross-Region Replication",
      "S3 Versioning"
    ]
  },
  {
    "No": "187",
    "question": "A company has an IoT platform that runs in an on-premises environment. The platform consists of a server that connects to IoT devices by using\nthe MQTT protocol. The platform collects telemetry data from the devices at least once every 5 minutes. The platform also stores device\nmetadata in a MongoDB cluster.\nAn application that is installed on an on-premises machine runs periodic jobs to aggregate and transform the telemetry and device metadata. The\napplication creates reports that users view by using another web application that runs on the same on-premises machine. The periodic jobs take\n120-600 seconds to run. However, the web application is always running.\nThe company is moving the platform to AWS and must reduce the operational overhead of the stack.\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "question_jp": "ある企業には、オンプレミス環境で稼働しているIoTプラットフォームがあります。このプラットフォームは、MQTTプロトコルを使用してIoTデバイスに接続するサーバーで構成されています。プラットフォームは、デバイスからのテレメトリーデータを少なくとも5分ごとに収集します。また、プラットフォームはデバイスのメタデータをMongoDBクラスタに保存します。オンプレミスのコンピュータにインストールされたアプリケーションは、テレメトリーおよびデバイスメタデータを集約および変換する定期的なジョブを実行します。このアプリケーションは、同じオンプレミスのコンピュータ上で実行される別のウェブアプリケーションによってユーザーが確認するレポートを生成します。定期的なジョブの実行には120-600秒かかりますが、ウェブアプリケーションは常に稼働しています。企業はプラットフォームをAWSに移行して、スタックの運用オーバーヘッドを削減する必要があります。どのステップの組み合わせが、最も運用オーバーヘッドを少なくしてこれらの要件を満たすでしょうか？ (3つ選択してください)",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Lambda functions to connect to the IoT devices",
        "text_jp": "AWS Lambda関数を利用してIoTデバイスに接続する"
      },
      {
        "key": "B",
        "text": "Configure the IoT devices to publish to AWS IoT Core",
        "text_jp": "IoTデバイスをAWS IoT Coreに公開するように設定する"
      },
      {
        "key": "C",
        "text": "Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance",
        "text_jp": "Amazon EC2インスタンス上の自己管理MongoDBデータベースにメタデータを書き込む"
      },
      {
        "key": "D",
        "text": "Write the metadata to Amazon DocumentDB (with MongoDB compatibility)",
        "text_jp": "Amazon DocumentDB（MongoDB互換）にメタデータを書き込む"
      },
      {
        "key": "E",
        "text": "Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use",
        "text_jp": "AWS Step Functionsのステートマシンを使用してレポートを準備し、Amazon S3に書き込む"
      },
      {
        "key": "F",
        "text": "Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress",
        "text_jp": "Amazon EKSクラスターとAmazon EC2インスタンスを使用してレポートを準備する"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BDE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is 'B'. By configuring the IoT devices to publish to AWS IoT Core, the connection and telemetry ingestion process is managed by AWS, which reduces operational overhead.",
        "situation_analysis": "The company currently runs an IoT platform on-premises, requiring substantial maintenance. They need to reduce operational overhead while migrating to AWS.",
        "option_analysis": "Option B allows devices to directly integrate with AWS IoT Core, simplifying device management and telemetry ingestion. Option D could be considered, but it would maintain some level of operational overhead due to self-management.",
        "additional_knowledge": "In addition to option B, using AWS Lambda or Step Functions could further automate processes, but the question specifically asks for the least operational overhead.",
        "key_terminology": "IoT Core, telemetry, DocumentDB, operational overhead, managed service",
        "overall_assessment": "Selecting option B is crucial for reducing management complexity and improving efficiency in the AWS ecosystem. The community votes confirm this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は「B」である。IoTデバイスをAWS IoT Coreに公開するように設定することにより、接続およびテレメトリーデータの取り込みプロセスがAWSによって管理され、運用オーバーヘッドが削減される。",
        "situation_analysis": "現在企業はプラットフォームをオンプレミス上で運営しており、多大なメンテナンスが必要である。AWSへの移行時に運用オーバーヘッドの削減が求められている。",
        "option_analysis": "オプションBは、デバイスがAWS IoT Coreと直接統合されることを可能にし、デバイス管理とテレメトリーの取り込みを簡素化する。オプションDも考慮できるが、自己管理による運用オーバーヘッドを維持することになる。",
        "additional_knowledge": "オプションBに加えて、AWS LambdaやStep Functionsを使用することでプロセスをさらに自動化できるが、この質問では最も運用オーバーヘッドの少ないものを求めている。",
        "key_terminology": "IoT Core, テレメトリー, DocumentDB, 運用オーバーヘッド, マネージドサービス",
        "overall_assessment": "オプションBを選択することは、AWSエコシステム内での管理の複雑さを削減し、効率を高めるために重要である。コミュニティの投票もこの選択を確認している。"
      }
    ],
    "keywords": [
      "AWS IoT Core",
      "telemetry",
      "DocumentDB",
      "operational overhead",
      "managed service"
    ]
  },
  {
    "No": "188",
    "question": "A global manufacturing company plans to migrate the majority of its applications to AWS. However, the company is concerned about applications\nthat need to remain within a specific country or in the company's central on-premises data center because of data regulatory requirements or\nrequirements for latency of single-digit milliseconds. The company also is concerned about the applications that it hosts in some of its factory\nsites, where limited network infrastructure exists.\nThe company wants a consistent developer experience so that its developers can build applications once and deploy on premises, in the cloud, or\nin a hybrid architecture. The developers must be able to use the same tools, APIs, and services that are familiar to them.\nWhich solution will provide a consistent hybrid experience to meet these requirements?",
    "question_jp": "グローバルな製造企業は、アプリケーションの大部分をAWSに移行する計画を立てている。しかし、同社は特定の国内または中央のオンプレミスデータセンター内に残しておく必要があるアプリケーションについて、データ規制要件やミリ秒単位の単一遅延に関する要件から懸念している。また、工場の一部にホストされているアプリケーションについても、限られたネットワークインフラが存在するため懸念している。同社は、開発者が一度アプリケーションを構築し、オンプレミス、クラウド、またはハイブリッドアーキテクチャで展開できるよう、一貫した開発者体験を求めている。開発者は、自身に馴染みのある同じツール、API、およびサービスを使用できる必要がある。この要件を満たす一貫したハイブリッド体験を提供するソリューションはどれか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on-",
        "text_jp": "すべてのアプリケーションを準拠している最寄りのAWSリージョンに移行し、中央のオンプレミス間にAWS Direct Connect接続を設定する。"
      },
      {
        "key": "B",
        "text": "Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency",
        "text_jp": "データ規制要件や遅延要件のあるアプリケーションには、AWS Snowball Edge Storage Optimizedデバイスを使用する。"
      },
      {
        "key": "C",
        "text": "Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds.",
        "text_jp": "データ規制要件やミリ秒単位の遅延要件があるアプリケーションには、AWS Outpostsをインストールする。"
      },
      {
        "key": "D",
        "text": "Migrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local",
        "text_jp": "データ規制要件やミリ秒単位の遅延要件があるアプリケーションをAWS Localに移行する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (86%) 14%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds.",
        "situation_analysis": "The company has specific applications that cannot leave the country due to regulatory compliance and requires low latency. AWS Outposts can provide a seamless hybrid cloud experience by allowing these applications to run on-premises with the same infrastructure as AWS cloud.",
        "option_analysis": "Option A may not allow certain applications to remain on-premises. Option B, while useful for data transfer, does not provide a solution for low-latency applications. Option D does not solve the requirements for applications needing to stay on-premises.",
        "additional_knowledge": "AWS Outposts allows businesses to extend their AWS infrastructure and services on-premises, making it suitable for businesses needing hybrid architecture.",
        "key_terminology": "AWS Outposts, hybrid cloud, low latency, regulatory compliance, on-premises infrastructure.",
        "overall_assessment": "This question effectively identifies the necessary requirements for a company's hybrid architecture and assesses the solutions offered. The community vote strongly aligns with the correct answer, highlighting the preference for AWS Outposts as a solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはC: データ規制要件やミリ秒単位の遅延要件があるアプリケーションには、AWS Outpostsをインストールすることです。",
        "situation_analysis": "この企業には、規制上の理由から国を出ることができない特定のアプリケーションがあり、低遅延も必要です。AWS Outpostsは、これらのアプリケーションをAWSクラウドと同じインフラでオンプレミスで実行できることから、シームレスなハイブリッドクラウド体験を提供します。",
        "option_analysis": "選択肢Aは、特定のアプリケーションがオンプレミスに留まることを許可しない可能性があります。選択肢Bはデータ転送には有用ですが、低遅延アプリケーションの解決にはなりません。選択肢Dは、オンプレミスに留める必要があるアプリケーションの要件には合致していません。",
        "additional_knowledge": "AWS Outpostsは、企業がAWSのインフラとサービスをオンプレミスに拡張できるようにするため、ハイブリッドアーキテクチャを必要とする企業に適しています。",
        "key_terminology": "AWS Outposts、ハイブリッドクラウド、低遅延、規制準拠、オンプレミスインフラ。",
        "overall_assessment": "この質問は、企業のハイブリッドアーキテクチャに必要な要件を明確に特定しており、提供されたソリューションを評価しています。コミュニティの投票は、AWS Outpostsを解決策として好む傾向がある正しい答えに強く一致しています。"
      }
    ],
    "keywords": [
      "AWS Outposts",
      "hybrid cloud",
      "low latency",
      "regulatory compliance",
      "on-premises infrastructure"
    ]
  },
  {
    "No": "189",
    "question": "A company is updating an application that customers use to make online orders. The number of attacks on the application by bad actors has\nincreased recently.\nThe company will host the updated application on an Amazon Elastic Container Service (Amazon ECS) cluster. The company will use Amazon\nDynamoDB to store application data. A public Application Load Balancer (ALB) will provide end users with access to the application. The company\nmust prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "question_jp": "顧客がオンライン注文を行うために使用するアプリケーションの更新を行っている会社がある。最近、悪意のある行為者によるアプリケーションへの攻撃が増加している。この会社は、更新されたアプリケーションをAmazon Elastic Container Service（Amazon ECS）クラスター上にホストし、アプリケーションデータをAmazon DynamoDBに保存することにした。パブリックApplication Load Balancer（ALB）がエンドユーザーにアプリケーションへのアクセスを提供する。会社は攻撃を防ぎ、継続的な攻撃の最中に最小限のサービス中断でビジネスの継続性を確保しなければならない。これらの要件を最もコスト効果高く満たす手順の組み合わせはどれか。（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain.",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、ALBをオリジンとして設定する。CloudFrontドメインにカスタムヘッダーとランダムな値を追加する。"
      },
      {
        "key": "B",
        "text": "Deploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight.",
        "text_jp": "アプリケーションを2つのAWSリージョンにデプロイする。Amazon Route 53を設定して、両方のリージョンに同じ重みでルーティングする。"
      },
      {
        "key": "C",
        "text": "Configure auto scaling for Amazon ECS tasks Create a DynamoDB Accelerator (DAX) cluster.",
        "text_jp": "Amazon ECSタスクの自動スケーリングを構成し、DynamoDB Accelerator（DAX）クラスターを作成する。"
      },
      {
        "key": "D",
        "text": "Configure Amazon ElastiCache to reduce overhead on DynamoDB.",
        "text_jp": "Amazon ElastiCacheを設定して、DynamoDBのオーバーヘッドを削減する。"
      },
      {
        "key": "E",
        "text": "Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution.",
        "text_jp": "AWS WAFウェブACLをデプロイし、適切なルールグループを含める。ウェブACLをAmazon CloudFrontディストリビューションに関連付ける。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and E.",
        "situation_analysis": "The company faces increased attacks on its application and needs to ensure  business continuity while minimizing service interruptions.",
        "option_analysis": "Option A helps improve security by using CloudFront as a content delivery network (CDN) to mitigate attacks before they reach the ALB. Option E implements AWS WAF to create web access control lists that selectively allow or block traffic, enhancing overall security.",
        "additional_knowledge": "It's also important to ensure that both solutions are correctly configured to balance cost efficiency with security and performance.",
        "key_terminology": "Amazon CloudFront, AWS WAF, Application Load Balancer, Denial of Service protection, content delivery network.",
        "overall_assessment": "While options B, C, and D provide some level of redundancy and performance enhancement, they do not directly address the immediate security needs as effectively as options A and E."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAとEである。",
        "situation_analysis": "会社はアプリケーションに対する攻撃の増加に直面しており、サービスの中断を最小限に抑えながらビジネスの継続性を確保する必要がある。",
        "option_analysis": "選択肢Aは、CloudFrontをコンテンツ配信ネットワーク（CDN）として使用し、攻撃がALBに到達する前に軽減することでセキュリティを向上させる。選択肢EはAWS WAFを実装して、トラフィックを選択的に許可またはブロックするウェブアクセス制御リストを作成し、全体的なセキュリティを強化する。",
        "additional_knowledge": "両方のソリューションがコスト効率とセキュリティおよびパフォーマンスのバランスを取るように正しく設定されていることを確認することも重要である。",
        "key_terminology": "Amazon CloudFront, AWS WAF, Application Load Balancer, DDoS保護, コンテンツ配信ネットワーク。",
        "overall_assessment": "選択肢B、C、Dは冗長性やパフォーマンスの向上を提供するが、AとEの選択肢ほど効果的に直面するセキュリティニーズに対処しない。"
      }
    ],
    "keywords": [
      "Amazon CloudFront",
      "AWS WAF",
      "Application Load Balancer",
      "DDoS protection",
      "content delivery network"
    ]
  },
  {
    "No": "190",
    "question": "A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon\nCloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fieet of\nAmazon EC2 instances in Auto Scaling groups. The application uses a domain name setup in Amazon Route 53.\nSome users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the\nALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors\noccur. The page should be displayed immediately for this error code.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がAWS上でウェブアプリケーションを運営しています。このウェブアプリケーションは、Amazon S3バケットから静的コンテンツを配信しており、Amazon CloudFrontディストリビューションの背後に配置されています。アプリケーションは、アプリケーションロードバランサー（ALB）を使用して動的コンテンツを配信し、自動スケーリンググループ内のAmazon EC2インスタンスにリクエストを分散しています。アプリケーションは、Amazon Route 53でセットアップされたドメイン名を使用しています。一部のユーザーは、ピーク時間中にウェブサイトにアクセスしようとした際に問題が発生したと報告しました。運用チームは、ALBが時々HTTP 503 Service Unavailableエラーを返すことを発見しました。同社は、これらのエラーが発生した際にカスタムエラーメッセージページを表示したいと考えています。このページは、エラーコードの発生時に直ちに表示される必要があります。最も運用上のオーバーヘッドが少ない要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the",
        "text_jp": "Route 53フェイルオーバールーティングポリシーを設定します。ALBエンドポイントのステータスを判断するためにヘルスチェックを構成し、フェイルオーバーします。"
      },
      {
        "key": "B",
        "text": "Create a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy.",
        "text_jp": "カスタムエラーページをホストするために、2つ目のCloudFrontディストリビューションとS3静的ウェブサイトを作成します。Route 53フェイルオーバールーティングポリシーを設定します。"
      },
      {
        "key": "C",
        "text": "Create a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket",
        "text_jp": "2つのオリジンを持つCloudFrontオリジングループを作成します。ALBエンドポイントをプライマリオリジンとして設定します。セカンダリオリジンにはS3バケットを設定します。"
      },
      {
        "key": "D",
        "text": "Create a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket.",
        "text_jp": "ALBが返す各HTTPレスポンスコードを検証するCloudFrontファンクションを作成します。S3バケットにS3静的ウェブサイトを作成します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. By creating a CloudFront origin group with ALB as the primary origin and an S3 bucket as the secondary origin, AWS can automatically redirect requests to the S3 bucket when the ALB returns a 503 error.",
        "situation_analysis": "The company is experiencing backend issues with the ALB that sometimes results in a 503 error. A seamless solution that requires the least operational overhead and provides a custom error message is needed.",
        "option_analysis": "Option A involves Route 53 which adds complexity and does not directly provide a custom error message. Option B also increases complexity by creating a separate CloudFront distribution. Option D creates a CloudFront function, but managing functions can introduce additional operational overhead. Option C efficiently handles the scenario by leveraging CloudFront's origin failover capabilities.",
        "additional_knowledge": "Using an S3 bucket for static error pages is a common practice to improve user experience when backend issues occur.",
        "key_terminology": "CloudFront, Application Load Balancer, origin group, HTTP 503, S3 bucket",
        "overall_assessment": "Overall, option C is the most straightforward solution, fulfilling the requirement with minimal additional management while providing the necessary custom error handling. The community overwhelmingly supports this solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。ALBをプライマリオリジンとして、S3バケットをセカンダリオリジンとするCloudFrontオリジングループを作成することで、ALBが503エラーを返した際に自動的にS3バケットへのリクエストをリダイレクトできる。",
        "situation_analysis": "この企業は、時折ALBのバックエンドで503エラーが発生し、これにより問題が生じている。最小の運用オーバーヘッドでカスタムエラーメッセージを提供するシームレスなソリューションが求められている。",
        "option_analysis": "選択肢AはRoute 53を関与させ、複雑さを加える上、カスタムエラーメッセージを直接提供するものではない。選択肢Bは別のCloudFrontディストリビューションを作成し、複雑さを増す。選択肢DはCloudFrontファンクションを作成するが、ファンクションの管理は追加の運用オーバーヘッドをもたらす可能性がある。選択肢CはCloudFrontのオリジンフェイルオーバー機能を利用することで、このシナリオを効率的に処理する。",
        "additional_knowledge": "静的エラーページにS3バケットを使用することは、バックエンドの問題が発生した際のユーザー体験を向上させる一般的な実践である。",
        "key_terminology": "CloudFront、アプリケーションロードバランサー、オリジングループ、HTTP 503、S3バケット",
        "overall_assessment": "総合的に見て、選択肢Cは最も単純な解決策であり、最小の追加管理で必要なカスタムエラーハンドリングを提供する要件を満たしている。コミュニティからの支持も圧倒的である。"
      }
    ],
    "keywords": [
      "CloudFront",
      "Application Load Balancer",
      "origin group",
      "HTTP 503",
      "S3 bucket"
    ]
  },
  {
    "No": "191",
    "question": "A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.\nA solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the\nunderlying infrastructure.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がアプリケーションをAWSに移行することを計画しています。このアプリケーションはDockerコンテナとして実行され、NFSバージョン4のファイル共有を使用しています。ソリューションアーキテクトは、基盤となるインフラストラクチャのプロビジョニングや管理を必要としない、安全でスケーラブルなコンテナ化されたソリューションを設計しなければなりません。どのソリューションがこれらの要件を満たすことができますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon",
        "text_jp": "Amazon Elastic Container Service (Amazon ECS) を Fargate 起動タイプを利用してアプリケーションコンテナをデプロイします。Amazon"
      },
      {
        "key": "B",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx",
        "text_jp": "Amazon Elastic Container Service (Amazon ECS) を Fargate 起動タイプを利用してアプリケーションコンテナをデプロイします。Amazon FSx"
      },
      {
        "key": "C",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto",
        "text_jp": "Amazon Elastic Container Service (Amazon ECS) を Amazon EC2 起動タイプを利用してアプリケーションコンテナをデプロイし、自動"
      },
      {
        "key": "D",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto",
        "text_jp": "Amazon Elastic Container Service (Amazon ECS) を Amazon EC2 起動タイプを利用してアプリケーションコンテナをデプロイし、自動"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Utilizing Amazon ECS with the Fargate launch type enables serverless deployment, which fits the requirement of not managing underlying infrastructure.",
        "situation_analysis": "The application runs in Docker containers and needs to migrate without significant changes to its architecture or the need for infrastructure management.",
        "option_analysis": "A is correct as Fargate supports serverless container deployments; B incorrectly suggests using FSx, which is not optimal for NFS shares in this context. C and D require EC2 management, which is against the stated requirements.",
        "additional_knowledge": "Utilizing Fargate simplifies deployment intricacies as it abstracts away infrastructure concerns.",
        "key_terminology": "Amazon ECS, Fargate, NFS, Docker Containers, Serverless Computing",
        "overall_assessment": "The question effectively assesses knowledge of AWS services related to container management. The unanimous community vote aligns with the expert recommendation for serverless solutions in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。Amazon ECSをFargate起動タイプで利用することで、インフラストラクチャを管理せずにサーバーレスなデプロイメントを可能にするため、必要な要件に適合する。",
        "situation_analysis": "アプリケーションはDockerコンテナで実行されており、アーキテクチャに大きな変更を加えず、インフラストラクチャ管理なしで移行する必要がある。",
        "option_analysis": "Aが正しい。Fargateはサーバーレスのコンテナデプロイメントをサポートする。一方、BはFSxの使用を示唆するが、NFS共有に最適ではない。CとDはEC2管理が必要であり、要求に反する。",
        "additional_knowledge": "Fargateを利用することで、インフラストラクチャに関する懸念を抽象化し、デプロイメントの複雑さを軽減する。",
        "key_terminology": "Amazon ECS, Fargate, NFS, Dockerコンテナ, サーバーレスコンピューティング",
        "overall_assessment": "この質問は、コンテナ管理に関連するAWSサービスについての知識を効果的に評価している。全体的なコミュニティの投票は、このシナリオにおけるサーバーレスソリューションの専門的な勧告と一致している。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "Fargate",
      "NFS",
      "Docker Containers",
      "Serverless Computing"
    ]
  },
  {
    "No": "192",
    "question": "A company is running an application in the AWS Cloud. The core business logic is running on a set of Amazon EC2 instances in an Auto Scaling\ngroup. An Application Load Balancer (ALB) distributes trafic to the EC2 instances. Amazon Route 53 record api.example.com is pointing to the\nALB.\nThe company's development team makes major updates to the business logic. The company has a rule that when changes are deployed, only 10%\nof customers can receive the new logic during a testing window. A customer must use the same version of the business logic during the testing\nwindow.\nHow should the company deploy the updates to meet these requirements?",
    "question_jp": "ある企業がAWSクラウドでアプリケーションを運用しています。コアビジネスロジックは、Auto Scalingグループ内の一連のAmazon EC2インスタンス上で実行されています。Application Load Balancer（ALB）は、EC2インスタンスにトラフィックを分配します。Amazon Route 53のレコードapi.example.comは、ALBを指しています。この企業の開発チームは、ビジネスロジックに大規模な更新を行います。変更がデプロイされる際には、10％の顧客のみが新しいロジックを受け取ることができるという規則があります。顧客は、テストウィンドウ中にビジネスロジックの同じバージョンを使用する必要があります。企業は、これらの要件を満たすために、どのように更新をデプロイするべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling group. Configure the ALB to distribute trafic",
        "text_jp": "2つ目のALBを作成し、新しいロジックを新しいAuto Scalingグループ内のEC2インスタンスにデプロイします。ALBをトラフィックを分配するように設定します。"
      },
      {
        "key": "B",
        "text": "Create a second target group that is referenced by the ALDeploy the new logic to EC2 instances in this new target group. Update the ALB",
        "text_jp": "ALBが参照する2つ目のターゲットグループを作成し、この新しいターゲットグループにEC2インスタンスに新しいロジックをデプロイします。ALBを更新します。"
      },
      {
        "key": "C",
        "text": "Create a new launch configuration for the Auto Scaling group. Specify the launch configuration to use the AutoScalingRollingUpdate policy,",
        "text_jp": "Auto Scalingグループのために新しいローンチ構成を作成します。AutoScalingRollingUpdateポリシーを使用するようにローンチ構成を指定します。"
      },
      {
        "key": "D",
        "text": "Create a second Auto Scaling group that is referenced by the ALB. Deploy the new logic on a set of EC2 instances in this new Auto Scaling",
        "text_jp": "ALBが参照する2つ目のAuto Scalingグループを作成し、この新しいAuto Scalingグループ内のEC2インスタンスに新しいロジックをデプロイします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating a second target group and deploying the new logic to it allows for controlled traffic routing during testing.",
        "situation_analysis": "The scenario requires deploying updates to an application while ensuring that only a portion (10%) of customers receive the new logic. The goal is to ensure that all customers who are in the testing window use the same version of the business logic.",
        "option_analysis": "Option A would create a second ALB, which is unnecessary overhead. Option C does not address the need for partitioning traffic. Option D involves multiple Auto Scaling groups, which complicates the setup without meeting the traffic requirements. Option B allows for a clean solution by utilizing a separate target group for the new logic.",
        "additional_knowledge": "Using a second target group also allows greater flexibility for future updates without significant architectural changes.",
        "key_terminology": "Application Load Balancer, Target Group, Auto Scaling, Canary Deployment, AWS Route 53.",
        "overall_assessment": "Overall, option B is the simplest and most effective approach in this scenario, aligning well with AWS best practices for deployment and traffic management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBです。2つ目のターゲットグループを作成し、新しいロジックをそのグループにデプロイすることで、テスト中のトラフィックを制御することができます。",
        "situation_analysis": "このシナリオでは、アプリケーションの更新をデプロイしながら、顧客の10％のみが新しいロジックを受け取るようにする必要があります。テストウィンドウ中に顧客が同じバージョンのビジネスロジックを使用することが求められています。",
        "option_analysis": "オプションAは、新しいALBを作成することになり、余分な負担となります。オプションCはトラフィックの分割の必要性に対処していません。オプションDは複数のAuto Scalingグループを含むため、トラフィック要件を満たさずに複雑になります。オプションBは新しいロジックのための別のターゲットグループを活用し、クリーンな解決策を提供します。",
        "additional_knowledge": "2つ目のターゲットグループを使用することで、将来の更新に対しても大きな柔軟性を持たせることができ、重要なアーキテクチャの変更を避けることができます。",
        "key_terminology": "アプリケーションロードバランサー、ターゲットグループ、オートスケーリング、カナリアデプロイメント、AWS Route 53。",
        "overall_assessment": "全体的に見て、オプションBはこのシナリオで最も簡単かつ効果的なアプローチであり、デプロイメントとトラフィック管理に関するAWSのベストプラクティスに適しています。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "Target Group",
      "Auto Scaling",
      "Canary Deployment",
      "AWS Route 53"
    ]
  },
  {
    "No": "193",
    "question": "A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The\ncompany is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is\nconnected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels.\nAn investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput\nof 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.\nWhat should the solutions architect do to meet these requirements with the LEAST administrative effort?",
    "question_jp": "大手教育会社が最近、複数の大学にわたる内部アプリケーションへのアクセスを提供するために Amazon Workspaces を導入しました。同社は、ユーザープロフィールを Amazon FSx for Windows File Server ファイルシステムに保存しています。このファイルシステムは DNS エイリアスで構成されており、自己管理の Active Directory に接続されています。より多くのユーザーが Workspaces を使用し始めると、ログイン時間が許容できないレベルに増加しました。調査の結果、ファイルシステムのパフォーマンスが低下していることが明らかになりました。同社は、スループット 16 MBps の HDD ストレージでファイルシステムを作成しました。ソリューションアーキテクトは、定義されたメンテナンスウィンドウ中にファイルシステムのパフォーマンスを改善する必要があります。最も管理労力を抑えた方法でこの要件を満たすために、ソリューションアーキテクトは何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system.",
        "text_jp": "AWS Backup を使用してファイルシステムの時点バックアップを作成します。バックアップを新しい FSx for Windows File Server ファイルシステムに復元します。"
      },
      {
        "key": "B",
        "text": "Disconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to",
        "text_jp": "ユーザーをファイルシステムから切断します。Amazon FSx コンソールでスループット容量を 32 MBps に更新します。ストレージタイプを更新します。"
      },
      {
        "key": "C",
        "text": "Deploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location.",
        "text_jp": "新しい Amazon EC2 インスタンスに AWS DataSync エージェントをデプロイします。タスクを作成します。既存のファイルシステムをソースロケーションとして構成します。"
      },
      {
        "key": "D",
        "text": "Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a",
        "text_jp": "Windows PowerShell コマンドを使用して既存のファイルシステムでシャドウコピーを有効にします。シャドウコピージョブをスケジュールして作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (63%) B (37%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using AWS Backup to create a point-in-time backup and restoring it to a new FSx for Windows File Server file system allows for an immediate upgrade in performance with minimal administrative effort.",
        "situation_analysis": "The problem centers on performance degradation due to the storage type (HDD) and throughput limitations (16 MBps). The business needs a solution to improve performance quickly.",
        "option_analysis": "Option A is the best since it allows for backup and restoration to improved performance while minimizing administrative burden. Option B requires user disconnection and is not guaranteed to be a permanent fix. Option C adds complexity with EC2 and DataSync, and option D does not resolve primary performance issues.",
        "additional_knowledge": "Importantly, architectural decisions should consider the access patterns and storage needs of applications using FSx.",
        "key_terminology": "Amazon FSx, SSD storage, AWS Backup, performance degradation, Active Directory",
        "overall_assessment": "This question effectively assesses knowledge of AWS file storage solutions and the importance of performance in application access. The community vote suggests that while A is more favorable, B is considered by some as a viable alternative."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は A です。AWS Backup を使用して時点バックアップを作成し、新しい FSx for Windows File Server ファイルシステムに復元することで、管理の手間を最小限に抑えながらパフォーマンスを即座に向上させることができます。",
        "situation_analysis": "問題はストレージタイプ（HDD）とスループット制限（16 MBps）によるパフォーマンスの低下に関連しています。ビジネスは迅速にパフォーマンスを改善するソリューションを必要としています。",
        "option_analysis": "オプション A は、バックアップと復元によりパフォーマンスを向上させることができ、管理の負担を最小限に抑えられるため最適です。オプション B はユーザーの切断を必要とし、永続的な解決策とは限りません。オプション C は EC2 と DataSync を利用するため複雑になり、オプション D は主要なパフォーマンス問題を解決しません。",
        "additional_knowledge": "特に、アーキテクチャ上の決定は、FSx を使用するアプリケーションのアクセスパターンとストレージニーズを考慮すべきです。",
        "key_terminology": "Amazon FSx、SSD ストレージ、AWS Backup、パフォーマンス低下、Active Directory",
        "overall_assessment": "この質問は、AWS ファイルストレージソリューションの知識とアプリケーションアクセスにおけるパフォーマンスの重要性を効果的に評価します。コミュニティ投票では、Aがより支持されている一方で、Bも一部の人には妥当な選択肢と見なされています。"
      }
    ],
    "keywords": [
      "Amazon FSx",
      "AWS Backup",
      "HDD storage",
      "performance degradation",
      "Active Directory"
    ]
  },
  {
    "No": "194",
    "question": "A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company\nmust modify the application to deploy the application in two AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がAWS上でアプリケーションをホストしています。このアプリケーションは、単一のAmazon S3バケットに格納されているオブジェクトを読み書きします。企業は、アプリケーションを2つのAWSリージョンに展開するようにアプリケーションを変更しなければなりません。どのソリューションが最も運用上のオーバーヘッドを少なくしてこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region Modify the application",
        "text_jp": "S3バケットをオリジンとしてAmazon CloudFrontディストリビューションを設定します。2番目のリージョンにアプリケーションを展開し、アプリケーションを変更します。"
      },
      {
        "key": "B",
        "text": "Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the",
        "text_jp": "2番目のリージョンに新しいS3バケットを作成します。元のS3バケットと新しいS3バケットの間で双方向S3クロスリージョンレプリケーション（CRR）を設定します。"
      },
      {
        "key": "C",
        "text": "Create a new S3 bucket in a second Region Deploy the application in the second Region. Configure the application to use the new S3",
        "text_jp": "2番目のリージョンに新しいS3バケットを作成します。2番目のリージョンにアプリケーションを展開します。アプリケーションを新しいS3バケットを使用するように構成します。"
      },
      {
        "key": "D",
        "text": "Set up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the",
        "text_jp": "S3バケットをオリジンとしてS3ゲートウェイエンドポイントを設定します。2番目のリージョンにアプリケーションを展開します。アプリケーションを使用するように変更します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (83%) C (17%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This solution minimizes operational overhead by utilizing S3 Cross-Region Replication (CRR) for automated synchronization of data between two S3 buckets across different regions.",
        "situation_analysis": "The requirement is to deploy the application across two AWS Regions while ensuring data consistency and minimizing manual workload.",
        "option_analysis": "Option A introduces unnecessary complexity with CloudFront without addressing data synchronization. Option C requires manual updates to switch S3 usage, which adds operational complexity. Option D does not directly address the synchronization of S3 objects.",
        "additional_knowledge": "Consideration should be given to the initial setup of CRR, but once established, it significantly reduces ongoing maintenance requirements.",
        "key_terminology": "Amazon S3, Cross-Region Replication (CRR), operational overhead, AWS Regions",
        "overall_assessment": "Option B is the best choice, aligning with AWS best practices for deploying applications across multiple regions with reduced operational tasks. The community largely supports this choice with 83% voting."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。このソリューションは、異なるリージョン間でのデータの自動同期にS3クロスリージョンレプリケーション（CRR）を利用することで、運用上のオーバーヘッドを最小限に抑える。",
        "situation_analysis": "要件は、データの整合性を確保し、手動作業を最小化しながら、アプリケーションを2つのAWSリージョンに展開することである。",
        "option_analysis": "選択肢Aは、データの同期に対処せずにCloudFrontの複雑さを追加する。選択肢Cは、S3の使用を切り替える手動更新を必要とし、運用の複雑さを追加する。選択肢Dは、S3オブジェクトの同期の直接的な対処をしない。",
        "additional_knowledge": "CRRの初期設定には注意を要するが、一度設定すると、継続的なメンテナンス要件が大幅に減少する。",
        "key_terminology": "Amazon S3, クロスリージョンレプリケーション（CRR）, 運用オーバーヘッド, AWSリージョン",
        "overall_assessment": "選択肢Bは、複数のリージョンにアプリケーションを展開し、運用タスクを削減するためのAWSのベストプラクティスに合致しているため、最良の選択である。コミュニティは83%がこの選択を支持している。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Cross-Region Replication (CRR)",
      "operational overhead",
      "AWS Regions"
    ]
  },
  {
    "No": "195",
    "question": "An online gaming company needs to rehost its gaming platform on AWS. The company's gaming application requires high performance computing\n(HPC) processing and has a leaderboard that changes frequently. An Ubuntu instance that is optimized for compute generation hosts a Node.js\napplication for game display. Game state is tracked in an on-premises Redis instance.\nThe company needs a migration strategy that optimizes application performance.\nWhich solution will meet these requirements?",
    "question_jp": "オンラインゲーム会社は、AWS上でゲームプラットフォームを再ホスティングする必要があります。会社のゲームアプリケーションは、高性能コンピューティング（HPC）処理を必要とし、頻繁に変更されるリーダーボードがあります。計算生成に最適化されたUbuntuインスタンスが、ゲーム表示用のNode.jsアプリケーションをホストしています。ゲーム状態は、オンプレミスのRedisインスタンスで追跡されています。会社はアプリケーションのパフォーマンスを最適化する移行戦略が必要です。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Auto Scaling group of m5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon ElastlCache",
        "text_jp": "m5.large Amazon EC2スポットインスタンスのAuto Scalingグループを作成し、アプリケーションロードバランサーの背後に配置する。Amazon ElastiCacheを使用する。"
      },
      {
        "key": "B",
        "text": "Create an Auto Scaling group of c5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon OpenSearch",
        "text_jp": "c5.large Amazon EC2スポットインスタンスのAuto Scalingグループを作成し、アプリケーションロードバランサーの背後に配置する。Amazon OpenSearchを使用する。"
      },
      {
        "key": "C",
        "text": "Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon",
        "text_jp": "c5.large Amazon EC2オンデマンドインスタンスのAuto Scalingグループを作成し、アプリケーションロードバランサーの背後に配置する。Amazon"
      },
      {
        "key": "D",
        "text": "Create an Auto Scaling group of m5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon",
        "text_jp": "m5.large Amazon EC2オンデマンドインスタンスのAuto Scalingグループを作成し、アプリケーションロードバランサーの背後に配置する。Amazon"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer.",
        "situation_analysis": "The gaming application demands high-performance computing (HPC), and the use of c5.large instances is recommended for compute-optimized workloads. The company's requirement for an On-Demand model ensures consistent availability and capacity.",
        "option_analysis": "Option A suggests using m5.large Spot Instances, which may not provide the required compute optimization for HPC tasks. Option B suggests using OpenSearch instead of Redis, which is not suitable for game state tracking. Option D uses m5.large On-Demand Instances, which are not optimized for compute performance.",
        "additional_knowledge": "Using Amazon ElastiCache could further optimize performance in the future.",
        "key_terminology": "High Performance Computing, Auto Scaling, Application Load Balancer, EC2 On-Demand Instances, Redis.",
        "overall_assessment": "The choice of c5.large On-Demand Instances with an Auto Scaling group aligned with Application Load Balancer represents a viable solution by ensuring computation performance and flexibility. The community vote overwhelmingly supports this decision."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです：c5.large Amazon EC2オンデマンドインスタンスのAuto Scalingグループを作成し、アプリケーションロードバランサーの背後に配置します。",
        "situation_analysis": "ゲームアプリケーションは高性能コンピューティング（HPC）の要求があり、c5.largeインスタンスの使用が計算最適化されたワークロードに推奨されます。オンデマンドモデルの利用は一貫した可用性とキャパシティを保証します。",
        "option_analysis": "オプションAはm5.largeスポットインスタンスを使用することを提案しており、HPCタスクのための必要な計算最適化を提供しない可能性があります。オプションBはRedisの代わりにOpenSearchを使用する提案をしており、ゲーム状態の追跡には適していません。オプションDはm5.largeオンデマンドインスタンスを使用しており、計算性能の最適化がされていません。",
        "additional_knowledge": "将来的には、Amazon ElastiCacheを使用することでパフォーマンスをさらに最適化することが可能です。",
        "key_terminology": "高性能コンピューティング、オートスケーリング、アプリケーションロードバランサー、EC2オンデマンドインスタンス、Redis。",
        "overall_assessment": "c5.largeオンデマンドインスタンスをAuto Scalingグループで使用することにより、計算性能と柔軟性を両立した適切な解決策となります。コミュニティの投票もこの決定を圧倒的に支持しています。"
      }
    ],
    "keywords": [
      "High Performance Computing",
      "Auto Scaling",
      "Application Load Balancer",
      "EC2 On-Demand Instances",
      "Redis"
    ]
  },
  {
    "No": "196",
    "question": "A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be\nsubmitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run\nmonthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.\nWhich combination of steps meets these requirements while minimizing operational overhead? (Choose two.)",
    "question_jp": "あるソリューションアーキテクトが、従業員がモバイルデバイスからタイムシートの入力を受け付けるアプリケーションを設計しています。タイムシートは毎週提出され、ほとんどの提出は金曜日に行われます。データは、給与管理者が月次レポートを実行できる形式で保存されなければなりません。インフラストラクチャは高い可用性を持ち、受信データと報告要求のペースに合わせてスケールする必要があります。オペレーショナルオーバーヘッドを最小限に抑えながらこれらの要件を満たす手順の組み合わせはどれですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled",
        "text_jp": "複数のアベイラビリティゾーンにわたってロードバランシングを行うオンデマンドAmazon EC2インスタンスにアプリケーションをデプロイします。スケジュールされた"
      },
      {
        "key": "B",
        "text": "Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability",
        "text_jp": "複数のアベイラビリティゾーンにわたってロードバランシングを行うために、Amazon Elastic Container Service（Amazon ECS）を使用してアプリケーションをコンテナにデプロイします"
      },
      {
        "key": "C",
        "text": "Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API",
        "text_jp": "アプリケーションのフロントエンドをAmazon S3バケットにデプロイし、Amazon CloudFrontから配信します。アプリケーションバックエンドをAmazon APIを使用してデプロイします。"
      },
      {
        "key": "D",
        "text": "Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the",
        "text_jp": "タイムシートの提出データをAmazon Redshiftに保存します。Amazon QuickSightを使用してAmazon Redshiftを利用したレポートを生成します"
      },
      {
        "key": "E",
        "text": "Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3",
        "text_jp": "タイムシートの提出データをAmazon S3に保存します。Amazon AthenaとAmazon QuickSightを使用してAmazon S3を利用したレポートを生成します"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CE (57%) AE (17%) 13% 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "正しい選択肢はB、Eである。Amazon ECSを使用し、コンテナ化されたアプリケーションをデプロイすることで、管理オーバーヘッドを減少させることができ、Amazon S3とAthenaを利用することで、データストレージとレポート作成も効率的になる。",
        "situation_analysis": "アプリケーションは、高可用性を提供し、スケールに対応する必要がある。大部分のタイムシートが金曜日に提出されるため、そのピークにも対応できる必要がある。",
        "option_analysis": "選択肢BではECSを使用することで容器化されているため管理が容易になり、オートスケール機能も活用できる。選択肢EはデータストレージにS3を利用し、Athenaでクエリを管理しやすくするため、データの分析も簡単である。",
        "additional_knowledge": "コンテナを使用したアプローチは、様々な規模のアプリケーションにおける現代的なアーキテクチャのベストプラクティスとされています。",
        "key_terminology": "Amazon ECS, Amazon S3, Amazon Athena, Amazon QuickSight, 高可用性",
        "overall_assessment": "この質問は、現代的なアプリケーションアーキテクチャにおいて、効率よくスケーラブルなソリューションを選択する能力を測るものとして適切である。コミュニティの投票は選択肢Eを支持しているが、選択肢Bも強力な選択肢である。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい選択肢はB、Eである。Amazon ECSを使用し、コンテナ化されたアプリケーションをデプロイすることで、管理オーバーヘッドを減少させることができ、Amazon S3とAthenaを利用することで、データストレージとレポート作成も効率的になる。",
        "situation_analysis": "アプリケーションは、高可用性を提供し、スケールに対応する必要がある。大部分のタイムシートが金曜日に提出されるため、そのピークにも対応できる必要がある。",
        "option_analysis": "選択肢BではECSを使用することで容器化されているため管理が容易になり、オートスケール機能も活用できる。選択肢EはデータストレージにS3を利用し、Athenaでクエリを管理しやすくするため、データの分析も簡単である。",
        "additional_knowledge": "コンテナを使用したアプローチは、様々な規模のアプリケーションにおける現代的なアーキテクチャのベストプラクティスとされています。",
        "key_terminology": "Amazon ECS, Amazon S3, Amazon Athena, Amazon QuickSight, 高可用性",
        "overall_assessment": "この質問は、現代的なアプリケーションアーキテクチャにおいて、効率よくスケーラブルなソリューションを選択する能力を測るものとして適切である。コミュニティの投票は選択肢Eを支持しているが、選択肢Bも強力な選択肢である。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "Amazon S3",
      "Amazon Athena",
      "Amazon QuickSight",
      "High Availability"
    ]
  },
  {
    "No": "197",
    "question": "A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the\nlogs for 5 years. The company's security team also must receive an email notification every time there is an attempt to delete data in the S3\nbucket.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "question_jp": "ある企業が敏感なデータをAmazon S3バケットに保存しています。この企業は、S3バケット内のオブジェクトに対するすべてのアクティビティをログに記録し、5年間ログを保持する必要があります。また、企業のセキュリティチームは、S3バケット内のデータ削除が試みられるたびにメール通知を受け取る必要があります。これらの要件を最も費用効果が高い形で満たすための手順の組み合わせはどれですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS CloudTrail to log S3 data events.",
        "text_jp": "AWS CloudTrailを設定し、S3データイベントをログします。"
      },
      {
        "key": "B",
        "text": "Configure S3 server access logging for the S3 bucket.",
        "text_jp": "S3バケットのサーバーアクセスログを設定します。"
      },
      {
        "key": "C",
        "text": "Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES).",
        "text_jp": "Amazon S3にオブジェクト削除イベントをAmazon Simple Email Service（Amazon SES）に送信させます。"
      },
      {
        "key": "D",
        "text": "Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification",
        "text_jp": "Amazon S3にオブジェクト削除イベントをAmazon EventBridgeのイベントバスに送信させ、Amazon Simple Notificationに公開します。"
      },
      {
        "key": "E",
        "text": "Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering.",
        "text_jp": "Amazon S3にログをAmazon Timestreamに送信させ、データストレージ層を設定します。"
      },
      {
        "key": "F",
        "text": "Configure a new S3 bucket to store the logs with an S3 Lifecycle policy.",
        "text_jp": "ログを保存するための新しいS3バケットを設定し、S3ライフサイクルポリシーを設定します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDF (53%) ADF (45%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, B, and D. AWS CloudTrail logs S3 data events, providing an audit trail of API calls. S3 server access logging will capture access requests for the bucket. Sending deletion events to EventBridge enables setting up a notification mechanism.",
        "situation_analysis": "The company needs to monitor sensitive data operations and ensure security compliance by logging and being notified of deletion attempts.",
        "option_analysis": "A is correct as CloudTrail can log S3 events; B is helpful for additional access logging; D is suitable for email notifications on deletions. C offers a notification mechanism but is less integrated; E is not cost-effective; F does not address notification requirements.",
        "additional_knowledge": "Ensure logs are retained according to compliance requirements.",
        "key_terminology": "AWS CloudTrail, Amazon S3, S3 Server Access Logging, Amazon EventBridge, Notification",
        "overall_assessment": "The combination of A, B, and D meets the requirement of logging and notification effectively. Community votes show that A is highly supported, suggesting it is a favorable choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はA、B、およびDである。AWS CloudTrailはS3データイベントをログに記録し、APIコールの監査証跡を提供する。S3サーバーアクセスログは、バケットへのアクセス要求をキャプチャする。削除イベントをEventBridgeに送信することで、通知メカニズムを設定できる。",
        "situation_analysis": "企業は、敏感なデータの操作を監視し、ログ記録と削除試行の通知を受けることでセキュリティコンプライアンスを確保する必要がある。",
        "option_analysis": "AはCloudTrailがS3イベントをログするため正しい；Bは追加のアクセスログに役立つ；Dは削除時の通知に適している。Cは通知メカニズムを提供するが、統合性が低く；Eはコスト効果が薄く；Fは通知要件に応えない。",
        "additional_knowledge": "ログはコンプライアンス要件に従って保持されることを確認すること。",
        "key_terminology": "AWS CloudTrail、Amazon S3、S3サーバーアクセスログ、Amazon EventBridge、通知",
        "overall_assessment": "A、B、およびDの組み合わせは、ログ記録と通知の要件を効果的に満たしている。コミュニティの投票はAが高く支持されていることを示しており、有望な選択肢である。"
      }
    ],
    "keywords": [
      "AWS CloudTrail",
      "Amazon S3",
      "S3 Server Access Logging",
      "Amazon EventBridge",
      "Notification"
    ]
  },
  {
    "No": "198",
    "question": "A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed\nAmazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct. Connect connection to\nthe data center from the Region that is closest to the data center.\nThe company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-\npremises data center also must have access to AWS public services.\nWhich combination of steps will meet these requirements with the LEAST cost? (Choose two.)",
    "question_jp": "ある企業が、オンプレミスのデータセンターとAWSクラウドを含むハイブリッド環境を構築しています。企業は、3つのVPCにAmazon EC2インスタンスを展開しました。それぞれのVPCは異なるAWSリージョンにあります。企業は、データセンターに最も近いリージョンからデータセンターへのAWS Direct Connect接続を確立しています。企業は、オンプレミスのデータセンターにあるサーバーが、すべての3つのVPC内のEC2インスタンスにアクセスできる必要があります。また、オンプレミスのデータセンターにあるサーバーはAWSのパブリックサービスにもアクセスできなければなりません。これらの要件を最小のコストで満たすための手順の組み合わせはどれですか？（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect",
        "text_jp": "データセンターに最も近いリージョンにDirect Connectゲートウェイを作成します。Direct Connect接続をDirect Connectゲートウェイに接続します"
      },
      {
        "key": "B",
        "text": "Set up additional Direct Connect connections from the on-premises data center to the other two Regions.",
        "text_jp": "オンプレミスのデータセンターから他の2つのリージョンへの追加のDirect Connect接続を設定します"
      },
      {
        "key": "C",
        "text": "Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions.",
        "text_jp": "プライベートVIFを作成します。プライベートVIFを介して他の2つのリージョンのVPCへのAWSサイト間VPN接続を確立します"
      },
      {
        "key": "D",
        "text": "Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions.",
        "text_jp": "パブリックVIFを作成します。パブリックVIFを介して他の2つのリージョンのVPCへのAWSサイト間VPN接続を確立します"
      },
      {
        "key": "E",
        "text": "Use VPC peering to establish a connection between the VPCs across the Regions Create a private VIF with the existing Direct Connect",
        "text_jp": "VPCピアリングを使用して、リージョン間でVPCを接続します。既存のDirect ConnectでプライベートVIFを作成します"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and C. These steps enable access for the on-premises servers to all EC2 instances in the specified VPCs with minimal cost.",
        "situation_analysis": "The company's servers need to communicate with EC2 instances in different AWS Regions and also need access to AWS public services. The existing Direct Connect connection supports this scenario.",
        "option_analysis": "Option A is necessary as it establishes a Direct Connect gateway which allows a connection to multiple VPCs across regions. Option C also provides a viable solution with a VPN, reducing additional costs from setting up multiple Direct Connect connections.",
        "additional_knowledge": "Choosing to set up direct connections only for the closest regional VPC minimizes costs compared to probing into multiple connections.",
        "key_terminology": "AWS Direct Connect, Direct Connect gateway, Site-to-Site VPN, VPC.",
        "overall_assessment": "The combination of A and C ensures that all necessary connectivity is established at the lowest possible cost. This approach aligns with AWS’s best practices for hybrid cloud architectures."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAとCである。これらの手順は、オンプレミスのサーバーが指定されたVPC内のすべてのEC2インスタンスにアクセスできるようにし、最小限のコストで実現できる。",
        "situation_analysis": "企業のサーバーは異なるAWSリージョンにあるEC2インスタンスと通信する必要があり、AWSのパブリックサービスへのアクセスも必要である。既存のDirect Connect接続がこのシナリオをサポートしている。",
        "option_analysis": "選択肢Aは、異なるリージョンにある複数のVPCに接続を提供するDirect Connectゲートウェイを確立するために必要である。選択肢Cは、コストを削減しつつVPN接続を介して接続を提供するためにも viable である。",
        "additional_knowledge": "最も近いリージョンのVPCに対する直接接続のみを設定することで、複数の接続を調整するよりもコストを最小限に抑える。",
        "key_terminology": "AWS Direct Connect, Direct Connectゲートウェイ, Site-to-Site VPN, VPC。",
        "overall_assessment": "AとCの組み合わせは、必要な接続性を最低コストで確立する。これはハイブリッドクラウドアーキテクチャに関するAWSのベストプラクティスに従っている。"
      }
    ],
    "keywords": [
      "AWS Direct Connect",
      "Direct Connect gateway",
      "Site-to-Site VPN",
      "VPC"
    ]
  },
  {
    "No": "199",
    "question": "A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to\nprovide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect\nis using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.\nWhich combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)",
    "question_jp": "ある企業がAWS Organizationsを利用して数百のAWSアカウントを管理しています。ソリューションアーキテクトがOpen Web Application Security Project（OWASP）のトップ10のWebアプリケーション脆弱性に対して基本的な保護を提供するソリューションに取り組んでいます。ソリューションアーキテクトは、組織内に展開されているすべての既存および新しいAmazon CloudFrontディストリビューションに対してAWS WAFを使用しています。基本的な保護を提供するために、ソリューションアーキテクトはどの組み合わせの手順を取るべきでしょうか？（3つを選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Enable AWS Config in all accounts",
        "text_jp": "すべてのアカウントでAWS Configを有効にする"
      },
      {
        "key": "B",
        "text": "Enable Amazon GuardDuty in all accounts",
        "text_jp": "すべてのアカウントでAmazon GuardDutyを有効にする"
      },
      {
        "key": "C",
        "text": "Enable all features for the organization",
        "text_jp": "組織のすべての機能を有効にする"
      },
      {
        "key": "D",
        "text": "Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions",
        "text_jp": "AWS Firewall Managerを使用して、すべてのアカウントでAWS WAFルールをすべてのCloudFrontディストリビューションに展開する"
      },
      {
        "key": "E",
        "text": "Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions",
        "text_jp": "AWS Shield Advancedを使用して、すべてのアカウントでAWS WAFルールをすべてのCloudFrontディストリビューションに展開する"
      },
      {
        "key": "F",
        "text": "Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions",
        "text_jp": "AWS Security Hubを使用して、すべてのアカウントでAWS WAFルールをすべてのCloudFrontディストリビューションに展開する"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ACD (64%) 8% 8% Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is 'C' - Enable all features for the organization. This is essential because it provides comprehensive governance and security management across multiple AWS accounts.",
        "situation_analysis": "The organization is managing numerous AWS accounts and must ensure security against OWASP top 10 vulnerabilities using AWS WAF in CloudFront distributions.",
        "option_analysis": "Options A, B, D, E, and F pertain more to monitoring and individual rule applications rather than a comprehensive solution across all accounts. Enabling all features allows better control and visibility.",
        "additional_knowledge": "Using AWS services in conjunction strengthens the overall security model in a multi-account setup.",
        "key_terminology": "AWS Organizations, AWS WAF, CloudFront, OWASP, governance",
        "overall_assessment": "The question effectively evaluates the understanding of AWS Organizations and security measures applicable across multiple accounts. In light of community voting, while the other options may seem valuable, focusing on enabling all features is the foundational management approach."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは「C」です。組織のすべての機能を有効にすることは、複数のAWSアカウントにわたる包括的なガバナンスとセキュリティ管理を提供するために不可欠である。",
        "situation_analysis": "組織は多数のAWSアカウントを管理しており、CloudFrontディストリビューションにおけるOWASPトップ10の脆弱性に対してAWS WAFを使用してセキュリティを確保する必要がある。",
        "option_analysis": "オプションA、B、D、E、Fは、個別のルールの適用や監視に関連しており、包括的な解決策よりも個別の対応になりがちである。すべての機能を有効にすることで、より良い管理と可視性が得られる。",
        "additional_knowledge": "AWSのサービスを組み合わせて使用することで、マルチアカウント構成における全体的なセキュリティモデルが強化される。",
        "key_terminology": "AWS Organizations、AWS WAF、CloudFront、OWASP、ガバナンス",
        "overall_assessment": "この質問は、AWS Organizationsと複数のアカウントに適用可能なセキュリティ対策に関する理解を評価する効果的なものである。コミュニティの投票を考慮すると、他のオプションも価値があるように見えるが、すべての機能を有効にすることが基本的な管理アプローチである。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS WAF",
      "CloudFront",
      "OWASP",
      "governance"
    ]
  },
  {
    "No": "200",
    "question": "A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to\nauthenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal,\naccess to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are\nnot able to access the AWS environment.\nWhich items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)",
    "question_jp": "ソリューションアーキテクトは、社内のアイデンティティプロバイダー（IdP）を使用してSAML 2.0のフェデレーテッドアイデンティティソリューションを実装し、AWS環境へのユーザーアクセスを認証した。ソリューションアーキテクトがフェデレーティッドアイデンティティウェブポータルを通じて認証をテストしたところ、AWS環境へのアクセスが許可された。しかし、テストユーザーがフェデレーティッドアイデンティティウェブポータルを通じて認証を試みると、AWS環境へのアクセスができなかった。ソリューションアーキテクトがアイデンティティフェデレーションが適切に構成されていることを確認するためにチェックすべき項目はどれか。 （3つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "The IAM user's permissions policy has allowed the use of SAML federation for that user.",
        "text_jp": "IAMユーザーのアクセス権限ポリシーが、そのユーザーのSAMLフェデレーションの使用を許可している。"
      },
      {
        "key": "B",
        "text": "The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.",
        "text_jp": "フェデレーテッドユーザーまたはフェデレーテッドグループのために作成されたIAMロールの信頼ポリシーがSAMLプロバイダーをプリンシパルとして設定している。"
      },
      {
        "key": "B",
        "text": "Test users are not in the AWSFederatedUsers group in the company's IdP.",
        "text_jp": "フェデレーテッドユーザーまたはフェデレーテッドグループのために作成されたIAMロールの信頼ポリシーがSAMLプロバイダーをプリンシパルとして設定している。"
      },
      {
        "key": "C",
        "text": "The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML",
        "text_jp": "ウェブポータルがAWS STS AssumeRoleWithSAML APIをSAMLプロバイダーのARN、IAMロールのARN、およびSAMLと共に呼び出す。"
      },
      {
        "key": "D",
        "text": "The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.",
        "text_jp": "オンプレミスのIdPのDNSホスト名がAWS環境のVPCから到達可能である。"
      },
      {
        "key": "E",
        "text": "The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions.",
        "text_jp": "会社のIdPがユーザーまたはグループをIAMロールに適切な権限でマッピングするSAMLアサーションを定義している。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BCE (43%) B (29%) BD (29%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct options are B, D, and E. The correct details ensure proper SAML federation setup.",
        "situation_analysis": "The issue arises when test users cannot access AWS despite successful access for the architect, indicating potential misconfiguration in identity mapping or permissions.",
        "option_analysis": "Option B is critical as it verifies that SAML roles are accessible by the identity provider. Options D and E also support correct configuration, focusing on the IdP and assertion mappings.",
        "additional_knowledge": "Constant review of the configuration in IdP is paramount in ensuring access problems are minimized.",
        "key_terminology": "SAML, AWS STS, AssumeRoleWithSAML, IdP, IAM roles",
        "overall_assessment": "The focus on SAML federation requires verifying multiple linked components to assure functionality. Community votes support this analysis, reflecting alignment with AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい選択肢はB、D、Eであり、これらはSAMLフェデレーションの設定が適切に行われていることを確認するために重要である。",
        "situation_analysis": "テストユーザーがアクセスできない問題が発生しており、これはアイデンティティマッピングや権限の誤設定が考えられる。",
        "option_analysis": "選択肢Bは、SAMLロールがアイデンティティプロバイダーによってアクセス可能であることを確認するために重要である。選択肢DやEも適切な設定をサポートし、IdPやアサーションマッピングに焦点を当てている。",
        "additional_knowledge": "IdPの設定を定期的にレビューすることが、アクセス問題を最小限に抑えるためには必須である。",
        "key_terminology": "SAML、AWS STS、AssumeRoleWithSAML、IdP、IAMロール",
        "overall_assessment": "SAMLフェデレーションにおいては、機能を確保するために関連する複数のコンポーネントを確認する必要がある。コミュニティ投票もこの分析を支持し、AWSのベストプラクティスとの整合性を反映している。"
      }
    ],
    "keywords": [
      "SAML",
      "AWS STS",
      "AssumeRoleWithSAML",
      "IdP",
      "IAM roles"
    ]
  },
  {
    "No": "201",
    "question": "A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB\ninstance that is experiencing overloaded connections. Most of the application's operations insert records into the database. The application\ncurrently stores credentials in a text-based configuration file.\nThe solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the\ncredentials secure and must provide the ability to rotate the credentials automatically on a regular basis.\nWhich solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、AWSクラウドでホストされているアプリケーションを改善する必要があります。このアプリケーションは、接続が過負荷となっているAmazon Aurora MySQL DBインスタンスを使用しています。アプリケーションの操作のほとんどは、データベースにレコードを挿入します。アプリケーションは現在、テキスト形式の構成ファイルに資格情報を保存しています。ソリューションアーキテクトは、アプリケーションが現在の接続負荷を処理できるようにするソリューションを実装する必要があります。このソリューションは、資格情報を安全に保持し、資格情報を定期的に自動的にローテーションする機能を提供する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy an Amazon RDS Proxy layer. In front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager.",
        "text_jp": "Amazon RDS ProxyレイヤーをDBインスタンスの前にデプロイします。接続資格情報をAWS Secrets Managerのシークレットとして保存します。"
      },
      {
        "key": "B",
        "text": "Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials in AWS Systems Manager Parameter Store",
        "text_jp": "Amazon RDS ProxyレイヤーをDBインスタンスの前にデプロイします。接続資格情報をAWS Systems Manager Parameter Storeに保存します。"
      },
      {
        "key": "C",
        "text": "Create an Aurora Replica. Store the connection credentials as a secret in AWS Secrets Manager",
        "text_jp": "Auroraレプリカを作成します。接続資格情報をAWS Secrets Managerのシークレットとして保存します。"
      },
      {
        "key": "D",
        "text": "Create an Aurora Replica. Store the connection credentials in AWS Systems Manager Parameter Store.",
        "text_jp": "Auroraレプリカを作成します。接続資格情報をAWS Systems Manager Parameter Storeに保存します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Deploying an Amazon RDS Proxy provides a managed database proxy with automatic scaling and improved connection pooling, which addresses the issue of overloaded connections. Storing credentials in AWS Secrets Manager ensures they are securely managed and can be rotated automatically.",
        "situation_analysis": "The application has overloaded connections to an existing MySQL DB and insecure handling of credentials.",
        "option_analysis": "Option A improves connection management with RDS Proxy and secures credentials. B provides less security and does not address connection overload as effectively. C offers a replica but neglects connection pooling. D also fails to address connection management efficiently.",
        "additional_knowledge": "Understanding how to manage connection loads and secure credentials is essential for designing resilient applications on AWS.",
        "key_terminology": "Amazon RDS Proxy, AWS Secrets Manager, connection pooling",
        "overall_assessment": "Option A clearly meets all the outlined requirements better than the others, ensuring both performance improvement and secure credential handling."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。Amazon RDS Proxyをデプロイすることで、接続のオーバーロードの問題に対処するための自動スケーリングと改善された接続プーリングを提供する管理されたデータベースプロキシが得られます。AWS Secrets Managerに資格情報を保存することで、資格情報が安全に管理され、自動的にローテーションされることが保証されます。",
        "situation_analysis": "アプリケーションは既存のMySQL DBへの接続が過負荷となっており、資格情報の取り扱いが不十分です。",
        "option_analysis": "AオプションはRDS Proxyを使用して接続管理を改善し、資格情報を確保します。Bは安全性が劣り、接続のオーバーロードを適切に解決しません。Cはレプリカを提供しますが、接続プール管理を無視します。Dも接続管理を効率的に解決しません。",
        "additional_knowledge": "接続負荷を管理し、資格情報を安全に保つ方法を理解することは、AWS上で回復力のあるアプリケーションを設計するために不可欠です。",
        "key_terminology": "Amazon RDS Proxy, AWS Secrets Manager, 接続プーリング",
        "overall_assessment": "Aオプションは、他の選択肢よりも明確にすべての要件を満たしており、パフォーマンスの向上と資格情報管理の安全性を確保しています。"
      }
    ],
    "keywords": [
      "Amazon RDS Proxy",
      "AWS Secrets Manager",
      "connection pooling"
    ]
  },
  {
    "No": "202",
    "question": "A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fieet of t3.large\nAmazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across\nmultiple Availability Zones.\nIn the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "会社は、eコマースウェブサイトのための災害復旧（DR）ソリューションを構築する必要があります。ウェブアプリケーションは、複数のAvailability ZoneにまたがるAuto Scalingグループ内で、t3.largeのAmazon EC2インスタンスの集まりとしてホストされています。また、Amazon RDS for MySQL DBインスタンスを使用しています。災害が発生した場合、ウェブアプリケーションは、30秒のRPOと10分のRTOで二次環境にフェイルオーバーする必要があります。どのソリューションが最もコスト効果的にこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB",
        "text_jp": "新しいインフラストラクチャをDRリージョンにプロビジョニングするためにコードとしてのインフラストラクチャ（IaC）を使用します。DBのためにクロスリージョンのリードレプリカを作成します"
      },
      {
        "key": "B",
        "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB",
        "text_jp": "新しいインフラストラクチャをDRリージョンにプロビジョニングするためにコードとしてのインフラストラクチャ（IaC）を使用します。DBのためにクロスリージョンのリードレプリカを作成します"
      },
      {
        "key": "C",
        "text": "Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression",
        "text_jp": "AWS Backupでバックアッププランを設定し、EC2インスタンスとDBインスタンスのためにクロスリージョンバックアップを作成します。Cron式を作成します"
      },
      {
        "key": "D",
        "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up",
        "text_jp": "新しいインフラストラクチャをDRリージョンにプロビジョニングするためにコードとしてのインフラストラクチャ（IaC）を使用します。Amazon Auroraグローバルデータベースを作成します"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (83%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Using Infrastructure as Code (IaC) allows for quick and repeatable provisioning of the necessary infrastructure in the DR Region and cross-Region read replicas for business continuity.",
        "situation_analysis": "The company needs a cost-effective DR solution that meets strict RPO and RTO requirements for its e-commerce application hosted on Amazon EC2 and RDS.",
        "option_analysis": "Option A and B are similar and provide an effective solution. Option C focuses on backups instead of real-time replication, which does not meet the RPO requirement. Option D involves setting up an Amazon Aurora global database, which may be more complex and costly without sufficient justification.",
        "additional_knowledge": "Organizations often prefer cost-effective solutions that don't sacrifice performance or availability.",
        "key_terminology": "Disaster Recovery, RPO, RTO, Infrastructure as Code (IaC), Amazon RDS, Read Replica.",
        "overall_assessment": "The community votes indicate strong support for answer B. It aligns well with best practices in DR strategies, and its similarity to option A confirms the validity of this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。インフラストラクチャをコードとして扱う（IaC）は、DRリージョン内の必要なインフラストラクチャを迅速かつ再現可能にプロビジョニングし、ビジネス継続性のためにクロスリージョンのリードレプリカを使うことができる。",
        "situation_analysis": "この会社は、Amazon EC2およびRDS上でホストされているeコマースアプリケーションに対して、厳格なRPOおよびRTO要件を満たすコスト効果の高いDRソリューションを必要としている。",
        "option_analysis": "選択肢AとBは類似しており、効果的なソリューションを提供する。選択肢Cは、リアルタイムのレプリケーションではなくバックアップに焦点を当てているため、RPO要件を満たしていない。選択肢Dは、Amazon Auroraグローバルデータベースの設定を含んでおり、十分な根拠がない場合はより複雑でコストがかかる可能性がある。",
        "additional_knowledge": "組織は、パフォーマンスや可用性を犠牲にすることなく、コスト効果の高いソリューションをしばしば好む。",
        "key_terminology": "災害復旧、RPO、RTO、インフラストラクチャとしてのコード（IaC）、Amazon RDS、リードレプリカ。",
        "overall_assessment": "コミュニティの投票は、選択肢Bの強力な支持を示している。DR戦略におけるベストプラクティスと適切に一致しており、Aとの類似性によりこの選択肢の妥当性が確認される。"
      }
    ],
    "keywords": [
      "Disaster Recovery",
      "RPO",
      "RTO",
      "Infrastructure as Code",
      "Amazon RDS",
      "Read Replica"
    ]
  },
  {
    "No": "203",
    "question": "A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's\ncurrent internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a\nmonth to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database\nmore quickly.\nWhich solution will migrate the database in the LEAST amount of time?",
    "question_jp": "企業は、オンプレミスのMySQLデータベースをus-east-1リージョンのAmazon Aurora MySQLに一度限り移行する計画を立てています。企業の現行のインターネット接続は帯域幅が限られています。オンプレミスのMySQLデータベースは60TBのサイズです。企業は、現在のインターネット接続を介してAWSにデータを転送するのに1ヶ月かかると見積もっています。企業は、データベースをより迅速に移行できるソリューションが必要です。どのソリューションが最も短時間でデータベースを移行できるでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service",
        "text_jp": "オンプレミスのデータセンターとAWS間に1 GbpsのAWS Direct Connect接続を依頼します。AWSデータベース移行サービスを使用します"
      },
      {
        "key": "B",
        "text": "Use AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use",
        "text_jp": "AWS DataSyncを現在のインターネット接続で使用して、オンプレミスのデータセンターとAWS間のデータ転送を加速します"
      },
      {
        "key": "C",
        "text": "Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration",
        "text_jp": "AWS Snowball Edgeデバイスを注文します。S3インターフェースを使用して、データをAmazon S3バケットにロードします。AWSデータベース移行サービスを使用します"
      },
      {
        "key": "D",
        "text": "Order an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application",
        "text_jp": "AWS Snowballデバイスを注文します。S3アダプターを使用してデータをAmazon S3バケットにロードします。AWSアプリケーションを使用します"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C - Order an AWS Snowball Edge device, as this approach allows for significant data transfer without being hindered by limited bandwidth.",
        "situation_analysis": "The size of the database (60 TB) is substantial, and the limited bandwidth of the current internet connection poses a significant delay in migration. Traditional upload methods would be inadequate.",
        "option_analysis": "Option A might improve transfer rates, but it is still dependent on the existing internet bandwidth. Option B also relies on the internet connection which is not sufficient for such a large database. Option D involves a Snowball device but suggests using the S3 Adapter, which is less efficient in this context compared to the Edge device that can directly integrate with the database migration service.",
        "additional_knowledge": "Other migration tools like AWS Database Migration Service can facilitate the process further after the data is physically moved.",
        "key_terminology": "AWS Snowball, AWS Database Migration Service, bandwidth, data transfer.",
        "overall_assessment": "Given the constraints of time and bandwidth, option C is the best approach. Community vote aligns with this conclusion, indicating strong preference for this solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです - AWS Snowball Edgeデバイスを注文する方法は、限られた帯域幅に妨げられることなく大量データ転送を可能にします。",
        "situation_analysis": "データベースのサイズ（60 TB）は非常に大きく、現行のインターネット接続の限られた帯域幅は移行に大きな遅延を引き起こします。従来のアップロード方法は不適切です。",
        "option_analysis": "選択肢Aは転送速度を改善できる可能性がありますが、既存のインターネット帯域幅に依存します。選択肢Bもインターネット接続に依存し、このような大規模データベースには不十分です。選択肢DはSnowballデバイスを使用する提案ですが、S3アダプターの使用はこの文脈ではEdgeデバイスに比べて効率が劣ります。",
        "additional_knowledge": "AWSデータベース移行サービスのような他の移行ツールは、データが物理的に移動された後のプロセスをさらに促進できます。",
        "key_terminology": "AWS Snowball, AWSデータベース移行サービス, 帯域幅, データ転送。",
        "overall_assessment": "時間と帯域幅の制約を考慮すると、選択肢Cが最良のアプローチです。コミュニティの投票もこの結論に沿っており、このソリューションに強い支持を示しています。"
      }
    ],
    "keywords": [
      "AWS Snowball",
      "AWS Database Migration Service",
      "bandwidth",
      "data transfer"
    ]
  },
  {
    "No": "204",
    "question": "A company has an application in the AWS Cloud. The application runs on a fieet of 20 Amazon EC2 instances. The EC2 instances are persistent\nand store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.\nThe company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration\nwithin 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes\noperational eficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network\nconfiguration in a secondary Region.\nWhich solution will meet these requirements?",
    "question_jp": "企業はAWSクラウドにアプリケーションを持っています。このアプリケーションは、20台のAmazon EC2インスタンスのフリートで動作しています。EC2インスタンスは永続的で、複数のアタッチされたAmazon Elastic Block Store（Amazon EBS）ボリュームにデータを保存しています。企業は、別のAWSリージョンにバックアップを維持しなければなりません。企業は、EC2インスタンスおよびその構成を1営業日以内に回復できる必要があり、データ損失は1日分以内でなければなりません。企業には限られたスタッフがおり、運用効率とコストを最適化するバックアップソリューションが必要です。企業はすでに二次リージョンに必要なネットワーク構成をデプロイできるAWS CloudFormationテンプレートを作成しています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots",
        "text_jp": "二次リージョンにEC2インスタンスを再作成できる第2のCloudFormationテンプレートを作成します。毎日マルチボリュームスナップショットを実行します。"
      },
      {
        "key": "B",
        "text": "Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure,",
        "text_jp": "Amazon Data Lifecycle Manager（Amazon DLM）を使用して、EBSボリュームの毎日のマルチボリュームスナップショットを作成します。障害発生時には、"
      },
      {
        "key": "C",
        "text": "Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in",
        "text_jp": "AWS Backupを使用して、EC2インスタンスのスケジュールされた毎日のバックアッププランを作成します。バックアップタスクを構成して、バックアップを保存します。"
      },
      {
        "key": "D",
        "text": "Deploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the",
        "text_jp": "同じサイズと構成のEC2インスタンスを二次リージョンにデプロイします。AWS DataSyncを毎日構成して、データをコピーします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (74%) B (26%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using AWS Backup allows for an automated and efficient backup solution tailored specifically for EC2 instances and EBS volumes, ensuring optimal operational efficiency and cost savings.",
        "situation_analysis": "The company needs to backup EC2 instances and their configurations, while ensuring that backups are maintained in a separate region and comply with specific restoration time and data loss criteria.",
        "option_analysis": "Option A requires manual recreation of instances, while B does not specify sufficient recovery controls. Option D focuses more on data transfer rather than complete instance management. C directly meets all backup and restoration requirements with an automated solution.",
        "additional_knowledge": "The use of AWS Backup also provides compliance with backup retention policies and efficient storage management.",
        "key_terminology": "AWS Backup, EC2, EBS, backup plan, automated backups",
        "overall_assessment": "This question assesses understanding of backup solutions specific to AWS services, highlighting the advantages of automated solutions over manual configurations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。AWS Backupを使用することで、EC2インスタンスおよびEBSボリュームに特化した自動化された効率的なバックアップソリューションが実現でき、運用効率とコストの最適化が図られる。",
        "situation_analysis": "企業はEC2インスタンスとその構成のバックアップを必要としており、別のリージョンでバックアップを維持し、特定の復元時間とデータ損失基準に従う必要がある。",
        "option_analysis": "選択肢Aはインスタンスの手動再作成を必要とし、Bは十分な復旧管理を指定していない。選択肢Dはデータ転送に重点を置いており、完全なインスタンス管理には対応していない。Cは、自動化されたソリューションであるため、すべてのバックアップと復元要件を直接満たしている。",
        "additional_knowledge": "AWS Backupの使用は、バックアップ保持ポリシーへの準拠と効率的なストレージ管理の提供も可能にする。",
        "key_terminology": "AWS Backup、EC2、EBS、バックアッププラン、自動バックアップ",
        "overall_assessment": "この質問は、AWSサービス特有のバックアップソリューションの理解を評価し、自動化されたソリューションが手動構成よりも利点があることを強調している。"
      }
    ],
    "keywords": [
      "AWS Backup",
      "EC2",
      "EBS",
      "backup plan",
      "automated backups"
    ]
  },
  {
    "No": "205",
    "question": "A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files.\nAccording to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using\nAmazon S3 and Amazon CloudFront.\nWhich combination of steps will meet the encryption requirements? (Choose three.)",
    "question_jp": "ある企業が静的コンテンツをホスティングする新しいウェブサイトを設計しています。このウェブサイトは、ユーザーが大きなファイルをアップロードおよびダウンロードできる機能を提供します。\n企業の要件に従い、すべてのデータは転送中と保存中に暗号化される必要があります。ソリューションアーキテクトは、Amazon S3およびAmazon CloudFrontを使用してソリューションを構築しています。\nどの組み合わせの手順が暗号化要件を満たすことになりますか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Turn on S3 server-side encryption for the S3 bucket that the web application uses.",
        "text_jp": "S3バケットに対してサーバー側暗号化をオンにする。"
      },
      {
        "key": "B",
        "text": "Add a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations in the S3 ACLs.",
        "text_jp": "S3 ACLの読み書き操作に\"aws:SecureTransport\": \"true\"のポリシー属性を追加する。"
      },
      {
        "key": "C",
        "text": "Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses.",
        "text_jp": "ウェブアプリケーションが使用するS3バケットで、暗号化されていない操作を拒否するバケットポリシーを作成する。"
      },
      {
        "key": "D",
        "text": "Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).",
        "text_jp": "AWS KMSキーを使用してCloudFrontの保存時暗号化を設定する（SSE-KMS）。"
      },
      {
        "key": "E",
        "text": "Configure redirection of HTTP requests to HTTPS requests in CloudFront.",
        "text_jp": "CloudFrontでHTTPリクエストをHTTPSリクエストにリダイレクトするよう設定する。"
      },
      {
        "key": "F",
        "text": "Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses.",
        "text_jp": "ウェブアプリケーションが使用するS3バケットのために、署名付きURLの作成時にRequireSSLオプションを使用する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACE (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, B, and E. These options collectively ensure that data is encrypted in transit and at rest.",
        "situation_analysis": "The organization must ensure that data is encrypted both in transit (during upload/download) and at rest (when stored). This is crucial for data security and compliance with industry regulations.",
        "option_analysis": "Option A activates server-side encryption for the stored data. Option B enforces encryption during file transfers by requiring secure transport. Option E redirects HTTP traffic to HTTPS, ensuring secure data transfers. Options C, D, and F contribute to security but are not part of the essential steps to meet encryption requirements.",
        "additional_knowledge": "AWS provides various methods to manage keys and encryption, including using KMS for managing encryption keys securely.",
        "key_terminology": "AWS key management service, server-side encryption, secure transport",
        "overall_assessment": "This question effectively evaluates the understanding of AWS security practices regarding data encryption. The community largely supports the selected answer, indicating a consensus on the correct steps."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA、B、およびEである。これらの選択肢は、データが転送中と保存中に暗号化されることを確実にする。",
        "situation_analysis": "組織は、データが転送中（アップロード/ダウンロード中）と保存中（保存時）の両方で暗号化されていることを確認する必要がある。これは、データの安全性と業界の規制への準拠にとって重要である。",
        "option_analysis": "選択肢Aは保存されたデータのサーバー側暗号化を有効にする。選択肢Bは、ファイル転送中に暗号化を強制することによって安全な伝送を要求する。選択肢Eは、HTTPトラフィックをHTTPSにリダイレクトし、安全なデータ転送を確実にする。選択肢C、D、およびFはセキュリティに貢献するが、暗号化要件を満たすための重要な手順の一部ではない。",
        "additional_knowledge": "AWSは、KMSを使用して暗号化キーを安全に管理するなど、キーと暗号化を管理するためのさまざまな方法を提供している。",
        "key_terminology": "AWSキーマネジメントサービス、サーバー側暗号化、安全な伝送",
        "overall_assessment": "この質問は、データ暗号化に関するAWSのセキュリティプラクティスの理解を効果的に評価している。コミュニティは選択した答えを広く支持しており、正しい手順に関する合意を示している。"
      }
    ],
    "keywords": [
      "S3",
      "CloudFront",
      "encryption",
      "AWS KMS",
      "HTTPS"
    ]
  },
  {
    "No": "206",
    "question": "A company is implementing a serverless architecture by using AWS Lambda functions that need to access a Microsoft SQL Server DB instance on\nAmazon RDS. The company has separate environments for development and production, including a clone of the database system.\nThe company's developers are allowed to access the credentials for the development database. However, the credentials for the production\ndatabase must be encrypted with a key that only members of the IT security team's IAM user group can access. This key must be rotated on a\nregular basis.\nWhat should a solutions architect do in the production environment to meet these requirements?",
    "question_jp": "ある企業がAWS Lambda関数を使用したサーバーレスアーキテクチャを実装しており、Microsoft SQL Server DBインスタンスにAmazon RDS上でアクセスする必要があります。この企業には、開発環境と本番環境用の別々の環境があり、データベースシステムのクローンも含まれています。開発データベースの認証情報には開発者がアクセスできるが、本番データベースの認証情報はITセキュリティチームのIAMユーザーグループのメンバーのみがアクセスできるキーで暗号化されなければなりません。このキーは定期的にローテーションする必要があります。解決策アーキテクトは、本番環境でこれらの要件を満たすために何をすべきか？",
    "choices": [
      {
        "key": "A",
        "text": "Store the database credentials in AWS Systems Manager Parameter Store by using a SecureString parameter that is encrypted by an AWS",
        "text_jp": "AWS Systems Manager Parameter Storeにデータベース認証情報を保存し、AWSによって暗号化されたSecureStringパラメータを使用する"
      },
      {
        "key": "B",
        "text": "Encrypt the database credentials by using the AWS Key Management Service (AWS KMS) default Lambda key. Store the credentials in the",
        "text_jp": "AWS Key Management Service (AWS KMS)のデフォルトのLambdaキーを使用してデータベース認証情報を暗号化し、認証情報を保存する"
      },
      {
        "key": "C",
        "text": "Store the database credentials in the environment variables of each Lambda function. Encrypt the environment variables by using an AWS",
        "text_jp": "各Lambda関数の環境変数にデータベース認証情報を保存し、AWSを使用して環境変数を暗号化する"
      },
      {
        "key": "D",
        "text": "Store the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS)",
        "text_jp": "AWS Secrets Managerにデータベース認証情報を秘密として保存し、AWS Key Management Service (AWS KMS)に関連付ける"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (77%) A (23%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using AWS Secrets Manager allows for secure storage and automatic handling of sensitive information such as database credentials, especially when integrated with AWS KMS for encryption.",
        "situation_analysis": "The requirements specify that production database credentials must be encrypted and accessible only to the IT security team, with regular key rotation. Secrets Manager supports these features natively.",
        "option_analysis": "Option D is correct because it allows for secure storage and access control via IAM policies. Option A does not provide sufficient security since access might be broader than the specified IAM group. Option B uses default keys, which may not meet the access restriction requirements. Option C does not allow for easy management or rotation of secrets and exposes credentials directly in code.",
        "additional_knowledge": "Utilizing AWS Secrets Manager helps simplify the management of sensitive data and reduces the risk of exposure compared to hard-coding credentials.",
        "key_terminology": "AWS Secrets Manager, AWS Key Management Service (AWS KMS), IAM user group, encryption, secure storage",
        "overall_assessment": "D is the most compliant solution, given it fully addresses the security and operational requirements while supporting AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。AWS Secrets Managerを使用することで、データベースの認証情報などの機密情報を安全に保存し、AWS KMSとの統合を通じて暗号化を自動的に処理できる。",
        "situation_analysis": "要件は、本番データベースの認証情報が暗号化され、ITセキュリティチームのみにアクセス可能で、定期的なキーのローテーションが必要であることを明示している。Secrets Managerはこれらの機能をネイティブにサポートしている。",
        "option_analysis": "選択肢Dが正しい理由は、セキュアなストレージとIAMポリシーによるアクセス制御を可能にするからである。選択肢Aは、指定されたIAMグループよりも広いアクセスが許可される可能性があるため、十分なセキュリティを提供しない。選択肢Bはデフォルトのキーを使用しており、アクセス制限要件を満たさない可能性がある。選択肢Cは、シークレットの管理やローテーションが容易ではなく、コード内で認証情報を直接公開する点で問題がある。",
        "additional_knowledge": "AWS Secrets Managerを活用することで、機密データの管理が簡素化され、認証情報をハードコーディングすることによるリスクが軽減される。",
        "key_terminology": "AWS Secrets Manager、AWS Key Management Service (AWS KMS)、IAMユーザーグループ、暗号化、安全なストレージ",
        "overall_assessment": "Dは最も要件を満たしており、セキュリティと運用の要件に完全に対応し、AWSのベストプラクティスをサポートする最適なソリューションである。"
      }
    ],
    "keywords": [
      "AWS Secrets Manager",
      "AWS Key Management Service",
      "IAM user group",
      "encryption",
      "secure storage"
    ]
  },
  {
    "No": "207",
    "question": "An online retail company is migrating its legacy on-premises .NET application to AWS. The application runs on load-balanced frontend web\nservers, load-balanced application servers, and a Microsoft SQL Server database.\nThe company wants to use AWS managed services where possible and does not want to rewrite the application. A solutions architect needs to\nimplement a solution to resolve scaling issues and minimize licensing costs as the application scales.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "オンライン小売企業がレガシーなオンプレミスの.NETアプリケーションをAWSに移行しています。このアプリケーションは、負荷分散されたフロントエンドのウェブサーバー、負荷分散されたアプリケーションサーバー、およびMicrosoft SQL Serverデータベースで動作します。この企業は、可能な限りAWSのマネージドサービスを使用したいと考えており、アプリケーションの書き換えは望んでいません。ソリューションアーキテクトは、スケーリングの問題を解決し、アプリケーションのスケールに伴うライセンス費用を最小限に抑えるためのソリューションを実装する必要があります。どのソリューションが最もコスト効果的にこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier.",
        "text_jp": "ウェブ層およびアプリケーション層のために、アプリケーションロードバランサーの背後にAuto Scalingグループ内にAmazon EC2インスタンスをデプロイします。"
      },
      {
        "key": "B",
        "text": "Create images of all the servers by using AWS Database Migration Service (AWS DMS). Deploy Amazon EC2 instances that are based on the",
        "text_jp": "AWS Database Migration Service (AWS DMS) を使用してすべてのサーバーのイメージを作成します。Amazon EC2インスタンスをそれに基づいてデプロイします。"
      },
      {
        "key": "C",
        "text": "Containerize the web frontend tier and the application tier. Provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create",
        "text_jp": "ウェブフロントエンド層とアプリケーション層をコンテナ化します。Amazon Elastic Kubernetes Service (Amazon EKS) クラスタをプロビジョニングします。"
      },
      {
        "key": "D",
        "text": "Separate the application functions into AWS Lambda functions. Use Amazon API Gateway for the web frontend tier and the application tier.",
        "text_jp": "アプリケーションの機能をAWS Lambda関数に分離します。ウェブフロントエンド層およびアプリケーション層にはAmazon API Gatewayを使用します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer allows for flexibility in scaling while managing licensing costs effectively.",
        "situation_analysis": "The company seeks to migrate an existing .NET application without rewriting it, utilizing managed AWS services where possible while addressing scaling issues.",
        "option_analysis": "Option A uses managed services and allows for scaling without code changes. Options B and C involve more complexity and potential licensing concerns, while option D requires significant refactoring.",
        "additional_knowledge": "It is crucial to review and optimize the workload for cloud compatibility to maximize the benefits of AWS.",
        "key_terminology": "Amazon EC2, Auto Scaling, Application Load Balancer, licensing costs, AWS managed services",
        "overall_assessment": "Overall, option A is aligned with best practices for AWS cloud migration with emphasis on cost-effectiveness and minimal disruption."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。アプリケーションロードバランサーの背後にあるAuto Scalingグループ内のAmazon EC2インスタンスを使用することで、ライセンス費用を効果的に管理しながらスケーリングの柔軟性を提供する。",
        "situation_analysis": "企業は、コードの書き換えをせずに既存の.NETアプリケーションを移行したいと考えており、可能な限りマネージドAWSサービスを利用しながらスケーリングの問題に対処したい。",
        "option_analysis": "選択肢Aは、マネージドサービスを使用し、コードの変更なしでスケーリングを実現する。選択肢BとCは、複雑さが増し、ライセンスに関する懸念が生じる可能性がある。一方、選択肢Dは大規模なリファクタリングを必要とする。",
        "additional_knowledge": "AWSのメリットを最大限に引き出すために、ワークロードのクラウド適合性を確認し最適化することが重要である。",
        "key_terminology": "Amazon EC2, Auto Scaling, Application Load Balancer, ライセンス費用, AWSマネージドサービス",
        "overall_assessment": "選択肢Aは、コスト効果と最小限の干渉を強調しつつ、AWSクラウド移行のベストプラクティスに沿っている。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "licensing costs",
      "AWS managed services"
    ]
  },
  {
    "No": "208",
    "question": "A software-as-a-service (SaaS) provider exposes APIs through an Application Load Balancer (ALB). The ALB connects to an Amazon Elastic\nKubernetes Service (Amazon EKS) cluster that is deployed in the us-east-1 Region. The exposed APIs contain usage of a few non-standard REST\nmethods: LINK, UNLINK, LOCK, and UNLOCK.\nUsers outside the United States are reporting long and inconsistent response times for these APIs. A solutions architect needs to resolve this\nproblem with a solution that minimizes operational overhead.\nWhich solution meets these requirements?",
    "question_jp": "ソフトウェア・アズ・ア・サービス（SaaS）プロバイダーが、アプリケーションロードバランサー（ALB）を介してAPIを公開しています。ALBは、us-east-1リージョンにデプロイされたAmazon Elastic Kubernetes Service（Amazon EKS）クラスタに接続します。公開されているAPIには、いくつかの非標準RESTメソッドであるLINK、UNLINK、LOCK、およびUNLOCKが含まれています。アメリカ合衆国以外のユーザーから、これらのAPIに対して長く一貫性のない応答時間が報告されています。ソリューションアーキテクトは、運用オーバーヘッドを最小限に抑えたソリューションでこの問題を解決する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Add an Amazon CloudFront distribution. Configure the ALB as the origin.",
        "text_jp": "Amazon CloudFrontディストリビューションを追加します。ALBをオリジンとして構成します。"
      },
      {
        "key": "B",
        "text": "Add an Amazon API Gateway edge-optimized API endpoint to expose the APIs. Configure the ALB as the target.",
        "text_jp": "Amazon API Gatewayのエッジ最適化APIエンドポイントを追加してAPIを公開します。ALBをターゲットとして構成します。"
      },
      {
        "key": "C",
        "text": "Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin.",
        "text_jp": "AWS Global Acceleratorにアクセラレーターを追加します。ALBをオリジンとして構成します。"
      },
      {
        "key": "D",
        "text": "Deploy the APIs to two additional AWS Regions: eu-west-1 and ap-southeast-2. Add latency-based routing records in Amazon Route 53.",
        "text_jp": "APIを追加の2つのAWSリージョン（eu-west-1およびap-southeast-2）にデプロイします。Amazon Route 53にレイテンシーベースのルーティングレコードを追加します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (66%) B (31%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin.",
        "situation_analysis": "Users outside the United States are experiencing long and inconsistent response times for APIs accessed through an ALB connected to an Amazon EKS cluster. This indicates a latency issue likely due to geographic distance from the us-east-1 Region.",
        "option_analysis": "Option A (CloudFront) is typically used for static content, not dynamic API calls. Option B (API Gateway) could improve performance but adds complexity. Option D (deploying to multiple regions) increases operational overhead without necessarily solving the latency issue. Option C effectively addresses latency by directing traffic through the closest regional endpoint.",
        "additional_knowledge": "AWS services like Route 53 for DNS and CloudFront for content delivery can complement Global Accelerator when implementing a multi-region strategy.",
        "key_terminology": "AWS Global Accelerator, Application Load Balancer, latency, endpoint.",
        "overall_assessment": "Given the requirements of minimizing operational overhead while resolving latency for international users, Option C is the most appropriate solution. Additionally, the community vote aligned with this choice, indicating strong support for the use of AWS Global Accelerator in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCです：AWS Global Acceleratorにアクセラレーターを追加し、ALBをオリジンとして構成します。",
        "situation_analysis": "アメリカ合衆国以外のユーザーがALBを介してアクセスされるAPIに対して長く一貫性のない応答時間を経験しています。これは、us-east-1リージョンからの地理的距離によるレイテンシーの問題を示唆しています。",
        "option_analysis": "選択肢A（CloudFront）は通常、静的コンテンツに使用され、動的APIコールには適していません。選択肢B（API Gateway）はパフォーマンスを向上させる可能性がありますが、複雑さが増します。選択肢D（複数のリージョンへのデプロイ）は運用のオーバーヘッドを増加させ、必ずしもレイテンシー問題を解決するわけではありません。選択肢Cは、最寄りのリージョナルエンドポイントを通じてトラフィックを誘導することにより、レイテンシーに効果的に対処します。",
        "additional_knowledge": "Route 53やCloudFrontといったAWSサービスは、 Global Acceleratorを実装するときに補完的な役割を果たすことができます。",
        "key_terminology": "AWS Global Accelerator、アプリケーションロードバランサー、レイテンシー、エンドポイント。",
        "overall_assessment": "国際ユーザーに対するレイテンシーを解決しつつ運用オーバーヘッドを最小限に抑える要件を考慮すると、選択肢Cが最も適切なソリューションです。さらに、コミュニティの投票もこの選択を支持しており、このシナリオにおけるAWS Global Acceleratorの使用が強く支持されています。"
      }
    ],
    "keywords": [
      "AWS Global Accelerator",
      "Application Load Balancer",
      "latency",
      "endpoint"
    ]
  },
  {
    "No": "209",
    "question": "A company runs an IoT application in the AWS Cloud. The company has millions of sensors that collect data from houses in the United States. The\nsensors use the MQTT protocol to connect and send data to a custom MQTT broker. The MQTT broker stores the data on a single Amazon EC2\ninstance. The sensors connect to the broker through the domain named iot.example.com. The company uses Amazon Route 53 as its DNS\nservice. The company stores the data in Amazon DynamoDB.\nOn several occasions, the amount of data has overloaded the MQTT broker and has resulted in lost sensor data. The company must improve the\nreliability of the solution.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWSクラウドでIoTアプリケーションを実行しています。この企業は、米国の家屋からデータを収集する数百万のセンサーを持っています。これらのセンサーは、MQTTプロトコルを使用して接続し、カスタムMQTTブローカーにデータを送信します。MQTTブローカーは、単一のAmazon EC2インスタンスにデータを保存します。センサーは、iot.example.comというドメイン名を介してブローカーに接続します。企業は、DNSサービスとしてAmazon Route 53を使用しています。また、企業はAmazon DynamoDBにデータを保存しています。数回にわたり、データの量がMQTTブローカーをオーバーロードさせ、センサーのデータが失われる結果となっています。企業は、ソリューションの信頼性を向上させる必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. Use the Auto Scaling group as the target for the",
        "text_jp": "Application Load Balancer (ALB)とMQTTブローカー用のAuto Scalingグループを作成する。Auto Scalingグループをターゲットとして使用する。"
      },
      {
        "key": "B",
        "text": "Set up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record",
        "text_jp": "センサーのデータを受信するためにAWS IoT Coreを設定する。AWS IoT Coreに接続するためのカスタムドメインを作成および構成する。DNSレコードを更新します。"
      },
      {
        "key": "C",
        "text": "Create a Network Load Balancer (NLB). Set the MQTT broker as the target. Create an AWS Global Accelerator accelerator. Set the NLB as",
        "text_jp": "Network Load Balancer (NLB)を作成する。MQTTブローカーをターゲットとして設定する。AWS Global Acceleratorを作成する。NLBを設定する。"
      },
      {
        "key": "D",
        "text": "Set up AWS IoT Greengrass to receive the sensor data. Update the DNS record in Route 53 to point to the AWS IoT Greengrass endpoint.",
        "text_jp": "センサーのデータを受信するためにAWS IoT Greengrassを設定する。Route 53のDNSレコードをAWS IoT Greengrassエンドポイントに向けて更新します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Create a Network Load Balancer (NLB) and set the MQTT broker as the target. This enhances reliability by distributing traffic across multiple endpoints.",
        "situation_analysis": "The company faces data overload on the MQTT broker, causing data loss. They need a reliable solution to handle large volumes of incoming sensor data.",
        "option_analysis": "Option A would require managing multiple instances, which may not efficiently distribute the load. Option B involves shifting to AWS IoT Core, which is not specified as a requirement. Option D suggests AWS IoT Greengrass, which is more suitable for edge computing than for addressing the core issue of broker overload.",
        "additional_knowledge": "Implementing monitoring and scaling policies would be essential to ensure the solution continues to meet performance needs.",
        "key_terminology": "Network Load Balancer, AWS Global Accelerator, MQTT, Auto Scaling",
        "overall_assessment": "While community votes favored option B, creating NLB with appropriate configurations is the better architectural approach for scaling and reliability in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです: Network Load Balancer (NLB)を作成し、MQTTブローカーをターゲットとして設定することです。これにより、トラフィックを複数のエンドポイントに分散し、信頼性が向上します。",
        "situation_analysis": "企業はMQTTブローカーのデータオーバーロードに直面しており、データが失われています。彼らは、大量のセンサーからのデータを処理するために信頼性の高いソリューションを必要としています。",
        "option_analysis": "選択肢Aは、複数のインスタンスを管理する必要があり、負荷を効率的に分散しない可能性があります。選択肢BはAWS IoT Coreへの移行を検討していますが、これは要件として明示されていません。選択肢DはAWS IoT Greengrassを提案していますが、これはエッジコンピューティングに適しており、ブローカーのオーバーロードの根本的な問題には対処していません。",
        "additional_knowledge": "モニタリングとスケーリングポリシーを実装することが、ソリューションが引き続きパフォーマンスニーズを満たすために重要です。",
        "key_terminology": "Network Load Balancer, AWS Global Accelerator, MQTT, Auto Scaling",
        "overall_assessment": "コミュニティの投票は選択肢Bが支持されていますが、NLBを適切に構成して作成することが、このシナリオにおいて拡張性と信頼性に優れた建築的アプローチです。"
      }
    ],
    "keywords": [
      "Network Load Balancer",
      "AWS Global Accelerator",
      "MQTT",
      "Auto Scaling"
    ]
  },
  {
    "No": "210",
    "question": "A company has Linux-based Amazon EC2 instances. Users must access the instances by using SSH with EC2 SSH key pairs. Each machine\nrequires a unique EC2 key pair.\nThe company wants to implement a key rotation policy that will, upon request, automatically rotate all the EC2 key pairs and keep the keys in a\nsecurely encrypted place. The company will accept less than 1 minute of downtime during key rotation.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がLinuxベースのAmazon EC2インスタンスを運用しています。ユーザーはEC2 SSHキーペアを使用してインスタンスにSSHでアクセスする必要があります。各マシンにはユニークなEC2キーペアが必要です。この企業は、リクエストに応じてすべてのEC2キーペアを自動的にローテーションし、キーを安全に暗号化された場所に保管するキー回転ポリシーを実装したいと考えています。企業は、キー回転中のダウンタイムを1分未満で受け入れます。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new",
        "text_jp": "AWS Secrets Managerにすべてのキーを保存します。Secrets Managerのローテーションスケジュールを定義して、新しいキーを生成するためにAWS Lambda関数を呼び出します。"
      },
      {
        "key": "B",
        "text": "Store all the keys in Parameter Store, a capability of AWS Systems Manager, as a string. Define a Systems Manager maintenance window to",
        "text_jp": "AWS Systems Managerの機能であるParameter Storeにすべてのキーを文字列として保存します。Systems Managerのメンテナンスウィンドウを定義して"
      },
      {
        "key": "C",
        "text": "Import the EC2 key pairs into AWS Key Management Service (AWS KMS). Configure automatic key rotation for these key pairs. Create an",
        "text_jp": "EC2キーペアをAWS Key Management Service（AWS KMS）にインポートします。これらのキーペアの自動キー回転を設定します。"
      },
      {
        "key": "D",
        "text": "Add all the EC2 instances to Fleet Manager, a capability of AWS Systems Manager. Define a Systems Manager maintenance window to",
        "text_jp": "すべてのEC2インスタンスをAWS Systems Managerの機能であるFleet Managerに追加します。Systems Managerのメンテナンスウィンドウを定義します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (69%) D (31%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. By using AWS Secrets Manager, the company can automatically rotate EC2 key pairs with minimal downtime.",
        "situation_analysis": "The company requires automatic key rotation of EC2 key pairs, with less than 1 minute of downtime acceptable. Storing keys securely and rotating them effectively is crucial.",
        "option_analysis": "Option A leverages AWS Secrets Manager for secure key storage and automatic rotation using Lambda. Option B and D are less effective for this specific requirement. Option C does not directly support automatic EC2 key rotation.",
        "additional_knowledge": "It is recommended to have a proper backup of the previous key pairs before rotation.",
        "key_terminology": "AWS Secrets Manager, EC2 key pairs, AWS Lambda, key rotation, encryption",
        "overall_assessment": "Considering the requirements, option A is the most suitable solution, while community votes also favor this choice with 69%."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。AWS Secrets Managerを使用することで、企業はダウンタイムを最小限に抑えた状態でEC2キーペアを自動的にローテーションすることができる。",
        "situation_analysis": "企業はEC2キーペアの自動キー回転が必要であり、1分未満のダウンタイムが許容できる。キーを安全に保管し、効果的にローテーションすることが重要である。",
        "option_analysis": "選択肢AはAWS Secrets Managerを利用してキーを安全に保管し、Lambdaを使用して自動ローテーションを行う。選択肢BおよびDはこの特定の要件には十分ではない。選択肢Cは自動的なEC2キー回転を直接サポートしていない。",
        "additional_knowledge": "ローテーションの前に、以前のキーペアの適切なバックアップを取ることを推奨する。",
        "key_terminology": "AWS Secrets Manager, EC2キーペア, AWS Lambda, キーローテーション, 暗号化",
        "overall_assessment": "要件を考慮すると、選択肢Aが最も適したソリューションであり、コミュニティの票も69%でこの選択を支持している。"
      }
    ],
    "keywords": [
      "AWS Secrets Manager",
      "EC2 key pairs",
      "AWS Lambda",
      "key rotation",
      "encryption"
    ]
  },
  {
    "No": "211",
    "question": "A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no\nconfiguration management database and has little knowledge about the utilization of the VMware portfolio.\nA solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がAWSに移行したいと考えています。この企業は、VMware ESXi環境で数千のVMを運用しています。企業には構成管理データベースがなく、VMwareポートフォリオの利用状況についての知識がほとんどありません。ソリューションアーキテクトは、企業がコスト効率の良い移行を計画できるよう、正確なインベントリを提供しなければなりません。最も運用オーバーヘッドが少ない要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight.",
        "text_jp": "AWS Systems Manager Patch Managerを使用して各VMにMigration Evaluatorをデプロイし、収集したデータをAmazon QuickSightでレビューします。"
      },
      {
        "key": "B",
        "text": "Export the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the",
        "text_jp": "VMwareポートフォリオを.csvファイルにエクスポートし、各サーバーのディスク利用率を確認します。利用率が高いサーバーを削除します。"
      },
      {
        "key": "C",
        "text": "Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive",
        "text_jp": "Migration EvaluatorのエージェントレスコレクターをESXiハイパーバイザーにデプロイし、収集したデータをMigration Evaluatorでレビューします。非アクティブなサーバーを特定します。"
      },
      {
        "key": "D",
        "text": "Deploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze",
        "text_jp": "AWS Application Migration Serviceエージェントを各VMにデプロイし、データが収集されたら、Amazon Redshiftを使用してインポートおよび分析します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, deploying the Migration Evaluator agentless collector to the ESXi hypervisor, which helps in gathering the necessary inventory data with least operational overhead.",
        "situation_analysis": "The company lacks a configuration management database and has limited knowledge about its VMware environment, which necessitates a solution that minimizes operational workload while still delivering an accurate inventory.",
        "option_analysis": "Option C allows for agentless data collection, meaning it can collect data without needing to deploy anything on each VM, making it significantly easier than options A and D. Option B requires manual effort to check disk utilization and may miss non-utilized VMs.",
        "additional_knowledge": "In environments with multiple VMs, leveraging agentless methods usually results in a reduced need for management and intervention, allowing for a streamlined process.",
        "key_terminology": "Migration Evaluator, agentless collector, VMware ESXi, operational overhead, inventory assessment",
        "overall_assessment": "Considering the requirements, option C aligns perfectly with the need for operational efficiency and effective inventory assessment, while options A and D introduce more complexity, thus not being optimal solutions."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCであり、Migration EvaluatorのエージェントレスコレクターをESXiハイパーバイザーにデプロイすることで、最も低い運用オーバーヘッドで必要なインベントリデータを収集することができます。",
        "situation_analysis": "企業には構成管理データベースがなく、VMware環境に関する知識が限られているため、業務負荷を最小限に抑えつつ、正確なインベントリを提供するソリューションが必要です。",
        "option_analysis": "オプションCはエージェントレスのデータ収集を可能にし、各VMに何かをデプロイする必要がないため、オプションAやDよりもはるかに簡単です。オプションBは手動でディスク利用状況を確認する必要があり、非利用のVMを見逃す可能性があります。",
        "additional_knowledge": "複数のVMを持つ環境では、エージェントレスの方法を利用することで管理および介入の必要が減少し、スムーズなプロセスが可能になります。",
        "key_terminology": "Migration Evaluator, エージェントレスコレクター, VMware ESXi, 運用オーバーヘッド, インベントリ評価",
        "overall_assessment": "要件を考慮すると、オプションCは運用効率と効果的なインベントリ評価の必要性に完璧に合致しており、オプションAやDはより複雑さをもたらすため最適なソリューションではありません。"
      }
    ],
    "keywords": [
      "Migration Evaluator",
      "agentless collector",
      "VMware ESXi",
      "operational overhead",
      "inventory assessment"
    ]
  },
  {
    "No": "212",
    "question": "A company runs a microservice as an AWS Lambda function. The microservice writes data to an on-premises SQL database that supports a\nlimited number of concurrent connections. When the number of Lambda function invocations is too high, the database crashes and causes\napplication downtime. The company has an AWS Direct Connect connection between the company's VPC and the on-premises data center. The\ncompany wants to protect the database from crashes.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、AWS Lambda 関数としてマイクロサービスを運用しています。そのマイクロサービスは、同時接続数が限られているオンプレミスの SQL データベースにデータを書き込みます。Lambda 関数の呼び出し回数が多すぎる場合、データベースがクラッシュし、アプリケーションがダウンしてしまいます。同社は、同社の VPC とオンプレミスのデータセンターの間に AWS Direct Connect 接続があります。同社は、データベースをクラッシュから保護したいと考えています。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write",
        "text_jp": "データを Amazon Simple Queue Service (Amazon SQS) キューに書き込みます。Lambda 関数を構成して、キューから読み取り、書き込みます。"
      },
      {
        "key": "B",
        "text": "Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the data from the existing database to Aurora Serverless.",
        "text_jp": "新しい Amazon Aurora Serverless DB クラスターを作成します。既存のデータベースから Aurora Serverless にデータを移行するために AWS DataSync を使用します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the Amazon RDS DB instance. Reconfigure the Lambda",
        "text_jp": "Amazon RDS Proxy DB インスタンスを作成します。RDS Proxy DB インスタンスを Amazon RDS DB インスタンスにアタッチします。Lambda を再構成します。"
      },
      {
        "key": "D",
        "text": "Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the Lambda function to write to the existing database",
        "text_jp": "データを Amazon Simple Notification Service (Amazon SNS) トピックに書き込みます。Lambda 関数を呼び出して、既存のデータベースに書き込みます。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Writing data to an Amazon SNS topic allows for decoupling the process and handling spikes in Lambda invocations without overwhelming the database.",
        "situation_analysis": "The existing SQL database has a limit on concurrent connections, which causes it to crash when invoked by multiple Lambda functions simultaneously. The reason to use SNS is that it can buffer the writes and prevent direct overload on the database.",
        "option_analysis": "Option A suggests using SQS, but it would not fully address the real-time requirements of the microservice compared to SNS. Option B introduces a new database service which may not solve current load issues directly. Option C could improve manageability, but it requires setting up RDS Proxy, which may not be necessary in this scenario.",
        "additional_knowledge": "Using SNS can also help in scenarios where different processing services can subscribe to notifications.",
        "key_terminology": "AWS Lambda, AWS Direct Connect, Amazon SNS, concurrency, microservices",
        "overall_assessment": "While option D is the best choice, the community vote heavily favors option A due to its buffering capabilities; however, SNS is more appropriate in this scalability scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは D です。データを Amazon SNS トピックに書き込むことで、プロセスをデカップリングし、Lambda の呼び出しのスパイクを処理しながらデータベースへの負荷を軽減することができます。",
        "situation_analysis": "既存の SQL データベースは同時接続数に制限があり、複数の Lambda 関数が同時に呼び出される場合にクラッシュします。SNS を使用する理由は、書き込みをバッファリングし、データベースへの過負荷を直接防ぐことができるためです。",
        "option_analysis": "オプション A は SQS の使用を提案していますが、SNS に比べてマイクロサービスのリアルタイム要件に完全には対処できません。オプション B は新しいデータベースサービスを導入しますが、現在の負荷の問題を直接解決することはできません。オプション C は管理を改善する可能性がありますが、RDS Proxy のセットアップが必要であり、今回のシナリオでは必要ない可能性があります。",
        "additional_knowledge": "SNS を使用することで、異なる処理サービスが通知にサブスクライブできるシナリオにも対応できます。",
        "key_terminology": "AWS Lambda、AWS Direct Connect、Amazon SNS、同時実行、マイクロサービス",
        "overall_assessment": "オプション D が最適な選択肢ですが、コミュニティ投票はバッファリング能力のためにオプション A に大きく偏っています。しかし、スケーラビリティシナリオでは SNS の方が適切です。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "AWS Direct Connect",
      "Amazon SNS",
      "concurrency",
      "microservices"
    ]
  },
  {
    "No": "213",
    "question": "A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS\nworkloads. The company has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be\nhighly available and cannot be down for longer than 10 minutes. The company needs to minimize ongoing maintenance.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業は、AWSワークロードの健康を監視するために単一のAmazon EC2インスタンス上で実行されているGrafanaデータビジュアライゼーションソリューションを使用しています。企業は、作成したダッシュボードを保存するために多くの時間と労力を投資してきました。ダッシュボードは高可用性である必要があり、10分以上ダウンしてはなりません。企業は継続的なメンテナンスを最小限に抑える必要があります。この要件を最も少ない運用オーバーヘッドで満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate to Amazon CloudWatch dashboards. Recreate the dashboards to match the existing Grafana dashboards. Use automatic",
        "text_jp": "Amazon CloudWatchダッシュボードに移行する。既存のGrafanaダッシュボードに合わせてダッシュボードを再作成する。自動的に使用する"
      },
      {
        "key": "B",
        "text": "Create an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing",
        "text_jp": "Amazon Managed Grafanaワークスペースを作成する。新しいAmazon CloudWatchデータソースを構成する。既存のダッシュボードをエクスポートする"
      },
      {
        "key": "C",
        "text": "Create an AMI that has Grafana pre-installed. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto",
        "text_jp": "Grafanaが事前にインストールされたAMIを作成する。既存のダッシュボードをAmazon Elastic File System（Amazon EFS）に保存する。Autoを作成する"
      },
      {
        "key": "D",
        "text": "Configure AWS Backup to back up the EC2 instance that runs Grafana once each hour. Restore the EC2 instance from the most recent",
        "text_jp": "Grafanaを実行しているEC2インスタンスを毎時バックアップするようにAWS Backupを構成する。最新のバックアップからEC2インスタンスを復元する"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Create an Amazon Managed Grafana workspace and export existing dashboards, which minimizes operational overhead and supports high availability.",
        "situation_analysis": "The company requires a solution that maintains high availability and minimizes downtime and maintenance efforts for the Grafana dashboards.",
        "option_analysis": "Option B is the most suitable as it utilizes Amazon Managed Grafana, which requires less operational management compared to managing an EC2 instance. Options A and C involve recreation of dashboards or doing AMI management, which increases operational overhead. Option D relies on backup and restore, which may not meet the stringent uptime requirement.",
        "additional_knowledge": "Using Amazon Managed services typically leads to reduced operational overhead, freeing teams to focus on higher value tasks.",
        "key_terminology": "Amazon Managed Grafana, High Availability, Amazon CloudWatch, Dashboard Management, EC2.",
        "overall_assessment": "Option B is supported by the community with 100% votes, which indicates strong agreement on the effectiveness of this solution. The other options, while valid, do not meet the operational efficiency and availability requirements as well as option B."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBです：Amazon Managed Grafanaワークスペースを作成し、既存のダッシュボードをエクスポートする方法が、運用オーバーヘッドを最小限に抑え、高可用性をサポートします。",
        "situation_analysis": "企業は、Grafanaダッシュボードの高可用性を維持し、ダウンタイムとメンテナンスの負担を最小限に抑えるソリューションを必要としています。",
        "option_analysis": "Bオプションは、Amazon Managed Grafanaを利用し、EC2インスタンスの管理を軽減するため、最も適しています。AおよびCの選択肢は、ダッシュボードを再作成したりAMI管理を行ったりする必要があり、運用オーバーヘッドを増加させます。Dオプションはバックアップと復元に依存しており、厳しい稼働時間要件を満たさない可能性があります。",
        "additional_knowledge": "Amazonのマネージドサービスを利用すると、運用オーバーヘッドが軽減され、チームがより価値の高い業務に集中する余裕が生まれます。",
        "key_terminology": "Amazon Managed Grafana, 高可用性, Amazon CloudWatch, ダッシュボード管理, EC2。",
        "overall_assessment": "Bオプションはコミュニティで100％の支持を受けており、このソリューションの効果について強い合意を示しています。他の選択肢も有効ですが、Bオプションほど運用効率と可用性の要件を満たすものではありません。"
      }
    ],
    "keywords": [
      "Amazon Managed Grafana",
      "High Availability",
      "Amazon CloudWatch",
      "Dashboard Management",
      "EC2"
    ]
  },
  {
    "No": "214",
    "question": "A company needs to migrate its customer transactions database from on premises to AWS. The database resides on an Oracle DB instance that\nruns on a Linux server. According to a new security requirement, the company must rotate the database password each year.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業が、オンプレミスからAWSに顧客取引データベースを移行する必要があります。このデータベースは、Linuxサーバー上で実行されているOracle DBインスタンスに存在します。新たなセキュリティ要件により、企業は毎年データベースのパスワードをローテーションする必要があります。どのソリューションが、最も管理オーバーヘッドを少なくしてこの要件を満たすことができますか？",
    "choices": [
      {
        "key": "A",
        "text": "Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool (AWS SCT). Store the password in AWS Systems",
        "text_jp": "AWS Schema Conversion Tool (AWS SCT) を使用してデータベースを Amazon DynamoDB に変換します。パスワードを AWS Systems に保存します"
      },
      {
        "key": "B",
        "text": "Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a",
        "text_jp": "データベースを Amazon RDS for Oracle に移行します。パスワードを AWS Secrets Manager に保存します。自動ローテーションをオンにします。"
      },
      {
        "key": "C",
        "text": "Migrate the database to an Amazon EC2 instance. Use AWS Systems Manager Parameter Store to keep and rotate the connection string by",
        "text_jp": "データベースを Amazon EC2 インスタンスに移行します。AWS Systems Manager Parameter Store を使用して接続文字列を保持し、ローテーションします。"
      },
      {
        "key": "D",
        "text": "Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool (AWS SCT). Create an Amazon CloudWatch alarm to",
        "text_jp": "AWS Schema Conversion Tool (AWS SCT) を使用してデータベースを Amazon Neptune に移行します。Amazon CloudWatch アラームを作成します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Migrate the database to an Amazon EC2 instance and use AWS Systems Manager Parameter Store to manage and rotate the database connection string. This approach allows for flexible management and minimal operational overhead, fulfilling the requirement to rotate the password annually.",
        "situation_analysis": "The company requires a migration of an Oracle database while also needing to implement an annual password rotation for security compliance.",
        "option_analysis": "Option C is ideal as it utilizes AWS Systems Manager Parameter Store, which is designed for managing configuration data and secrets. This allows the password to be rotated automatically with little management effort. Option B is also viable, but it requires additional setup with AWS Secrets Manager. Options A and D either involve significant changes to the database architecture or do not directly facilitate password management.",
        "additional_knowledge": "Considering the operational practices and the alignment with AWS best practices, managing database credentials while hosting on EC2 ensures resilience and adaptability.",
        "key_terminology": "AWS Systems Manager, Parameter Store, Oracle, EC2, Secrets Manager, password rotation.",
        "overall_assessment": "While the community vote overwhelmingly supports Answer B, it may not be the least operationally demanding solution. Answer C represents a practical balance between operational overhead and compliance with requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです：データベースをAmazon EC2インスタンスに移行し、AWS Systems Manager Parameter Storeを使用してデータベース接続文字列を管理およびローテーションします。このアプローチは、柔軟な管理を可能にし、年間パスワードローテーションの要件を満たす最小の運用オーバーヘッドを提供します。",
        "situation_analysis": "企業は、Oracleデータベースの移行を必要としており、同時にセキュリティコンプライアンスのために年間パスワードローテーションを実施する必要があります。",
        "option_analysis": "選択肢Cは、AWS Systems Manager Parameter Storeを利用し、構成データと秘密を管理するために設計されたものであるため、理想的です。これにより、パスワードを自動的にローテーションして少ない管理労力で行えます。選択肢Bも実行可能ですが、AWS Secrets Managerでの追加設定が必要です。選択肢AとDは、データベースアーキテクチャに大幅な変更を伴うか、パスワード管理を直接提供しません。",
        "additional_knowledge": "運用慣行とAWSのベストプラクティスに沿った場合、EC2上でホストされているデータベースの認証情報を管理することは、堅牢性と適応性を保証します。",
        "key_terminology": "AWS Systems Manager、Parameter Store、Oracle、EC2、Secrets Manager、パスワードローテーション。",
        "overall_assessment": "コミュニティの投票は圧倒的に選択肢Bを支持していますが、運用的に最も負担の少ないソリューションではないかもしれません。選択肢Cは、運用オーバーヘッドと要件の遵守との実用的なバランスを示しています。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "Parameter Store",
      "Oracle",
      "EC2",
      "Secrets Manager",
      "password rotation"
    ]
  },
  {
    "No": "215",
    "question": "A solutions architect is designing an AWS account structure for a company that consists of multiple teams. All the teams will work in the same\nAWS Region. The company needs a VPC that is connected to the on-premises network. The company expects less than 50 Mbps of total trafic to\nand from the on-premises network.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "question_jp": "ソリューションアーキテクトが、複数のチームで構成される企業のためにAWSアカウント構造を設計しています。すべてのチームは同じAWSリージョンで作業します。企業にはオンプレミスネットワークに接続されたVPCが必要です。企業は、オンプレミスネットワークへの送受信トラフィックが合計50 Mbps未満であることを期待しています。この要件を最も費用対効果の高い方法で満たすための手順の組み合わせはどれですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account.",
        "text_jp": "AWS CloudFormationテンプレートを作成し、VPCと必要なサブネットをプロビジョニングします。テンプレートを各AWSアカウントにデプロイします。"
      },
      {
        "key": "B",
        "text": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account.",
        "text_jp": "AWS CloudFormationテンプレートを作成し、VPCと必要なサブネットをプロビジョニングします。テンプレートを共有サービスアカウントにデプロイします。"
      },
      {
        "key": "C",
        "text": "Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by",
        "text_jp": "AWS Transit Gatewayを使用し、AWS Site-to-Site VPNを介してオンプレミスネットワークとの接続を確立します。"
      },
      {
        "key": "D",
        "text": "Use AWS Site-to-Site VPN for connectivity to the on-premises network.",
        "text_jp": "AWS Site-to-Site VPNを使用してオンプレミスネットワークとの接続を確立します。"
      },
      {
        "key": "E",
        "text": "Use AWS Direct Connect for connectivity to the on-premises network.",
        "text_jp": "AWS Direct Connectを使用してオンプレミスネットワークとの接続を確立します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BD (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D. Using AWS CloudFormation to provision the VPC is an efficient way to ensure consistency, and AWS Site-to-Site VPN provides the necessary connectivity to on-premises at low bandwidth requirements.",
        "situation_analysis": "The company requires a scalable VPC setup connected to its on-premises network with less than 50 Mbps traffic needs. Cost-effectiveness is a priority in designing this structure.",
        "option_analysis": "Option A allows for the creation of a reusable template that can be applied across multiple accounts, ensuring uniformity. Option D directly meets the connectivity need without incurring high costs associated with options like Direct Connect which is more suitable for higher bandwidth requirements.",
        "additional_knowledge": "Direct Connect offers a dedicated connection but is not ideal in this scenario due to the low traffic estimate.",
        "key_terminology": "AWS CloudFormation, AWS VPC, AWS Site-to-Site VPN, on-premises network, cost-effective solution",
        "overall_assessment": "Considering the low bandwidth and multiple teams' requirement, using CloudFormation for provisioning and Site-to-Site VPN is strategically aligned with best practices, while also ensuring low operational costs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとDである。AWS CloudFormationを使用してVPCをプロビジョニングすることは、一貫性を確保するための効率的な方法であり、AWS Site-to-Site VPNは、低帯域幅要件で必要な接続を提供する。",
        "situation_analysis": "企業には、オンプレミスネットワークに接続されたスケーラブルなVPCのセットアップが必要であり、トラフィック要件は50 Mbps未満である。コスト効果が設計の優先事項である。",
        "option_analysis": "選択肢Aは、複数のアカウントで適用できる再利用可能なテンプレートを作成することを可能にし、一貫性を確保する。選択肢Dは、費用のかかるDirect Connectなどのオプションに伴う高コストを回避し、接続ニーズを直接満たす。",
        "additional_knowledge": "Direct Connectは専用接続を提供するが、このシナリオでは低トラフィック推定のために理想的ではない。",
        "key_terminology": "AWS CloudFormation, AWS VPC, AWS Site-to-Site VPN, オンプレミスネットワーク, コスト効果の高いソリューション",
        "overall_assessment": "低帯域幅と複数のチームの要件を考慮すると、プロビジョニングにはCloudFormationを、接続にはSite-to-Site VPNを使用することは、戦略的に最適で、低い運用コストを確保する。 "
      }
    ],
    "keywords": [
      "AWS CloudFormation",
      "AWS VPC",
      "AWS Site-to-Site VPN",
      "on-premises network",
      "cost-effective solution"
    ]
  },
  {
    "No": "216",
    "question": "A solutions architect at a large company needs to set up network security for outbound trafic to the internet from all AWS accounts within an\norganization in AWS Organizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a\ncentralized AWS Transit Gateway. Each account has both an internet gateway and a NAT gateway for outbound trafic to the internet. The company\ndeploys resources only into a single AWS Region.\nThe company needs the ability to add centrally managed rule-based filtering on all outbound trafic to the internet for all AWS accounts in the\norganization. The peak load of outbound trafic will not exceed 25 Gbps in each Availability Zone.\nWhich solution meets these requirements?",
    "question_jp": "大規模企業のソリューションアーキテクトは、AWS Organizations内のすべてのAWSアカウントからインターネットへのアウトバウンドトラフィックに対してネットワークセキュリティを設定する必要があります。組織は100以上のAWSアカウントを持ち、アカウント間のルーティングは中央集権的なAWS Transit Gatewayを使用しています。各アカウントには、インターネットへのアウトバウンドトラフィック用にインターネットゲートウェイとNATゲートウェイの両方があります。会社は単一のAWSリージョンにのみリソースを展開します。この会社は、組織内のすべてのAWSアカウントに対してインターネットへのすべてのアウトバウンドトラフィックに対する中央管理のルールベースのフィルタリングを追加できる能力を必要としています。アウトバウンドトラフィックのピーク負荷は、各アベイラビリティゾーンで25 Gbpsを超えることはありません。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new VPC for outbound trafic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway.",
        "text_jp": "インターネットへのアウトバウンドトラフィック用に新しいVPCを作成します。既存のトランジットゲートウェイを新しいVPCに接続します。新しいNATゲートウェイを設定します。"
      },
      {
        "key": "B",
        "text": "Create a new VPC for outbound trafic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway.",
        "text_jp": "インターネットへのアウトバウンドトラフィック用に新しいVPCを作成します。既存のトランジットゲートウェイを新しいVPCに接続します。新しいNATゲートウェイを設定します。"
      },
      {
        "key": "C",
        "text": "Create an AWS Network Firewall firewall for rule-based filtering in each AWS account. Modify all default routes to point to the Network",
        "text_jp": "各AWSアカウントに対して、ルールベースのフィルタリングのためにAWS Network Firewallを作成します。すべてのデフォルトルートをNetwork Firewallに指すように変更します。"
      },
      {
        "key": "D",
        "text": "In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for",
        "text_jp": "各AWSアカウントにおいて、オープンソースのインターネットプロキシを実行するネットワーク最適化されたAmazon EC2インスタンスのAuto Scalingグループを作成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Creating an Auto Scaling group of network-optimized Amazon EC2 instances to run an open-source internet proxy allows for centralized management and rule-based filtering for all outbound traffic.",
        "situation_analysis": "The requirement is to manage outbound traffic from over 100 AWS accounts centrally, using a configuration that adheres to network security best practices.",
        "option_analysis": "Option A and B create a new VPC without addressing the need for centralized rule-based filtering. Option C, while useful, does not provide the proxy functionality needed for filtering all traffic efficiently. Option D meets the requirement by leveraging open-source proxy software to enforce filtering and manage throughput effectively.",
        "additional_knowledge": "Utilizing auto-scaling allows for resource optimization and cost efficiency, adapting to varying loads.",
        "key_terminology": "AWS Transit Gateway, NAT Gateway, Amazon EC2, Network Optimization, Open-source Internet Proxy",
        "overall_assessment": "While community votes suggest options B might be favored, option D provides the practical requirement of a proxy server facilitating central management and rule filtering."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はD: ネットワーク最適化されたAmazon EC2インスタンスのAuto Scalingグループを作成し、オープンソースのインターネットプロキシを実行することで、すべてのアウトバウンドトラフィックに対する中央管理のルールベースのフィルタリングが可能になります。",
        "situation_analysis": "100以上のAWSアカウントからのアウトバウンドトラフィックを中央管理する必要があるため、ネットワークセキュリティのベストプラクティスに従った構成が求められます。",
        "option_analysis": "選択肢AとBは新しいVPCを作成しますが、中央のルールベースのフィルタリングの必要性に対処していません。選択肢Cは役立つ可能性がありますが、すべてのトラフィックを効率的にフィルタリングするためのプロキシ機能を提供しません。選択肢Dは、オープンソースプロキシソフトウェアを活用してフィルタリングを強制し、スループットを効果的に管理する要件を満たします。",
        "additional_knowledge": "Auto Scalingの利用により、リソースの最適化とコスト効率が可能となり、負荷の変動に対応することができます。",
        "key_terminology": "AWSトランジットゲートウェイ, NATゲートウェイ, Amazon EC2, ネットワーク最適化, オープンソースインターネットプロキシ",
        "overall_assessment": "コミュニティの投票はBを支持することを示唆していますが、選択肢Dは中央管理とルールフィルタリングを促進するプロキシサーバーの実用的な要件を提供します。"
      }
    ],
    "keywords": [
      "AWS Transit Gateway",
      "NAT Gateway",
      "Amazon EC2",
      "Network Optimization",
      "Open-source Internet Proxy"
    ]
  },
  {
    "No": "217",
    "question": "A company uses a load balancer to distribute trafic to Amazon EC2 instances in a single Availability Zone. The company is concerned about\nsecurity and wants a solutions architect to re-architect the solution to meet the following requirements:\n• Inbound requests must be filtered for common vulnerability attacks.\n• Rejected requests must be sent to a third-party auditing application.\n• All resources should be highly available.\nWhich solution meets these requirements?",
    "question_jp": "ある企業が、単一のアベイラビリティーゾーン内の Amazon EC2 インスタンスにトラフィックを分配するためにロードバランサーを使用しています。この企業はセキュリティを懸念しており、ソリューションアーキテクトに次の要件を満たすようにソリューションを再構築することを求めています:\n• 受信リクエストは、一般的な脆弱性攻撃に対してフィルタリングされなければなりません。\n• 拒否されたリクエストは、サードパーティの監査アプリケーションに送信されなければなりません。\n• すべてのリソースは高可用性でなければなりません。\nこの要件を満たす解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously",
        "text_jp": "アプリケーションの AMI を使用して Multi-AZ Auto Scaling グループを構成します。アプリケーションロードバランサー (ALB) を作成し、以前に選択したものを選択します。"
      },
      {
        "key": "B",
        "text": "Configure an Application Load Balancer (ALB) and add the EC2 instances as targets. Create a web ACL in WAF. Create an AWS WAF using",
        "text_jp": "アプリケーションロードバランサー (ALB) を構成し、EC2 インスタンスをターゲットとして追加します。ウェブ ACL を WAF に作成します。"
      },
      {
        "key": "C",
        "text": "Configure an Application Load Balancer (ALB) along with a target group adding the EC2 instances as targets. Create an Amazon Kinesis",
        "text_jp": "アプリケーションロードバランサー (ALB) を構成し、ターゲットグループに EC2 インスタンスをターゲットとして追加します。Amazon Kinesis を作成します。"
      },
      {
        "key": "D",
        "text": "Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously",
        "text_jp": "アプリケーションの AMI を使用して Multi-AZ Auto Scaling グループを構成します。アプリケーションロードバランサー (ALB) を作成し、以前に選択したものを選択します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (80%) A (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. By configuring the ALB and integrating with AWS WAF, inbound requests can be filtered for vulnerabilities, and rejected requests can be processed as required.",
        "situation_analysis": "The company needs a robust solution that balances traffic across instances while ensuring security through filtering and auditing of requests.",
        "option_analysis": "Option A and D do not mention security layers such as WAF, making them less suitable. Option C includes Kinesis but doesn't address the core requirements effectively.",
        "additional_knowledge": "It's important to maintain both scaling and availability while ensuring security measures are in place.",
        "key_terminology": "Application Load Balancer, AWS WAF, Auto Scaling, Security Layer, Inbound Traffic Filtering",
        "overall_assessment": "Despite community votes favoring D, B is the only option that fully meets the outlined security and availability requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は B である。ALB を構成し、AWS WAF と統合することで、受信リクエストを脆弱性に対してフィルタリングし、拒否されたリクエストを必要に応じて処理できる。",
        "situation_analysis": "企業は、トラフィックをインスタンス間でバランスさせる一方で、リクエストのフィルタリングと監査を通じて安全性を確保する堅牢なソリューションが必要である。",
        "option_analysis": "A および D は WAF のようなセキュリティ層に言及していないため、適切さが低い。C は Kinesis を含んでいるが、主要な要件に効果的に対処していない。",
        "additional_knowledge": "スケーリングと可用性を維持しつつ、セキュリティ対策を講じることが重要である。",
        "key_terminology": "アプリケーションロードバランサー, AWS WAF, Auto Scaling, セキュリティ層, 受信トラフィックフィルタリング",
        "overall_assessment": "コミュニティの投票が D を支持しているにもかかわらず、B は示されたセキュリティと可用性の要件を完全に満たす唯一の選択肢である。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "AWS WAF",
      "Auto Scaling"
    ]
  },
  {
    "No": "218",
    "question": "A company is running an application in the AWS Cloud. The application consists of microservices that run on a fieet of Amazon EC2 instances in\nmultiple Availability Zones behind an Application Load Balancer. The company recently added a new REST API that was implemented in Amazon\nAPI Gateway. Some of the older microservices that run on EC2 instances need to call this new API.\nThe company does not want the API to be accessible from the public internet and does not want proprietary data to traverse the public internet.\nWhat should a solutions architect do to meet these requirements?",
    "question_jp": "ある企業がAWSクラウドでアプリケーションを運用している。このアプリケーションは、複数のアベイラビリティゾーンにあるAmazon EC2インスタンスのファイート上で実行されるマイクロサービスで構成されており、アプリケーションロードバランサーの背後にある。最近、企業はAmazon API Gatewayで実装された新しいREST APIを追加した。古いマイクロサービスの中には、この新しいAPIを呼び出す必要があるEC2インスタンス上で動作しているものもある。企業はこのAPIが公共インターネットからアクセスできないようにしたいと考えており、プロプライエタリデータが公共インターネットを通過することを望んでいない。この要件を満たすために、ソリューションアーキテクトは何を行うべきか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Site-to-Site VPN connection between the VPC and the API Gateway. Use API Gateway to generate a unique API Key for each",
        "text_jp": "VPCとAPI Gatewayの間にAWS Site-to-Site VPN接続を作成し、API Gatewayを使用して各APIキーを生成する。"
      },
      {
        "key": "B",
        "text": "Create an interface VPC endpoint for API Gateway, and set an endpoint policy to only allow access to the specific API. Add a resource",
        "text_jp": "API GatewayのためのインターフェースVPCエンドポイントを作成し、特定のAPIへのアクセスのみを許可するエンドポイントポリシーを設定する。"
      },
      {
        "key": "C",
        "text": "Modify the API Gateway to use IAM authentication. Update the IAM policy for the IAM role that is assigned to the EC2 instances to allow",
        "text_jp": "API Gatewayの認証をIAM認証を使用するように変更し、EC2インスタンスに割り当てられたIAMロールのIAMポリシーを更新して許可する。"
      },
      {
        "key": "D",
        "text": "Create an accelerator in AWS Global Accelerator, and connect the accelerator to the API Gateway. Update the route table for all VPC subnets",
        "text_jp": "AWS Global Acceleratorにアクサレレーターを作成し、アクサレレーターをAPI Gatewayに接続する。すべてのVPCサブネットのルートテーブルを更新する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. By using IAM authentication, the API can be securely accessed by the EC2 instances without exposing it to the public internet.",
        "situation_analysis": "The company wants to ensure that the new API remains private and not accessible from the internet, while allowing certain EC2 instances to access it.",
        "option_analysis": "Option A involves setting up a VPN, which is overly complex for this need. Option B could work but does not directly address IAM authentication requirements. Option D is irrelevant for private access.",
        "additional_knowledge": "It is crucial to have the IAM policy precisely configured to restrict access correctly.",
        "key_terminology": "IAM authentication, API Gateway, EC2 instances, VPC, public internet",
        "overall_assessment": "While community voting favors option B, option C meets the requirements of secure access without internet exposure effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。IAM認証を使用することで、EC2インスタンスから安全にAPIにアクセスでき、公共インターネットにさらすことなく運用を行うことができる。",
        "situation_analysis": "企業は新しいAPIがインターネットからのアクセスを受けず、特定のEC2インスタンスがそれにアクセスできることを望んでいる。",
        "option_analysis": "選択肢AはVPNの設定を含むため、必要に対して過剰な複雑さをもたらす。選択肢Bも有効だが、IAM認証の要件には直接対処していない。選択肢Dはプライベートアクセスには関係がない。",
        "additional_knowledge": "IAMポリシーを正確に設定することが、アクセスを適切に制限する上で重要である。",
        "key_terminology": "IAM認証、API Gateway、EC2インスタンス、VPC、公共インターネット",
        "overall_assessment": "コミュニティの投票は選択肢Bを支持しているが、選択肢Cは公共インターネットにさらさずに安全なアクセスの要件を満たしている。"
      }
    ],
    "keywords": [
      "IAM authentication",
      "API Gateway",
      "EC2 instances",
      "VPC",
      "public internet"
    ]
  },
  {
    "No": "219",
    "question": "A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses\nAmazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account.\nOccasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment.\nA solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make\nnoncompliant changes to the security settings for the EC2 instances.\nWhat is the FASTEST way for the solutions architect to meet these requirements?",
    "question_jp": "ある企業は、AWS上に全てのインフラストラクチャを構築しています。企業は、eコマースウェブサイトをホストするためにAmazon EC2インスタンスを使用し、静的データを格納するためにAmazon S3を使用しています。3人のエンジニアが1つのAWSアカウントを通じてクラウド管理と開発を担当しています。時折、あるエンジニアが別のエンジニアのEC2セキュリティグループの設定を変更し、環境内でのコンプライアンス問題を引き起こします。ソリューションアーキテクトは、エンジニアが行う変更を追跡するシステムを構築する必要があります。このシステムは、エンジニアがEC2インスタンスのセキュリティ設定に対して非準拠の変更を行った場合に警告を送信しなければなりません。ソリューションアーキテクトがこれらの要件を満たすための最も迅速な方法は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the",
        "text_jp": "会社のためにAWS Organizationsを設定する。SCPを適用して、行われた非準拠のセキュリティグループ変更を管理および追跡する。"
      },
      {
        "key": "B",
        "text": "Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when",
        "text_jp": "AWS CloudTrailを有効にして、EC2セキュリティグループの変更をキャプチャする。Amazon CloudWatchルールを有効にして、非準拠の変更が行われたときに警告をを提供する。"
      },
      {
        "key": "C",
        "text": "Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment.",
        "text_jp": "AWSアカウントでSCPを有効にして、環境に対して行われた非準拠のセキュリティグループ変更の際に警告を提供する。"
      },
      {
        "key": "D",
        "text": "Enable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple",
        "text_jp": "EC2セキュリティグループでAWS Configを有効にして、非準拠の変更を追跡する。変更をAmazon Simpleを通じて警告として送信する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, enabling AWS CloudTrail to capture changes and using Amazon CloudWatch rules for alerts on noncompliant changes.",
        "situation_analysis": "The company has compliance issues due to engineers altering security group settings, necessitating tracking and alerting.",
        "option_analysis": "Option B provides a direct solution by capturing changes and alerting when noncompliance occurs, aligning with AWS best practices. Option A complicates governance without immediate alerting. Option C does not track changes directly. Option D, while useful, might introduce delays in alerting.",
        "additional_knowledge": "Enabling AWS Config could be useful for long-term compliance tracking, but for immediate alerting, CloudTrail is more effective.",
        "key_terminology": "AWS CloudTrail, Amazon CloudWatch, compliance, security group, alerts",
        "overall_assessment": "Although the community vote favors option D, option B is faster by actively capturing changes as they happen, ensuring immediate compliance alerts."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBであり、AWS CloudTrailを有効にして変更をキャプチャし、Amazon CloudWatchルールを使用して非準拠の変更に対して警告を提供する方法である。",
        "situation_analysis": "企業はエンジニアがセキュリティグループの設定を変更することでコンプライアンスの問題を抱えているため、追跡と警告が必要である。",
        "option_analysis": "選択肢Bは、変更をキャプチャし、非準拠が発生した際に警告を行うという直接的な解決策を提供し、AWSのベストプラクティスに適合している。選択肢Aはガバナンスを複雑にするだけで即時の警告を提供しない。選択肢Cは直接的な変更の追跡を行わない。選択肢Dは有用であるが、警告までの遅延を引き起こす可能性がある。",
        "additional_knowledge": "AWS Configを有効にすることは長期的なコンプライアンス追跡に有用であるが、即時の警告にはCloudTrailがより効果的である。",
        "key_terminology": "AWS CloudTrail, Amazon CloudWatch, コンプライアンス, セキュリティグループ, 警告",
        "overall_assessment": "コミュニティの票は選択肢Dを支持しているが、選択肢Bは変更を即座にキャプチャすることで速やかなコンプライアンス警告を確保するため、より迅速である。"
      }
    ],
    "keywords": [
      "AWS CloudTrail",
      "Amazon CloudWatch",
      "compliance",
      "security group",
      "alerts"
    ]
  },
  {
    "No": "220",
    "question": "A company has IoT sensors that monitor trafic patterns throughout a large city. The company wants to read and collect data from the sensors and\nperform aggregations on the data.\nA solutions architect designs a solution in which the IoT devices are streaming to Amazon Kinesis Data Streams. Several applications are reading\nfrom the stream. However, several consumers are experiencing throttling and are periodically encountering a\nReadProvisionedThroughputExceeded error.\nWhich actions should the solutions architect take to resolve this issue? (Choose three.)",
    "question_jp": "ある企業が大都市全体の交通パターンを監視するIoTセンサーを設置しています。企業はセンサーからデータを読み取り、収集し、データに対して集計を行いたいと考えています。ソリューションアーキテクトは、IoTデバイスがAmazon Kinesis Data Streamsにデータをストリーミングするソリューションを設計しました。複数のアプリケーションがストリームからデータを読み取っていますが、いくつかのコンシューマはスロットリングを経験しており、定期的にReadProvisionedThroughputExceededエラーに遭遇しています。この問題を解決するために、ソリューションアーキテクトはどのようなアクションを取るべきでしょうか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Reshard the stream to increase the number of shards in the stream.",
        "text_jp": "ストリームのシャーディングを行い、ストリーム内のシャードの数を増やす。"
      },
      {
        "key": "B",
        "text": "Use the Kinesis Producer Library (KPL). Adjust the polling frequency.",
        "text_jp": "Kinesis Producer Library (KPL)を使用し、ポーリング頻度を調整する。"
      },
      {
        "key": "C",
        "text": "Use consumers with the enhanced fan-out feature.",
        "text_jp": "強化ファンアウト機能を持つコンシューマを使用する。"
      },
      {
        "key": "D",
        "text": "Reshard the stream to reduce the number of shards in the stream.",
        "text_jp": "ストリームのシャーディングを行い、ストリーム内のシャードの数を減らす。"
      },
      {
        "key": "E",
        "text": "Use an error retry and exponential backoff mechanism in the consumer logic.",
        "text_jp": "コンシューマのロジックにエラーレトライとエクスポネンシャルバックオフメカニズムを使用する。"
      },
      {
        "key": "F",
        "text": "Configure the stream to use dynamic partitioning.",
        "text_jp": "ストリームを動的パーティショニングを使用するように構成する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct actions to resolve the issues with throttling are to reshard the stream to increase the number of shards, as more shards will allow for higher throughput.",
        "situation_analysis": "The company aims to efficiently collect and aggregate data from IoT sensors streaming data into Amazon Kinesis Data Streams. Throttling suggests that the current shard provisioned throughput is insufficient.",
        "option_analysis": "Option A is correct as increasing shards increases the throughput. Option B does not address the core issue of shard limitation. Option C can help if used, but primarily it’s about addressing the shard limit. Option D is incorrect as it reduces capacity. Option E provides a workaround but does not solve capacity issues directly. Option F does not apply as dynamic partitioning is not a feature of Kinesis Data Streams.",
        "additional_knowledge": "Utilizing Kinesis Producer Library can help optimize data ingestion but is secondary to addressing shard capacity.",
        "key_terminology": "Kinesis Data Streams, shards, throttling, provisioned throughput, enhanced fan-out.",
        "overall_assessment": "This question effectively tests knowledge of scaling Kinesis Data Streams and understanding of how to manage throughput issues. The answer key aligns with community voting patterns, affirming A as the most supported resolution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "スロットリングの問題を解決するための正しいアクションは、ストリームのシャーディングを行い、シャードの数を増やすことです。より多くのシャードがあると、スループットが向上します。",
        "situation_analysis": "企業は、IoTセンサーからのデータを効率的に収集および集約することを目指しています。スロットリングは、現在のシャードのプロビジョニングスループットが不十分であることを示唆しています。",
        "option_analysis": "オプションAは正しいです。シャードを増やすことでスループットが向上します。オプションBはシャードの制限という核心を解決しません。オプションCは使用すれば役立つかもしれませんが、主にシャードの制限に対処することです。オプションDは能力を減少させるため不正解です。オプションEは回避策を提供しますが、容量問題を直接解決しません。オプションFはKinesis Data Streamsの機能としては適用できません。",
        "additional_knowledge": "Kinesis Producer Libraryを利用することでデータの取り込みを最適化できますが、シャード容量への対処が優先されます。",
        "key_terminology": "Kinesis Data Streams, シャード, スロットリング, プロビジョニングスループット, 強化ファンアウト。",
        "overall_assessment": "この質問は、Kinesis Data Streamsのスケーリングとスループットの問題管理に関する知識を効果的にテストしています。回答キーはコミュニティ投票パターンと一致しており、Aが最も支持される解決策であると認められています。"
      }
    ],
    "keywords": [
      "Kinesis Data Streams",
      "shards",
      "throttling",
      "provisioned throughput",
      "enhanced fan-out"
    ]
  },
  {
    "No": "221",
    "question": "A company uses AWS Organizations to manage its AWS accounts. The company needs a list of all its Amazon EC2 instances that have\nunderutilized CPU or memory usage. The company also needs recommendations for how to downsize these underutilized instances.\nWhich solution will meet these requirements with the LEAST effort?",
    "question_jp": "企業はAWS Organizationsを使用してAWSアカウントを管理しています。企業は、CPUまたはメモリ使用率が十分に活用されていないすべてのAmazon EC2インスタンスのリストが必要です。また、企業はこれらの十分に活用されていないインスタンスをダウンサイジングする方法に関する推奨事項も必要です。最も労力をかけずにこの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Install a CPU and memory monitoring tool from AWS Marketplace on all the EC2 instances. Store the findings in Amazon S3. Implement a",
        "text_jp": "AWS MarketplaceからすべてのEC2インスタンスにCPUとメモリの監視ツールをインストールします。結果をAmazon S3に保存します。実装します。"
      },
      {
        "key": "B",
        "text": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization",
        "text_jp": "AWS Systems Managerを使用してすべてのEC2インスタンスにAmazon CloudWatchエージェントをインストールします。リソースの最適化を取得します。"
      },
      {
        "key": "C",
        "text": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization",
        "text_jp": "AWS Systems Managerを使用してすべてのEC2インスタンスにAmazon CloudWatchエージェントをインストールします。リソースの最適化を取得します。"
      },
      {
        "key": "D",
        "text": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Create an AWS Lambda function to extract",
        "text_jp": "AWS Systems Managerを使用してすべてのEC2インスタンスにAmazon CloudWatchエージェントをインストールします。抽出するためのAWS Lambda関数を作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which utilizes the Amazon CloudWatch agent installed via AWS Systems Manager to monitor resource usage efficiently.",
        "situation_analysis": "The company needs an overview of underutilized EC2 instances and recommendations for downsizing, which calls for a centralized monitoring solution.",
        "option_analysis": "Option A requires manual installation and thus adds more effort. Options C and D are similar to B, but B provides the best straightforward solution.",
        "additional_knowledge": "Resource optimization is critical in managing costs and performance in AWS environments.",
        "key_terminology": "AWS Systems Manager, Amazon CloudWatch, EC2 instances, resource optimization.",
        "overall_assessment": "All community votes support option B as the preferred solution, aligning best with AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、AWS Systems Managerを使用してインストールされたAmazon CloudWatchエージェントを利用してリソース使用状況を効率的に監視します。",
        "situation_analysis": "企業は十分に活用されていないEC2インスタンスの概要とダウンサイジングに関する推奨事項が必要であるため、集中監視ソリューションが必要です。",
        "option_analysis": "選択肢Aは手動インストールを必要とし、より多くの労力がかかります。選択肢CとDはBと似ていますが、Bは最も簡素な解決策を提供します。",
        "additional_knowledge": "リソースの最適化は、AWS環境におけるコストとパフォーマンスの管理において重要です。",
        "key_terminology": "AWS Systems Manager, Amazon CloudWatch, EC2インスタンス, リソース最適化。",
        "overall_assessment": "コミュニティのすべての投票は、AWSベストプラクティスに最も一致する解決策としてオプションBを支持しています。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "Amazon CloudWatch",
      "EC2 instances",
      "resource optimization"
    ]
  },
  {
    "No": "222",
    "question": "A company wants to run a custom network analysis software package to inspect trafic as trafic leaves and enters a VPC. The company has\ndeployed the solution by using AWS CloudFormation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been\nestablished to direct trafic to the EC2 instances.\nWhenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the\ninstance replacement occurs.\nWhich combination of steps will resolve this issue? (Choose three.)",
    "question_jp": "ある企業は、VPCの出入りするトラフィックを検査するためにカスタムネットワーク分析ソフトウェアパッケージを実行したいと考えています。企業は、CloudFormationを使用して3つのAmazon EC2インスタンスをAuto Scalingグループ内にデプロイしました。すべてのネットワークリーティングは、EC2インスタンスにトラフィックを向けるように確立されています。分析ソフトウェアが動作しなくなると、Auto Scalingグループはインスタンスを置き換えます。インスタンス置き換えが発生してもネットワークルートは更新されません。この問題を解決するためのステップの組み合わせはどれですか？（3つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance.",
        "text_jp": "EC2ステータスチェックメトリクスに基づいてアラームを作成し、失敗したインスタンスを置き換えるようにAuto Scalingグループを設定する。"
      },
      {
        "key": "B",
        "text": "Update the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to",
        "text_jp": "CloudFormationテンプレートを更新して、EC2インスタンスにAmazon CloudWatchエージェントをインストールする。CloudWatchエージェントの設定を行う。"
      },
      {
        "key": "C",
        "text": "Update the CloudFormation template to install AWS Systems Manager Agent on the EC2 instances. Configure Systems Manager Agent to",
        "text_jp": "CloudFormationテンプレートを更新して、EC2インスタンスにAWS Systems Managerエージェントをインストールする。Systems Managerエージェントの設定を行う。"
      },
      {
        "key": "D",
        "text": "Create an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an",
        "text_jp": "失敗シナリオに対するカスタムメトリクスのアラームをAmazon CloudWatchに作成し、アラームがメッセージを発行するように設定する。"
      },
      {
        "key": "E",
        "text": "Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out",
        "text_jp": "Amazon Simple Notification Service（Amazon SNS）メッセージに応じてインスタンスを削除するAWS Lambda関数を作成する。"
      },
      {
        "key": "F",
        "text": "In the CloudFormation template, write a condition that updates the network routes when a replacement instance is launched.",
        "text_jp": "CloudFormationテンプレートに条件を書き、置き換えインスタンスが起動されるときにネットワークルートを更新する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Creating alarms based on EC2 status check metrics will allow the Auto Scaling group to monitor the health of instances effectively.",
        "situation_analysis": "The main requirement is to ensure that network routes are updated whenever an instance is replaced within the Auto Scaling group in relation to the custom network analysis software.",
        "option_analysis": "Option A provides a direct solution to monitor instance health and replace failed instances. Other options may address installation or configuration, but they do not address the core issue of routing updates during instance replacement.",
        "additional_knowledge": "Implementing a robust monitoring strategy with CloudWatch can greatly enhance the overall resilience of the application.",
        "key_terminology": "Auto Scaling, Amazon EC2, CloudWatch, instance health checks, network routing",
        "overall_assessment": "Option A is the most directly relevant solution to the problem. The other options may improve monitoring or installation processes but do not fundamentally solve the routing issue."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA：EC2ステータスチェックメトリクスに基づいてアラームを作成することにより、Auto Scalingグループがインスタンスの健康状態を効果的に監視できるようになります。",
        "situation_analysis": "主な要件は、カスタムネットワーク分析ソフトウェアに関連して、Auto Scalingグループ内のインスタンスが置き換えられるたびにネットワークルートが更新されることを確保することです。",
        "option_analysis": "選択肢Aは、インスタンスの健康を監視し、失敗したインスタンスを置き換えるための直接的な解決策を提供します。他の選択肢は、インストールや設定に関して改善を提供するかもしれませんが、インスタンス置き換え時のルーティング更新のコアな問題を扱っていません。",
        "additional_knowledge": "CloudWatchを使用した堅牢な監視戦略の実装は、アプリケーションの全体的な信頼性を大いに向上させる可能性があります。",
        "key_terminology": "Auto Scaling、Amazon EC2、CloudWatch、インスタンスの健康チェック、ネットワークリーティング",
        "overall_assessment": "選択肢Aは問題に最も直接関連する解決策です。他の選択肢はモニタリングやインストールプロセスを改善するかもしれませんが、ルーティングの問題を根本的に解決するものではありません。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "Amazon EC2",
      "CloudWatch",
      "instance health checks",
      "network routing"
    ]
  },
  {
    "No": "223",
    "question": "A company is developing a new on-demand video application that is based on microservices. The application will have 5 million users at launch\nand will have 30 million users after 6 months. The company has deployed the application on Amazon Elastic Container Service (Amazon ECS) on\nAWS Fargate. The company developed the application by using ECS services that use the HTTPS protocol.\nA solutions architect needs to implement updates to the application by using blue/green deployments. The solution must distribute trafic to each\nECS service through a load balancer. The application must automatically adjust the number of tasks in response to an Amazon CloudWatch alarm.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がマイクロサービスに基づいた新しいオンデマンドビデオアプリケーションを開発しています。このアプリケーションは、開始時に500万人のユーザーを持ち、6ヶ月後には3000万人のユーザーを持つ予定です。企業は、AWS Fargate上のAmazon Elastic Container Service (Amazon ECS)にアプリケーションをデプロイしました。企業は、HTTPSプロトコルを使用するECSサービスを活用してアプリケーションを開発しました。ソリューションアーキテクトは、青/緑デプロイメントを使用してアプリケーションに更新を実装する必要があります。このソリューションは、ロードバランサーを介して各ECSサービスにトラフィックを分散する必要があります。また、アプリケーションは、Amazon CloudWatchアラームに応じてタスクの数を自動的に調整する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Request increases to the service quota for",
        "text_jp": "ECSサービスを青/緑デプロイメントタイプとネットワークロードバランサーを使用するように構成し、サービスクォータの増加をリクエストします。"
      },
      {
        "key": "B",
        "text": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Implement Auto Scaling group for each",
        "text_jp": "ECSサービスを青/緑デプロイメントタイプとネットワークロードバランサーを使用するように構成し、各サービスのためにオートスケーリンググループを実装します。"
      },
      {
        "key": "C",
        "text": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement an Auto Scaling group for",
        "text_jp": "ECSサービスを青/緑デプロイメントタイプとアプリケーションロードバランサーを使用するように構成し、各サービスのためにオートスケーリンググループを実装します。"
      },
      {
        "key": "D",
        "text": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for",
        "text_jp": "ECSサービスを青/緑デプロイメントタイプとアプリケーションロードバランサーを使用するように構成し、サービスオートスケーリングを実装します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (81%) C (19%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This option provides a blue/green deployment using a Network Load Balancer (NLB), which is suitable for non-HTTP traffic; however, it meets the requirement of distributing traffic and can leverage ECS service updates effectively.",
        "situation_analysis": "The company requires a deployment mechanism that can handle a significant increase in users and ensure traffic is efficiently routed to the appropriate ECS deployment.",
        "option_analysis": "Option A aligns well with the requirements as it uses the blue/green deployment strategy and includes traffic distribution through a load balancer. Options B, C, and D provide reasonable configurations but do not satisfy every requirement as effectively.",
        "additional_knowledge": "Auto Scaling can also be used to ensure that the application reacts to changes in workload automatically.",
        "key_terminology": "Amazon ECS, AWS Fargate, Blue/Green Deployment, Network Load Balancer, Auto Scaling.",
        "overall_assessment": "Despite community voting favoring D, option A effectively meets the question's requirements through the appropriate use of a Network Load Balancer and blue/green deployment. Community preferences may not always reflect the technical solution that best fits the scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。この選択肢は、トラフィックを分散し、ECSサービスの更新を効果的に活用できる非HTTPトラフィックに適したネットワークロードバランサー(NLB)を使用した青/緑デプロイメントを提供する。",
        "situation_analysis": "企業は、ユーザーの大幅な増加を処理し、トラフィックが適切なECSデプロイメントへ効率的にルーティングされるデプロイメントメカニズムを必要としている。",
        "option_analysis": "選択肢Aは、青/緑デプロイメント戦略を採用し、ロードバランサーを介してトラフィック分散を含むため、要件と良く一致する。選択肢B、C、Dも理由のある構成を提供するが、すべての要件を同等に満たしていない。",
        "additional_knowledge": "オートスケーリングも、アプリケーションがワークロードの変化に自動的に反応できるようにするために使用される。",
        "key_terminology": "Amazon ECS、AWS Fargate、青/緑デプロイメント、ネットワークロードバランサー、オートスケーリング。",
        "overall_assessment": "コミュニティ投票がDを支持しているにもかかわらず、選択肢Aはネットワークロードバランサーと青/緑デプロイメントの適切な使用により、問題の要件を確実に満たす。コミュニティの選好は、シナリオに最適な技術的解決策を常に反映するわけではないかもしれない。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "AWS Fargate",
      "Blue/Green Deployment",
      "Network Load Balancer",
      "Auto Scaling"
    ]
  },
  {
    "No": "224",
    "question": "A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service\n(Amazon ECS) on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group.\nThe company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the\nnew image version receives a unique tag.\nThe company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically\ndelete new image tags that have Critical or High severity findings. The solution also must notify the development team when such a deletion\noccurs.\nWhich solution meets these requirements?",
    "question_jp": "ある企業がAWSクラウドでコンテナ化されたアプリケーションを運用しています。このアプリケーションは、Amazon Elastic Container Service (Amazon ECS) を使用して、Amazon EC2インスタンスのセット上で実行されています。EC2インスタンスはオートスケーリンググループ内で実行されています。この企業は、Amazon Elastic Container Registry (Amazon ECR)を使用してコンテナイメージを保存しています。新しいイメージバージョンがアップロードされると、新しいイメージバージョンには一意のタグが付与されます。この企業は、新しいイメージバージョンを共通の脆弱性や露出について検査するソリューションが必要です。このソリューションは、重大または高の深刻度の検出結果がある新しいイメージタグを自動的に削除し、そのような削除が行われた際に開発チームに通知する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure scan on push on the repository. Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is",
        "text_jp": "リポジトリでプッシュ時にスキャンを設定します。スキャンが常に行われたときにAmazon EventBridgeを使用してAWS Step Functionsステートマシンを起動します。"
      },
      {
        "key": "B",
        "text": "Configure scan on push on the repository. Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queue.",
        "text_jp": "リポジトリでプッシュ時にスキャンを設定します。スキャン結果をAmazon Simple Queue Service (Amazon SQS)キューにプッシュされるよう設定します。"
      },
      {
        "key": "C",
        "text": "Schedule an AWS Lambda function to start a manual image scan every hour. Configure Amazon EventBridge to invoke another Lambda",
        "text_jp": "AWS Lambda関数をスケジュールして毎時手動でイメージスキャンを開始します。Amazon EventBridgeを設定して、別のLambdaを起動するようにします。"
      },
      {
        "key": "D",
        "text": "Configure periodic image scan on the repository. Configure scan results to be added to an Amazon Simple Queue Service (Amazon SQS)",
        "text_jp": "リポジトリで定期的なイメージスキャンを設定します。スキャン結果がAmazon Simple Queue Service (Amazon SQS)に追加されるように設定します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It involves scheduling an AWS Lambda function to manually scan images for vulnerabilities, with notifications potentially handled through Amazon EventBridge.",
        "situation_analysis": "The company requires a solution that automatically inspects image versions and handles notifications upon deletions. Manual scans provide the flexibility needed.",
        "option_analysis": "Option C allows for manual scans and integration with EventBridge for notifications. Other options either rely on automatic scans or don't provide sufficient notifications.",
        "additional_knowledge": "Understanding how to securely manage container images is vital for maintaining application integrity.",
        "key_terminology": "AWS Lambda, Amazon EventBridge, Amazon ECR, Amazon ECS, vulnerability scanning",
        "overall_assessment": "Option C is the most suitable due to its flexibility and alignment with AWS packaging and development practices. The community seems to support option A."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。これは、AWS Lambda関数をスケジュールしてイメージの脆弱性を手動でスキャンし、通知をAmazon EventBridgeを通じて処理する方法を含む。",
        "situation_analysis": "この企業は、イメージバージョンを自動的に検査し、削除が行われた際に通知を処理する必要がある。手動スキャンは必要な柔軟性を提供する。",
        "option_analysis": "オプションCは手動スキャンを可能にし、通知のためにEventBridgeとの統合を提供する。他のオプションは自動スキャンに依存しているか、十分な通知を提供しない。",
        "additional_knowledge": "コンテナイメージを安全に管理する方法を理解することは、アプリケーションの整合性を維持するために重要である。",
        "key_terminology": "AWS Lambda、Amazon EventBridge、Amazon ECR、Amazon ECS、脆弱性スキャン",
        "overall_assessment": "オプションCはその柔軟性とAWSのパッケージングおよび開発プラクティスとの整合性から最も適切である。コミュニティはオプションAを支持しているようである。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon EventBridge",
      "Amazon ECR",
      "Amazon ECS",
      "vulnerability scanning"
    ]
  },
  {
    "No": "225",
    "question": "A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. The workloads are hosted on Amazon EC2. AWS\nFargate. and AWS Lambda. Some of the workloads have unpredictable demand. Accounts record high usage in some months and low usage in\nother months.\nThe company wants to optimize its compute costs over the next 3 years. A solutions architect obtains a 6-month average for each of the accounts\nacross the organization to calculate usage.\nWhich solution will provide the MOST cost savings for all the organization's compute usage?",
    "question_jp": "ある企業は、AWS上で多くのワークロードを実行しており、AWS Organizationsを使用してアカウントを管理しています。ワークロードはAmazon EC2、AWS Fargate、AWS Lambdaでホストされています。いくつかのワークロードは予測不可能な需要があります。アカウントは特定の月に高い使用率を記録し、他の月には低い使用率を記録します。企業は今後3年間のコンピューティングコストを最適化したいと考えています。ソリューションアーキテクトは、組織全体の各アカウントの使用状況を計算するために、各アカウントの6ヶ月間の平均を取得します。どの解決策が組織のコンピューティング使用状況に対して最もコスト削減を提供しますか？",
    "choices": [
      {
        "key": "A",
        "text": "Purchase Reserved Instances for the organization to match the size and number of the most common EC2 instances from the member",
        "text_jp": "最も一般的なEC2インスタンスのサイズと数に合うように、組織全体でリザーブドインスタンスを購入すること。"
      },
      {
        "key": "B",
        "text": "Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management",
        "text_jp": "管理アカウントから組織のためにコンピュートのセービングプランを購入し、管理者の推奨を使用すること。"
      },
      {
        "key": "C",
        "text": "Purchase Reserved Instances for each member account that had high EC2 usage according to the data from the last 6 months.",
        "text_jp": "過去6ヶ月のデータに基づいて、高いEC2使用率を持つ各メンバーアカウントのためにリザーブドインスタンスを購入すること。"
      },
      {
        "key": "D",
        "text": "Purchase an EC2 Instance Savings Plan for each member account from the management account based on EC2 usage data from the last 6",
        "text_jp": "過去6ヶ月のEC2使用データに基づいて、管理アカウントから各メンバーアカウントのためにEC2インスタンスのセービングプランを購入すること。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Purchasing a Compute Savings Plan allows for flexible usage across different instance types and sizes, optimizing the cost over various workloads and unpredictable demand.",
        "situation_analysis": "The company experiences fluctuating workloads, so a flexible plan will accommodate these variations while still providing cost savings.",
        "option_analysis": "Option A limits savings to specific instance types, while Option C and D may not provide as much flexibility across accounts and workloads.",
        "additional_knowledge": "Understanding the difference between Compute Savings Plans and Reserved Instances is crucial for optimizing AWS costs.",
        "key_terminology": "Compute Savings Plan, AWS Organizations, EC2, Reserved Instances, cost optimization.",
        "overall_assessment": "Given the community support for option B, it aligns well with best practices for handling variable workloads effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。コンピュートのセービングプランを購入することで、異なるインスタンスタイプやサイズに対して柔軟に使用でき、様々なワークロードや予測不可能な需要に対してコストを最適化できます。",
        "situation_analysis": "企業はワークロードの変動を経験しているため、柔軟なプランがこれらの変動に対応しつつ、コスト削減を提供します。",
        "option_analysis": "選択肢Aは特定のインスタンスタイプに限られるため、コスト削減を制限します。また、選択肢CやDは、アカウント間やワークロードに対して同様の柔軟性を提供しない可能性があります。",
        "additional_knowledge": "コンピュートのセービングプランとリザーブドインスタンスの違いを理解することが、AWSコストの最適化にとって重要です。",
        "key_terminology": "コンピュートのセービングプラン、AWS Organizations、EC2、リザーブドインスタンス、コスト最適化。",
        "overall_assessment": "コミュニティの支持がオプションBに寄せられているため、変動するワークロードに効果的に対応するためのベストプラクティスにしっかりと一致しています。"
      }
    ],
    "keywords": [
      "Compute Savings Plan",
      "AWS Organizations",
      "EC2",
      "Reserved Instances",
      "cost optimization"
    ]
  },
  {
    "No": "226",
    "question": "A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations to manage all the accounts. The company\nhas turned on all features.\nA finance team has allocated a daily budget for AWS costs. The finance team must receive an email notification if the organization's AWS costs\nexceed 80% of the allocated budget. A solutions architect needs to implement a solution to track the costs and deliver the notifications.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が数百のAWSアカウントを持っています。この企業は、すべてのアカウントを管理するためにAWS Organizationsを使用しています。企業はすべての機能をオンにしています。財務チームはAWSコストのために日次予算を割り当てました。財務チームは、組織のAWSコストが割り当てられた予算の80％を超える場合に、電子メール通知を受け取る必要があります。ソリューションアーキテクトは、コストを追跡し通知を配信するソリューションを実装する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "In the organization's management account, use AWS Budgets to create a budget that has a daily period. Add an alert threshold and set the",
        "text_jp": "組織の管理アカウントで、AWS Budgetsを使用して日次期間を持つ予算を作成します。アラートしきい値を追加し、設定します。"
      },
      {
        "key": "B",
        "text": "In the organization's management account, set up the organizational view feature for AWS Trusted Advisor. Create an organizational view",
        "text_jp": "組織の管理アカウントで、AWS Trusted Advisorの組織ビュー機能を設定します。組織ビューを作成します。"
      },
      {
        "key": "C",
        "text": "Register the organization with AWS Control Tower. Activate the optional cost control (guardrail). Set a control (guardrail) parameter of 80%.",
        "text_jp": "組織をAWS Control Towerに登録します。オプションのコスト制御（ガードレール）を有効化します。80％の制御（ガードレール）パラメータを設定します。"
      },
      {
        "key": "D",
        "text": "Configure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket in the organization's management",
        "text_jp": "メンバーアカウントを構成して、組織の管理アカウントのAmazon S3バケットに日次AWSコストと使用状況レポートを保存します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. AWS Budgets allows you to create a budget with a specified period, set thresholds, and send notifications.",
        "situation_analysis": "The company needs a system to notify the finance team when spending exceeds 80% of their budget.",
        "option_analysis": "Option A is suitable as it directly addresses the requirements using AWS Budgets. Options B, C, and D do not provide a direct mechanism for budget tracking and notification.",
        "additional_knowledge": "Setting alerts in AWS Budgets ensures timely responses to budgetary concerns.",
        "key_terminology": "AWS Budgets, notifications, email alerts, cost tracking, AWS Organizations",
        "overall_assessment": "The solution proposed in option A is the best practice for budget management and notification within AWS environments, ensuring finance teams can monitor costs accurately."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。AWS Budgetsを使用すると、指定された期間の予算を作成し、しきい値を設定して通知を送信することができます。",
        "situation_analysis": "企業は、支出が予算の80%を超えたときに財務チームに通知するシステムが必要です。",
        "option_analysis": "選択肢Aは、AWS Budgetsを使用して直接要件に対処しているため、適しています。選択肢B、C、Dは予算追跡と通知の直接的なメカニズムを提供していません。",
        "additional_knowledge": "AWS Budgetsでのアラート設定は、予算の懸念に対して迅速に対応できるようにします。",
        "key_terminology": "AWS Budgets、通知、電子メールアラート、コスト追跡、AWS Organizations",
        "overall_assessment": "選択肢Aに提案されたソリューションは、AWS環境内での予算管理と通知のベストプラクティスであり、財務チームがコストを正確に監視できるようにします。"
      }
    ],
    "keywords": [
      "AWS Budgets",
      "AWS Organizations",
      "cost tracking",
      "notifications",
      "budget management"
    ]
  },
  {
    "No": "227",
    "question": "A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon\nEC2 instances in the us-east-1 Region. Artists upload photos of their work as large-size. high-resolution image files from their mobile phones to a\ncentralized Amazon S3 bucket created in the us-east-1 Region. The users in Europe are reporting slow performance for their image uploads.\nHow can a solutions architect improve the performance of the image upload process?",
    "question_jp": "ある企業はアートワークのオークションサービスを提供しており、北米とヨーロッパにユーザーがいます。この企業は、us-east-1リージョンのAmazon EC2インスタンスでアプリケーションをホスティングしています。アーティストは、自分のモバイルフォンから作成した大サイズ・高解像度の画像ファイルを中央集中的に設けられたus-east-1リージョンのAmazon S3バケットにアップロードしています。ヨーロッパのユーザーが画像のアップロードのパフォーマンスが遅いと報告しています。どのようにソリューションアーキテクトは画像アップロードプロセスのパフォーマンスを向上させることができますか？",
    "choices": [
      {
        "key": "A",
        "text": "Redeploy the application to use S3 multipart uploads.",
        "text_jp": "アプリケーションをS3のマルチパートアップロードを使用するように再展開する。"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudFront distribution and point to the application as a custom origin.",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、カスタムオリジンとしてアプリケーションを指す。"
      },
      {
        "key": "C",
        "text": "Configure the buckets to use S3 Transfer Acceleration.",
        "text_jp": "S3転送アクセラレーションを使用するようにバケットを設定する。"
      },
      {
        "key": "D",
        "text": "Create an Auto Scaling group for the EC2 instances and create a scaling policy.",
        "text_jp": "EC2インスタンス用のオートスケーリンググループを作成し、スケーリングポリシーを作成する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (82%) A (18%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Configure the buckets to use S3 Transfer Acceleration, which speeds up uploads by utilizing Amazon CloudFront's globally distributed edge locations.",
        "situation_analysis": "The company faces slow upload speeds for users in Europe, likely due to the geographical distance from the S3 bucket in the us-east-1 Region.",
        "option_analysis": "Option C is the best choice as S3 Transfer Acceleration uses a network of CloudFront edge locations to transfer uploads faster. Option A focuses on improving upload methods, but does not resolve network latencies. Option B adds a CDN but does not address upload performance directly. Option D focuses on scaling which is not necessary for the upload speed issue.",
        "additional_knowledge": "There are additional ways to enhance performance, like establishing direct connections or using AWS Direct Connect for larger enterprises.",
        "key_terminology": "S3 Transfer Acceleration, CloudFront, Edge Locations, Multipart Uploads, Auto Scaling",
        "overall_assessment": "This question effectively assesses knowledge of AWS services related to performance optimization. C is the most supported option by community votes, highlighting its effectiveness in real-world scenarios."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はC: S3転送アクセラレーションを使用するようにバケットを設定することであり、これはAmazon CloudFrontの世界中に分散されたエッジロケーションを利用してアップロードを高速化します。",
        "situation_analysis": "この企業は、ヨーロッパのユーザーに対してアップロード速度が遅い問題を抱えており、これはus-east-1リージョンのS3バケットまでの地理的距離が原因と考えられます。",
        "option_analysis": "選択肢Cはベストな選択肢であり、S3転送アクセラレーションはCloudFrontのエッジロケーションのネットワークを使用してアップロードをより速くします。選択肢Aはアップロード手段の改善に焦点を当てているが、ネットワークの遅延問題を解決しません。選択肢BはCDNを追加しますが、アップロードパフォーマンスに直接影響しません。選択肢Dはスケーリングに重点を置いていますが、アップロード速度の問題には必要ありません。",
        "additional_knowledge": "パフォーマンス向上のための追加の方法として、ダイレクト接続の確立や、AWS Direct Connectを使用してより大規模な企業向けの選択肢があります。",
        "key_terminology": "S3転送アクセラレーション, CloudFront, エッジロケーション, マルチパートアップロード, オートスケーリング",
        "overall_assessment": "この質問は、パフォーマンス最適化に関連するAWSサービスの知識を効果的に評価しています。Cはコミュニティ投票によって最も支持されており、実際のシナリオでの効果を示しています。"
      }
    ],
    "keywords": [
      "S3 Transfer Acceleration",
      "CloudFront",
      "Edge Locations",
      "Multipart Uploads",
      "Auto Scaling"
    ]
  },
  {
    "No": "228",
    "question": "A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application\nincludes web. application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed\ndata must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in\ntrafic.\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?",
    "question_jp": "ある企業がマルチティアのウェブアプリケーションをコンテナ化し、アプリケーションをオンプレミスのデータセンターからAWSに移行したいと考えています。このアプリケーションには、ウェブ、アプリケーション、およびデータベースのティアが含まれています。企業は、アプリケーションをフェイルオーバー対応かつスケーラブルにする必要があります。頻繁にアクセスされるデータがアプリケーションサーバーの間で常に利用可能である必要があります。フロントエンドのウェブサーバーにはセッションの永続性が必要であり、トラフィックの増加に対応してスケールしなければなりません。どのソリューションが最も少ない運用オーバーヘッドでこれらの要件を満たすことができますか？",
    "choices": [
      {
        "key": "A",
        "text": "Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS)",
        "text_jp": "Amazon Elastic Container Service（Amazon ECS）をAWS Fargate上で実行し、Amazon Elastic File System（Amazon EFS）を使用する"
      },
      {
        "key": "B",
        "text": "Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache",
        "text_jp": "Amazon Elastic Container Service（Amazon ECS）をAmazon EC2上で実行し、Amazon ElastiCache for Redisを使用してキャッシュする"
      },
      {
        "key": "C",
        "text": "Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use",
        "text_jp": "Amazon Elastic Kubernetes Service（Amazon EKS）を使用してアプリケーションを実行し、Amazon EKSをマネージドノードグループを使用して構成する"
      },
      {
        "key": "D",
        "text": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the",
        "text_jp": "Amazon Elastic Kubernetes Service（Amazon EKS）上でアプリケーションをデプロイし、Amazon EKSをマネージドノードグループを使用して構成する。実行する"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (82%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. It provides a scalable architecture with low operational overhead using Amazon ElastiCache.",
        "situation_analysis": "The company needs to ensure fault tolerance and scalability while maintaining session persistence and availability of frequently accessed data.",
        "option_analysis": "Option B allows for caching of frequently accessed data, reducing database load and improving performance. Option A requires management of EFS, which can increase operational overhead. Option C and D may introduce complexities with Kubernetes management.",
        "additional_knowledge": "In microservices architecture, caching can simplify state management and session persistence significantly.",
        "key_terminology": "Amazon ElastiCache, Amazon ECS, containerization, fault tolerance, scalability",
        "overall_assessment": "The community vote heavily favors option D, likely due to familiarity with Kubernetes; however, B meets the requirements with less operational complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。Amazon ElastiCacheを使用することで、スケーラブルなアーキテクチャを低い運用オーバーヘッドで提供する。",
        "situation_analysis": "企業は、セッションの永続性と頻繁にアクセスされるデータの可用性を維持しながら、フェイルオーバー対応とスケーラブルである必要がある。",
        "option_analysis": "Bオプションは、頻繁にアクセスされるデータのキャッシュを可能にし、データベースの負荷を軽減し、パフォーマンスを向上させる。AオプションはEFSの管理が必要であり、運用オーバーヘッドが増加する可能性がある。CおよびDオプションはKubernetesの管理の複雑さをもたらす可能性がある。",
        "additional_knowledge": "マイクロサービスアーキテクチャでは、キャッシングが状態管理やセッションの永続性を大幅に簡素化できる。",
        "key_terminology": "Amazon ElastiCache, Amazon ECS, コンテナ化, フェイルオーバー対応, スケーラビリティ",
        "overall_assessment": "コミュニティの投票はDオプションに大きく偏っているが、これはKubernetesへの親しみから来ていると思われる。しかし、Bは運用複雑性が少なく要件を満たしている。"
      }
    ],
    "keywords": [
      "Amazon ElastiCache",
      "Amazon ECS",
      "containerization",
      "fault tolerance",
      "scalability"
    ]
  },
  {
    "No": "229",
    "question": "A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the\nsolutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero\ndowntime.\nWhich solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、重要なMicrosoft SQL ServerデータベースをAWSに移行する計画を立てています。データベースはレガシーシステムであるため、ソリューションアーキテクトはデータベースを現代のデータアーキテクチャに移行します。ソリューションアーキテクトは、ほぼダウンタイムゼロでデータベースを移行しなければなりません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the",
        "text_jp": "AWSアプリケーションマイグレーションサービスとAWSスキーマ変換ツール（AWS SCT）を使用します。インプレースアップグレードを実行します。"
      },
      {
        "key": "B",
        "text": "Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC)",
        "text_jp": "AWSデータベースマイグレーションサービス（AWS DMS）を使用してデータベースを再ホストします。ターゲットとしてAmazon S3を設定します。変更データキャプチャ（CDC）を設定します。"
      },
      {
        "key": "C",
        "text": "Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure",
        "text_jp": "ネイティブデータベースの高可用性ツールを使用します。ソースシステムをAmazon RDS for Microsoft SQL Server DBインスタンスに接続します。構成します。"
      },
      {
        "key": "D",
        "text": "Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database",
        "text_jp": "AWSアプリケーションマイグレーションサービスを使用します。データベースサーバーをAmazon EC2に再ホストします。データレプリケーションが完了したら、データベースを切り離します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (67%) B (33%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Native database high availability tools enable migration with near-zero downtime by allowing the source database to remain operational during the transition.",
        "situation_analysis": "Key requirements are to migrate legacy Microsoft SQL Server databases to a modern architecture with minimal downtime.",
        "option_analysis": "Option A involves an in-place upgrade, which does not guarantee near-zero downtime. Option B suggests setting S3 as a target, which isn't suitable for SQL Server. Option D involves rehosting, which may have downtime during the detachment.",
        "additional_knowledge": "This approach can also leverage replication technologies to further minimize disruption.",
        "key_terminology": "Amazon RDS, High Availability, Microsoft SQL Server, Migration, Change Data Capture",
        "overall_assessment": "Option C is best aligned with best practices for ensuring high availability during migration. It addresses downtime concerns effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。ネイティブデータベースの高可用性ツールを使用することで、移行中にソースデータベースを稼働したままにでき、ほぼダウンタイムゼロでの移行が可能になります。",
        "situation_analysis": "主要な要件は、レガシーのMicrosoft SQL Serverデータベースを現代のアーキテクチャに移行し、最小限のダウンタイムで行うことです。",
        "option_analysis": "オプションAはインプレースアップグレードを含み、ほぼダウンタイムゼロを保証しません。オプションBはターゲットとしてS3を設定する提案ですが、SQL Serverには適していません。オプションDは再ホストを含みますが、切り離しの際にダウンタイムが発生する可能性があります。",
        "additional_knowledge": "このアプローチは、混乱をさらに最小限に抑えるためにレプリケーション技術を活用することもできます。",
        "key_terminology": "Amazon RDS, 高可用性, Microsoft SQL Server, マイグレーション, 変更データキャプチャ",
        "overall_assessment": "オプションCは、移行中の高可用性を確保するためのベストプラクティスと最も整合性があります。ダウンタイムの懸念に対処しています。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "High Availability",
      "Microsoft SQL Server",
      "Migration",
      "Change Data Capture"
    ]
  },
  {
    "No": "230",
    "question": "A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability\nZones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has\ncreated multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created\nmultiple service consumer applications in the other organization.\nData transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect\nmust recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the\nwhole environment.\nWhich guidelines meet these requirements? (Choose two.)",
    "question_jp": "ある企業のソリューションアーキテクトが複数アプリケーション環境のコストを分析しています。この環境は、単一のAWSリージョン内の複数のアベイラビリティゾーンに展開されています。最近の買収により、同社はAWS Organizationsで二つの組織を管理しています。同社は、1つの組織に複数のサービスプロバイダーアプリケーションをAWS PrivateLinkによるVPCエンドポイントサービスとして作成しました。同社は、他の組織において複数のサービスコンシューマアプリケーションを作成しました。データ転送料金が予想よりも高く、ソリューションアーキテクトはコストを削減する必要があります。ソリューションアーキテクトは、サービスを展開する際に開発者が従うべきガイドラインを推奨しなければなりません。これらのガイドラインは、全体の環境のデータ転送料金を最小限に抑える必要があります。どのガイドラインがこれらの要件を満たしますか？（二つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the",
        "text_jp": "AWS Resource Access Managerを使用して、サービスプロバイダーアプリケーションをホストするサブネットを他のアカウントと共有します。"
      },
      {
        "key": "B",
        "text": "Place the service provider applications and the service consumer applications in AWS accounts in the same organization.",
        "text_jp": "サービスプロバイダーアプリケーションとサービスコンシューマアプリケーションを同じ組織内のAWSアカウントに配置します。"
      },
      {
        "key": "C",
        "text": "Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.",
        "text_jp": "すべてのサービスプロバイダーアプリケーションデプロイメントにおいて、Network Load Balancerのクロスゾーンロードバランシングをオフにします。"
      },
      {
        "key": "D",
        "text": "Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS",
        "text_jp": "サービスコンシューマ計算リソースがエンドポイントのローカルDNSを使用してアベイラビリティゾーン特有のエンドポイントサービスを利用することを確認します。"
      },
      {
        "key": "E",
        "text": "Create a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage.",
        "text_jp": "組織の計画されたアベイラビリティゾーン間データ転送使用に対して十分なカバレッジを提供するSavings Planを作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (37%) BD (33%) CD (30%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D. A emphasizes sharing resources effectively within organizations, while D highlights utilizing local endpoint services to reduce data transfer costs.",
        "situation_analysis": "The company manages two organizations and has created applications that need to communicate across VPCs. High data transfer charges are occurring between these applications necessitating a solution.",
        "option_analysis": "Option A allows resource sharing, potentially reducing unnecessary data transfer across organizations. Option D ensures that data stays within the same Availability Zone, further decreasing costs. Options B, C, and E do not adequately address the data transfer charge issue.",
        "additional_knowledge": "Each option should be evaluated against AWS architecture best practices for cost optimization.",
        "key_terminology": "AWS PrivateLink, VPC, Data Transfer Costs, Availability Zone, Resource Access Manager",
        "overall_assessment": "The question is well-framed, focusing on cost-reduction strategies essential for large multi-account AWS environments. Given the community support for options A and D, they are valid approaches."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとDである。Aは組織内でのリソースの効果的な共有を強調しており、Dはデータ転送料金を減らすためにローカルエンドポイントサービスを利用することを強調している。",
        "situation_analysis": "この企業は二つの組織を管理しており、VPC間で通信する必要があるアプリケーションを作成している。高いデータ転送料金がこれらのアプリケーション間で発生しており、解決策が求められている。",
        "option_analysis": "選択肢Aはリソースの共有を可能にし、組織を横断する不必要なデータ転送を減少させる可能性がある。選択肢Dは、データが同じアベイラビリティゾーン内に留まることを保証し、コストをさらに減少させる。選択肢B、C、Eはデータ転送料金の問題を十分に解決しない。",
        "additional_knowledge": "各選択肢は、コスト最適化に関するAWSアーキテクチャのベストプラクティスに照らして評価されるべきである。",
        "key_terminology": "AWS PrivateLink, VPC, データ転送コスト, アベイラビリティゾーン, リソースアクセスマネージャ",
        "overall_assessment": "この質問は適切に構成されており、大規模なマルチアカウントAWS環境におけるコスト削減戦略に焦点を当てている。コミュニティがAとDを支持していることを考慮すると、これらは有効なアプローチである。"
      }
    ],
    "keywords": [
      "AWS PrivateLink",
      "VPC",
      "Data Transfer Costs",
      "Availability Zone",
      "Resource Access Manager"
    ]
  },
  {
    "No": "231",
    "question": "A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move\nthe backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-\npremises data center and AWS.\nWhich solution meets these requirements MOST cost-effectively?",
    "question_jp": "ある企業がオンプレミスのMicrosoft SQL Serverデータベースを持ち、毎晩200GBのエクスポートをローカルドライブに書き込んでいます。企業はバックアップをAmazon S3のより堅牢なクラウドストレージに移動したいと考えています。企業はオンプレミスのデータセンターとAWSの間に10 GbpsのAWS Direct Connect接続を設定しました。この要件を最もコスト効率の良い方法で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection.",
        "text_jp": "新しいS3バケットを作成する。Direct Connect接続に接続されたVPC内にAWS Storage Gatewayファイルゲートウェイを展開する。"
      },
      {
        "key": "B",
        "text": "Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection.",
        "text_jp": "Direct Connect接続に接続されたVPC内にAmazon FSx for Windows File ServerのSingle-AZファイルシステムを作成する。"
      },
      {
        "key": "C",
        "text": "Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection.",
        "text_jp": "Direct Connect接続に接続されたVPC内にAmazon FSx for Windows File ServerのMulti-AZファイルシステムを作成する。"
      },
      {
        "key": "D",
        "text": "Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect",
        "text_jp": "新しいS3バケットを作成する。Direct Connectに接続されたVPC内にAWS Storage Gatewayボリュームゲートウェイを展開する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create a new S3 bucket and deploy an AWS Storage Gateway file gateway within the VPC connected to the Direct Connect connection.",
        "situation_analysis": "The company's requirement is to store large backups in Amazon S3 efficiently and cost-effectively, utilizing a Direct Connect connection for high-speed data transfer.",
        "option_analysis": "Option A is the most cost-effective solution as it uses an AWS Storage Gateway file gateway to transfer files directly to S3, which is optimized for such scenarios. Options B and C involve FSx, which is a more expensive solution for just storing backups. Option D introduces a volume gateway, which might not be the best fit for this use case.",
        "additional_knowledge": "Other options, while potentially viable, do not offer the same cost efficiency and suitability for the specific requirement of nightly large exports.",
        "key_terminology": "AWS Storage Gateway, Amazon S3, AWS Direct Connect, file gateway, cost efficiency",
        "overall_assessment": "Option A aligns perfectly with the requirements and provides a seamless transition to AWS S3 for backup storage without incurring high costs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はA：新しいS3バケットを作成し、Direct Connect接続に接続されたVPC内にAWS Storage Gatewayファイルゲートウェイを展開することです。",
        "situation_analysis": "企業の要件は、大規模なバックアップを効率的かつコスト効果の高い方法でAmazon S3に保存し、高速データ転送のためにDirect Connect接続を利用することです。",
        "option_analysis": "選択肢Aは、AWS Storage Gatewayファイルゲートウェイを使用してファイルを直接S3に転送するため、このシナリオに最適化されているため、最もコスト効率の良いソリューションです。選択肢BとCは、バックアップを保存するためのより高価なFSxを含んでいます。選択肢Dはボリュームゲートウェイを導入していますが、このユースケースに最適ではないかもしれません。",
        "additional_knowledge": "他の選択肢は、有効かもしれませんが、夜間の大規模エクスポートの特定の要件に対するコスト効率と適合性を提供しません。",
        "key_terminology": "AWS Storage Gateway, Amazon S3, AWS Direct Connect, ファイルゲートウェイ, コスト効率",
        "overall_assessment": "選択肢Aは要件と完全に一致し、バックアップストレージとしてAWS S3へのシームレスな移行を高コストをかけることなく提供します。"
      }
    ],
    "keywords": [
      "AWS Storage Gateway",
      "Amazon S3",
      "AWS Direct Connect",
      "file gateway",
      "cost efficiency"
    ]
  },
  {
    "No": "232",
    "question": "A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are\nlocated in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound\ntrafic costs, increase bandwidth throughput, and provide a consistent network experience for end users.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、自社のオンプレミスデータセンターからAWSへの接続を確立する必要があります。この企業は、異なるAWSリージョンにあるすべてのVPCを、VPCネットワーク間の経路転送機能を持って接続する必要があります。また、ネットワークのアウトバウンドトラフィックコストを削減し、帯域幅を増加させ、エンドユーザーに対して一貫したネットワーク体験を提供する必要があります。これらの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections",
        "text_jp": "オンプレミスデータセンターと新しい中央VPCの間にAWS Site-to-Site VPN接続を作成します。VPCピアリング接続を作成します"
      },
      {
        "key": "B",
        "text": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct",
        "text_jp": "オンプレミスデータセンターとAWSの間にAWS Direct Connect接続を作成します。トランジットVIFをプロビジョニングし、これを接続します"
      },
      {
        "key": "C",
        "text": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPUse a transit gateway with dynamic",
        "text_jp": "オンプレミスデータセンターと新しい中央VPCの間にAWS Site-to-Site VPN接続を作成します。トランジットゲートウェイを使用し、動的な"
      },
      {
        "key": "D",
        "text": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection",
        "text_jp": "オンプレミスデータセンターとAWSの間にAWS Direct Connect接続を作成します。AWS Site-to-Site VPN接続を確立します"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves creating an AWS Direct Connect connection. This is essential for establishing a dedicated network connection that reduces costs and increases throughput.",
        "situation_analysis": "The company requires a connection between its on-premises data center and AWS with transitive routing capabilities between multiple VPCs across different regions. High bandwidth and low cost are critical requirements.",
        "option_analysis": "Option A does not facilitate transitive routing between multiple VPCs efficiently. Option C, while introducing a Site-to-Site VPN, does not provide the bandwidth benefits of Direct Connect. Option D combines both solutions but does not fully utilize the Direct Connect benefits.",
        "additional_knowledge": "Direct Connect provides improved performance for applications by reducing latency and improving reliability.",
        "key_terminology": "AWS Direct Connect, transitive routing, bandwidth, VPC peering, Site-to-Site VPN",
        "overall_assessment": "Given the requirements, option B is the most effective solution. The approach aligns perfectly with AWS best practices for connectivity across multiple regions with cost efficiency."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBで、AWS Direct Connect接続の作成を伴います。これは、コストを削減し、スループットを増加させる専用のネットワーク接続を確立するために不可欠です。",
        "situation_analysis": "この企業は、オンプレミスデータセンターとAWS間の接続を必要としており、異なるリージョンにわたる複数のVPC間での経路転送機能を必要としています。高帯域幅と低コストが重要な要件です。",
        "option_analysis": "オプションAは、複数のVPC間で効率的に経路転送を行うことができません。オプションCは、Site-to-Site VPNを導入しますが、Direct Connectの帯域幅の利点を提供していません。オプションDは両方のソリューションを組み合わせていますが、Direct Connectの利点を完全には活用していません。",
        "additional_knowledge": "Direct Connectはアプリケーションのパフォーマンスを向上させ、遅延を減少させ、信頼性を向上させます。",
        "key_terminology": "AWS Direct Connect, 経路転送, 帯域幅, VPCピアリング, Site-to-Site VPN",
        "overall_assessment": "要件を考慮すると、オプションBが最も効果的なソリューションです。このアプローチは、コスト効率で複数のリージョン間の接続性に関するAWSのベストプラクティスと完全に一致しています。"
      }
    ],
    "keywords": [
      "AWS Direct Connect",
      "transitive routing",
      "bandwidth",
      "VPC peering",
      "Site-to-Site VPN"
    ]
  },
  {
    "No": "233",
    "question": "A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a\nseparate member account for development and a separate member account for production. Consolidated billing is linked to the management\naccount. In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in both member\naccounts.\nWhich solution will meet this requirement?",
    "question_jp": "ある企業はAWS Organizations内の新しい組織に開発および生産ワークロードを移行しています。この企業は開発用のメンバーアカウントと生産用のメンバーアカウントを別々に作成しました。統合請求は管理アカウントにリンクされています。管理アカウント内で、ソリューションアーキテクトは両方のメンバーアカウント内のリソースを停止または終了できるIAMユーザーを作成する必要があります。この要件を満たすための解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to",
        "text_jp": "管理アカウントにIAMユーザーとクロスアカウントロールを作成します。クロスアカウントロールを最小権限アクセスで設定します。"
      },
      {
        "key": "B",
        "text": "Create an IAM user in each member account. In the management account, create a cross-account role that has least privilege access. Grant",
        "text_jp": "各メンバーアカウントにIAMユーザーを作成します。管理アカウントで最小権限アクセスを持つクロスアカウントロールを作成します。"
      },
      {
        "key": "C",
        "text": "Create an IAM user in the management account. In the member accounts, create an IAM group that has least privilege access. Add the IAM",
        "text_jp": "管理アカウントにIAMユーザーを作成します。メンバーアカウント内に最小権限アクセスを持つIAMグループを作成します。IAM"
      },
      {
        "key": "D",
        "text": "Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant",
        "text_jp": "管理アカウントにIAMユーザーを作成します。メンバーアカウント内に最小権限アクセスを持つクロスアカウントロールを作成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Create an IAM user in the management account and create cross-account roles in the member accounts with least privilege access.",
        "situation_analysis": "The requirement is to manage resources across multiple accounts in AWS Organizations while adhering to security best practices by using least privilege.",
        "option_analysis": "Option D is the best choice as it allows centralized management from the management account while also ensuring that each member account has specific roles with minimal access permissions. Option A does not provide the necessary access in the member accounts, while B and C incorrectly suggest creating users instead of roles in member accounts.",
        "additional_knowledge": "Properly configured IAM roles can allow for auditing and can simplify permission management across multiple AWS accounts.",
        "key_terminology": "IAM, least privilege, cross-account roles, AWS Organizations, resource management",
        "overall_assessment": "Option D effectively balances the need for access and security, which is crucial in a multi-account AWS environment. The community vote confirms the choice as correct."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはD：管理アカウントにIAMユーザーを作成し、メンバーアカウント内に最小権限アクセスを持つクロスアカウントロールを作成することです。",
        "situation_analysis": "要件はAWS Organizations内の複数アカウントのリソースを管理しながら、安全性のベストプラクティスに従って最小権限を使用することです。",
        "option_analysis": "選択肢Dは、管理アカウントから中央管理を可能にし、各メンバーアカウントに特定の権限を持つロールを設定し、最小限のアクセス許可を保証します。選択肢Aは、メンバーアカウントで必要なアクセスを提供せず、BとCはメンバーアカウント内でユーザーを作成することを誤って示唆しています。",
        "additional_knowledge": "適切に構成されたIAMロールは監査を可能にし、複数のAWSアカウント間での権限管理を簡素化できます。",
        "key_terminology": "IAM, 最小権限, クロスアカウントロール, AWS Organizations, リソース管理",
        "overall_assessment": "選択肢Dは、アクセスとセキュリティのニーズを効果的にバランスさせており、マルチアカウントAWS環境で重要です。コミュニティ投票もこの選択が正しいことを確認しています。"
      }
    ],
    "keywords": [
      "IAM",
      "least privilege",
      "cross-account roles",
      "AWS Organizations",
      "resource management"
    ]
  },
  {
    "No": "234",
    "question": "A company wants to use AWS for disaster recovery for an on-premises application. The company has hundreds of Windows-based servers that run\nthe application. All the servers mount a common share.\nThe company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support native failover and fallback capabilities.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業がオンプレミスアプリケーションの災害復旧にAWSを使用したいと考えています。この企業はアプリケーションを実行する何百ものWindowsサーバーを持っています。すべてのサーバーは共通の共有ストレージをマウントしています。\n企業には15分のRTOと5分のRPOがあります。このソリューションはネイティブのフェイルオーバーおよびフォールバック機能をサポートする必要があります。\nどのソリューションが最もコスト効率よくこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Storage Gateway File Gateway. Schedule daily Windows server backups. Save the data to Amazon S3. During a disaster,",
        "text_jp": "AWSストレージゲートウェイファイルゲートウェイを作成します。毎日のWindowsサーバーのバックアップをスケジュールします。データをAmazon S3に保存します。災害時には、"
      },
      {
        "key": "B",
        "text": "Create a set of AWS CloudFormation templates to create infrastructure. Replicate all data to Amazon Elastic File System (Amazon EFS) by",
        "text_jp": "AWS CloudFormationテンプレートのセットを作成してインフラストラクチャを作成します。すべてのデータをAmazon Elastic File System (Amazon EFS) にレプリケートします。"
      },
      {
        "key": "C",
        "text": "Create an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-active environment on AWS. Replicate data into",
        "text_jp": "AWS Cloud Development Kit (AWS CDK)パイプラインを作成して、AWS上にマルチサイトアクティブ-アクティブ環境を立ち上げます。データをレプリケートします。"
      },
      {
        "key": "D",
        "text": "Use AWS Elastic Disaster Recovery to replicate the on-premises servers. Replicate data to an Amazon FSx for Windows File Server file",
        "text_jp": "AWS Elastic Disaster Recoveryを使用してオンプレミスサーバーをレプリケートします。データをAmazon FSx for Windows File Serverファイルにレプリケートします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves creating a set of AWS CloudFormation templates to establish infrastructure and replicate all data to Amazon EFS.",
        "situation_analysis": "The company needs a solution with a recovery time objective (RTO) of 15 minutes and a recovery point objective (RPO) of 5 minutes, which necessitates real-time data replication and quick failover capabilities.",
        "option_analysis": "Option B meets the requirements for cost-effectiveness and capability. Option A does not provide continuous data synchronization, and while options C and D may offer robust solutions, they involve more complexity and potential costs.",
        "additional_knowledge": "The decision should focus on leveraging Amazon EFS for file-level access while implementing CloudFormation for quick deployment.",
        "key_terminology": "AWS CloudFormation, Amazon EFS, RTO, RPO, Elastic Disaster Recovery",
        "overall_assessment": "The chosen option aligns with AWS best practices for disaster recovery while balancing cost and required capabilities. The community vote suggests confusion or lack of understanding regarding the correct approach, focusing instead on managing the underlying servers rather than optimizing the infrastructure."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBであり、AWS CloudFormationテンプレートのセットを作成してインフラストラクチャを構築し、すべてのデータをAmazon EFSにレプリケートする方法です。",
        "situation_analysis": "企業は15分のRTOと5分のRPOの要件を満たすソリューションを必要としており、これにはリアルタイムのデータレプリケーションと迅速なフェイルオーバー機能が求められます。",
        "option_analysis": "Bの選択肢はコスト効率と能力を兼ね備えています。選択肢Aは継続的なデータ同期を提供せず、選択肢CとDは堅牢なソリューションを提供するかもしれませんが、複雑さと潜在的なコストが増加します。",
        "additional_knowledge": "決定は、ファイルレベルアクセスのためにAmazon EFSを活用し、迅速な展開のためにCloudFormationを実装すべきです。",
        "key_terminology": "AWS CloudFormation、Amazon EFS、RTO、RPO、Elastic Disaster Recovery",
        "overall_assessment": "選ばれたオプションは、災害復旧のためのAWSのベストプラクティスに沿っており、コストと必要な機能のバランスを保っています。コミュニティの投票は、正しいアプローチに対する混乱や理解不足を示唆しており、基盤サーバーの管理に焦点を当てすぎています。"
      }
    ],
    "keywords": [
      "AWS CloudFormation",
      "Amazon EFS",
      "RTO",
      "RPO",
      "Elastic Disaster Recovery"
    ]
  },
  {
    "No": "235",
    "question": "A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared\nfiles stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when\nthe company increased the cluster size to 1.000 EC2 instances, overall performance was well below expectations.\nWhich collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose\nthree.)",
    "question_jp": "ある企業が、Amazon EFS に保存された多数の共有ファイルを生成する密に結合したワークロードのために、AWS に高性能コンピューティング (HPC) クラスターを構築しました。クラスターが 100 の Amazon EC2 インスタンスの場合は良好に動作していましたが、クラスターのサイズを 1,000 の EC2 インスタンスに増やした際、全体のパフォーマンスは期待を大きく下回りました。HPC クラスターから最大のパフォーマンスを引き出すために、ソリューションアーキテクトはどのデザインの選択肢を選ぶべきですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Ensure the HPC cluster is launched within a single Availability Zone.",
        "text_jp": "HPC クラスターが単一のアベイラビリティーゾーン内で起動されるようにする。"
      },
      {
        "key": "B",
        "text": "Launch the EC2 instances and attach elastic network interfaces in multiples of four.",
        "text_jp": "EC2 インスタンスを起動し、弾力的ネットワークインターフェイスを 4 の倍数で接続する。"
      },
      {
        "key": "C",
        "text": "Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.",
        "text_jp": "Elastic Fabric Adapter (EFA) が有効な EC2 インスタンスタイプを選択する。"
      },
      {
        "key": "D",
        "text": "Ensure the cluster is launched across multiple Availability Zones.",
        "text_jp": "クラスターが複数のアベイラビリティーゾーンにまたがって起動されるようにする。"
      },
      {
        "key": "E",
        "text": "Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array.",
        "text_jp": "Amazon EFS を RAID 配列における複数の Amazon EBS ボリュームに置き換える。"
      },
      {
        "key": "F",
        "text": "Replace Amazon EFS with Amazon FSx for Lustre.",
        "text_jp": "Amazon EFS を Amazon FSx for Lustre に置き換える。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACF (80%) CDF (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct choices for maximizing the HPC cluster performance are A, C, and F. Choice A ensures that the HPC cluster is launched within a single Availability Zone to minimize network latency.",
        "situation_analysis": "The HPC cluster's performance decreased when scaling from 100 to 1,000 EC2 instances, highlighting the need for optimizing network communication and data access.",
        "option_analysis": "Option A minimizes inter-instance communication latency. Option C, selecting EC2 instances with EFA, enhances interconnect performance for tightly coupled applications. Option F, using Amazon FSx for Lustre, offers high throughput and low-latency file storage. Options B, D, and E do not directly address performance optimization.",
        "additional_knowledge": "Scaling requires careful consideration of both architecture and data flow, particularly for workloads requiring high performance.",
        "key_terminology": "High Performance Computing, Elastic Fabric Adapter, Amazon FSx for Lustre, AWS EFS, EC2 instances.",
        "overall_assessment": "The analysis indicates that focusing on minimizing latency and maximizing throughput are crucial for the HPC workloads. Community voting aligns with the recommendation of using EFA and FSx for Lustre."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "最大の HPC クラスターのパフォーマンスを引き出すための正しい選択肢は A、C、および F である。選択肢 A は、HPC クラスターが単一のアベイラビリティーゾーン内で起動されることを保証し、ネットワークの遅延を最小限に抑える。",
        "situation_analysis": "HPC クラスターは、100 から 1,000 の EC2 インスタンスにスケールする際にパフォーマンスが低下し、ネットワーク通信とデータアクセスの最適化が必要であることを示している。",
        "option_analysis": "選択肢 A は、インスタンス間通信の遅延を最小限に抑える。選択肢 C の EFA を持つ EC2 インスタンスを選ぶことは、密結合型アプリケーションのインターコネクトパフォーマンスを向上させる。選択肢 F の Amazon FSx for Lustre は、高スループットで低遅延のファイルストレージを提供する。選択肢 B、D、および E は、パフォーマンスの最適化に直接関係しない。",
        "additional_knowledge": "スケーリングには、特に高性能を必要とするワークロードにおいて、アーキテクチャとデータフローの両方に関する慎重な考慮が必要である。",
        "key_terminology": "高性能コンピューティング、Elastic Fabric Adapter、Amazon FSx for Lustre、AWS EFS、EC2 インスタンス。",
        "overall_assessment": "分析は、遅延を最小限に抑え、スループットを最大化することが、HPC ワークロードにとって重要であることを示している。コミュニティ投票は、EFA および FSx for Lustre の使用推奨と一致している。"
      }
    ],
    "keywords": [
      "High Performance Computing",
      "Elastic Fabric Adapter",
      "Amazon FSx for Lustre",
      "AWS EFS",
      "EC2 instances"
    ]
  },
  {
    "No": "236",
    "question": "A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire\norganization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique\ntag values.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWS Organizationsの構造を設計しています。この企業は、組織全体でタグを適用するプロセスの標準化を求めています。新しいリソースを作成する際に、ユーザーに特定の値を持つタグを要求する必要があります。各OUにはユニークなタグ値があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the",
        "text_jp": "必要なタグがないリソースの作成を拒否するSCPを使用します。タグの値が含まれたタグポリシーを作成します。"
      },
      {
        "key": "B",
        "text": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the",
        "text_jp": "必要なタグがないリソースの作成を拒否するSCPを使用します。タグの値が含まれたタグポリシーを作成します。"
      },
      {
        "key": "C",
        "text": "Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag",
        "text_jp": "リソースに必要なタグがある場合のみリソースの作成を許可するSCPを使用します。タグが含まれたタグポリシーを作成します。"
      },
      {
        "key": "D",
        "text": "Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs.",
        "text_jp": "必要なタグがないリソースの作成を拒否するSCPを使用します。タグのリストを定義します。SCPをOUにアタッチします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (77%) B (23%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This approach allows the organization to control resource creation based on the presence of necessary tags.",
        "situation_analysis": "The organization requires the use of specific tags whenever resources are created, and these tags must differ across different organizational units (OUs).",
        "option_analysis": "Option C allows for resource creation only when required tags are present, thereby enforcing the tagging policy effectively. Options A and D deny resources without the required tags but do not enforce creation based on tags. Option B is a duplicate of A, providing no additional information.",
        "additional_knowledge": "Understanding how SCPs and tagging work together within AWS Organizations can greatly enhance management and compliance.",
        "key_terminology": "AWS Organizations, Service Control Policies (SCPs), Tag Policies, Resource Tagging",
        "overall_assessment": "Despite community voting indicating a preference for options A and D, option C aligns directly with the requirement of allowing resource creation based on necessary tags, making it the most appropriate choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。このアプローチにより、組織は必要なタグの存在に基づいてリソースの作成を制御できる。",
        "situation_analysis": "組織はリソースが作成される際に特定のタグを使用することを求めており、これらのタグは異なる組織単位（OU）ごとに異なる必要がある。",
        "option_analysis": "選択肢Cは、必要なタグが存在する場合のみリソースの作成を許可するため、タグ付けポリシーを効果的に強制する。選択肢AとDは必要なタグがないリソースを拒否するが、タグに基づいて作成を強制しない。選択肢BはAの重複であり、追加情報を提供しない。",
        "additional_knowledge": "AWS Organizations内でSCPとタグ付けがどのように連携するかを理解することが、管理およびコンプライアンスの向上に役立つ。",
        "key_terminology": "AWS Organizations, サービスコントロールポリシー(SCP), タグポリシー, リソースタグ付け",
        "overall_assessment": "コミュニティの投票が選択肢AとDを好む傾向があるにもかかわらず、選択肢Cは必要なタグに基づいてリソース作成を許可する要件に直接合致しており、最も適切な選択である。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Service Control Policies",
      "Tag Policies",
      "Resource Tagging"
    ]
  },
  {
    "No": "237",
    "question": "A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry\nTransport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket.\nRecently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new\ndesign on AWS that is highly available and scalable to prevent a similar occurrence.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、10,000以上のセンサーがあり、メッセージキューイングテレメトリートランスポート（MQTT）プロトコルを使用して、オンプレミスのApache Kafkaサーバーにデータを送信しています。オンプレミスのKafkaサーバーはデータを変換し、結果をAmazon S3バケット内のオブジェクトとして保存します。最近、Kafkaサーバーがクラッシュしました。企業はサーバーの復旧中にセンサーデータを失いました。ソリューションアーキテクトは、同様の発生を防ぐために、高可用性かつスケーラブルな新しい設計をAWS上で作成する必要があります。どのソリューションがこれらの要求を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a",
        "text_jp": "二つのAmazon EC2インスタンスを起動し、二つのアベイラビリティゾーンにまたがるアクティブ/スタンバイ構成でKafkaサーバーをホストします。"
      },
      {
        "key": "B",
        "text": "Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer",
        "text_jp": "オンプレミスのKafkaサーバーをAmazon Managed Streaming for Apache Kafka（Amazon MSK）に移行します。ネットワークロードバランサーを作成します。"
      },
      {
        "key": "C",
        "text": "Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data",
        "text_jp": "AWS IoT Coreを展開し、Amazon Kinesis Data Firehoseデリバリーストリームに接続します。データを処理するためにAWS Lambda関数を使用します。"
      },
      {
        "key": "D",
        "text": "Deploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2",
        "text_jp": "AWS IoT Coreを展開し、Amazon EC2インスタンスを起動してKafkaサーバーをホストします。AWS IoT Coreを構成してデータをEC2に送信します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (84%) B (16%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. By launching two Amazon EC2 instances in an active/standby configuration across two Availability Zones, the architecture gains redundancy and high availability, making it resilient to failures.",
        "situation_analysis": "The company must ensure that there is no data loss during processing, maintain high availability of the Kafka service, and be able to scale if additional sensors are added in the future.",
        "option_analysis": "Option A directly addresses the need for high availability and fault tolerance with an active/standby setup. Option B would also work by migrating to a managed service, but it would not guarantee the same configuration as Option A with regard to active/passive setup. Option C offers a different architecture but doesn't mention Kafka, which is essential in this case. Option D does not provide sufficient redundancy, as it relies on a single EC2 instance.",
        "additional_knowledge": "Designing for failure is a core principle in cloud architecture, ensuring applications are built to withstand failures.",
        "key_terminology": "Active/Standby, High Availability, AWS EC2, Apache Kafka, Fault Tolerance",
        "overall_assessment": "The community's preference for option C indicates a misunderstanding of the need for Kafka in this specific situation. Option A is technically sound for the requirements stated."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。二つのAmazon EC2インスタンスをアクティブ/スタンバイ構成で二つのアベイラビリティゾーンにまたがって起動することにより、アーキテクチャは冗長性と高可用性を得て、障害に対して耐性をもたらす。",
        "situation_analysis": "企業は、処理中にデータの損失がないことを確保し、Kafkaサービスの高可用性を維持し、将来的に追加のセンサーが追加された場合にスケーラビリティを確保する必要がある。",
        "option_analysis": "選択肢Aは、高可用性と障害耐性の必要性に直接対処している。選択肢Bは、管理サービスに移行することで機能するが、アクティブ/スタンバイ設定に関しては選択肢Aと同じ構成を保証しない。選択肢Cは異なるアーキテクチャを提供するが、Kafkaに言及していないため、このケースでは重要である。選択肢Dは単一のEC2インスタンスに依存するため、冗長性が不十分である。",
        "additional_knowledge": "失敗に対して設計することは、クラウドアーキテクチャの核心的原則であり、アプリケーションは障害に耐えるように構築されるべきである。",
        "key_terminology": "アクティブ/スタンバイ、高可用性、AWS EC2、Apache Kafka、障害耐性",
        "overall_assessment": "コミュニティの選択肢Cに対する優先度は、その特定の状況におけるKafkaの必要性についての誤解を示している。選択肢Aは、示された要件に対して技術的に健全である。"
      }
    ],
    "keywords": [
      "Active/Standby",
      "High Availability",
      "AWS EC2",
      "Apache Kafka",
      "Fault Tolerance"
    ]
  },
  {
    "No": "238",
    "question": "A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances. Amazon Elastic\nFile System (Amazon EFS) file systems, and Amazon RDS DB instances.\nTo meet regulatory and business requirements, the company must make the following changes for data backups:\n• Backups must be retained based on custom daily, weekly, and monthly requirements.\n• Backups must be replicated to at least one other AWS Region immediately after capture.\n• The backup solution must provide a single source of backup status across the AWS environment.\n• The backup solution must send immediate notifications upon failure of any resource backup.\nWhich combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)",
    "question_jp": "ある企業が最近AWSクラウドで新しいアプリケーションワークロードをホストし始めました。この企業は、Amazon EC2インスタンス、Amazon Elastic File System（Amazon EFS）ファイルシステム、およびAmazon RDS DBインスタンスを使用しています。規制およびビジネス要件を満たすために、企業はデータバックアップに関して以下の変更を行う必要があります： • バックアップはカスタムの毎日、毎週、毎月の要件に基づいて保持する必要があります。 • バックアップは、キャプチャ後に少なくとも1つの他のAWSリージョンに即座にレプリケートされる必要があります。 • バックアップソリューションは、AWS環境全体でのバックアップ状況の単一のソースを提供する必要があります。 • バックアップソリューションは、リソースバックアップの失敗が発生した場合に即座に通知を送信する必要があります。これらの要件を運用負荷が最も少ない状態で満たすために、どのステップの組み合わせを選択しますか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Backup plan with a backup rule for each of the retention requirements.",
        "text_jp": "AWSバックアッププランを作成し、各保存要件のためのバックアップルールを設定する。"
      },
      {
        "key": "B",
        "text": "Configure an AWS Backup plan to copy backups to another Region.",
        "text_jp": "バックアップを別のリージョンにコピーするようにAWSバックアッププランを設定する。"
      },
      {
        "key": "C",
        "text": "Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.",
        "text_jp": "バックアップを別のリージョンにレプリケートし、失敗が発生した場合に通知を送信するAWS Lambda関数を作成する。"
      },
      {
        "key": "D",
        "text": "Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any",
        "text_jp": "バックアッププランにAmazon Simple Notification Service（Amazon SNS）トピックを追加して、いかなる完了したジョブについても通知を送信する。"
      },
      {
        "key": "E",
        "text": "Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements.",
        "text_jp": "各保存要件のためのAmazon Data Lifecycle Manager（Amazon DLM）スナップショットライフサイクルポリシーを作成する。"
      },
      {
        "key": "F",
        "text": "Set up RDS snapshots on each database.",
        "text_jp": "各データベースにRDSスナップショットを設定する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ABD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A, B, and D. These steps align best with the requirements to ensure efficient backup management with minimal overhead.",
        "situation_analysis": "The company requires a backup strategy that is both compliant with regulatory standards and operationally efficient. Immediate replication to another region is critical for disaster recovery and compliance.",
        "option_analysis": "Option A creates a structured backup plan tailored to different retention needs, ensuring compliance. Option B addresses the need for immediate replication, which is essential as per the requirements. Option D ensures communication and timely alerts in case of failures.",
        "additional_knowledge": "Using AWS Backup can help mitigate risks associated with backup failures and streamline the backup process through automation.",
        "key_terminology": "AWS Backup, Amazon SNS, Backup Plan, Data Lifecycle Manager",
        "overall_assessment": "The community vote aligns with the operationally efficient approach as it supports the best practices recommended by AWS for backup strategies. Options C, E and F introduce more complexity and overhead."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はA、B、およびDである。これらのステップは、運用負荷を最小限に抑えつつ、効率的なバックアップ管理を確保する要件に最適に合致している。",
        "situation_analysis": "企業は、規制基準を遵守しつつ運用効率を重視したバックアップ戦略を求めている。別のリージョンへの即時レプリケーションは、災害復旧とコンプライアンス上、重要である。",
        "option_analysis": "選択肢Aは、異なる保存ニーズに合わせた構造的なバックアッププランを作成し、コンプライアンスを確保する。選択肢Bは、要件に従った即時レプリケーションを取り扱っている。選択肢Dは、失敗が発生した場合の通知を保証する。",
        "additional_knowledge": "AWS Backupを使用することで、バックアップ障害に関連するリスクを軽減し、自動化を通じてバックアッププロセスを簡素化することができる。",
        "key_terminology": "AWS Backup, Amazon SNS, バックアッププラン, データライフサイクルマネージャー",
        "overall_assessment": "コミュニティの投票は、AWSのバックアップ戦略に推奨されるベストプラクティスを支持する運用効率的アプローチに一致している。選択肢C、E、Fは、より多くの複雑さと負担を引き起こす。"
      }
    ],
    "keywords": [
      "AWS Backup",
      "Amazon SNS",
      "Backup Plan",
      "Data Lifecycle Manager"
    ]
  },
  {
    "No": "239",
    "question": "A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data\nfrom a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the\ndata and provide information back to researchers. The data platform must meet the following requirements:\n• Provide near-real-time analytics of the inbound genomic data\n• Ensure the data is fiexible, parallel, and durable\n• Deliver results of processing to a data warehouse\nWhich strategy should a solutions architect use to meet these requirements?",
    "question_jp": "ある企業が研究者が多様な人口からの大量のデータサンプルを収集するのを支援するために、遺伝子報告デバイスを開発しています。このデバイスは、毎秒8 KBの遺伝子データをデータプラットフォームにプッシュし、そのデータを処理・分析し、研究者に情報を提供する必要があります。データプラットフォームは以下の要件を満たさなければなりません。\n• インバウンドの遺伝子データのほぼリアルタイム分析を提供すること\n• データが柔軟性、並列性、耐久性を確保すること\n• 処理結果をデータウェアハウスに配信すること\nソリューションアーキテクトはどの戦略を使用してこれらの要件を満たすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an",
        "text_jp": "Amazon Kinesis Data Firehoseを使用してインバウンドセンサーデータを収集し、Kinesisクライアントでデータを分析し、結果を保存します。"
      },
      {
        "key": "B",
        "text": "Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an",
        "text_jp": "Amazon Kinesis Data Streamsを使用してインバウンドセンサーデータを収集し、Kinesisクライアントでデータを分析し、結果を保存します。"
      },
      {
        "key": "C",
        "text": "Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon",
        "text_jp": "Amazon S3を使用してインバウンドデバイスデータを収集し、Amazon SQSからKinesisでデータを分析し、結果をAmazonに保存します。"
      },
      {
        "key": "D",
        "text": "Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the",
        "text_jp": "Amazon API Gatewayを使用してリクエストをAmazon SQSキューに送信し、AWS Lambda関数でデータを分析し、保存します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which utilizes Amazon Kinesis Data Streams for real-time processing of genomic data.",
        "situation_analysis": "The requirement to analyze data in near real-time and handle large volume streaming data is a critical aspect for this solution.",
        "option_analysis": "Option B is ideal because Kinesis Data Streams can scale to handle the necessary throughput while providing low-latency access to streaming data. Options A and C may not provide the same level of real-time processing required, while D introduces unnecessary complexity.",
        "additional_knowledge": "Using Kinesis Data Streams allows for durability and flexibility in handling diverse data formats.",
        "key_terminology": "Amazon Kinesis, real-time data processing, data analytics",
        "overall_assessment": "In conclusion, option B aligns perfectly with the given requirements for real-time genomic data processing and analysis."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、Amazon Kinesis Data Streamsを使用して遺伝子データをリアルタイムで処理します。",
        "situation_analysis": "データをほぼリアルタイムで分析し、大量のストリーミングデータを処理する必要があることが、このソリューションの重要な側面です。",
        "option_analysis": "Bは理想的な選択で、Kinesis Data Streamsは必要なスループットを処理するためにスケールでき、ストリーミングデータへの低遅延アクセスを提供します。AとCのオプションは、要求されるリアルタイム処理を提供しない可能性があり、Dは不必要な複雑さを引き起こします。",
        "additional_knowledge": "Kinesis Data Streamsを使用することで、さまざまなデータ形式の処理において耐久性と柔軟性を確保できます。",
        "key_terminology": "Amazon Kinesis, リアルタイムデータ処理, データ分析",
        "overall_assessment": "結論として、選択肢Bは、与えられた要求に完全に合致し、遺伝子データのリアルタイム処理と分析に適しています。"
      }
    ],
    "keywords": [
      "Amazon Kinesis",
      "real-time data processing",
      "data analytics"
    ]
  },
  {
    "No": "240",
    "question": "A solutions architect needs to define a reference architecture for a solution for three-tier applications with web. application, and NoSQL data\nlayers. The reference architecture must meet the following requirements:\n• High availability within an AWS Region\n• Able to fail over in 1 minute to another AWS Region for disaster recovery\n• Provide the most eficient solution while minimizing the impact on the user experience\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ソリューションアーキテクトは、ウェブアプリケーション及びNoSQLデータ層を含む三層アプリケーションのソリューションの参照アーキテクチャを定義する必要がある。参照アーキテクチャは、以下の要件を満たさなければならない:\n• AWSリージョン内での高可用性\n• 災害復旧のために他のAWSリージョンに1分でフェイルオーバーできること\n• ユーザーエクスペリエンスへの影響を最小限にしながら、最も効率的なソリューションを提供すること\nこれらの要件を満たすための手順の組み合わせはどれか？（三つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.",
        "text_jp": "Amazon Route 53のウェイテッドルーティングポリシーを使用して、二つの選択したリージョン間で100/0に設定する。TTLを1時間に設定する。"
      },
      {
        "key": "B",
        "text": "Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL)",
        "text_jp": "プライマリーリージョンから災害復旧リージョンへのフェイルオーバーのために、Amazon Route 53のフェイルオーバールーティングポリシーを使用する。TTLを設定する。"
      },
      {
        "key": "C",
        "text": "Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.",
        "text_jp": "データが二つの選択したリージョンでアクセスできるように、Amazon DynamoDB内でグローバルテーブルを使用する。"
      },
      {
        "key": "D",
        "text": "Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3",
        "text_jp": "プライマリーリージョンでのAmazon DynamoDBテーブルからデータを60分ごとにバックアップし、その後、データをAmazon S3に書き込む。S3を使用する。"
      },
      {
        "key": "E",
        "text": "Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the",
        "text_jp": "ウェブおよびアプリケーション層のために、複数のアベイラビリティゾーンにわたるAuto Scalingグループを使用してホットスタンバイモデルを実装する。"
      },
      {
        "key": "F",
        "text": "Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the",
        "text_jp": "ウェブおよびアプリケーション層のために、リージョン内の複数のアベイラビリティゾーンにわたるAuto Scalingグループを使用する。"
      }
    ],
    "answer_key": "F",
    "community_vote_distribution": "BCE (88%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is F. It involves using Auto Scaling groups across multiple Availability Zones for the web and application layers, ensuring high availability and resilience.",
        "situation_analysis": "The requirements demand high availability within an AWS Region and failover capability to another Region within a minute, indicating the need for redundancy and quick response.",
        "option_analysis": "Option F provides a scalable and resilient architecture. Option E is incorrect because it doesn't fulfill the failover requirement to a different Region. Other options relate to routing or backup strategies which do not satisfy all requirements.",
        "additional_knowledge": "Proper management of failover strategies is essential in designing resilient architectures in AWS.",
        "key_terminology": "Auto Scaling, Availability Zones, High Availability, Disaster Recovery, AWS Region",
        "overall_assessment": "The question effectively tests knowledge of architectural patterns and principles in the AWS environment; however, community votes favor other options that do not meet all requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはFである。ウェブおよびアプリケーション層のために、複数のアベイラビリティゾーンにわたるAuto Scalingグループを使用し、高可用性と耐障害性を確保している。",
        "situation_analysis": "要件は、AWSリージョン内での高可用性と1分以内に別のリージョンへのフェイルオーバー能力を要求しており、冗長性と迅速な対応の必要性を示している。",
        "option_analysis": "選択肢Fは、スケーラブルで耐障害性のあるアーキテクチャを提供する。選択肢Eは、異なるリージョンへのフェイルオーバー要件を満たしていないため不正解である。他の選択肢は、ルーティングやバックアップ戦略に関連しており、すべての要件を満たしていない。",
        "additional_knowledge": "耐障害性のあるアーキテクチャを設計する上で、フェイルオーバー戦略の適切な管理が不可欠である。",
        "key_terminology": "Auto Scaling, アベイラビリティゾーン, 高可用性, 災害復旧, AWSリージョン",
        "overall_assessment": "この質問は、AWS環境におけるアーキテクチャパターンや原則の知識を効果的に試験するものであるが、コミュニティの投票はすべての要件を満たさない他の選択肢を支持している。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "Availability Zones",
      "High Availability",
      "Disaster Recovery",
      "AWS Region"
    ]
  },
  {
    "No": "241",
    "question": "A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to\nconnect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-\npremises storage. Custom applications analyze this data to detect anomalies.\nThe number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not\nable to scale for peak trafic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the\nscaling challenges.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がスマート車両を製造しています。この企業は、車両データを収集するためにカスタムアプリケーションを使用しています。車両はMQTTプロトコルを使用してアプリケーションに接続します。企業は5分間隔でデータを処理しています。その後、企業は車両のテレマティクスデータをオンプレミスストレージにコピーします。カスタムアプリケーションがこのデータを分析して異常を検出します。データを送信する車両の数は常に増加しています。新しい車両は大量のデータを生成します。オンプレミスストレージソリューションはピークトラフィックにスケールできず、データ損失が発生しています。企業はソリューションを近代化し、スケーリングの課題を解決するためにAWSに移行する必要があります。どのソリューションが最小の運用オーバーヘッドでこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache",
        "text_jp": "AWS IoT Greengrassを使用して車両データをAmazon Managed Streaming for Apache Kafka (Amazon MSK)に送信する。Apacheを作成する"
      },
      {
        "key": "B",
        "text": "Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores",
        "text_jp": "AWS IoT Coreを使用して車両データを受信する。ルールを構成してデータをAmazon Kinesis Data Firehose配信ストリームにルーティングする"
      },
      {
        "key": "C",
        "text": "Use AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose",
        "text_jp": "AWS IoT FleetWiseを使用して車両データを収集する。データをAmazon Kinesisデータストリームに送信する。Amazon Kinesis Data Firehoseを使用する"
      },
      {
        "key": "D",
        "text": "Use Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the",
        "text_jp": "Amazon MQ for RabbitMQを使用して車両データを収集する。データをAmazon Kinesis Data Firehose配信ストリームに送信する"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (79%) C (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which utilizes AWS IoT FleetWise for data collection and scales effectively within AWS infrastructure.",
        "situation_analysis": "The company faces challenges due to increasing numbers of vehicles and the high volume of data generated by newer vehicles, leading to data loss with the current on-premises solution.",
        "option_analysis": "Option C is ideal since AWS IoT FleetWise is specifically designed for automotive data collection and can integrate smoothly with Kinesis for real-time data processing. Other options either introduce more complexity or do not scale as effectively.",
        "additional_knowledge": "Kinesis Data Firehose can be utilized for more extensive data analytics and storage options.",
        "key_terminology": "AWS IoT FleetWise, Kinesis Data Stream, JSON",
        "overall_assessment": "Despite community votes favoring option B, C's operational overhead is lower due to AWS IoT FleetWise's tailored features for automotive data."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。AWS IoT FleetWiseを使用してデータを収集し、AWSインフラの中で効果的にスケールします。",
        "situation_analysis": "企業は、車両の数が増加し、新しい車両から生成される高ボリュームのデータにより、現在のオンプレミスソリューションでデータ損失が発生している課題に直面しています。",
        "option_analysis": "オプションCは理想的です。AWS IoT FleetWiseは、自動車データ収集のために特別に設計されており、Kinesisとの統合が円滑に行えるため、リアルタイムデータ処理が可能です。他のオプションは複雑さを増すか、効果的にスケールしません。",
        "additional_knowledge": "Kinesis Data Firehoseは、より広範なデータ分析やストレージオプションのためにも使用できます。",
        "key_terminology": "AWS IoT FleetWise, Kinesis Data Stream, JSON",
        "overall_assessment": "コミュニティの投票ではBが支持されていますが、Cは自動車データ専用の機能によって運用オーバーヘッドが低いため優れています。"
      }
    ],
    "keywords": [
      "AWS IoT FleetWise",
      "Kinesis Data Stream",
      "Kinesis Data Firehose"
    ]
  },
  {
    "No": "242",
    "question": "During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it\nto an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.\nWhich solution will ensure that the credentials are appropriately secured automatically?",
    "question_jp": "監査中に、セキュリティチームは開発チームがIAMユーザーのシークレットアクセスキーをコードに埋め込み、AWS CodeCommitリポジトリにコミットしていることを発見しました。セキュリティチームは、このセキュリティ脆弱性のインスタンスを自動的に検出し修正することを望んでいます。この資格情報が適切に自動的に保護されることを保証する解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS",
        "text_jp": "AWS Systems Manager Run Commandを使用して、開発インスタンス上で資格情報を検索するスクリプトを毎晩実行します。見つかった場合は、AWSを使用して修正します"
      },
      {
        "key": "B",
        "text": "Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate",
        "text_jp": "スケジュールされたAWS Lambda関数を使用して、CodeCommitからアプリケーションコードをダウンロードしてスキャンします。資格情報が見つかった場合は、生成します"
      },
      {
        "key": "C",
        "text": "Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to",
        "text_jp": "Amazon Macieを設定して、CodeCommitリポジトリ内の資格情報をスキャンします。資格情報が見つかった場合は、AWS Lambda関数をトリガーします"
      },
      {
        "key": "D",
        "text": "Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found,",
        "text_jp": "CodeCommitトリガーを設定して、新しいコードの提出を資格情報のためにスキャンするAWS Lambda関数を呼び出します。資格情報が見つかった場合は、"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This approach utilizes AWS Systems Manager's Run Command to automate the detection of IAM user credentials stored in code, providing the necessary remediation when such instances are found.",
        "situation_analysis": "The development team inadvertently included sensitive IAM user credentials in their source code, which poses a significant security risk. The need is to prevent such practices and ensure sensitive information is not exposed in repositories.",
        "option_analysis": "Option A suggests using AWS Systems Manager to automatically search for credentials in the development instances, which is a robust solution. Option B focuses on scanning the application code from CodeCommit but lacks real-time detection. Option C mentions Amazon Macie, which is effective but may not cover all cases in integration with CodeCommit directly. Option D involves manual triggers based on newly submitted code, which may delay detection and remediation actions.",
        "additional_knowledge": "Regular audits and scanning processes enhance security posture and compliance in the development lifecycle.",
        "key_terminology": "AWS Systems Manager, IAM, Security best practices, CodeCommit, Automated remediation",
        "overall_assessment": "While the community vote indicates a preference for option D, the automated continuous search approach of option A aligns best with security best practices for preventing exposure of sensitive information."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。このアプローチでは、AWS Systems ManagerのRun Commandを使用して、コードに保存されたIAMユーザーの資格情報を自動的に検出し、そのようなインスタンスが見つかった場合に必要な修正を行う。",
        "situation_analysis": "開発チームは誤ってソースコードに機密のIAMユーザー資格情報を含めてしまい、これは重大なセキュリティリスクを伴う。この問題を防ぎ、機密情報がリポジトリに公開されないようにする必要がある。",
        "option_analysis": "選択肢Aは、AWS Systems Managerを使用して開発インスタンスにおける資格情報を自動的に検索することを提案しており、これは信頼性の高い解決策である。選択肢BはCodeCommitからアプリケーションコードをスキャンすることに焦点を当てているが、リアルタイムの検出が欠けている。選択肢CはAmazon Macieを挙げているが、CodeCommitとの直接的な統合によるカバー範囲が限られる可能性がある。選択肢Dは新たに提出されたコードに対する手動トリガーを含んでおり、検出や修正アクションが遅延する可能性がある。",
        "additional_knowledge": "定期的な監査とスキャンプロセスは、開発ライフサイクルにおけるセキュリティの姿勢とコンプライアンスを高める。",
        "key_terminology": "AWS Systems Manager, IAM, セキュリティのベストプラクティス, CodeCommit, 自動修正",
        "overall_assessment": "コミュニティ投票では選択肢Dが好まれているが、選択肢Aの自動的かつ継続的な検索アプローチが機密情報の露出防止に関するセキュリティのベストプラクティスに最も合致している。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "IAM",
      "Security best practices",
      "CodeCommit",
      "Automated remediation"
    ]
  },
  {
    "No": "243",
    "question": "A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's\ninformation security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the\nminimum permissions necessary to function.\nTo meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.\nWhich combination of steps should the solutions architect take to implement this solution? (Choose two.)",
    "question_jp": "ある企業は、数百のアプリケーションが多くのAWSアカウントでアクセスする必要があるAmazon S3内にデータレイクを持っています。この企業の情報セキュリティポリシーでは、S3バケットはインターネットを介してアクセスできないこと、各アプリケーションは機能に必要な最小限の権限を持つべきであるとされています。これらの要件を満たすために、ソリューションアーキテクトは、各アプリケーションに特定のVPCに制限されたS3アクセス点を使用する計画を立てています。このソリューションを実装するために、ソリューションアーキテクトが取るべき手順の組み合わせはどれですか？（2つ選んでください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible",
        "text_jp": "AWSアカウント内の各アプリケーション用にS3アクセスポイントを作成します。各アクセスポイントがアクセス可能であるように構成します"
      },
      {
        "key": "B",
        "text": "Create an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point.",
        "text_jp": "各アプリケーションのVPCにAmazon S3のインターフェースエンドポイントを作成します。エンドポイントポリシーを構成してS3アクセスポイントへのアクセスを許可します。"
      },
      {
        "key": "C",
        "text": "Create a gateway endpoint for Amazon S3 in each application's VPConfigure the endpoint policy to allow access to an S3 access point.",
        "text_jp": "各アプリケーションのVPCにAmazon S3のゲートウェイエンドポイントを作成します。エンドポイントポリシーを構成してS3アクセスポイントへのアクセスを許可します。"
      },
      {
        "key": "D",
        "text": "Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access",
        "text_jp": "各AWSアカウントごとに各アプリケーション用のS3アクセスポイントを作成し、そのアクセスポイントをS3バケットに接続します。各アクセスポイントがアクセスできるように構成します"
      },
      {
        "key": "E",
        "text": "Create a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the",
        "text_jp": "データレイクのVPCにAmazon S3のゲートウェイエンドポイントを作成します。S3バケットへのアクセスを許可するエンドポイントポリシーをアタッチします。指定される"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "AC (68%) 14% Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, where the solutions architect creates an S3 access point for each application in each AWS account, connecting the access points to the S3 bucket while ensuring they are properly configured for access.",
        "situation_analysis": "The company requires access to its S3 data lake from multiple applications across different AWS accounts, while adhering to strict security policies.",
        "option_analysis": "Option A does not address multi-account access. Option B proposes the use of an interface endpoint which isn't ideal as it introduces complexity without meeting the requirement of restricting public access. Option C leverages a gateway endpoint which is appropriate, but lacks the multi-account access needed. Option D correctly creates an S3 access point for each application in each account. Option E only establishes a gateway endpoint in the data lake's VPC without enabling granular permissions.",
        "additional_knowledge": "Implementing granular access control through IAM policies on the S3 access points further enhances security.",
        "key_terminology": "S3 Access Points, VPC, Gateway Endpoint, Endpoint Policy, Multi-account Access",
        "overall_assessment": "The solution proposed in option D effectively meets the security policy requirements and ensures minimal permissions are granted, making it the best approach here. The community vote's majority support for option C (68%) represents a misunderstanding of the multi-account access needs involved."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDであり、ソリューションアーキテクトは各AWSアカウント内の各アプリケーション用にS3アクセスポイントを作成し、アクセスポイントをS3バケットに接続し、適切にアクセスできるように構成します。",
        "situation_analysis": "この企業は、複数のAWSアカウントにわたる多くのアプリケーションからS3データレイクにアクセスする必要があり、厳格なセキュリティポリシーを遵守しています。",
        "option_analysis": "オプションAはマルチアカウントアクセスに対応していません。オプションBはインターフェースエンドポイントの使用を提案しており、これは公的アクセス制限の要件を満たさないため、複雑さを増すだけです。オプションCはゲートウェイエンドポイントを利用していますが、必要なマルチアカウントアクセスが欠けています。オプションDは各アプリケーション用に各アカウントでS3アクセスポイントを正しく作成します。オプションEはデータレイクのVPCにゲートウェイエンドポイントを設定するだけで、きめ細かな権限を有効にすることはできません。",
        "additional_knowledge": "S3アクセスポイントに対するIAMポリシーを通じてきめ細かなアクセス制御を実施することもセキュリティを向上させる要因となります。",
        "key_terminology": "S3アクセスポイント, VPC, ゲートウェイエンドポイント, エンドポイントポリシー, マルチアカウントアクセス",
        "overall_assessment": "オプションDで提案されたソリューションは、セキュリティポリシー要件を効果的に満たし、付与される権限を最小限に抑えるため、ここでの最良のアプローチです。コミュニティ投票の68%の支持を得たオプションCは、関与するマルチアカウントアクセスのニーズに対する誤解を示しています。"
      }
    ],
    "keywords": [
      "S3 Access Points",
      "VPC",
      "Gateway Endpoint",
      "Endpoint Policy",
      "Multi-account Access"
    ]
  },
  {
    "No": "244",
    "question": "A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that\nsend application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.\nThe company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring\nsolution that uses Splunk on premises. A solutions architect needs to determine how to send networking trafic to Splunk.\nHow should the solutions architect meet these requirements?",
    "question_jp": "企業は、データセンターとAWSとの間にハイブリッドソリューションを開発しました。企業はAmazon VPCとAmazon EC2インスタンスを使用しており、それらのインスタンスはアプリケーションログをAmazon CloudWatchに送信します。EC2インスタンスは、オンプレミスにホストされている複数の関係データベースからデータを読み取ります。企業は、近いリアルタイムで、どのEC2インスタンスがデータベースに接続されているかを監視したいと考えています。企業はすでにオンプレミスでSplunkを使用した監視ソリューションを持っています。ソリューションアーキテクトは、ネットワークトラフィックをSplunkに送信する方法を決定する必要があります。ソリューションアーキテクトは、これらの要件を満たすためにどのようにすべきであるか？",
    "choices": [
      {
        "key": "A",
        "text": "Enable VPC fiows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an",
        "text_jp": "VPCフローログを有効にし、CloudWatchに送信します。AWS Lambda関数を作成して、CloudWatchログを定期的にエクスポートします。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function",
        "text_jp": "Amazon Kinesis Data Firehose配信ストリームを作成し、Splunkを宛先として設定します。前処理用のAWS Lambda関数を設定します。"
      },
      {
        "key": "C",
        "text": "Ask the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to",
        "text_jp": "企業に、EC2インスタンスのIPアドレスと共にデータベースへのリクエストをすべてログに記録するように依頼します。CloudWatchログをエクスポートします。"
      },
      {
        "key": "D",
        "text": "Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1-",
        "text_jp": "CloudWatchログをAmazon Kinesisデータストリームに送信し、Amazon Kinesis Data Analytics for SQLアプリケーションを構成します。1-"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Enable VPC flow logs and send them to CloudWatch.",
        "situation_analysis": "The company requires monitoring of EC2 instances connected to the on-premises databases in near real-time.",
        "option_analysis": "Option A allows for collecting and storing traffic logs that can be sent to an existing monitoring solution (Splunk). Other options either do not directly address the requirement or are more complex, making them less desirable.",
        "additional_knowledge": "Monitoring real-time traffic effectively requires understanding the flow of data and the use of appropriate logging tools.",
        "key_terminology": "VPC Flow Logs, CloudWatch, AWS Lambda",
        "overall_assessment": "The solution architect's decision to choose Option A aligns with AWS best practices for cloud monitoring while seamlessly integrating with the existing on-premises Splunk solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はA：VPCフローログを有効にし、CloudWatchに送信することです。",
        "situation_analysis": "企業は、オンプレミスのデータベースに接続されているEC2インスタンスを近いリアルタイムで監視する必要があります。",
        "option_analysis": "選択肢Aは、トラフィックログを収集し、既存の監視ソリューション（Splunk）に送信できるため、要件に対応しています。他の選択肢は、要件に直接対応していないか、より複雑なため、望ましくありません。",
        "additional_knowledge": "リアルタイムトラフィックを効果的に監視するには、データの流れと適切なログツールの使用を理解する必要があります。",
        "key_terminology": "VPCフローログ、CloudWatch、AWS Lambda",
        "overall_assessment": "ソリューションアーキテクトが選択肢Aを選んだことは、クラウド監視のAWSベストプラクティスと一致しており、既存のオンプレミスSplunkソリューションとも統合が容易です。"
      }
    ],
    "keywords": [
      "VPC Flow Logs",
      "CloudWatch",
      "AWS Lambda"
    ]
  },
  {
    "No": "245",
    "question": "A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the\ndevelopment teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide\nthe information to the company's finance team.\nThe company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States.\nHowever, some resources have been created in other Regions.\nA solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the\naccounts. The solution also must ensure that the company can create resources only in Regions in the United States.\nWhich combination of steps will meet these requirements in the MOST operationally eficient way? (Choose three.)",
    "question_jp": "ある企業には、アプリケーションを開発およびホストするために、それぞれ5つのAWSアカウントを作成した5つの開発チームがあります。経費を追跡するために、開発チームは毎月各アカウントにログインし、AWSの請求およびコスト管理コンソールから現在のコストを記録し、その情報を企業の財務チームに提供しています。この企業には厳格なコンプライアンス要件があり、リソースが米国のAWS領域にのみ作成されることを確認する必要があります。しかし、他の領域にもいくつかのリソースが作成されています。ソリューションアーキテクトは、財務チームがすべてのアカウントの支出を追跡および統合する能力を持つソリューションを実装する必要があります。また、企業は米国の領域にのみリソースを作成できることを保証する必要があります。これらの要件を最も運用効率的に満たすステップの組み合わせはどれですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage",
        "text_jp": "新しいアカウントを作成して管理アカウントとして使用します。財務チームのためにAmazon S3バケットを作成します。AWSコストおよび使用状況"
      },
      {
        "key": "B",
        "text": "Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all",
        "text_jp": "新しいアカウントを作成して管理アカウントとして使用します。すべての機能が有効なAWS Organizationsに組織を展開します。すべてを招待します。"
      },
      {
        "key": "C",
        "text": "Create an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the",
        "text_jp": "すべての開発チームを含むOUを作成します。米国の領域でのみリソースが作成されることを許可するSCPを作成します。"
      },
      {
        "key": "D",
        "text": "Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the",
        "text_jp": "すべての開発チームを含むOUを作成します。米国の外にある領域でのリソース作成を拒否するSCPを作成します。"
      },
      {
        "key": "E",
        "text": "Create an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management",
        "text_jp": "管理アカウントにIAMロールを作成します。請求およびコスト管理コンソールを表示するための権限を含むポリシーを追加します。"
      },
      {
        "key": "F",
        "text": "Create an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow",
        "text_jp": "各AWSアカウントにIAMロールを作成します。請求およびコスト管理コンソールを表示するための権限を含むポリシーを追加します。許可する"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDE (81%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This option outlines creating a management account and setting up an S3 bucket for centralizing financial data. It is operationally efficient and complies with AWS best practices.",
        "situation_analysis": "The company needs accurate tracking of expenses across multiple AWS accounts and compliance for resource creation in US regions only. Current manual tracking is inefficient.",
        "option_analysis": "Option A allows for centralized cost management. Option B might incorporate organization-level features but does not specify effective cost management. Option C could partially meet requirements but lacks efficient tracking. Option D denies access but does not centralize visibility. Options E and F focus on IAM roles without addressing financial tracking comprehensively.",
        "additional_knowledge": "Implementing AWS Budgets can further enhance tracking expenses across accounts.",
        "key_terminology": "AWS Organizations, Service Control Policies, Amazon S3, centralization, compliance.",
        "overall_assessment": "Overall, option A is the most comprehensive solution, emphasizing both financial tracking and compliance."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。この選択肢は、管理アカウントを作成し、財務データを中央集約するためのS3バケットを設定することを概説している。これは運用効率が高く、AWSのベストプラクティスに従っている。",
        "situation_analysis": "企業は、複数のAWSアカウントにわたる費用を正確に追跡し、リソースが米国の領域にのみ作成されることを確認する必要がある。現在の手動追跡は非効率的である。",
        "option_analysis": "オプションAは、中央集約のコスト管理を可能にする。オプションBは組織レベルの機能を取り入れるかもしれないが、効果的なコスト管理を指定していない。オプションCは要件の一部を満たすが、効率的な追跡が欠けている。オプションDはアクセスを拒否するが、可視性を中央集約しない。オプションEとFは、IAMロールに焦点を当てているが、財務追跡に関する包括的なアプローチが不足している。",
        "additional_knowledge": "AWS Budgetsを実装することで、アカウント間の費用追跡をさらに強化することができる。",
        "key_terminology": "AWS Organizations、サービスコントロールポリシー、Amazon S3、中央集約、コンプライアンス。",
        "overall_assessment": "全体として、オプションAは、財務追跡とコンプライアンスを強調した最も包括的なソリューションである。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Service Control Policies",
      "Amazon S3",
      "centralization",
      "compliance"
    ]
  },
  {
    "No": "246",
    "question": "A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires\nread-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security\nteam.\nHow should a solutions architect meet these requirements?",
    "question_jp": "ある企業は、中央の場所から複数のAWSアカウントを作成および管理する必要があります。セキュリティチームは、自分のAWSアカウントからすべてのアカウントへの読み取り専用アクセスを要求しています。この企業はAWS Organizationsを使用しており、セキュリティチーム用のアカウントを作成しました。ソリューションアーキテクトは、これらの要件をどのように満たすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a",
        "text_jp": "OrganizationAccountAccessRole IAMロールを使用して、各メンバーアカウントに読み取り専用アクセスを持つ新しいIAMポリシーを作成します。設立する"
      },
      {
        "key": "B",
        "text": "Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a",
        "text_jp": "OrganizationAccountAccessRole IAMロールを使用して、各メンバーアカウントに読み取り専用アクセスを持つ新しいIAMロールを作成します。設立する"
      },
      {
        "key": "C",
        "text": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole",
        "text_jp": "セキュリティチームにAWS Security Token Service (AWS STS)を使用してAssumeRole APIを呼び出させます。"
      },
      {
        "key": "D",
        "text": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole",
        "text_jp": "セキュリティチームにAWS Security Token Service (AWS STS)を使用してAssumeRole APIを呼び出させます。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. By using the OrganizationAccountAccessRole IAM role to create a new IAM role in each member account with read-only access, the security team can obtain the required access without compromising security.",
        "situation_analysis": "The requirement is to allow the security team to have read-only access to multiple AWS accounts managed centrally through AWS Organizations. The security team already has an account within this structure.",
        "option_analysis": "Option B is correct as it establishes a new IAM role with read-only access. Option A suggests creating a new IAM policy instead of a role, which is not actionable directly by the security team. Options C and D are incorrect because they propose an assumption of a role without the creation of a dedicated role per the requirements.",
        "additional_knowledge": "In practice, ensuring the principle of least privilege while providing necessary access is critical.",
        "key_terminology": "AWS Organizations, OrganizationAccountAccessRole, IAM role, read-only access, AWS STS",
        "overall_assessment": "The question assesses the knowledge of AWS Organizations and IAM cross-account access. Option B provides the best solution based on the centralized security requirement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。OrganizationAccountAccessRole IAMロールを使用して各メンバーアカウントに読み取り専用アクセスを持つ新しいIAMロールを作成することで、セキュリティチームは必要なアクセスを取得でき、安全性を損なうことなく実現できる。",
        "situation_analysis": "要件は、セキュリティチームがAWS Organizationsを通じて中央管理された複数のAWSアカウントに読み取り専用アクセスを持つことを許可することである。セキュリティチームはすでにこの構造内にアカウントを持っている。",
        "option_analysis": "オプションBは、新しいIAMロールを作成し、読み取り専用アクセスを設定するため正しい。オプションAは、ロールではなくポリシーを作成することを提案しており、これはセキュリティチームが直接実行できるものではない。オプションCとDは、要求に基づいた専用ロールの作成を提案せず、誤りである。",
        "additional_knowledge": "実践においては、必要なアクセスを提供しながら最小権限の原則を確保することが重要である。",
        "key_terminology": "AWS Organizations、OrganizationAccountAccessRole、IAMロール、読み取り専用アクセス、AWS STS",
        "overall_assessment": "この質問は、AWS OrganizationsとIAMのクロスアカウントアクセスに関する知識を評価するものである。オプションBは、セキュリティ要件に基づく最良のソリューションを提供する。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "OrganizationAccountAccessRole",
      "IAM role",
      "read-only access",
      "AWS STS"
    ]
  },
  {
    "No": "247",
    "question": "A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private\nsubnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the\ninternet from the private subnets.\nA solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route trafic to the internet through an\negress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.\nWhich set of additional steps should the solutions architect take to meet these requirements?",
    "question_jp": "大規模な企業は、何百ものAWSアカウントにデプロイされたVPCでワークロードを運用しています。各VPCは、複数のアベイラビリティゾーンにまたがるパブリックサブネットとプライベートサブネットで構成されています。NATゲートウェイがパブリックサブネットにデプロイされており、プライベートサブネットからインターネットへのアウトバウンド接続を可能にしています。\nソリューションアーキテクトはハブアンドスポーク設計に取り組んでいます。すべてのスポークVPC内のプライベートサブネットは、エグレスVPCを介してインターネットにトラフィックをルーティングしなければなりません。ソリューションアーキテクトは、中央のAWSアカウント内のエグレスVPCにNATゲートウェイを既にデプロイしています。\nこれらの要件を満たすためにソリューションアーキテクトが取るべき追加のステップのセットはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.",
        "text_jp": "エグレスVPCとスポークVPC間にピアリング接続を作成します。インターネットアクセスを許可するために必要なルーティングを構成します。"
      },
      {
        "key": "B",
        "text": "Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required",
        "text_jp": "トランジットゲートウェイを作成し、既存のAWSアカウントと共有します。既存のVPCをトランジットゲートウェイに接続します。必要なルーティングを構成します。"
      },
      {
        "key": "C",
        "text": "Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access",
        "text_jp": "各アカウントにトランジットゲートウェイを作成します。NATゲートウェイをトランジットゲートウェイに接続します。インターネットアクセスを許可するために必要なルーティングを構成します。"
      },
      {
        "key": "D",
        "text": "Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the",
        "text_jp": "エグレスVPCとスポークVPC間にAWS PrivateLink接続を作成します。アクセスを許可するために必要なルーティングを構成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.",
        "situation_analysis": "The situation describes a hub-and-spoke model where traffic from private subnets in spoke VPCs needs to reach the internet through a central egress VPC. NAT gateways are in place for outbound internet access from private subnets.",
        "option_analysis": "Option A is correct as it establishes direct connectivity via peering, which is efficient for routing. Option B introduces a transit gateway, which may be more complex without necessity. Option C unnecessarily multiplies gateways. Option D uses PrivateLink, which is inappropriate for outbound internet access.",
        "additional_knowledge": "When establishing peering relationships, it's important to consider the limitations on the number of peering connections and manage CIDR overlap.",
        "key_terminology": "VPC Peering, NAT Gateway, Egress, Hub-and-Spoke, Routing",
        "overall_assessment": "Option A aligns well with AWS best practices for efficient VPC interconnection. Community vote indicating preference for B suggests some might favor a more centralized approach, but A is optimal given the requirements outlined."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである: エグレスVPCとスポークVPC間にピアリング接続を作成し、インターネットアクセスを許可するために必要なルーティングを構成する。",
        "situation_analysis": "状況は、スポークVPC内のプライベートサブネットからのトラフィックが、中央のエグレスVPCを介してインターネットに到達する必要があるハブアンドスポークモデルについて説明している。NATゲートウェイがプライベートサブネットからのアウトバウンドインターネットアクセスのために配置されている。",
        "option_analysis": "選択肢Aが正しい理由は、ピアリングを通じて直接接続を確立し、ルーティングを効率的に行えるからである。選択肢Bはトランジットゲートウェイを導入するが、必要がない場合はより複雑になる可能性がある。選択肢Cはゲートウェイを無駄に増やしている。選択肢DはPrivateLinkを利用するが、アウトバウンドインターネットアクセスには不適切である。",
        "additional_knowledge": "ピアリング関係を確立する際には、ピアリング接続の数に制限があることを考慮し、CIDRの重複を管理することが重要である。",
        "key_terminology": "VPCピアリング、NATゲートウェイ、エグレス、ハブアンドスポーク、ルーティング",
        "overall_assessment": "選択肢Aは、VPCの相互接続に関するAWSのベストプラクティスに良く合致している。コミュニティ投票でBが好まれたことは、より集中管理されたアプローチを好む人がいることを示唆しているが、要求された要件に基づきAが最適である。"
      }
    ],
    "keywords": [
      "VPC Peering",
      "NAT Gateway",
      "Egress",
      "Hub-and-Spoke",
      "Routing"
    ]
  },
  {
    "No": "248",
    "question": "An education company is running a web application used by college students around the world. The application runs in an Amazon Elastic\nContainer Service (Amazon ECS) cluster in an Auto Scaling group behind an Application Load Balancer (ALB). A system administrator detects a\nweekly spike in the number of failed login attempts, which overwhelm the application's authentication service. All the failed login attempts\noriginate from about 500 different IP addresses that change each week. A solutions architect must prevent the failed login attempts from\noverwhelming the authentication service.\nWhich solution meets these requirements with the MOST operational eficiency?",
    "question_jp": "教育会社は、世界中の大学生によって使用されるウェブアプリケーションを運営しています。このアプリケーションは、Amazon Elastic Container Service（Amazon ECS）クラスター内で、アプリケーションロードバランサー（ALB）の背後にある自動スケーリンググループで実行されています。システム管理者は、毎週、アプリケーションの認証サービスを圧倒する失敗したログイン試行の急増を検出しました。すべての失敗したログイン試行は、毎週変更される約500の異なるIPアドレスから発生しています。ソリューションアーキテクトは、失敗したログイン試行が認証サービスを圧倒しないようにしなければなりません。どのソリューションが最も運用効率を高める要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses.",
        "text_jp": "AWS Firewall Managerを使用して、IPアドレスからのアクセスを拒否するセキュリティグループとセキュリティグループポリシーを作成する。"
      },
      {
        "key": "B",
        "text": "Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB.",
        "text_jp": "AWS WAF Web ACLを作成し、レートベースのルールを設定し、ルールアクションをブロックに設定する。Web ACLをALBに接続する。"
      },
      {
        "key": "C",
        "text": "Use AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges.",
        "text_jp": "AWS Firewall Managerを使用して、特定のCIDR範囲のみへのアクセスを許可するセキュリティグループとセキュリティグループポリシーを作成する。"
      },
      {
        "key": "D",
        "text": "Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB.",
        "text_jp": "AWS WAF Web ACLを作成し、IPセットマッチルールを設定し、ルールアクションをブロックに設定する。Web ACLをALBに接続する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, as it effectively uses AWS WAF to manage a high number of failed login attempts through a rate-based rule.",
        "situation_analysis": "The application is facing a significant number of failed login attempts from many varying IP addresses weekly, requiring an efficient response to mitigate potential attacks.",
        "option_analysis": "Option B's rate-based rule dynamically blocks IP addresses that exceed a specific threshold, thereby managing traffic efficiently. Options A and C involve static approaches that may not address the variable nature of the attacks. Option D could block specific addresses but lacks the dynamic rate-limiting feature.",
        "additional_knowledge": "Using rate-based rules in AWS WAF is a scalable solution to prevent DDoS attacks and brute-force login attempts.",
        "key_terminology": "AWS WAF, rate-based rules, web ACL, Application Load Balancer, security groups",
        "overall_assessment": "Given the details in the scenario, Option B not only addresses the problem effectively but also aligns with best practices for operational efficiency in cloud architectures."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、レートベースのルールを使用して大量の失敗したログイン試行を効果的に管理している。",
        "situation_analysis": "アプリケーションは、さまざまなIPアドレスから毎週大量の失敗したログイン試行に直面しており、効率的な対応が求められている。",
        "option_analysis": "選択肢Bのレートベースのルールは、特定のしきい値を超えたIPアドレスを動的にブロックすることで、トラフィックを効率的に管理する。選択肢AとCは、攻撃の変動性に対応していない静的アプローチを含んでいる。選択肢Dは特定のアドレスをブロックできるが、動的なレート制限機能が欠けている。",
        "additional_knowledge": "AWS WAFにおけるレートベースのルールの使用は、DDoS攻撃やブルートフォースログイン試行を防ぐためのスケーラブルなソリューションである。",
        "key_terminology": "AWS WAF、レートベースのルール、Web ACL、アプリケーションロードバランサー、セキュリティグループ",
        "overall_assessment": "シナリオの詳細を考慮すると、選択肢Bは問題に効果的に対処するだけでなく、クラウドアーキテクチャにおける運用効率のベストプラクティスにも合致している。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "rate-based rules",
      "web ACL",
      "Application Load Balancer",
      "security groups"
    ]
  },
  {
    "No": "249",
    "question": "A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public\nSFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for\noutbound trafic. Changes to the SFTP endpoint IP addresses are not permitted.\nThe company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.\nWhich solution meets these requirements?",
    "question_jp": "企業は、毎日複数のファイルを取り込むオンプレミスのソフトウェア・アズ・ア・サービス（SaaS）ソリューションを運用しています。企業は、ファイル転送を円滑にするために、顧客に複数のパブリックSFTPエンドポイントを提供しています。顧客は、ファイアウォールの許可リストにSFTPエンドポイントのIPアドレスを追加します。SFTPエンドポイントのIPアドレスを変更することはできません。企業は、SaaSソリューションをAWSに移行し、ファイル転送サービスの運用負荷を軽減したいと考えています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and",
        "text_jp": "顧客所有のIPアドレスブロックを企業のAWSアカウントに登録します。アドレスプールからElastic IPアドレスを作成します。"
      },
      {
        "key": "B",
        "text": "Add a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign",
        "text_jp": "顧客所有のIPアドレスブロックを含むサブネットをVPCに追加します。アドレスプールからElastic IPアドレスを作成し、割り当てます。"
      },
      {
        "key": "C",
        "text": "Register the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load",
        "text_jp": "顧客所有のIPアドレスブロックをAmazon Route 53に登録します。Route 53でエイリアスレコードを作成し、Network Loadにポイントします。"
      },
      {
        "key": "D",
        "text": "Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and",
        "text_jp": "顧客所有のIPアドレスブロックを企業のAWSアカウントに登録します。アドレスプールからElastic IPアドレスを作成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution allows the company to retain the existing IP addresses, facilitating the transition to AWS without requiring changes on the customers' firewalls.",
        "situation_analysis": "The company operates with strict constraints on their SFTP endpoint IP addresses and needs a solution that maintains these addresses while migrating to AWS.",
        "option_analysis": "Option A suggests registering IP addresses in AWS but does not clearly address retaining their functionality through a block of IPs directly. Option B introduces creating a subnet, which might complicate matters, while C misuses Route 53 as the primary mechanism for SFTP, which relies on direct IP access.",
        "additional_knowledge": "It is crucial when migrating services to maintain existing access methods to minimize friction for end-users.",
        "key_terminology": "Elastic IP, AWS account, SFTP, network migration, static IP",
        "overall_assessment": "Option D stands out as the only solution that fulfills all requirements without introducing additional complexity or potential disruptions."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。このソリューションにより、企業は既存のIPアドレスを保持でき、顧客のファイアウォールに変更を要求することなくAWSへの移行が可能となる。",
        "situation_analysis": "企業はSFTPエンドポイントのIPアドレスに厳しい制約があり、AWSへの移行中にこれらのアドレスを維持するソリューションが必要である。",
        "option_analysis": "選択肢AはAWSにIPアドレスを登録することを提案しているが、直接的にIPブロックとしての機能を保持することには言及していない。選択肢Bはサブネットの作成を導入しているため、複雑さを増す可能性がある。一方、CはSFTPのためにRoute 53を利用することを誤用している。",
        "additional_knowledge": "サービスの移行時には、エンドユーザーの摩擦を最小限に抑えるために、既存のアクセス方法を保持することが重要である。",
        "key_terminology": "Elastic IP、AWSアカウント、SFTP、ネットワーク移行、静的IP",
        "overall_assessment": "選択肢Dは、すべての要件を満たし、追加の複雑さや潜在的な中断を引き起こさない唯一のソリューションとして際立っている。"
      }
    ],
    "keywords": [
      "Elastic IP",
      "AWS account",
      "SFTP",
      "network migration",
      "static IP"
    ]
  },
  {
    "No": "250",
    "question": "A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high-\nthroughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the\napplication to be fault tolerant.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、新しいアプリケーションを単一のAWSリージョン内で5つのAmazon EC2インスタンスで実行する必要があります。このアプリケーションは、高スループットで低遅延のネットワーク接続が必要であり、すべてのEC2インスタンス間でアプリケーションが実行されます。ただし、アプリケーションにフォールトトレラントである必要はありません。\nどのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.",
        "text_jp": "5つの新しいEC2インスタンスをクラスタープレースメントグループに展開します。EC2インスタンスタイプがエンハンストネットワーキングをサポートしていることを確認します。"
      },
      {
        "key": "B",
        "text": "Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each",
        "text_jp": "同じアベイラビリティゾーンにAuto Scalingグループで5つの新しいEC2インスタンスを展開します。各インスタンスに追加のElasticネットワークインターフェースを接続します。"
      },
      {
        "key": "C",
        "text": "Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking.",
        "text_jp": "5つの新しいEC2インスタンスをパーティションプレースメントグループに展開します。EC2インスタンスタイプがエンハンストネットワーキングをサポートしていることを確認します。"
      },
      {
        "key": "D",
        "text": "Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance.",
        "text_jp": "5つの新しいEC2インスタンスをスプレッドプレースメントグループに展開します。各EC2インスタンスに追加のElasticネットワークインターフェースを接続します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Launch five new EC2 instances into a cluster placement group, ensuring the instance type supports enhanced networking.",
        "situation_analysis": "The application needs high-throughput and low-latency network connections among the EC2 instances, with no requirement for fault tolerance.",
        "option_analysis": "Option A meets the requirements by placing instances in a cluster placement group, optimizing networking. Option B and D do not provide the necessary low-latency connections. Option C uses a partition placement group, which isn't suitable for this application type.",
        "additional_knowledge": "In scenarios where fault tolerance is not required, focusing on network performance is crucial.",
        "key_terminology": "Cluster Placement Group, Enhanced Networking, High-Throughput, Low-Latency, EC2 Instances",
        "overall_assessment": "The community overwhelmingly supports this choice, reflecting its strong alignment with AWS best practices for high-performance applications."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです：エンハンストネットワーキングをサポートするEC2インスタンスタイプを確認しながら、5つの新しいEC2インスタンスをクラスタープレースメントグループに展開します。",
        "situation_analysis": "このアプリケーションは、EC2インスタンス間での高スループットおよび低遅延のネットワーク接続を必要としており、フォールトトレラントである必要はありません。",
        "option_analysis": "選択肢Aは、インスタンスをクラスタープレースメントグループに配置することで要件を満たし、ネットワーキングを最適化します。選択肢BとDは、必要な低遅延接続を提供しません。選択肢Cはパーティションプレースメントグループを使用しており、このアプリケーションタイプには適していません。",
        "additional_knowledge": "フォールトトレラントが必要ないシナリオでは、ネットワークパフォーマンスに焦点を当てることが重要です。",
        "key_terminology": "クラスタープレースメントグループ、エンハンストネットワーキング、高スループット、低遅延、EC2インスタンス",
        "overall_assessment": "コミュニティはこの選択肢を圧倒的に支持しており、高性能アプリケーションに対するAWSのベストプラクティスに強く一致しています。"
      }
    ],
    "keywords": [
      "Cluster Placement Group",
      "Enhanced Networking",
      "High-Throughput",
      "Low-Latency",
      "EC2 Instances"
    ]
  },
  {
    "No": "251",
    "question": "A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon\nAPI Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.\nAfter initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The\ncompany believes this trafic is originating from a botnet and wants to secure its API while minimizing cost.\nWhich approach should the company take to secure its API?",
    "question_jp": "ある企業が、アメリカに拠点を置く6つのパートナーと情報を共有するためのREST APIを作成しています。この企業は、Amazon API Gatewayのリージョナルエンドポイントを作成しました。6つのパートナーは、毎日1回、日々の売上データを投稿するためにAPIにアクセスします。初回のデプロイ後、同社は、世界中の500の異なるIPアドレスから発信される1秒あたり1,000件のリクエストを観察しました。同社は、このトラフィックがボットネットから発生していると考え、コストを最小限に抑えつつAPIを保護したいと考えています。同社は、APIを保護するためにどのアプローチを取るべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、APIをオリジンとします。AWS WAFウェブACLを作成し、クライアントが送信したリクエストをブロックするルールを設定します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、APIをオリジンとします。AWS WAFウェブACLを作成し、クライアントが送信したリクエストをブロックするルールを設定します。"
      },
      {
        "key": "C",
        "text": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API.",
        "text_jp": "AWS WAFウェブACLを作成し、6つのパートナーが使用するIPアドレスへのアクセスを許可するルールを設定します。ウェブACLをAPIに関連付けます。"
      },
      {
        "key": "D",
        "text": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API.",
        "text_jp": "AWS WAFウェブACLを作成し、6つのパートナーが使用するIPアドレスへのアクセスを許可するルールを設定します。ウェブACLをAPIに関連付けます。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which involves creating an AWS WAF web ACL that allows access from the IP addresses used by the six partners and associating it with the API. This method effectively secures the API while minimizing costs by restricting access only to known partners.",
        "situation_analysis": "The company is facing unusually high traffic to its API, believed to be generated by a botnet. The goal is to secure the API against unwanted access while keeping costs low.",
        "option_analysis": "Option C is correct as it specifies allowing only trusted IP addresses. Options A and B suggest using CloudFront but do not address the core issue of securing access effectively. Option D is identical to C and would provide similar benefits if associated correctly, but the community support favors D, indicating some preference for CloudFront integration.",
        "additional_knowledge": "Combining AWS services like WAF and CloudFront can enhance security and resilience against various attack vectors, taking advantage of AWS's robust infrastructure.",
        "key_terminology": "AWS WAF, Amazon CloudFront, IP Whitelisting, API Gateway, Botnet Protection",
        "overall_assessment": "While community voting leans heavily towards D, technically both C and D are valid approaches. C focuses on cost-effectiveness by avoiding unnecessary complexity, while D may be favored for its incorporation of CloudFront advantages."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCであり、6つのパートナーが使用するIPアドレスへのアクセスを許可するAWS WAFウェブACLを作成し、それをAPIに関連付けることが含まれます。この方法は、知られているパートナーのみへのアクセスを制限することによって、コストを最小限に抑えつつAPIを効果的に保護します。",
        "situation_analysis": "同社は、ボットネットによって生成されていると信じられる異常に高いトラフィックに直面しています。目標は、不要なアクセスからAPIを保護し、コストを抑えることです。",
        "option_analysis": "選択肢Cは、信頼できるIPアドレスのみを許可することを指定しているため正しいです。選択肢AとBはCloudFrontの使用を提案していますが、効果的にアクセスを保護する核となる問題に対処していません。選択肢DはCと同じ内容であり、適切に関連付ければ同様の利点を提供しますが、コミュニティのサポートはDを好んでいることを示しており、CloudFrontの統合に対する一部の好みがあります。",
        "additional_knowledge": "WAFとCloudFrontのようなAWSサービスを組み合わせることで、さまざまな攻撃ベクトルに対してセキュリティと耐障害性を強化し、AWSの堅牢なインフラストラクチャの利点を活用できます。",
        "key_terminology": "AWS WAF, Amazon CloudFront, IPホワイトリスト, API Gateway, ボットネット保護",
        "overall_assessment": "コミュニティ投票はDに重く傾いていますが、技術的にはCとDの両方が有効なアプローチです。Cは不必要な複雑さを避けることによってコスト効果に重点を置いており、DはCloudFrontの利点を取り入れている可能性が好まれている理由かもしれません。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "Amazon CloudFront",
      "IP Whitelisting",
      "API Gateway",
      "Botnet Protection"
    ]
  },
  {
    "No": "252",
    "question": "A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor\nall data activity on all the databases.\nWhich solution will achieve this goal?",
    "question_jp": "ある企業は、単一のAWSリージョン内でアプリケーション用のAmazon Aurora PostgreSQL DBクラスターを使用しています。企業のデータベースチームは、すべてのデータベースにおけるすべてのデータ活動を監視しなければなりません。この目標を達成するための解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source.",
        "text_jp": "AWS Database Migration Service (AWS DMS)の変更データキャプチャ (CDC) タスクを設定します。Aurora DBクラスターをソースとして指定します。"
      },
      {
        "key": "B",
        "text": "Start a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda",
        "text_jp": "Aurora DBクラスターでデータベースアクティビティストリームを開始して、Amazon EventBridgeにアクティビティストリームをキャプチャします。AWS Lambdaを定義します。"
      },
      {
        "key": "C",
        "text": "Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon",
        "text_jp": "Aurora DBクラスターでデータベースアクティビティストリームを開始して、アクティビティストリームをAmazon Kinesisデータストリームにプッシュします。Amazonを設定します。"
      },
      {
        "key": "D",
        "text": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source.",
        "text_jp": "AWS Database Migration Service (AWS DMS)の変更データキャプチャ (CDC) タスクを設定します。Aurora DBクラスターをソースとして指定します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. This allows monitoring of all data activity directly from the Aurora DB cluster.",
        "situation_analysis": "The database team at the company needs to monitor all data activities for compliance and performance analysis.",
        "option_analysis": "Option D is correct because AWS DMS can capture change data, allowing the DB team to monitor all activities. Options A, B, and C are not suitable as they do not provide comprehensive activity tracking in the context given.",
        "additional_knowledge": "Experience in using AWS DMS would enhance understanding of its monitoring capabilities.",
        "key_terminology": "AWS DMS, change data capture (CDC), Aurora DB cluster, data monitoring.",
        "overall_assessment": "Despite community vote indicating C as the preference, D offers a more direct approach to comprehensive monitoring of data activities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはD: AWS Database Migration Service (AWS DMS)の変更データキャプチャ (CDC) タスクを設定します。これにより、Aurora DBクラスターから直接すべてのデータ活動を監視できます。",
        "situation_analysis": "企業のデータベースチームは、コンプライアンスやパフォーマンス分析のために、すべてのデータ活動を監視する必要があります。",
        "option_analysis": "選択肢Dが正しい理由は、AWS DMSが変更データをキャプチャできるため、データベースチームがすべての活動を監視できるからです。選択肢A、B、Cは、与えられた文脈では包括的な活動追跡を提供しないため、不適切です。",
        "additional_knowledge": "AWS DMSの使用経験があれば、その監視機能の理解が深まるでしょう。",
        "key_terminology": "AWS DMS, 変更データキャプチャ (CDC), Aurora DBクラスター, データ監視。",
        "overall_assessment": "コミュニティの投票がCを支持しているにもかかわらず、Dはデータ活動の包括的な監視へのより直接的なアプローチを提供します。"
      }
    ],
    "keywords": [
      "AWS DMS",
      "change data capture",
      "Aurora DB cluster",
      "data monitoring"
    ]
  },
  {
    "No": "253",
    "question": "An entertainment company recently launched a new game. To ensure a good experience for players during the launch period, the company\ndeployed a static quantity of 12 r6g.16xlarge (memory optimized) Amazon EC2 instances behind a Network Load Balancer. The company's\noperations team used the Amazon CloudWatch agent and a custom metric to include memory utilization in its monitoring strategy.\nAnalysis of the CloudWatch metrics from the launch period showed consumption at about one quarter of the CPU and memory that the company\nexpected. Initial demand for the game has subsided and has become more variable. The company decides to use an Auto Scaling group that\nmonitors the CPU and memory consumption to dynamically scale the instance fieet. A solutions architect needs to configure the Auto Scaling\ngroup to meet demand in the most cost-effective way.\nWhich solution will meet these requirements?",
    "question_jp": "エンターテインメント会社は最近、新しいゲームを発売した。プレイヤーが良い体験をするために、会社は12台のr6g.16xlarge（メモリ最適化）Amazon EC2インスタンスをNetwork Load Balancerの背後にデプロイした。会社の運用チームは、Amazon CloudWatchエージェントおよびカスタムメトリックを使用してメモリ使用率を監視戦略に含めた。発売期間中のCloudWatchメトリックの分析では、消費が会社が予想したCPUとメモリの約4分の1であることが示された。ゲームへの初期の需要は減少し、より変動的になった。会社は、CPUとメモリの消費を監視してインスタンスのフリートを動的にスケールするAuto Scalingグループを使用することを決定した。ソリューションアーキテクトは、最もコスト効果の高い方法で需要を満たすためにAuto Scalingグループを構成する必要がある。この要件を満たすソリューションはどれか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the Auto Scaling group to deploy c6g.4xlarge (compute optimized) instances. Configure a minimum capacity of 3, a desired",
        "text_jp": "Auto Scalingグループをc6g.4xlarge（コンピュート最適化）インスタンスをデプロイするように構成する。最小容量を3、希望容量を構成する。"
      },
      {
        "key": "B",
        "text": "Configure the Auto Scaling group to deploy m6g.4xlarge (general purpose) instances. Configure a minimum capacity of 3, a desired",
        "text_jp": "Auto Scalingグループをm6g.4xlarge（汎用）インスタンスをデプロイするように構成する。最小容量を3、希望容量を構成する。"
      },
      {
        "key": "C",
        "text": "Configure the Auto Scaling group to deploy r6g.4xlarge (memory optimized) instances. Configure a minimum capacity of 3, a desired",
        "text_jp": "Auto Scalingグループをr6g.4xlarge（メモリ最適化）インスタンスをデプロイするように構成する。最小容量を3、希望容量を構成する。"
      },
      {
        "key": "D",
        "text": "Configure the Auto Scaling group to deploy r6g.8xlarge (memory optimized) instances. Configure a minimum capacity of 2, a desired",
        "text_jp": "Auto Scalingグループをr6g.8xlarge（メモリ最適化）インスタンスをデプロイするように構成する。最小容量を2、希望容量を構成する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, as deploying r6g.8xlarge instances provides the necessary memory optimization to handle variable demand efficiently.",
        "situation_analysis": "The company needs an Auto Scaling group that can adapt to variable demand while ensuring effective resource allocation.",
        "option_analysis": "Option D is the best choice because r6g.8xlarge instances are optimized for memory and provide sufficient resources for variable loads. Options A, B, and C do not meet the required memory optimization for the game's needs.",
        "additional_knowledge": "Regularly reviewing the Auto Scaling configurations and monitoring usage can help refine performance and cost efficiency further.",
        "key_terminology": "Auto Scaling, r6g.8xlarge, memory optimized, variable demand, resource allocation",
        "overall_assessment": "Overall, the solution architect should choose option D for effective cost management and resource utilization."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。r6g.8xlargeインスタンスをデプロイすることで、変動する需要に効率よく対応するためのメモリ最適化が提供される。",
        "situation_analysis": "会社は、変動する需要に適応できるAuto Scalingグループを必要としており、効果的なリソース配分を確保する必要がある。",
        "option_analysis": "選択肢Dは、r6g.8xlargeインスタンスがメモリに最適化されており、変動する負荷に対して十分なリソースを提供できるため、最良の選択肢である。選択肢A、B、Cはゲームのニーズに必要なメモリ最適化を満たしていない。",
        "additional_knowledge": "Auto Scalingの構成を定期的に見直し、使用状況を監視することで、パフォーマンスとコスト効率のさらなる改善が可能である。",
        "key_terminology": "Auto Scaling, r6g.8xlarge, メモリ最適化, 変動需要, リソース配分",
        "overall_assessment": "全体として、ソリューションアーキテクトはコスト管理とリソース利用のために選択肢Dを選択すべきである。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "r6g.8xlarge",
      "memory optimized",
      "variable demand",
      "resource allocation"
    ]
  },
  {
    "No": "254",
    "question": "A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity\nmode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts\nthroughout the day. and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.\nWhich strategy should a solutions architect recommend to meet this requirement?",
    "question_jp": "金融サービス会社が何百万もの過去の株式取引をAmazon DynamoDBテーブルにロードしました。このテーブルはオンデマンドキャパシティモードを使用しています。毎日真夜中に数百万の新しいレコードがテーブルにロードされます。アプリケーションの読取り活動は、日中にバーストで発生し、限定的なキーセットが繰り返し検索されます。この会社はDynamoDBに関連するコストを削減する必要があります。どの戦略をソリューションアーキテクトは推奨すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy an Amazon ElastiCache cluster in front of the DynamoDB table",
        "text_jp": "Amazon ElastiCacheクラスタをDynamoDBテーブルの前にデプロイする"
      },
      {
        "key": "B",
        "text": "Deploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer.",
        "text_jp": "DynamoDB Accelerator (DAX)をデプロイする。DynamoDBのオートスケーリングを設定する。Cost ExplorerでSavings Plansを購入する。"
      },
      {
        "key": "C",
        "text": "Use provisioned capacity mode. Purchase Savings Plans in Cost Explorer.",
        "text_jp": "プロビジョニングされたキャパシティモードを使用する。Cost ExplorerでSavings Plansを購入する。"
      },
      {
        "key": "D",
        "text": "Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling.",
        "text_jp": "DynamoDB Accelerator (DAX)をデプロイする。プロビジョニングされたキャパシティモードを使用する。DynamoDBのオートスケーリングを設定する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (79%) C (16%)5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Deploy an Amazon ElastiCache cluster in front of the DynamoDB table. This approach enhances performance by caching frequently accessed data and reduces read costs associated with DynamoDB.",
        "situation_analysis": "The application experiences burst read activity and accesses a limited set of keys multiple times throughout the day. Using Amazon ElastiCache can help alleviate the read load on DynamoDB and cut costs.",
        "option_analysis": "Option A is correct as it uses caching to improve performance and reduce costs. Option B involves DAX and auto-scaling, which might introduce additional complexity and costs. Option C does not capitalize on the performance benefits of caching. Option D also introduces unnecessary complexity with auto-scaling.",
        "additional_knowledge": "",
        "key_terminology": "Amazon ElastiCache, DynamoDB, caching, cost optimization, read activity",
        "overall_assessment": "The community's choice of D suggests that they may misunderstand the scenario's requirements. The high vote for D does not align with the cost-reduction strategy needed for the given use case. Caching with ElastiCache is a straightforward answer for reducing costs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA: Amazon ElastiCacheクラスタをDynamoDBテーブルの前にデプロイすることです。このアプローチは、頻繁にアクセスされるデータをキャッシュすることによってパフォーマンスを向上させ、DynamoDBに関連する読み取りコストを削減します。",
        "situation_analysis": "アプリケーションはバースト的な読み取り活動を経験し、日中に限られたキーセットに複数回アクセスしています。Amazon ElastiCacheを使用すれば、DynamoDBへの読み取り負荷を軽減し、コストを削減できます。",
        "option_analysis": "選択肢Aは、キャッシュを使用してパフォーマンスを向上させ、コストを削減するため正解です。選択肢BはDAXとオートスケーリングが含まれ、追加の複雑さとコストが発生する可能性があります。選択肢Cは、キャッシングのパフォーマンス利点を十分に活かしていません。選択肢Dもオートスケーリングによる不要な複雑さをもたらします。",
        "additional_knowledge": "",
        "key_terminology": "Amazon ElastiCache, DynamoDB, キャッシング, コスト最適化, 読み取り活動",
        "overall_assessment": "コミュニティのD選択は、シナリオの要件を誤解している可能性を示唆しています。Dに高投票が集まったことは、与えられたユースケースに必要なコスト削減戦略に合致していません。ElastiCacheでのキャッシングは、コスト削減に対する簡潔な回答です。"
      }
    ],
    "keywords": [
      "Amazon ElastiCache",
      "DynamoDB",
      "caching",
      "cost optimization",
      "read activity"
    ]
  },
  {
    "No": "255",
    "question": "A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts.\nAWS PrivateLink is being used to provide connectivity between the client services and the logging service.\nIn each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on\nEC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC\nendpoint.\nWhich combination of steps should a solutions architect take to resolve this issue? (Choose two.)",
    "question_jp": "ある会社が、数百のAWSアカウントからのログを受信・分析する、Amazon EC2上で稼働する集中型ログサービスを作成しています。AWS PrivateLinkを使用して、クライアントサービスとログサービス間の接続を提供しています。各AWSアカウントにおいて、ログサービス用のインターフェイスエンドポイントが作成され、利用可能となっています。EC2インスタンス上でネットワークロードバランサー（NLB）を使用しているログサービスは異なるサブネットにデプロイされています。クライアントは、VPCエンドポイントを使用してログを提出できません。この問題を解決するために、ソリューションアーキテクトはどの組み合わせのステップを取るべきですか？（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL",
        "text_jp": "ログサービスのサブネットにNACLがアタッチされていることを確認し、NLBサブネットとの通信を許可します。NACLを確認します。"
      },
      {
        "key": "B",
        "text": "Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets.",
        "text_jp": "ログサービスのサブネットにNACLがアタッチされていることを確認し、インターフェイスエンドポイントのサブネットとの通信を許可します。"
      },
      {
        "key": "C",
        "text": "Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets.",
        "text_jp": "EC2インスタンス上で稼働するログサービスのセキュリティグループを確認し、NLBサブネットからのインバウンドを許可していることを確認します。"
      },
      {
        "key": "D",
        "text": "Check the security group for the logging service running on EC2 instances to ensure it allows ingress from the clients.",
        "text_jp": "EC2インスタンス上で稼働するログサービスのセキュリティグループを確認し、クライアントからのインバウンドを許可していることを確認します。"
      },
      {
        "key": "E",
        "text": "Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets.",
        "text_jp": "NLBのセキュリティグループを確認し、インターフェイスエンドポイントサブネットからのインバウンドが許可されていることを確認します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (52%) BD (36%) 12%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A and B. Steps A and B ensure that the Network Access Control Lists (NACLs) are configured properly to allow communication between the logging service and relevant subnet configurations.",
        "situation_analysis": "The issue involves log submission failures from clients to a centralized logging service deployed across multiple AWS accounts. The communications need to cross various network components.",
        "option_analysis": "Option A checks NACL configuration for communication with NLB subnets, which is essential as the logging service relies on traffic from the NLB. Option B ensures connectivity between the logging service and interface endpoint subnets.",
        "additional_knowledge": "Understanding AWS networking can involve complex configurations; hence, ensuring both NACLs and Security Groups are correctly set up is critical.",
        "key_terminology": "Network Access Control Lists (NACLs), Security Groups, Interface Endpoints, VPC, AWS PrivateLink",
        "overall_assessment": "The question assesses understanding of VPC networking and security. Community votes suggest a variety of interpretations, but the technical configuration outlined in options A and B provide the correct pathway to resolution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAとBである。ステップAとBは、ログサービスと関連するサブネット構成間の通信を許可するためにNACLが適切に構成されていることを確実にする。",
        "situation_analysis": "クライアントから集中型ログサービスへのログ送信の失敗が問題となっている。通信は様々なネットワークコンポーネントを横断する必要がある。",
        "option_analysis": "選択肢AはNLBサブネットとの通信のためのNACL設定を確認し、ログサービスがNLBからのトラフィックに依存しているため、これは重要である。選択肢Bは、ログサービスとインターフェイスエンドポイントのサブネット間の接続を確保する。",
        "additional_knowledge": "AWSのネットワーキングは複雑な構成を伴う場合があるため、NACLとセキュリティグループの両方が正しく設定されていることを確認することが重要である。",
        "key_terminology": "ネットワークアクセスコントロールリスト（NACL）、セキュリティグループ、インターフェイスエンドポイント、VPC、AWS PrivateLink",
        "overall_assessment": "この質問はVPCネットワーキングとセキュリティの理解を評価する。コミュニティの投票では様々な解釈が示されるが、選択肢AとBに示された技術的構成が解決への正しい道を提供する。"
      }
    ],
    "keywords": [
      "Network Access Control Lists",
      "Security Groups",
      "Interface Endpoints",
      "VPC",
      "AWS PrivateLink"
    ]
  },
  {
    "No": "256",
    "question": "A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed\nfrequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side\nencryption with AWS KMS keys (SSE-KMS).\nA solutions architect reviews the company's monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of\nrequests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業は、Amazon S3 バケットに何百万のオブジェクトを保有しています。オブジェクトは S3 Standard ストレージクラスにあり、すべての S3 オブジェクトは頻繁にアクセスされています。オブジェクトにアクセスするユーザーおよびアプリケーションの数は急速に増加しています。オブジェクトは、AWS KMS キー（SSE-KMS）によるサーバー側暗号化で暗号化されています。ソリューションアーキテクトは、企業の月次 AWS 請求書をレビューし、Amazon S3 からのリクエストの多さによって AWS KMS コストが増加していることに気づきました。ソリューションアーキテクトは、アプリケーションへの変更を最小限に抑えてコストを最適化する必要があります。どのソリューションが最小限の運用オーバーヘッドでこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing",
        "text_jp": "顧客提供のキー（SSE-C）を暗号化タイプとして持つ新しい S3 バケットを作成し、既存のオブジェクトをコピーします"
      },
      {
        "key": "B",
        "text": "Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch",
        "text_jp": "Amazon S3 管理キー（SSE-S3）を暗号化タイプとして持つ新しい S3 バケットを作成し、S3 バッチを使用します"
      },
      {
        "key": "C",
        "text": "Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new",
        "text_jp": "AWS CloudHSM を使用して暗号化キーを保存します。新しい S3 バケットを作成し、S3 バッチオペレーションを使用して既存のオブジェクトを新しいバケットにコピーします"
      },
      {
        "key": "D",
        "text": "Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that",
        "text_jp": "S3 バケットに S3 インテリジェント階層化ストレージクラスを使用します。オブジェクトを遷移させるための S3 インテリジェント階層化アーカイブ設定を作成します"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, which involves creating a new S3 bucket using customer provided keys for encryption, reducing AWS KMS requests and associated costs.",
        "situation_analysis": "The company needs to minimize KMS costs due to frequent requests while ensuring seamless access to S3 objects.",
        "option_analysis": "Option A minimizes KMS calls by allowing customer managed encryption key use, while B, C, and D increase complexity and costs.",
        "additional_knowledge": "It's critical to have a strong key management policy when using customer-provided keys (SSE-C).",
        "key_terminology": "AWS KMS, SSE-C, encryption, cost optimization, Amazon S3",
        "overall_assessment": "Option A is the most suitable choice as it addresses cost control without significantly altering the application architecture."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAであり、暗号化用に顧客提供のキーを使用する新しいS3バケットを作成することで、AWS KMSへのリクエストと関連コストを削減します。",
        "situation_analysis": "企業は、S3オブジェクトへのアクセスを確保しつつ、頻繁なリクエストによるKMSコストを最小化する必要があります。",
        "option_analysis": "選択肢Aは、顧客管理の暗号化キーの使用により、KMSコールを最小限に抑えますが、B、C、Dは複雑さとコストを増加させます。",
        "additional_knowledge": "顧客提供のキー（SSE-C）を使用する際には、強固なキー管理ポリシーを持つことが重要です。",
        "key_terminology": "AWS KMS, SSE-C, 暗号化, コスト最適化, Amazon S3",
        "overall_assessment": "選択肢Aは、アプリケーションアーキテクチャを大きく変更することなくコストコントロールに対応しているため、最も適切な選択です。"
      }
    ],
    "keywords": [
      "AWS KMS",
      "SSE-C",
      "encryption",
      "cost optimization",
      "Amazon S3"
    ]
  },
  {
    "No": "257",
    "question": "A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions. Application state is stored in Amazon\nDynamoDB tables. Users are reporting that some uploaded photos are not being processed properly. The application developers trace the logs and\nfind that Lambda is experiencing photo processing issues when thousands of users upload photos simultaneously. The issues are the result of\nLambda concurrency limits and the performance of DynamoDB when data is saved.\nWhich combination of actions should a solutions architect take to increase the performance and reliability of the application? (Choose two.)",
    "question_jp": "ユーザーの写真を処理するために、メディアストレージアプリケーションがAmazon S3にアップロードします。アプリケーションの状態はAmazon DynamoDBテーブルに保存されています。ユーザーからは、アップロードした写真が適切に処理されないという報告があります。アプリケーション開発者はログを追跡し、数千のユーザーが同時に写真をアップロードしているときにLambdaが写真処理の問題を経験していることを発見しました。問題の原因は、Lambdaの同時実行制限とデータが保存される際のDynamoDBのパフォーマンスです。ソリューションアーキテクトは、アプリケーションのパフォーマンスと信頼性を向上させるために、どの組み合わせのアクションを取るべきですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Evaluate and adjust the RCUs for the DynamoDB tables.",
        "text_jp": "DynamoDBテーブルのRCUを評価し、調整する。"
      },
      {
        "key": "B",
        "text": "Evaluate and adjust the WCUs for the DynamoDB tables.",
        "text_jp": "DynamoDBテーブルのWCUを評価し、調整する。"
      },
      {
        "key": "C",
        "text": "Add an Amazon ElastiCache layer to increase the performance of Lambda functions.",
        "text_jp": "Lambda関数のパフォーマンスを向上させるために、Amazon ElastiCacheレイヤーを追加する。"
      },
      {
        "key": "D",
        "text": "Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions.",
        "text_jp": "Amazon S3とLambda関数の間にAmazon Simple Queue Service (SQS)キューと再処理ロジックを追加する。"
      },
      {
        "key": "E",
        "text": "Use S3 Transfer Acceleration to provide lower latency to users.",
        "text_jp": "S3 Transfer Accelerationを使用して、ユーザーへの遅延を低下させる。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are B and D. Adjusting the Write Capacity Units (WCU) for DynamoDB helps manage write operations, while adding an SQS queue can handle peak loads effectively.",
        "situation_analysis": "The application is facing performance issues due to Lambda concurrency limits and DynamoDB performance during simultaneous uploads by thousands of users.",
        "option_analysis": "Option B addresses the WCU, which is crucial for handling more write operations in DynamoDB. Option D introduces SQS to decouple the photo upload process from the immediate processing, allowing Lambda to process photos at a manageable rate.",
        "additional_knowledge": "Scaling strategies such as using AWS Lambda reserved concurrency can also be considered for managing concurrency limits effectively.",
        "key_terminology": "AWS Lambda, Amazon DynamoDB, Write Capacity Units (WCU), Amazon Simple Queue Service (SQS), concurrency limits.",
        "overall_assessment": "The question effectively tests knowledge on AWS service performance tuning and design patterns for decoupling application components. The community supports the answer by voting for B and D."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBとDです。DynamoDBの書き込みキャパシティ単位（WCU）を調整することで、書き込み操作を管理でき、SQSキューを追加することでピーク負荷を効果的に処理できます。",
        "situation_analysis": "アプリケーションは、数千のユーザーが同時にアップロードする際のLambdaの同時実行制限とDynamoDBのパフォーマンスにより、パフォーマンスの問題に直面しています。",
        "option_analysis": "選択肢Bは、DynamoDBでのより多くの書き込み操作を処理するために、WCUへの調整を行います。選択肢Dは、SQSを導入し、写真のアップロードプロセスを即時処理から分離し、Lambdaが管理可能なレートで写真を処理できるようにします。",
        "additional_knowledge": "AWS Lambdaの予約同時実行を使用するなど、同時実行制限を効果的に管理するためのスケーリング戦略も考慮できます。",
        "key_terminology": "AWS Lambda, Amazon DynamoDB, 書き込みキャパシティ単位（WCU）, Amazon Simple Queue Service（SQS）, 同時実行制限。",
        "overall_assessment": "この質問は、AWSサービスのパフォーマンスチューニングおよびアプリケーションコンポーネントのデカップリングの設計パターンに関する知識を効果的にテストしています。コミュニティはBとDに投票して答えを支持しています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon DynamoDB",
      "Write Capacity Units",
      "Amazon Simple Queue Service",
      "concurrency limits"
    ]
  },
  {
    "No": "258",
    "question": "A company runs an application in an on-premises data center. The application gives users the ability to upload media files. The files persist in a\nfile server. The web application has many users. The application server is overutilized, which causes data uploads to fail occasionally. The\ncompany frequently adds new storage to the file server. The company wants to resolve these challenges by migrating the application to AWS.\nUsers from across the United States and Canada access the application. Only authenticated users should have the ability to access the\napplication to upload files. The company will consider a solution that refactors the application, and the company needs to accelerate application\ndevelopment.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "企業はオンプレミスのデータセンターでアプリケーションを運用しています。このアプリケーションは、ユーザーにメディアファイルをアップロードする機能を提供します。ファイルはファイルサーバーに保存されます。Webアプリケーションには多くのユーザーがおり、アプリケーションサーバーは過負荷状態で、データのアップロードが時々失敗しています。企業はファイルサーバーに頻繁に新しいストレージを追加しています。企業は、アプリケーションをAWSに移行することでこれらの課題を解決したいと考えています。アメリカとカナダ全土からユーザーがアプリケーションにアクセスします。認証されたユーザーのみがファイルをアップロードできるようにする必要があります。企業はアプリケーションのリファクタリングを検討し、アプリケーション開発の加速が求められています。これらの要件を最小限の運用オーバーヘッドで満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the",
        "text_jp": "AWS Application Migration Serviceを使用して、アプリケーションサーバーをAmazon EC2インスタンスに移行します。Auto Scalingグループを作成します。"
      },
      {
        "key": "B",
        "text": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the",
        "text_jp": "AWS Application Migration Serviceを使用して、アプリケーションサーバーをAmazon EC2インスタンスに移行します。Auto Scalingグループを作成します。"
      },
      {
        "key": "C",
        "text": "Create a static website for uploads of media files. Store the static assets in Amazon S3. Use AWS AppSync to create an API. Use AWS",
        "text_jp": "メディアファイルのアップロード用の静的ウェブサイトを作成します。静的アセットをAmazon S3に保存します。AWS AppSyncを使用してAPIを作成します。"
      },
      {
        "key": "D",
        "text": "Use AWS Amplify to create a static website for uploads of media files. Use Amplify Hosting to serve the website through Amazon",
        "text_jp": "AWS Amplifyを使用して、メディアファイルのアップロード用の静的ウェブサイトを作成します。Amplify Hostingを使用して、ウェブサイトをAmazonを介して提供します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This option ensures minimal operational overhead while allowing the application to scale appropriately using Auto Scaling.",
        "situation_analysis": "The application has high resource usage and needs to handle many uploads. There is a need for secure access for authenticated users, and the environment needs to be capable of scaling efficiently.",
        "option_analysis": "Option A provides a migration path that retains the existing application structure while utilizing AWS services for scaling. Other options involve more refactoring, adding complexity and management overhead.",
        "additional_knowledge": "It is important to assess existing applications for compatibility with AWS services during migration.",
        "key_terminology": "Auto Scaling, AWS Application Migration Service, Amazon EC2",
        "overall_assessment": "Option A meets the requirements for operational efficiency and is well-suited for lifting and shifting with the intent to optimize later. The community vote largely favors option D, which may reflect a preference for newer architectures, but A remains appropriate given the specific needs stated in the question."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。この選択肢は、アプリケーションが適切にスケールするのを可能にしながら、最小限の運用オーバーヘッドを確保する。",
        "situation_analysis": "アプリケーションは高いリソース使用率を持ち、多くのアップロードを処理する必要があります。認証されたユーザーの安全なアクセスが求められ、環境は効率的にスケールする能力が必要である。",
        "option_analysis": "選択肢Aは、既存のアプリケーション構造を維持しつつAWSサービスを利用してスケーリングを行う移行ルートを提供する。他の選択肢は、より多くのリファクタリングを伴い、複雑さと管理オーバーヘッドを増加させる。",
        "additional_knowledge": "移行中にAWSサービスとの互換性について既存のアプリケーションを評価することが重要である。",
        "key_terminology": "Auto Scaling、AWS Application Migration Service、Amazon EC2",
        "overall_assessment": "選択肢Aは運用効率の要件を満たしており、リフトアンドシフトの目的に最も適している。コミュニティの投票は主に選択肢Dに支持されているが、これは新しいアーキテクチャへの好みを反映している可能性があるが、質問に明記された特定のニーズに対してAが適切である。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "AWS Application Migration Service",
      "Amazon EC2",
      "media file upload",
      "application migration"
    ]
  },
  {
    "No": "259",
    "question": "A company has an application that is deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are part of an\nAuto Scaling group. The application has unpredictable workloads and frequently scales out and in. The company's development team wants to\nanalyze application logs to find ways to improve the application's performance. However, the logs are no longer available after instances scale in.\nWhich solution will give the development team the ability to view the application logs after a scale-in event?",
    "question_jp": "ある会社には、Application Load Balancer (ALB) の背後にデプロイされたアプリケーションがあります。インスタンスはAuto Scalingグループに所属しています。アプリケーションは予測不可能な負荷があり、頻繁にスケールアウトおよびスケールインします。会社の開発チームは、アプリケーションのログを分析して、アプリケーションのパフォーマンスを改善する方法を見つけたいと考えています。ただし、インスタンスがスケールインした後、ログはもう利用できなくなります。スケールインイベント後にアプリケーションのログを表示する能力を開発チームに与える解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Enable access logs for the ALB. Store the logs in an Amazon S3 bucket.",
        "text_jp": "ALBのアクセスログを有効にし、ログをAmazon S3バケットに保存する。"
      },
      {
        "key": "B",
        "text": "Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent.",
        "text_jp": "EC2インスタンスを設定して、統合CloudWatchエージェントを使用してログをAmazon CloudWatch Logsに公開する。"
      },
      {
        "key": "C",
        "text": "Modify the Auto Scaling group to use a step scaling policy.",
        "text_jp": "Auto Scalingグループを変更して、ステップスケーリングポリシーを使用する。"
      },
      {
        "key": "D",
        "text": "Instrument the application with AWS X-Ray tracing.",
        "text_jp": "アプリケーションにAWS X-Rayトレースを取り入れる。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Configuring EC2 instances to publish logs to Amazon CloudWatch Logs allows the application logs to be retained even after the instances scale in.",
        "situation_analysis": "The application experiences unpredictable workloads with instances frequently scaling in and out. Without persistent logging, important operational information may be lost after scaling activities.",
        "option_analysis": "Option A would enable logging of requests going to the ALB but does not capture application-level logs from EC2 instances. Option C relates to scaling mechanics rather than logging. Option D provides tracing capabilities but does not address log retention. Thus, B is the correct choice for retaining logs through CloudWatch.",
        "additional_knowledge": "It is also beneficial to set up log retention policies in CloudWatch Logs to manage storage costs while keeping necessary logs for analysis.",
        "key_terminology": "Amazon EC2, Application Load Balancer, Auto Scaling, Amazon CloudWatch Logs.",
        "overall_assessment": "The question effectively addresses a common architectural concern in dynamic environments concerning log retention and access. The community supports option B unanimously, reinforcing its correctness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。EC2インスタンスを設定してログをAmazon CloudWatch Logsに公開することで、インスタンスがスケールインした後もアプリケーションログを保持できる。",
        "situation_analysis": "アプリケーションは予測不可能な負荷を経験しており、インスタンスは頻繁にスケールインおよびスケールアウトしている。持続的なログがなければ、重要な運用情報がスケーリング活動の後に失われる可能性がある。",
        "option_analysis": "選択肢AはALBに送信されるリクエストのログを有効にするが、EC2インスタンスからのアプリケーションレベルのログをキャプチャしない。選択肢Cはスケーリングメカニズムに関するもので、ログとは関連がない。選択肢Dはトレース機能を提供するが、ログの保持に関しては対処していない。このため、BがログをCloudWatchで保持するための正しい選択である。",
        "additional_knowledge": "CloudWatch Logsでログ保持ポリシーを設定することも、ストレージコストを管理しつつ、分析に必要なログを保持するのに役立つ。",
        "key_terminology": "Amazon EC2, Application Load Balancer, Auto Scaling, Amazon CloudWatch Logs.",
        "overall_assessment": "この質問は、ダイナミックな環境におけるログ保持とアクセスに関する一般的なアーキテクチャの問題を効果的に扱っている。コミュニティはBの選択肢を全員支持しており、その正しさを強調している。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Application Load Balancer",
      "Auto Scaling",
      "Amazon CloudWatch Logs"
    ]
  },
  {
    "No": "260",
    "question": "A company runs an unauthenticated static website (www.example.com) that includes a registration form for users. The website uses Amazon S3\nfor hosting and uses Amazon CloudFront as the content delivery network with AWS WAF configured. When the registration form is submitted, the\nwebsite calls an Amazon API Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward the payload to an\nexternal API call.\nDuring testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutions architect confirms that the CloudFront\ndistribution origin has the Access-Control-Allow-Origin header set to www.example.com.\nWhat should the solutions architect do to resolve the error?",
    "question_jp": "会社がユーザーの登録フォームを含む認証されていない静的ウェブサイト（www.example.com）を運営しています。このウェブサイトはAmazon S3を使用してホスティングされており、AWS WAFが設定されたAmazon CloudFrontをコンテンツ配信ネットワークとして使用しています。登録フォームが送信されると、ウェブサイトはAmazon API GatewayのAPIエンドポイントを呼び出し、AWS Lambda関数がペイロードを処理して外部API呼び出しに転送します。テスト中、ソリューションアーキテクトはクロスオリジンリソースシェアリング（CORS）エラーに遭遇しました。ソリューションアーキテクトはCloudFrontディストリビューションのオリジンにAccess-Control-Allow-Originヘッダーがwww.example.comに設定されていることを確認しました。このエラーを解決するためにソリューションアーキテクトは何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Change the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com.",
        "text_jp": "S3バケットのCORS設定を変更します。AllowedOrigin要素にwww.example.comのルールを追加します。"
      },
      {
        "key": "B",
        "text": "Enable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com.",
        "text_jp": "AWS WAFでCORS設定を有効にします。Access-Control-Allow-Originヘッダーがwww.example.comに設定されているウェブACLルールを作成します。"
      },
      {
        "key": "C",
        "text": "Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the",
        "text_jp": "API Gateway APIエンドポイントでCORS設定を有効にします。APIエンドポイントがすべてのレスポンスに対して"
      },
      {
        "key": "D",
        "text": "Enable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header",
        "text_jp": "Lambda関数でCORS設定を有効にします。関数の戻りコードにAccess-Control-Allow-Originヘッダーが含まれていることを確認します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. By enabling the CORS settings in AWS WAF and creating a web ACL rule specifying the Access-Control-Allow-Origin header, it ensures proper handling of CORS requests.",
        "situation_analysis": "The company is facing CORS errors when users try to submit the registration form. This indicates that the browser is blocking the request due to CORS policy violations.",
        "option_analysis": "Option A is incorrect because changing the S3 bucket's CORS configuration alone does not resolve the cross-origin requests being blocked. Option C does not provide a complete action and is incomplete. Option D may address Lambda's response, but the WAF setting is essential for controlling web traffic.",
        "additional_knowledge": "CORS issues are commonly managed at multiple points in AWS architecture, necessitating a coordinated approach.",
        "key_terminology": "CORS, Access-Control-Allow-Origin, AWS WAF, web ACL, API Gateway",
        "overall_assessment": "The choice of B is aligned with best practices for handling CORS at the API Gateway and WAF level, as this offers a comprehensive solution to the problem encountered."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。AWS WAFでCORS設定を有効にし、Access-Control-Allow-Originヘッダーがwww.example.comに設定されているウェブACLルールを作成することによって、CORSリクエストが適切に処理されることを保証する。",
        "situation_analysis": "会社はユーザーが登録フォームを送信しようとするとCORSエラーに直面している。これは、ブラウザがCORSポリシー違反のためにリクエストをブロックしていることを示している。",
        "option_analysis": "選択肢Aは不正である。S3バケットのCORS設定を変更するだけでは、クロスオリジンリクエストのブロックを解決できない。選択肢Cは完全な行動を示していないため不完全である。選択肢DはLambdaの応答に対応する可能性があるが、WAFの設定がウェブトラフィックを制御するために重要である。",
        "additional_knowledge": "CORSの問題はAWSアーキテクチャの複数のポイントで一般的に管理されており、調整されたアプローチが必要である。",
        "key_terminology": "CORS、Access-Control-Allow-Origin、AWS WAF、ウェブACL、API Gateway",
        "overall_assessment": "選択BはAPI GatewayおよびWAFレベルでのCORS処理のベストプラクティスと一致しており、遭遇した問題に対する包括的な解決策を提供する。"
      }
    ],
    "keywords": [
      "CORS",
      "Access-Control-Allow-Origin",
      "AWS WAF",
      "web ACL",
      "API Gateway"
    ]
  },
  {
    "No": "261",
    "question": "A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different\ndepartments in the company. The company has a Microsoft Azure Active Directory that is deployed.\nA solutions architect needs to centralize billing and management of the company's AWS accounts. The company wants to start using identity\nfederation instead of manual user management. The company also wants to use temporary credentials instead of long-lived access keys.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ある企業は多くの独立したAWSアカウントを持ち、中央管理や請求を行っていません。各AWSアカウントは、企業内の異なる部署向けのサービスをホストしています。企業には、展開されているMicrosoft Azure Active Directoryがあります。ソリューションアーキテクトは、企業のAWSアカウントの請求と管理を中央集約化する必要があります。また、企業は手動のユーザー管理の代わりにアイデンティティフェデレーションを使用し始めたく、長期的なアクセスキーの代わりに一時的な認証情報を使用したいと考えています。これらの要件を満たすために、どの組み合わせの手順を選択すればよいでしょうか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS",
        "text_jp": "管理アカウントとして新しいAWSアカウントを作成し、AWS Organizationsに組織を展開します。既存の各AWSアカウントを招待します。"
      },
      {
        "key": "B",
        "text": "Configure each AWS account's email address to be aws+@example.com so that account management email messages and invoices are",
        "text_jp": "各AWSアカウントのメールアドレスをaws+@example.comに設定し、アカウント管理のメールメッセージや請求書が送信されるようにします。"
      },
      {
        "key": "C",
        "text": "Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active",
        "text_jp": "管理アカウントにAWS IAM Identity Center（AWS Single Sign-On）を展開し、IAM Identity CenterをAzure Active Directoryに接続します。"
      },
      {
        "key": "D",
        "text": "Deploy an AWS Managed Microsoft AD directory in the management account. Share the directory with all other accounts in the organization",
        "text_jp": "管理アカウントにAWS Managed Microsoft ADディレクトリを展開し、組織内の他のすべてのアカウントとディレクトリを共有します。"
      },
      {
        "key": "E",
        "text": "Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center",
        "text_jp": "AWS IAM Identity Center（AWS Single Sign-On）で権限セットを作成し、適切なIAM Identity Centerユーザーに権限セットをアタッチします。"
      },
      {
        "key": "F",
        "text": "Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and",
        "text_jp": "各AWSアカウントでAWS Identity and Access Management（IAM）を設定し、認証にAWS Managed Microsoft ADを使用します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ACE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account and connect it to Azure Active Directory to enable identity federation.",
        "situation_analysis": "The company requires a centralized management and billing solution due to multiple AWS accounts serving different departments, with a need for identity federation and temporary credentials.",
        "option_analysis": "Option C is the most aligned with the requirements. Options A and D do not directly address identity federation and do not involve connecting Azure AD. Options E and F are additional actions but are not the primary steps needed to meet initial requirements.",
        "additional_knowledge": "Using identity federation helps simplify user management and enhance security through Temporary Credentials.",
        "key_terminology": "AWS IAM Identity Center, Azure Active Directory, identity federation, temporary credentials, AWS Organizations.",
        "overall_assessment": "The question effectively tests understanding of AWS identity solutions and account management. Option C is the most appropriate choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです: 管理アカウントにAWS IAM Identity Center（AWS Single Sign-On）を展開し、アイデンティティフェデレーションを有効にするためにAzure Active Directoryに接続します。",
        "situation_analysis": "企業は、異なる部署向けの複数のAWSアカウントがあるため、中央管理と請求のソリューションが必要です。また、アイデンティティフェデレーションと一時的な認証情報が必要とされています。",
        "option_analysis": "選択肢Cは要件に最も合致しています。選択肢AとDは、アイデンティティフェデレーションに直接関係なく、Azure ADとの接続もありません。選択肢EとFは追加の行動ですが、初期要件を満たすために必要な主要な手順ではありません。",
        "additional_knowledge": "アイデンティティフェデレーションを使用することで、ユーザー管理が簡素化され、一時的な認証情報によってセキュリティが強化されます。",
        "key_terminology": "AWS IAM Identity Center、Azure Active Directory、アイデンティティフェデレーション、一時的な認証情報、AWS Organizations。",
        "overall_assessment": "この質問は、AWSアイデンティティソリューションとアカウント管理の理解を効果的にテストしています。選択肢Cは最も適切な選択です。"
      }
    ],
    "keywords": [
      "AWS IAM Identity Center",
      "Azure Active Directory",
      "identity federation",
      "temporary credentials",
      "AWS Organizations"
    ]
  },
  {
    "No": "262",
    "question": "A company wants to manage the costs associated with a group of 20 applications that are infrequently used, but are still business-critical, by\nmigrating to AWS. The applications are a mix of Java and Node.js spread across different instance clusters. The company wants to minimize\ncosts while standardizing by using a single deployment methodology.\nMost of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other\ntimes. Average application memory consumption is less than 1 GB. though some applications use as much as 2.5 GB of memory during peak\nprocessing. The most important application in the group is a billing report written in Java that accesses multiple data sources and often runs for\nseveral hours.\nWhich is the MOST cost-effective solution?",
    "question_jp": "ある企業は、ビジネス上重要でありながら、稀にしか使用されない20のアプリケーション群に関連するコストを管理するために、AWSに移行したいと考えています。アプリケーションは、異なるインスタンスクラスターに分散したJavaとNode.jsの混合です。この企業は、コストを最小限に抑えつつ、単一のデプロイ方法論を使用して標準化したいと考えています。ほとんどのアプリケーションは月末処理ルーチンの一部で、同時に接続するユーザーは少数ですが、他の時間にも時折実行されます。アプリケーションの平均メモリ消費量は1GB未満ですが、一部のアプリケーションはピーク処理中に最大2.5GBのメモリを使用します。このグループで最も重要なアプリケーションは、複数のデータソースにアクセスし、数時間にわたり実行されることが多いJavaで書かれた請求書レポートです。最もコスト効果の高い解決策は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a separate AWS Lambda function for each application. Use AWS CloudTrail logs and Amazon CloudWatch alarms to verify",
        "text_jp": "各アプリケーションに対して別々のAWS Lambda関数をデプロイします。AWS CloudTrailログとAmazon CloudWatchアラームを使用して確認します"
      },
      {
        "key": "B",
        "text": "Deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. Deploy an ECS task for each",
        "text_jp": "Amazon ECSコンテナをAmazon EC2上にデプロイし、メモリ使用量の75%に設定されたオートスケーリングを構成します。各アプリケーション用にECSタスクをデプロイします"
      },
      {
        "key": "C",
        "text": "Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have suficient resources. Monitor each",
        "text_jp": "各アプリケーション用にAWS Elastic Beanstalkをデプロイし、オートスケーリングを使用して全てのリクエストに十分なリソースが確保されるようにします。"
      },
      {
        "key": "D",
        "text": "Deploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling and Application Load Balancers. Scale",
        "text_jp": "すべてのアプリケーションを共存させる新しいAmazon EC2インスタンスクラスターをデプロイし、EC2オートスケーリングとアプリケーションロードバランサーを使用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The most cost-effective solution is to deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. This allows for efficient resource management by adjusting the number of EC2 instances based on load, especially since the applications require varying memory at peak times.",
        "situation_analysis": "The company requires a solution that minimizes costs while supporting infrequently used but critical applications that can have variable memory requirements. The memory usage during peaks can go up to 2.5 GB.",
        "option_analysis": "Option B is preferable because it uses containerization, which is known for cost efficiency and provides scalability. Option A (AWS Lambda) may not be ideal due to potential cold start times for infrequently accessed applications. Option C (Elastic Beanstalk) may be over-provisioning resources and not the most cost-effective. Option D involves new EC2 instances but does not scale as efficiently as ECS.",
        "additional_knowledge": "ECS provides built-in service discovery and load balancing features, making it ideal for the application's architecture.",
        "key_terminology": "Amazon ECS, Amazon EC2, Auto Scaling, memory utilization, containerization",
        "overall_assessment": "Given the application characteristics, option B aligns with best practices for cost management and resource utilization."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "最もコスト効果の高い解決策は、Amazon ECSコンテナをAmazon EC2上にデプロイし、メモリ使用量を75%に設定したオートスケーリングを構成することです。これにより、負荷に応じてEC2インスタンスの数を調整することで効率的なリソース管理が可能になり、特にアプリケーションがピーク時に異なるメモリを必要とするために有効です。",
        "situation_analysis": "企業は、稀に使用されるが重要なアプリケーションに対し、コストを最小限に抑えつつ定期的にスケールさせる必要があります。ピーク時のメモリ使用量は最大2.5GBです。",
        "option_analysis": "選択肢Bが好まれます。なぜなら、コンテナ化を使用することで、コスト効率が良く、スケーラビリティを提供するからです。選択肢A（AWS Lambda）は、稀にアクセスされるアプリケーションに対してコールドスタートの問題が生じる可能性があるため、理想的ではありません。選択肢C（Elastic Beanstalk）は、オーバープロビジョニングになる可能性があり、最もコスト効果が高いとは言えません。選択肢Dは新しいEC2インスタンスを必要としますが、ECSほど効率的にスケールしません。",
        "additional_knowledge": "ECSは、サービス発見とロードバランシング機能を統合しているため、アプリケーションのアーキテクチャに理想的です。",
        "key_terminology": "Amazon ECS、Amazon EC2、オートスケーリング、メモリ使用量、コンテナ化",
        "overall_assessment": "アプリケーションの特性を考慮すると、選択肢Bは、コスト管理とリソース利用に関するベストプラクティスに合致しています。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "Amazon EC2",
      "Auto Scaling",
      "memory utilization",
      "containerization"
    ]
  },
  {
    "No": "263",
    "question": "A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs\ntasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, primary, and core\nnodes. The EMR tasks run each morning, starting at 1:00 AM. and take 6 hours to finish running. The amount of time to complete the processing is\nnot a priority because the data is not referenced until late in the day.\nThe solutions architect must review the architecture and suggest a solution to minimize the compute costs.\nWhich solution should the solutions architect recommend to meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、EMRファイルシステム（EMRFS）を使用しているAmazon EMRクラスターの設計をレビューする必要があります。このクラスターは、ビジネスニーズにとって重要なタスクを実行しています。クラスターは常にすべてのタスク、プライマリ、およびコアノードに対してAmazon EC2オンデマンドインスタンスを実行しています。EMRタスクは毎朝午前1時に開始され、実行に6時間かかります。処理を完了するのにかかる時間は優先事項ではありません。なぜなら、データは日中遅くまで参照されないからです。ソリューションアーキテクトはアーキテクチャをレビューし、コンピュートコストを最小限に抑えるための解決策を提案する必要があります。ソリューションアーキテクトはどの解決策を推奨するべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Launch all task, primary, and core nodes on Spot Instances in an instance fieet. Terminate the cluster, including all instances, when the",
        "text_jp": "すべてのタスク、プライマリ、およびコアノードをスポットインスタンスのインスタンスフリートで起動し、"
      },
      {
        "key": "B",
        "text": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fieet. Terminate the",
        "text_jp": "プライマリおよびコアノードをオンデマンドインスタンスで起動し、タスクノードをスポットインスタンスのインスタンスフリートで起動し、"
      },
      {
        "key": "C",
        "text": "Continue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed.",
        "text_jp": "すべてのノードをオンデマンドインスタンスで起動し、処理が完了したときにクラスターを終了する。"
      },
      {
        "key": "D",
        "text": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fieet. Terminate only",
        "text_jp": "プライマリおよびコアノードをオンデマンドインスタンスで起動し、タスクノードをスポットインスタンスのインスタンスフリートで起動し、終了する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (50%) D (50%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Continuing to launch all nodes on On-Demand Instances minimizes cost management without compromising performance.",
        "situation_analysis": "The cluster runs tasks critical to business needs every morning, and completion time is not a priority. Therefore, using On-Demand Instances provides the necessary resources without additional cost concerns.",
        "option_analysis": "Option A and B suggest using Spot Instances, which can reduce costs significantly during non-peak hours, but may lead to interruptions if instances are terminated unexpectedly. Option D does not provide a cost-effective approach since it mixes On-Demand and Spot instances with no clear benefit.",
        "additional_knowledge": "It’s essential to analyze when to utilize On-Demand versus Spot based on workload consistency.",
        "key_terminology": "On-Demand Instances, Spot Instances, Amazon EMR, EMRFS, cost management",
        "overall_assessment": "While the community vote was split between options B and D, option C aligns best with the requirement of minimizing compute costs without introducing risk to performance or task interruptions."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。すべてのノードをオンデマンドインスタンスで起動し続けることで、パフォーマンスを損なうことなくコスト管理を最小限に抑えることができる。",
        "situation_analysis": "クラスターは毎朝ビジネスニーズに重要なタスクを実行しており、完了時間は優先事項ではない。そのため、オンデマンドインスタンスを使用することで、追加のコスト懸念なしに必要なリソースを提供する。",
        "option_analysis": "選択肢AおよびBはスポットインスタンスの使用を提案しており、オフピーク時にコストを大幅に削減できるが、インスタンスが予期せず終了されると中断のリスクがある。選択肢Dは、オンデマンドとスポットインスタンスを混合しており明確な利益がないため、コスト効果の高いアプローチを提供していない。",
        "additional_knowledge": "ワークロードの一貫性に基づいてオンデマンドインスタンスとスポットインスタンスを使用するタイミングを分析することが重要である。",
        "key_terminology": "オンデマンドインスタンス、スポットインスタンス、Amazon EMR、EMRFS、コスト管理",
        "overall_assessment": "コミュニティの投票はBとDに分かれていたが、オプションCがコストを最小限に抑える要件と最も整合しておりパフォーマンスやタスクの中断リスクを引き起こさない。"
      }
    ],
    "keywords": [
      "On-Demand Instances",
      "Spot Instances",
      "Amazon EMR",
      "EMRFS",
      "cost management"
    ]
  },
  {
    "No": "264",
    "question": "A company has migrated a legacy application to the AWS Cloud. The application runs on three Amazon EC2 instances that are spread across three\nAvailability Zones. One EC2 instance is in each Availability Zone. The EC2 instances are running in three private subnets of the VPC and are set up\nas targets for an Application Load Balancer (ALB) that is associated with three public subnets.\nThe application needs to communicate with on-premises systems. Only trafic from IP addresses in the company's IP address range are allowed to\naccess the on-premises systems. The company's security team is bringing only one IP address from its internal IP address range to the cloud. The\ncompany has added this IP address to the allow list for the company firewall. The company also has created an Elastic IP address for this IP\naddress.\nA solutions architect needs to create a solution that gives the application the ability to communicate with the on-premises systems. The solution\nalso must be able to mitigate failures automatically.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がレガシーアプリケーションをAWSクラウドに移行しました。このアプリケーションは、3つのAmazon EC2インスタンスで動作しており、それぞれのインスタンスは異なる3つのアベイラビリティゾーンに分散しています。各アベイラビリティゾーンに1つのEC2インスタンスがあります。EC2インスタンスはVPCの3つのプライベートサブネットで実行されており、3つのパブリックサブネットに関連付けられたアプリケーションロードバランサー（ALB）のターゲットとして設定されています。アプリケーションはオンプレミスシステムと通信する必要があります。オンプレミスシステムにアクセスできるのは、社内のIPアドレス範囲にあるIPアドレスからのトラフィックのみです。企業のセキュリティチームは、社内IPアドレス範囲から1つのIPアドレスのみをクラウドに持ち込むことにしています。このIPアドレスは、企業のファイアウォールの許可リストに追加されました。企業はこのIPアドレスに対してElastic IPアドレスも作成しました。ソリューションアーキテクトは、アプリケーションがオンプレミスシステムと通信できるようにするソリューションを作成する必要があります。このソリューションは、故障を自動的に緩和できる必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy three NAT gateways, one in each public subnet. Assign the Elastic IP address to the NAT gateways. Turn on health checks for the",
        "text_jp": "3つのNATゲートウェイをデプロイし、各パブリックサブネットに1つずつ設置します。Elastic IPアドレスをNATゲートウェイに割り当てます。ヘルスチェックを有効にします。"
      },
      {
        "key": "B",
        "text": "Replace the ALB with a Network Load Balancer (NLB). Assign the Elastic IP address to the NLTurn on health checks for the NLIn the case of",
        "text_jp": "ALBをネットワークロードバランサー（NLB）に置き換えます。Elastic IPアドレスをNLBに割り当てます。NLBのヘルスチェックを有効にし、故障時には自動で検出します。"
      },
      {
        "key": "C",
        "text": "Deploy a single NAT gateway in a public subnet. Assign the Elastic IP address to the NAT gateway. Use Amazon CloudWatch with a custom",
        "text_jp": "パブリックサブネットに単一のNATゲートウェイをデプロイします。Elastic IPアドレスをNATゲートウェイに割り当てます。Amazon CloudWatchを使用してカスタム監視を行います。"
      },
      {
        "key": "D",
        "text": "Assign the Elastic IP address to the ALB. Create an Amazon Route 53 simple record with the Elastic IP address as the value. Create a Route",
        "text_jp": "Elastic IPアドレスをALBに割り当てます。Route 53のシンプルレコードを作成し、Elastic IPアドレスを値として設定します。Route 53でのルーティングを構成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Deploying three NAT gateways provides high availability and failover capabilities.",
        "situation_analysis": "The application requires communication with on-premises systems and must operate within defined IP restrictions.",
        "option_analysis": "Option A offers a robust solution with individual NAT gateways per Availability Zone, allowing for automatic failover. Options B and D don't provide the same level of redundancy, while Option C sacrifices availability by using a single NAT gateway.",
        "additional_knowledge": "It is important to consider the architecture's resilience to ensure reliable operations.",
        "key_terminology": "NAT Gateway, Elastic IP, High Availability, Automatic Failover, VPC, ALB",
        "overall_assessment": "Option A stands out as the best solution due to its resilience and ability to maintain connectivity even in case of a failure of one of the NAT gateways. Community support for option C indicates a misunderstanding, as it does not provide the necessary redundancy."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。3つのNATゲートウェイをデプロイすることで、高可用性とフェイルオーバー機能を提供します。",
        "situation_analysis": "アプリケーションはオンプレミスシステムとの通信が必要であり、定義されたIP制限内で動作する必要があります。",
        "option_analysis": "オプションAは各アベイラビリティゾーンに個別のNATゲートウェイを持つことにより、自動フェイルオーバーを可能にする堅牢なソリューションを提供します。オプションBおよびDは同等の冗長性を提供せず、オプションCは単一のNATゲートウェイを使用するため可用性を犠牲にします。",
        "additional_knowledge": "アーキテクチャの回復力を考慮し、信頼性のある運用を確保することが重要です。",
        "key_terminology": "NATゲートウェイ、Elastic IP、高可用性、自動フェイルオーバー、VPC、ALB",
        "overall_assessment": "オプションAはその耐障害性および接続性の維持能力から最良の解決策として際立っており、コミュニティのCの支持は誤解を示しています。Cは必要な冗長性を提供しません。"
      }
    ],
    "keywords": [
      "NAT Gateway",
      "Elastic IP",
      "High Availability",
      "Automatic Failover",
      "VPC",
      "ALB"
    ]
  },
  {
    "No": "265",
    "question": "A company uses AWS Organizations to manage more than 1,000 AWS accounts. The company has created a new developer organization. There\nare 540 developer member accounts that must be moved to the new developer organization. All accounts are set up with all the required\ninformation so that each account can be operated as a standalone account.\nWhich combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Choose\nthree.)",
    "question_jp": "ある会社がAWS Organizationsを使用して1,000以上のAWSアカウントを管理しています。会社は新しい開発者組織を作成しました。540の開発者メンバーアカウントを新しい開発者組織に移動させる必要があります。すべてのアカウントは独立したアカウントとして運用できるように、必要なすべての情報が設定されています。ソリューションアーキテクトがすべての開発者アカウントを新しい開発者組織に移動させるためにどのステップの組み合わせを取るべきですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Call the MoveAccount operation in the Organizations API from the old organization's management account to migrate the developer",
        "text_jp": "古い組織の管理アカウントからOrganizations APIのMoveAccount操作を呼び出して開発者アカウントを移行します"
      },
      {
        "key": "B",
        "text": "From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization",
        "text_jp": "管理アカウントから古い組織から各開発者アカウントをRemoveAccountFromOrganizationを使用して削除します"
      },
      {
        "key": "C",
        "text": "From each developer account, remove the account from the old organization using the RemoveAccountFromOrganization operation in the",
        "text_jp": "各開発者アカウントから、RemoveAccountFromOrganization操作を使用して古い組織からアカウントを削除します"
      },
      {
        "key": "D",
        "text": "Sign in to the new developer organization's management account and create a placeholder member account that acts as a target for the",
        "text_jp": "新しい開発者組織の管理アカウントにサインインし、ターゲットとなるプレースホルダーのメンバーアカウントを作成します"
      },
      {
        "key": "E",
        "text": "Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to",
        "text_jp": "新しい開発者組織の管理アカウントからOrganizations APIのInviteAccountToOrganization操作を呼び出します"
      },
      {
        "key": "F",
        "text": "Have each developer sign in to their account and confirm to join the new developer organization.",
        "text_jp": "各開発者が自分のアカウントにサインインし、新しい開発者組織に参加することを確認します"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "BEF (81%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is option C. To move accounts between organizations, each developer account must individually remove itself from the old organization.",
        "situation_analysis": "The company has 540 member accounts that need to be moved to a new organization. Each account is self-sufficient, meaning they can operate independently.",
        "option_analysis": "Option C is correct because it requires each developer account to actively remove itself from the old organization. Options A, B, D, E, and F do not directly achieve moving the accounts as they involve management account operations or unnecessary steps.",
        "additional_knowledge": "To ensure security and compliance, member accounts should have their permissions and policies reviewed to avoid any potential disruptions during the migration.",
        "key_terminology": "AWS Organizations, MoveAccount, RemoveAccountFromOrganization, InviteAccountToOrganization, management account",
        "overall_assessment": "The question tests the understanding of AWS Organizations account management. Option C directly addresses the requirement while other listed options do not."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は選択肢Cである。他の組織間でアカウントを移動させるには、各開発者アカウントが古い組織から自らを削除する必要がある。",
        "situation_analysis": "この会社は540のメンバーアカウントを新しい組織に移動させる必要がある。各アカウントは独立して運用できるため、自己完結性がある。",
        "option_analysis": "選択肢Cが正しいのは、各開発者アカウントが古い組織から自ら削除することを必要としているためである。選択肢A、B、D、E、Fは、管理アカウントの操作や不必要なステップを含んでおり、アカウントを移動させることにつながらない。",
        "additional_knowledge": "セキュリティとコンプライアンスを確保するため、メンバーアカウントの権限やポリシーを確認し、移行中の潜在的な中断を避けることが重要である。",
        "key_terminology": "AWS Organizations, MoveAccount, RemoveAccountFromOrganization, InviteAccountToOrganization, 管理アカウント",
        "overall_assessment": "この質問はAWS Organizationsのアカウント管理についての理解を試すものである。選択肢Cは要求に直接対応しており、他の選択肢はそうではない。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "RemoveAccountFromOrganization",
      "InviteAccountToOrganization",
      "management account"
    ]
  },
  {
    "No": "266",
    "question": "A company's interactive web application uses an Amazon CloudFront distribution to serve images from an Amazon S3 bucket. Occasionally, third-\nparty tools ingest corrupted images into the S3 bucket. This image corruption causes a poor user experience in the application later. The company\nhas successfully implemented and tested Python logic to detect corrupt images.\nA solutions architect must recommend a solution to integrate the detection logic with minimal latency between the ingestion and serving.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業のインタラクティブなWebアプリケーションは、Amazon S3バケットから画像を配信するためにAmazon CloudFrontディストリビューションを使用しています。時折、第三者のツールがS3バケットに破損した画像を取り込んでしまいます。この画像の破損は、後にアプリケーションでのユーザー体験を損なう原因となります。企業は、破損した画像を検出するためのPythonロジックを正常に実装し、テストしました。ソリューションアーキテクトは、取り込みと配信の間の遅延を最小限に抑えて、検出ロジックを統合するためのソリューションを推奨する必要があります。どのソリューションがこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use a Lambda@Edge function that is invoked by a viewer-response event.",
        "text_jp": "ビューワーレスポンスイベントによって起動されるLambda@Edge関数を使用する。"
      },
      {
        "key": "B",
        "text": "Use a Lambda@Edge function that is invoked by an origin-response event.",
        "text_jp": "オリジンレスポンスイベントによって起動されるLambda@Edge関数を使用する。"
      },
      {
        "key": "C",
        "text": "Use an S3 event notification that invokes an AWS Lambda function.",
        "text_jp": "AWS Lambda関数を起動するS3イベント通知を使用する。"
      },
      {
        "key": "D",
        "text": "Use an S3 event notification that invokes an AWS Step Functions state machine.",
        "text_jp": "AWS Step Functionsステートマシンを起動するS3イベント通知を使用する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Use a Lambda@Edge function that is invoked by an origin-response event. This method minimizes latency effectively by allowing the image integrity check to happen as soon as the content is fetched from the origin, thus ensuring only good images are served to the end-users.",
        "situation_analysis": "The company requires real-time detection of corrupt images in an S3 bucket that serves content through Amazon CloudFront, focusing on minimal latency between image ingestion and serving.",
        "option_analysis": "Option B is the best solution because it allows the integrity check to be performed directly in the origin-response phase, ensuring that corrupt images are not served. Option A would not be effective as it occurs too late in the request-response cycle. Options C and D do involve S3 events but introduce additional latency since they are triggered after the images are already in S3.",
        "additional_knowledge": "Using Lambda@Edge effectively requires examples of its invocation settings and performance monitoring to ensure real-time data processing.",
        "key_terminology": "Amazon CloudFront, Amazon S3, Lambda@Edge, origin-response event, event-driven architecture.",
        "overall_assessment": "Option B not only adheres to best practices regarding latency but also aligns with efficient content delivery mechanisms provided by AWS, making it the most suitable choice given the requirements. Community voting was overwhelmingly in favor of option C, which highlights a misunderstanding as that option does not provide the best results in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はB：オリジンレスポンスイベントによって起動されるLambda@Edge関数を使用することです。この方法は、コンテンツがオリジンから取得されるとすぐに画像の整合性チェックを行うことができるため、遅延を最小限に抑え、ユーザーに正常な画像だけが配信されることを保証します。",
        "situation_analysis": "企業は、Amazon CloudFrontを介してコンテンツを提供するS3バケット内の破損した画像をリアルタイムで検出する必要があり、画像の取り込みと配信の間の遅延を最小化することに焦点を当てています。",
        "option_analysis": "選択肢Bは、オリジンレスポンス段階で整合性チェックを実行するため、破損した画像が配信されないようにするための最良のソリューションです。選択肢Aはリクエスト-レスポンスサイクルが遅すぎるため効果的ではありません。選択肢CとDはS3イベントを含みますが、すでに画像がS3にある際にトリガーされるため、追加の遅延が発生します。",
        "additional_knowledge": "Lambda@Edgeを効果的に使用するには、その呼び出し設定やリアルタイムデータ処理を確保するためのパフォーマンスモニタリングの例が必要です。",
        "key_terminology": "Amazon CloudFront, Amazon S3, Lambda@Edge, オリジンレスポンスイベント, イベント駆動型アーキテクチャ。",
        "overall_assessment": "選択肢Bは、遅延に関するベストプラクティスに従い、AWSによって提供される効率的なコンテンツ配信メカニズムと整合するため、要件を考慮した場合最も適した選択となります。コミュニティの投票は圧倒的に選択肢Cに支持されていましたが、それはこのシナリオにおいて最良の結果を提供しないという誤解を強調しています。"
      }
    ],
    "keywords": [
      "Amazon CloudFront",
      "Amazon S3",
      "Lambda@Edge",
      "origin-response event",
      "event-driven architecture"
    ]
  },
  {
    "No": "267",
    "question": "A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group. The company uses AWS CodePipeline to\ndeploy the application. The instances that run in the Auto Scaling group are constantly changing because of scaling events.\nWhen the company deploys new application code versions, the company installs the AWS CodeDeploy agent on any new target EC2 instances and\nassociates the instances with the CodeDeploy deployment group. The application is set to go live within the next 24 hours.\nWhat should a solutions architect recommend to automate the application deployment process with the LEAST amount of operational overhead?",
    "question_jp": "ある企業がAmazon EC2インスタンス上で実行されるアプリケーションを持っています。アプリケーションはAmazon EC2 Auto Scalingグループ内で実行されており、企業はAWS CodePipelineを使用してアプリケーションをデプロイしています。Auto Scalingグループで実行されるインスタンスは、スケーリングイベントによって常に変化しています。企業が新しいアプリケーションコードバージョンをデプロイするとき、企業は新しいターゲットEC2インスタンスにAWS CodeDeployエージェントをインストールし、そのインスタンスをCodeDeployデプロイメントグループに関連付けます。アプリケーションは次の24時間以内に公開される予定です。ソリューションアーキテクトは、運用のオーバーヘッドを最小限に抑えてアプリケーションデプロイメントプロセスを自動化するために何を推奨すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure Amazon EventBridge to invoke an AWS Lambda function when a new EC2 instance is launched into the Auto Scaling group. Code",
        "text_jp": "新しいEC2インスタンスがAuto Scalingグループに起動されるときにAWS Lambda関数を呼び出すようにAmazon EventBridgeを構成します。"
      },
      {
        "key": "B",
        "text": "Write a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new code. When the deployment is complete,",
        "text_jp": "新しいコードのデプロイ前にAmazon EC2 Auto Scaling操作を一時停止するスクリプトを書く。デプロイが完了したら、"
      },
      {
        "key": "C",
        "text": "Create a new AWS CodeBuild project that creates a new AMI that contains the new code. Configure CodeBuild to update the Auto Scaling",
        "text_jp": "新しいコードを含む新しいAMIを作成するAWS CodeBuildプロジェクトを作成します。CodeBuildを構成してAuto Scalingを更新する。"
      },
      {
        "key": "D",
        "text": "Create a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group's launch template to use the new AMI.",
        "text_jp": "CodeDeployエージェントがインストールされた新しいAMIを作成します。Auto Scalingグループの起動テンプレートを新しいAMIを使用するように構成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Creating a new AMI with the CodeDeploy agent installed allows the Auto Scaling group to automatically provision new instances with the necessary software for deployments.",
        "situation_analysis": "The company needs to ensure that any new EC2 instances launched in the Auto Scaling group have the CodeDeploy agent installed, minimizing manual intervention during deployments.",
        "option_analysis": "Option A involves setting up an EventBridge rule, which adds complexity. Option B requires manual control and could lead to downtime. Option C also adds complexity as it involves creating a new build process. Option D is the simplest and most effective solution.",
        "additional_knowledge": "A well-constructed AMI simplifies the deployment process significantly. By ensuring that the AMI is updated regularly, the operational burden during deployments is greatly reduced.",
        "key_terminology": "AMI, CodeDeploy, Auto Scaling, deployment automation, AWS Lambda",
        "overall_assessment": "The quality of the question is high as it addresses a common scenario involving deployment in an Auto Scaling environment. The community strongly supports option D, which aligns with AWS best practices for minimizing operational overhead."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。CodeDeployエージェントがインストールされた新しいAMIを作成すると、Auto Scalingグループはデプロイメントに必要なソフトウェアを備えた新しいインスタンスを自動的にプロビジョニングできます。",
        "situation_analysis": "企業はAuto Scalingグループ内で起動される新しいEC2インスタンスにCodeDeployエージェントがインストールされることを確実にする必要があり、デプロイ中の手動介入を最小限に抑えることが求められています。",
        "option_analysis": "選択肢AはEventBridgeルールの設定を伴うため、複雑さが増します。選択肢Bは手動コントロールが必要で、ダウンタイムにつながる可能性があります。選択肢Cも新しいビルドプロセスの作成を必要とするため、複雑さが増します。選択肢Dは最もシンプルで効果的な解決策です。",
        "additional_knowledge": "良く構成されたAMIは、デプロイメントプロセスを大幅に簡素化します。AMIを定期的に更新することにより、デプロイメント中の運用負担が著しく軽減されます。",
        "key_terminology": "AMI、CodeDeploy、Auto Scaling、デプロイ自動化、AWS Lambda",
        "overall_assessment": "この質問の品質は高く、Auto Scaling環境でのデプロイに関する一般的なシナリオを扱っています。コミュニティは選択肢Dを強く支持しており、運用オーバーヘッドを最小化するためのAWSのベストプラクティスに一致します。"
      }
    ],
    "keywords": [
      "AMI",
      "CodeDeploy",
      "Auto Scaling",
      "deployment automation",
      "AWS Lambda"
    ]
  },
  {
    "No": "268",
    "question": "A company has a website that runs on four Amazon EC2 instances that are behind an Application Load Balancer (ALB). When the ALB detects that\nan EC2 instance is no longer available, an Amazon CloudWatch alarm enters the ALARM state. A member of the company's operations team then\nmanually adds a new EC2 instance behind the ALB.\nA solutions architect needs to design a highly available solution that automatically handles the replacement of EC2 instances. The company\nneeds to minimize downtime during the switch to the new solution.\nWhich set of steps should the solutions architect take to meet these requirements?",
    "question_jp": "ある企業が、Application Load Balancer (ALB) の背後で動作する4つのAmazon EC2インスタンスを持つウェブサイトを運営しています。ALBがEC2インスタンスが利用できなくなったことを検出すると、Amazon CloudWatchアラームがALARM状態に入ります。その企業の運用チームのメンバーは手動でALBの背後に新しいEC2インスタンスを追加します。ソリューションアーキテクトは、EC2インスタンスの交換を自動的に処理する高可用性ソリューションを設計する必要があります。企業は新しいソリューションへの切り替え中のダウンタイムを最小限に抑える必要があります。この要件を満たすために、ソリューションアーキテクトはどの手順を踏むべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Delete the existing ALB. Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template",
        "text_jp": "既存のALBを削除します。ウェブアプリケーショントラフィックを処理するように設定されたAuto Scalingグループを作成します。新しい起動テンプレートを添付します。"
      },
      {
        "key": "B",
        "text": "Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template to the Auto Scaling",
        "text_jp": "ウェブアプリケーショントラフィックを処理するように設定されたAuto Scalingグループを作成します。新しい起動テンプレートをAuto Scalingに添付します。"
      },
      {
        "key": "C",
        "text": "Delete the existing ALB and the EC2 instances. Create an Auto Scaling group that is configured to handle the web application trafic. Attach",
        "text_jp": "既存のALBとEC2インスタンスを削除します。ウェブアプリケーショントラフィックを処理するように設定されたAuto Scalingグループを作成します。"
      },
      {
        "key": "D",
        "text": "Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template to the Auto Scaling",
        "text_jp": "ウェブアプリケーショントラフィックを処理するように設定されたAuto Scalingグループを作成します。新しい起動テンプレートをAuto Scalingに添付します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. By deleting the existing ALB and EC2 instances, and creating a new Auto Scaling group that is responsible for managing the web application traffic, the solution will be highly available and automatically replace EC2 instances as needed.",
        "situation_analysis": "The company needs a solution that can automatically replace unhealthy EC2 instances to ensure minimal downtime.",
        "option_analysis": "Option C meets that requirement by leveraging Auto Scaling which automatically provisions EC2 instances. Option A and B do not include the necessary steps to replace the ALB, while option D does not specify removing the existing resources.",
        "additional_knowledge": "An Elastic Load Balancer (ELB) is typically used in conjunction with Auto Scaling groups to manage incoming traffic effectively.",
        "key_terminology": "Auto Scaling, Application Load Balancer, EC2 instance, CloudWatch, high availability",
        "overall_assessment": "Option C is the best approach because it ensures that both the load balancer and EC2 instances are replaced, establishing a robust, fault-tolerant architecture. Community votes seem to favor option B, which indicates some misunderstanding of the requirement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。既存のALBとEC2インスタンスを削除し、ウェブアプリケーショントラフィックを管理する新しいAuto Scalingグループを作成することで、高可用性のソリューションが実現し、必要に応じてEC2インスタンスが自動的に交換される。",
        "situation_analysis": "企業は、ダウンタイムを最小限に抑えるために、非正常のEC2インスタンスを自動的に交換できるソリューションを必要としている。",
        "option_analysis": "選択肢Cは、Auto Scalingを活用することによって、その要件を満たしている。選択肢AおよびBは、ALBを交換するために必要なステップを含んでいない一方で、選択肢Dは既存のリソースを削除することを明示していない。",
        "additional_knowledge": "Elastic Load Balancer (ELB)は、Auto Scalingグループと共に使用されることが一般的で、到着トラフィックを効果的に管理する。",
        "key_terminology": "Auto Scaling, Application Load Balancer, EC2インスタンス, CloudWatch, 高可用性",
        "overall_assessment": "選択肢Cは、ALBとEC2インスタンスの両方を交換することを保障し、堅牢でフォールトトレラントなアーキテクチャを確立するため、最も適切なアプローチである。コミュニティの投票は選択肢Bを支持しており、要件の誤解を示している。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "Application Load Balancer",
      "EC2 instance",
      "CloudWatch",
      "high availability"
    ]
  },
  {
    "No": "269",
    "question": "A company wants to optimize AWS data-transfer costs and compute costs across developer accounts within the company's organization in AWS\nOrganizations. Developers can configure VPCs and launch Amazon EC2 instances in a single AWS Region. The EC2 instances retrieve\napproximately 1 TB of data each day from Amazon S3.\nThe developer activity leads to excessive monthly data-transfer charges and NAT gateway processing charges between EC2 instances and S3\nbuckets, along with high compute costs. The company wants to proactively enforce approved architectural patterns for any EC2 instance and VPC\ninfrastructure that developers deploy within the AWS accounts. The company does not want this enforcement to negatively affect the speed at\nwhich the developers can perform their tasks.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業は、AWS Organizations内の開発者アカウント全体でAWSのデータ転送コストおよび計算コストを最適化したいと考えています。開発者は、単一のAWSリージョンでVPCを構成し、Amazon EC2インスタンスを起動できます。EC2インスタンスは、Amazon S3から毎日約1TBのデータを取得しています。この開発者の活動は、EC2インスタンスとS3バケット間の過剰な月間データ転送料金およびNATゲートウェイ処理料金に加えて、高い計算コストにつながっています。この企業は、開発者がAWSアカウント内でデプロイするすべてのEC2インスタンスとVPCインフラストラクチャに承認されたアーキテクチャパターンを積極的に施行したいと考えていますが、この施行が開発者のタスクのスピードに悪影響を与えないことを望んでいます。コスト効率の高い方法で、これらの要件を最も満たす解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create SCPs to prevent developers from launching unapproved EC2 instance types. Provide the developers with an AWS CloudFormation",
        "text_jp": "開発者が承認されていないEC2インスタンスタイプを起動するのを防ぐためにSCPを作成します。開発者にはAWS CloudFormationを提供します"
      },
      {
        "key": "B",
        "text": "Create a daily forecasted budget with AWS Budgets to monitor EC2 compute costs and S3 data-transfer costs across the developer",
        "text_jp": "AWS Budgetsを使用して、開発者アカウント全体のEC2計算コストおよびS3データ転送コストを監視するための予測された日次予算を作成します"
      },
      {
        "key": "C",
        "text": "Create an AWS Service Catalog portfolio that users can use to create an approved VPC configuration with S3 gateway endpoints and",
        "text_jp": "ユーザーがS3ゲートウェイエンドポイントを持つ承認されたVPC構成を作成できるAWS Service Catalogポートフォリオを作成します"
      },
      {
        "key": "D",
        "text": "Create and deploy AWS Config rules to monitor the compliance of EC2 and VPC resources in the developer AWS accounts. If developers",
        "text_jp": "EC2およびVPCリソースの準拠を監視するためにAWS Configルールを作成し、デプロイします。開発者が..."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating an AWS Service Catalog portfolio allows users to launch approved VPC configurations efficiently, which incorporates S3 gateway endpoints and can reduce data transfer costs.",
        "situation_analysis": "The company is facing excessive data transfer and compute costs. Establishing a framework for approved architectures can streamline the development process while managing costs.",
        "option_analysis": "Option A restricts EC2 types but does not address architecture patterns. Option B only monitors expenses without changing architecture. Option D monitors compliance but may lead to slower development due to compliance checks.",
        "additional_knowledge": "The use of S3 gateway endpoints can lead to significant cost savings as it removes the need for NAT usage for S3 access.",
        "key_terminology": "AWS Service Catalog, VPC, S3 gateway endpoints, data transfer costs, architecture governance.",
        "overall_assessment": "Considering community support, option C directly aligns with both the cost reduction target and the requirement of proactive architecture enforcement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。AWS Service Catalogポートフォリオを作成することで、ユーザーは効率的に承認されたVPC構成を起動でき、これにはS3ゲートウェイエンドポイントが含まれており、データ転送コストを削減できます。",
        "situation_analysis": "企業は過剰なデータ転送および計算コストに直面しています。承認されたアーキテクチャの枠組みを確立することで、コストを管理しながら開発プロセスを合理化できます。",
        "option_analysis": "選択肢AはEC2タイプを制限しますが、アーキテクチャパターンには対応していません。選択肢Bは構成を変更せずに費用を監視するだけです。選択肢Dは準拠を監視しますが、準拠チェックのために開発が遅れる可能性があります。",
        "additional_knowledge": "S3ゲートウェイエンドポイントの利用は、S3アクセスのためのNAT使用を不要にすることで、かなりのコスト削減につながります。",
        "key_terminology": "AWS Service Catalog、VPC、S3ゲートウェイエンドポイント、データ転送コスト、アーキテクチャガバナンス。",
        "overall_assessment": "コミュニティの支持を考慮すると、選択肢Cはコスト削減目標とプロアクティブなアーキテクチャ施行の要件の両方に直接整合しています。"
      }
    ],
    "keywords": [
      "AWS Service Catalog",
      "VPC",
      "S3 gateway endpoint",
      "data transfer costs",
      "architecture governance"
    ]
  },
  {
    "No": "270",
    "question": "A company is expanding. The company plans to separate its resources into hundreds of different AWS accounts in multiple AWS Regions. A\nsolutions architect must recommend a solution that denies access to any operations outside of specifically designated Regions.\nWhich solution will meet these requirements?",
    "question_jp": "企業が拡大しています。この企業はリソースを数百の異なるAWSアカウントに分離し、複数のAWSリージョンにまたがる計画を立てています。ソリューションアーキテクトは、特定の指定されたリージョン以外の操作へのアクセスを拒否する解決策を推奨する必要があります。どの解決策がこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create IAM roles for each account. Create IAM policies with conditional allow permissions that include only approved Regions for the",
        "text_jp": "各アカウントに対してIAMロールを作成します。承認されたリージョンのみを含む条件付き許可ポリシーを作成します。"
      },
      {
        "key": "B",
        "text": "Create an organization in AWS Organizations. Create IAM users for each account. Attach a policy to each user to block access to Regions",
        "text_jp": "AWS Organizationsに組織を作成します。各アカウントに対してIAMユーザーを作成します。各ユーザーに対してリージョンへのアクセスをブロックするポリシーを添付します。"
      },
      {
        "key": "C",
        "text": "Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions.",
        "text_jp": "AWS Control Towerランディングゾーンを立ち上げます。OUを作成し、承認されたリージョンの外でのサービスの実行を拒否するSCPを添付します。"
      },
      {
        "key": "D",
        "text": "Enable AWS Security Hub in each account. Create controls to specify the Regions where an account can deploy infrastructure.",
        "text_jp": "各アカウントでAWS Security Hubを有効にします。アカウントがインフラストラクチャを展開できるリージョンを指定するためのコントロールを作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "正解はBです。本ソリューションはAWS Organizationsを利用しており、これによって複数のアカウントを管理し、特定のリージョンに対するアクセスを制御できます。",
        "situation_analysis": "企業がリソースを分離し、指定されたリージョンの外で操作を行うことを禁止したいという明確な要件があります。",
        "option_analysis": "AはIAMロールを使用するものの、ユーザーのアクセス管理を独立して行うため、管理が複雑になる可能性があります。CはAWS Control Towerに頼ったアプローチですが、全体的なアプローチはAWS Organizationsよりも柔軟性が劣る可能性があります。DはAWS Security Hubを使用しますが、これに依存してリージョンを制御するのは不十分です。",
        "additional_knowledge": "Bの選択を支持する根拠には、AWSリソースを適切に分離し管理する重要性が含まれており、遵守性を高めることができる点も挙げられます。",
        "key_terminology": "AWS Organizations, IAM, Service Control Policies, Regions, Access Management",
        "overall_assessment": "Bは、組織全体の管理において強力で柔軟な解決策を提供し、要件を満たします。コミュニティ投票ではCが100%という結果ですが、それは多くの人がSCPの使用を推奨したからであり、正規の答案が強い理由を提供しています。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。本ソリューションはAWS Organizationsを利用しており、これによって複数のアカウントを管理し、特定のリージョンに対するアクセスを制御できます。",
        "situation_analysis": "企業がリソースを分離し、指定されたリージョンの外で操作を行うことを禁止したいという明確な要件があります。",
        "option_analysis": "AはIAMロールを使用するものの、ユーザーのアクセス管理を独立して行うため、管理が複雑になる可能性があります。CはAWS Control Towerに頼ったアプローチですが、全体的なアプローチはAWS Organizationsよりも柔軟性が劣る可能性があります。DはAWS Security Hubを使用しますが、これに依存してリージョンを制御するのは不十分です。",
        "additional_knowledge": "Bの選択を支持する根拠には、AWSリソースを適切に分離し管理する重要性が含まれており、遵守性を高めることができる点も挙げられます。",
        "key_terminology": "AWS Organizations, IAM, Service Control Policies, Regions, Access Management",
        "overall_assessment": "Bは、組織全体の管理において強力で柔軟な解決策を提供し、要件を満たします。コミュニティ投票ではCが100%という結果ですが、それは多くの人がSCPの使用を推奨したからであり、正規の答案が強い理由を提供しています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "IAM",
      "Service Control Policies",
      "Regions",
      "Access Management"
    ]
  },
  {
    "No": "271",
    "question": "A company wants to refactor its retail ordering web application that currently has a load-balanced Amazon EC2 instance fieet for web hosting,\ndatabase API services, and business logic. The company needs to create a decoupled, scalable architecture with a mechanism for retaining failed\norders while also minimizing operational costs.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、現在負荷分散されたAmazon EC2インスタンスのフィートを使用している小売注文ウェブアプリケーションをリファクタリングしたいと考えています。データベースAPIサービスとビジネスロジックを含んでいます。この企業は、失敗した注文を保持するメカニズムを備えた、分離されたスケーラブルなアーキテクチャを作成する必要があり、運用コストを最小限に抑える必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon S3 for web hosting with Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS)",
        "text_jp": "Amazon S3をウェブホスティングに使用し、データベースAPIサービスにAmazon API Gatewayを使用します。Amazon Simple Queue Service (Amazon SQS)を使用します"
      },
      {
        "key": "B",
        "text": "Use AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API services. Use Amazon MQ for order queuing. Use",
        "text_jp": "AWS Elastic Beanstalkをウェブホスティングに使用し、データベースAPIサービスにAmazon API Gatewayを使用します。注文のキューイングにAmazon MQを使用します"
      },
      {
        "key": "C",
        "text": "Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order",
        "text_jp": "Amazon S3をウェブホスティングに使用し、データベースAPIサービスにAWS AppSyncを使用します。注文のためにAmazon Simple Queue Service (Amazon SQS)を使用します"
      },
      {
        "key": "D",
        "text": "Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Amazon Simple Email Service (Amazon SES) for",
        "text_jp": "Amazon Lightsailをウェブホスティングに使用し、データベースAPIサービスにAWS AppSyncを使用します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct solution is A: Use Amazon S3 for web hosting, Amazon API Gateway for database API services, and Amazon SQS for order queuing.",
        "situation_analysis": "The company needs to refactor its web application to a decoupled architecture with the ability to retain failed orders and minimize operational costs.",
        "option_analysis": "Option A provides a serverless architecture that allows for scalability and cost efficiency through Amazon S3 and SQS. Other options involve additional services that may add complexity and cost.",
        "additional_knowledge": "Understanding the operational benefits of serverless architectures can significantly impact cost efficiency and management overhead.",
        "key_terminology": "Amazon S3, Amazon API Gateway, Amazon SQS, serverless architecture, scalability",
        "overall_assessment": "Option A aligns perfectly with the company's requirements for a scalable and cost-effective solution, whereas other options may introduce unnecessary complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しいソリューションはAです：Amazon S3をウェブホスティングに、Amazon API GatewayをデータベースAPIサービスに、Amazon SQSを注文のキューイングに使用します。",
        "situation_analysis": "企業は、失敗した注文を保持し、運用コストを最小限に抑える能力を持つ分離されたアーキテクチャへウェブアプリケーションをリファクタリングする必要があります。",
        "option_analysis": "オプションAは、Amazon S3とSQSを使用することでスケーラビリティとコスト効率を提供するサーバーレスアーキテクチャを提供します。他のオプションは、追加のサービスが必要となり、複雑さとコストが増加する可能性があります。",
        "additional_knowledge": "サーバーレスアーキテクチャの運用上の利点を理解することは、コスト効率と管理の負担に大きく影響を与える可能性があります。",
        "key_terminology": "Amazon S3、Amazon API Gateway、Amazon SQS、サーバーレスアーキテクチャ、スケーラビリティ",
        "overall_assessment": "オプションAは、企業の要求に完全に一致しており、スケーラブルでコスト効率の良いソリューションを提供しますが、他のオプションは不要な複雑さをもたらす可能性があります。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Amazon API Gateway",
      "Amazon SQS",
      "serverless architecture",
      "scalability"
    ]
  },
  {
    "No": "272",
    "question": "A company hosts a web application on AWS in the us-east-1 Region. The application servers are distributed across three Availability Zones behind\nan Application Load Balancer. The database is hosted in a MySQL database on an Amazon EC2 instance. A solutions architect needs to design a\ncross-Region data recovery solution using AWS services with an RTO of less than 5 minutes and an RPO of less than 1 minute. The solutions\narchitect is deploying application servers in us-west-2, and has configured Amazon Route 53 health checks and DNS failover to us-west-2.\nWhich additional step should the solutions architect take?",
    "question_jp": "ある企業がAWSのus-east-1リージョンでウェブアプリケーションをホストしています。アプリケーションサーバーは3つのアベイラビリティゾーンに分散しており、アプリケーションロードバランサーの背後に配置されています。データベースはAmazon EC2インスタンス上のMySQLデータベースにホストされています。ソリューションアーキテクトはAWSサービスを使用して、RTOが5分未満、RPOが1分未満のクロスリージョンデータ復旧ソリューションを設計する必要があります。ソリューションアーキテクトはus-west-2にアプリケーションサーバーをデプロイし、Amazon Route 53ヘルスチェックおよびDNSフェイルオーバーをus-west-2に構成しました。ソリューションアーキテクトが取るべき追加のステップはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica in us-west-2.",
        "text_jp": "データベースをAmazon RDS for MySQLインスタンスに移行し、us-west-2にクロスリージョンリードレプリカを作成する。"
      },
      {
        "key": "B",
        "text": "Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2.",
        "text_jp": "データベースをAmazon Auroraグローバルデータベースに移行し、プライマリをus-east-1に、セカンダリをus-west-2にする。"
      },
      {
        "key": "C",
        "text": "Migrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment.",
        "text_jp": "データベースをAmazon RDS for MySQLインスタンスに移行し、マルチAZデプロイメントを実施する。"
      },
      {
        "key": "D",
        "text": "Create a MySQL standby database on an Amazon EC2 instance in us-west-2.",
        "text_jp": "us-west-2のAmazon EC2インスタンス上にMySQLスタンバイデータベースを作成する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Creating a MySQL standby database on an Amazon EC2 instance in us-west-2 provides a local backup solution that can meet the RTO and RPO requirements.",
        "situation_analysis": "The company requires a cross-Region data recovery solution with specific RTO and RPO requirements. The application servers are already deployed in a secondary region with DNS failover configured.",
        "option_analysis": "Option D directly addresses the RTO and RPO requirement by having a standby database. Options A and B offer other database migration strategies but may not fulfill the recovery time objectives adequately as they typically introduce additional latency compared to a local standby. Option C improves availability but does not address cross-Region strategies.",
        "additional_knowledge": "In the event of a failure, the standby database can be quickly brought online.",
        "key_terminology": "RTO, RPO, standby database, cross-Region, Amazon EC2",
        "overall_assessment": "While options A and B provide robust solutions, creating a standby database on EC2 can be the quickest to set up for immediate disaster recovery needs based on the outlined criteria."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。us-west-2にAmazon EC2インスタンス上にMySQLスタンバイデータベースを作成することにより、RTOおよびRPOの要件を満たすローカルバックアップソリューションが提供される。",
        "situation_analysis": "企業は特定のRTOおよびRPO要件を持つクロスリージョンデータ復旧ソリューションを必要としている。アプリケーションサーバーはすでにセカンダリリージョンにデプロイされ、DNSフェイルオーバーが構成されている。",
        "option_analysis": "選択肢Dは、スタンバイデータベースを持つことでRTOおよびRPOの要件に直接対応している。選択肢AおよびBは他のデータベース移行戦略を提供するが、通常追加のレイテンシを引き起こすため、回復時間の目標を満たすのに適切でない可能性がある。選択肢Cは可用性を向上させるが、クロスリージョン戦略には対応していない。",
        "additional_knowledge": "障害が発生した場合、スタンバイデータベースは迅速にオンラインにすることができる。",
        "key_terminology": "RTO、RPO、スタンバイデータベース、クロスリージョン、Amazon EC2",
        "overall_assessment": "選択肢AおよびBは堅牢なソリューションを提供するが、EC2上にスタンバイデータベースを作成することは、記載された基準に基づいた即時災害復旧ニーズに対して最も迅速にセットアップできる。"
      }
    ],
    "keywords": [
      "RTO",
      "RPO",
      "standby database",
      "cross-Region",
      "Amazon EC2"
    ]
  },
  {
    "No": "273",
    "question": "A company is using AWS Organizations to manage multiple accounts. Due to regulatory requirements, the company wants to restrict specific\nmember accounts to certain AWS Regions, where they are permitted to deploy resources. The resources in the accounts must be tagged, enforced\nbased on a group standard, and centrally managed with minimal configuration.\nWhat should a solutions architect do to meet these requirements?",
    "question_jp": "ある企業がAWS Organizationsを使用して複数のアカウントを管理しています。規制の要件により、企業は特定のメンバーアカウントを、リソースをデプロイできる特定のAWSリージョンに制限したいと考えています。アカウント内のリソースは、グループ標準に基づいてタグ付けされ、最小限の設定で中央管理される必要があります。ソリューションアーキテクトはこれらの要件を満たすために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy.",
        "text_jp": "特定のメンバーアカウントにAWS Configルールを作成し、リージョンを制限し、タグポリシーを適用する。"
      },
      {
        "key": "B",
        "text": "From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and",
        "text_jp": "AWS Billing and Cost Managementコンソールから、管理アカウントで特定のメンバーアカウントのリージョンを無効にする。"
      },
      {
        "key": "C",
        "text": "Associate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions.",
        "text_jp": "特定のメンバーアカウントをルートに関連付け、タグポリシーとSCPを条件を使用して適用し、リージョンを制限する。"
      },
      {
        "key": "D",
        "text": "Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions.",
        "text_jp": "特定のメンバーアカウントを新しいOUに関連付け、タグポリシーとSCPを条件を使用して適用し、リージョンを制限する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy.",
        "situation_analysis": "The company needs to restrict member accounts to specific AWS Regions and enforce tagging based on group standards.",
        "option_analysis": "Option A allows for Region restriction and tagging compliance as it leverages AWS Config rules, which can monitor resource configurations against desired settings. Options B and C do not provide a centralized approach or allow for tagging enforcement.",
        "additional_knowledge": "Using a Service Control Policy (SCP) can also restrict access to regions; however, using AWS Config ensures compliance for existing resources.",
        "key_terminology": "AWS Organizations, AWS Config, Tagging, SCP (Service Control Policies), Compliance.",
        "overall_assessment": "Option A is well aligned with best practices for managing AWS environments under regulatory requirements. However, the community vote distribution is overwhelmingly in favor of option D, which might indicate a misunderstanding of the requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA：特定のメンバーアカウントにAWS Configルールを作成し、リージョンを制限し、タグポリシーを適用することです。",
        "situation_analysis": "企業は、特定のAWSリージョンにメンバーアカウントを制限し、グループ基準に基づくタグ付けを強制する必要があります。",
        "option_analysis": "オプションAは、AWS Configルールを活用し、リソース構成を望ましい設定に対して監視することができるため、リージョン制限とタグ付けの遵守を許可します。オプションBおよびCは、中央集中型のアプローチを提供せず、タグの遵守を許可しません。",
        "additional_knowledge": "サービス制御ポリシー（SCP）を使用することでリージョンへのアクセスを制限することもできますが、AWS Configを使用することで既存のリソースのコンプライアンスを確保できます。",
        "key_terminology": "AWS Organizations、AWS Config、タグ付け、SCP（サービス制御ポリシー）、コンプライアンス。",
        "overall_assessment": "オプションAは、規制要件の下でAWS環境を管理するためのベストプラクティスとよく一致しています。ただし、コミュニティの投票分布はオプションDを圧倒的に支持しており、要件の誤解を示す可能性があります。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS Config",
      "Tagging",
      "SCP",
      "Compliance"
    ]
  },
  {
    "No": "274",
    "question": "A company has an application that generates reports and stores them in an Amazon S3 bucket. When a user accesses their report, the application\ngenerates a signed URL to allow the user to download the report. The company's security team has discovered that the files are public and that\nanyone can download them without authentication. The company has suspended the generation of new reports until the problem is resolved.\nWhich set of actions will immediately remediate the security issue without impacting the application's normal workfiow?",
    "question_jp": "ある企業が、レポートを生成し、それをAmazon S3バケットに保存するアプリケーションを持っています。ユーザーが自身のレポートにアクセスすると、アプリケーションはユーザーがレポートをダウンロードできるように署名付きURLを生成します。企業のセキュリティチームは、ファイルがパブリックであり、誰でも認証なしでダウンロードできることを発見しました。企業は問題が解決されるまで新しいレポートの生成を中止しています。アプリケーションの通常のワークフローに影響を与えずに、セキュリティの問題を即座に軽減するために、どのアクションセットを実施すればよいですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Lambda function that applies a deny all policy for users who are not authenticated. Create a scheduled event to invoke the",
        "text_jp": "認証されていないユーザーにdeny allポリシーを適用するAWS Lambda関数を作成します。スケジュールイベントを作成してそれを呼び出します。"
      },
      {
        "key": "B",
        "text": "Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions.",
        "text_jp": "AWS Trusted Advisorのバケット権限チェックをレビューし、推奨されたアクションを実施します。"
      },
      {
        "key": "C",
        "text": "Run a script that puts a private ACL on all of the objects in the bucket.",
        "text_jp": "バケット内のすべてのオブジェクトにプライベートACLを設定するスクリプトを実行します。"
      },
      {
        "key": "D",
        "text": "Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcIs option to TRUE on the bucket.",
        "text_jp": "Amazon S3のBlock Public Access機能を使用して、バケットのIgnorePublicAclsオプションをTRUEに設定します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (78%) C (22%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. The AWS Trusted Advisor provides recommendations for security best practices, including S3 bucket permissions. By reviewing these permissions and implementing the recommended actions, the company can immediately address the security issue without impacting the current application workflow.",
        "situation_analysis": "The company has a security risk where files in the S3 bucket are publicly accessible, leading to unauthorized downloads.",
        "option_analysis": "Option B aligns with best practices for security and can help remediate the issue without disrupting application functionality. Options A and C may introduce complexities and are not focused on immediate resolution. Option D could help, but reviewing and following Trusted Advisor is the most comprehensive approach.",
        "additional_knowledge": "Employing AWS IAM policies and fine-grained S3 permissions is critical for protecting data in cloud environments.",
        "key_terminology": "AWS Trusted Advisor, S3 bucket permissions, security best practices.",
        "overall_assessment": "Option B is the most effective solution for the immediate issue as it leverages a comprehensive AWS service to ensure security compliance."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBです。AWS Trusted Advisorは、セキュリティのベストプラクティスについての推奨事項を提供し、S3バケットの権限に関する調査が含まれています。これらの権限を確認し、推奨されたアクションを実施することで、会社は現在のアプリケーションワークフローに影響を与えることなく、セキュリティの問題に即座に対処できます。",
        "situation_analysis": "会社には、S3バケット内のファイルが公共にアクセス可能であり、認証なしでダウンロードされることによるセキュリティリスクがあります。",
        "option_analysis": "選択肢Bは、セキュリティのベストプラクティスに沿っており、アプリケーションの機能に影響を与えずに問題を軽減できます。選択肢AおよびCは、複雑さをもたらす可能性があり、即時解決に焦点を当てていません。選択肢Dも助けになる可能性がありますが、Trusted Advisorをレビューし、それに従うことが最も包括的なアプローチです。",
        "additional_knowledge": "AWS IAMポリシーや細粒度のS3権限を活用することは、クラウド環境におけるデータ保護にとって非常に重要です。",
        "key_terminology": "AWS Trusted Advisor、S3バケット権限、セキュリティベストプラクティス。",
        "overall_assessment": "選択肢Bは、即時の問題解決に最も効果的なソリューションであり、AWSサービスを活用してセキュリティのコンプライアンスを確保します。"
      }
    ],
    "keywords": [
      "AWS Trusted Advisor",
      "S3 bucket permissions",
      "security best practices"
    ]
  },
  {
    "No": "275",
    "question": "A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance in another AWS account. A solutions\narchitect needs to design a migration strategy that will require no downtime and that will minimize the amount of time necessary to complete the\nmigration. The migration strategy must replicate all existing data and any new data that is created during the migration. The target database must\nbe identical to the source database at completion of the migration process.\nAll applications currently use an Amazon Route 53 CNAME record as their endpoint for communication with the RDS for Oracle DB instance. The\nRDS for Oracle DB instance is in a private subnet.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "ある企業が、Amazon RDS for Oracle データベースを別の AWS アカウントの RDS for PostgreSQL DB インスタンスに移行する計画を立てています。ソリューションアーキテクトは、ダウンタイムを必要とせず、移行を完了するために必要な時間を最小限に抑える移行戦略を設計する必要があります。この移行戦略は、既存のすべてのデータと、移行中に作成される新しいデータを複製する必要があります。移行プロセスが完了した時点で、ターゲットデータベースはソースデータベースと同一でなければなりません。現在、すべてのアプリケーションは、RDS for Oracle DB インスタンスとの通信のために Amazon Route 53 CNAME レコードをエンドポイントとして使用しています。RDS for Oracle DB インスタンスはプライベートサブネットにあります。これらの要件を満たすために、ソリューションアーキテクトはどの組み合わせのステップを取るべきでしょうか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the",
        "text_jp": "ターゲットアカウントに新しい RDS for PostgreSQL DB インスタンスを作成します。AWS スキーマ変換ツール (AWS SCT) を使用して移行します。"
      },
      {
        "key": "B",
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema",
        "text_jp": "AWS スキーマ変換ツール (AWS SCT) を使用して、ターゲットアカウントにスキーマを使用して新しい RDS for PostgreSQL DB インスタンスを作成します。"
      },
      {
        "key": "C",
        "text": "Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account.",
        "text_jp": "二つの AWS アカウント間で VPC ピアリングを設定して、ターゲットアカウントから両方の DB インスタンスへの接続を提供します。"
      },
      {
        "key": "D",
        "text": "Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account. Configure the",
        "text_jp": "ターゲットアカウントの VPC からの接続を提供するために、ソース DB インスタンスを一時的にパブリックにアクセス可能にします。"
      },
      {
        "key": "E",
        "text": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration",
        "text_jp": "ターゲットアカウントで AWS データベース移行サービス (AWS DMS) を使用して、フルロードおよび変更データキャプチャ (CDC) 移行を実行します。"
      },
      {
        "key": "F",
        "text": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a change data capture (CDC) migration from the source",
        "text_jp": "ターゲットアカウントで AWS データベース移行サービス (AWS DMS) を使用して、ソースから変更データキャプチャ (CDC) 移行を実行します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ACE (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Configuring VPC peering allows the two AWS accounts to communicate directly, which is necessary for the migration process.",
        "situation_analysis": "The migration requires no downtime and needs to replicate all current and new data, meaning that direct connectivity between the source and target databases is required during the migration.",
        "option_analysis": "Option C directly facilitates the connection needed for data migration. Options A, B, E, and F are all relevant to the migration process but do not address connectivity in the same way. Option D, while it may provide access, poses a security risk by making the DB publicly accessible.",
        "additional_knowledge": "This migration strategy commonly utilizes AWS Database Migration Service (DMS) alongside VPC peering for secure, efficient movement of data.",
        "key_terminology": "AWS DMS, VPC Peering, CDC",
        "overall_assessment": "The question addresses a common use case in cloud migrations, emphasizing network requirements. The community vote supports option C appropriately."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは C である。VPC ピアリングを設定することにより、二つの AWS アカウントが直接通信できるようになり、移行プロセスに必要な接続が確保される。",
        "situation_analysis": "移行中にダウンタイムが発生せず、現在のすべてのデータと新しいデータを複製する必要があるため、ソースとターゲットデータベース間の直接接続が必要である。",
        "option_analysis": "選択肢 C はデータ移行に必要な接続を直接的に促進する。選択肢 A、B、E、F はすべて移行プロセスに関連しているが、同様の方法で接続の問題に対処していない。選択肢 D はアクセスを提供するかもしれないが、DB を公開にすることでセキュリティリスクを引き起こす。",
        "additional_knowledge": "この移行戦略は、セキュアで効率的なデータ移動のために、AWS データベース移行サービス (DMS) と VPC ピアリングを組み合わせて利用することが一般的である。",
        "key_terminology": "AWS DMS、VPC ピアリング、CDC",
        "overall_assessment": "この問題はクラウド移行の一般的なユースケースに対処しており、ネットワーク要件を強調している。コミュニティの投票は選択肢 C を適切に支持している。"
      }
    ],
    "keywords": [
      "AWS DMS",
      "VPC Peering",
      "CDC"
    ]
  },
  {
    "No": "276",
    "question": "A company has implemented an ordering system using an event-driven architecture. During initial testing, the system stopped processing orders.\nFurther log analysis revealed that one order message in an Amazon Simple Queue Service (Amazon SQS) standard queue was causing an error on\nthe backend and blocking all subsequent order messages. The visibility timeout of the queue is set to 30 seconds, and the backend processing\ntimeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages and ensure that the system continues to process\nsubsequent messages.\nWhich step should the solutions architect take to meet these requirements?",
    "question_jp": "A company has implemented an ordering system using an event-driven architecture. During initial testing, the system stopped processing orders.\nFurther log analysis revealed that one order message in an Amazon Simple Queue Service (Amazon SQS) standard queue was causing an error on\nthe backend and blocking all subsequent order messages. The visibility timeout of the queue is set to 30 seconds, and the backend processing\ntimeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages and ensure that the system continues to process\nsubsequent messages.\nWhich step should the solutions architect take to meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Increase the backend processing timeout to 30 seconds to match the visibility timeout.",
        "text_jp": "Increase the backend processing timeout to 30 seconds to match the visibility timeout."
      },
      {
        "key": "B",
        "text": "Reduce the visibility timeout of the queue to automatically remove the faulty message.",
        "text_jp": "Reduce the visibility timeout of the queue to automatically remove the faulty message."
      },
      {
        "key": "C",
        "text": "Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages.",
        "text_jp": "Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages."
      },
      {
        "key": "D",
        "text": "Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages.",
        "text_jp": "Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (80%) C (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages.",
        "situation_analysis": "The scenario describes an event-driven architecture attempting to process orders where a faulty message causes subsequent messages to be blocked. The current visibility timeout of the SQS queue is longer than the backend processing timeout, leading to processing failures.",
        "option_analysis": "Option C is the best solution as it allows for isolating erroneous messages. Using a dead-letter queue enables analysis and reprocessing of these messages without blocking the order processing flow. Option A does not solve the blocking issue, option B only temporarily removes the message but does not help in isolating it, and option D, while workable, does not provide the benefits of FIFO ordering that C offers.",
        "additional_knowledge": "No additional knowledge available.",
        "key_terminology": "SQS, dead-letter queue, FIFO queue, visibility timeout, backend processing timeout",
        "overall_assessment": "The community vote demonstrates a preference for option D, but given the specific requirements of the case, option C is more appropriate due to its capability to handle errors in a structured manner. In scenarios where order precision and failure analysis are crucial, implementing a FIFO dead-letter queue is essential."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はC: 複雑なメッセージを隔離するために、新しいSQS FIFOキューをデッドレターキューとして構成することです。",
        "situation_analysis": "シナリオでは、故障したメッセージがその後のメッセージの処理をブロックする注文を処理しようとするイベント駆動型アーキテクチャについて述べられています。SQSキューの現在の可視性タイムアウトはバックエンド処理タイムアウトよりも長いため、処理の失敗が発生しています。",
        "option_analysis": "選択肢Cは、誤ったメッセージを隔離できるため、最良の解決策です。デッドレターキューを使用することで、これらのメッセージをモニターおよび分析しつつ、注文処理の流れをブロックせずに再処理できます。選択肢Aはブロックの問題を解決せず、選択肢Bはメッセージを一時的に削除するだけで、隔離には役立ちません。そして選択肢Dは機能的ですが、注文の処理が重要であるべきシナリオにはCが適しています。",
        "additional_knowledge": "追加の知識はありません。",
        "key_terminology": "SQS, デッドレターキュー, FIFOキュー, 可視性タイムアウト, バックエンド処理タイムアウト",
        "overall_assessment": "コミュニティの投票では選択肢Dが優先されていますが、特定の要件を考慮すると、故障分析が重要なシナリオでは、エラーを構造的に処理できるCが有効です。"
      }
    ],
    "keywords": [
      "SQS",
      "dead-letter queue",
      "FIFO queue",
      "visibility timeout",
      "backend processing timeout"
    ]
  },
  {
    "No": "277",
    "question": "A company has automated the nightly retraining of its machine learning models by using AWS Step Functions. The workfiow consists of multiple\nsteps that use AWS Lambda. Each step can fail for various reasons, and any failure causes a failure of the overall workfiow.\nA review reveals that the retraining has failed multiple nights in a row without the company noticing the failure. A solutions architect needs to\nimprove the workfiow so that notifications are sent for all types of failures in the retraining process.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "ある企業は、AWS Step Functionsを用いて機械学習モデルの夜間再訓練を自動化しています。このワークフローは、AWS Lambdaを使用する複数のステップで構成されており、各ステップはさまざまな理由で失敗する可能性があり、いずれかの失敗が全体のワークフローの失敗を引き起こします。調査の結果、再訓練は何度も失敗し、企業はその失敗に気付いていなかったことが判明しました。ソリューションアーキテクトは、再訓練プロセスのすべてのタイプの失敗に対して通知が送信されるようにワークフローを改善する必要があります。要件を満たすために、ソリューションアーキテクトが取るべき手順の組み合わせはどれですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type \"Email\" that targets the team's mailing list.",
        "text_jp": "Amazon Simple Notification Service (Amazon SNS) トピックを作成し、チームのメーリングリストを対象にした「Email」タイプのサブスクリプションを作成する。"
      },
      {
        "key": "B",
        "text": "Create a task named \"Email\" that forwards the input arguments to the SNS topic.",
        "text_jp": "SNSトピックに入力引数を転送する「Email」という名前のタスクを作成する。"
      },
      {
        "key": "C",
        "text": "Add a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.ALL\" ] and \"Next”: \"Email\".",
        "text_jp": "すべてのTask、Map、およびParallel状態に「ErrorEquals」として「States.ALL」を持ち、「Next」を「Email」に設定したCatchフィールドを追加する。"
      },
      {
        "key": "D",
        "text": "Add a new email address to Amazon Simple Email Service (Amazon SES). Verify the email address.",
        "text_jp": "Amazon Simple Email Service (Amazon SES) に新しいメールアドレスを追加し、そのメールアドレスを検証する。"
      },
      {
        "key": "E",
        "text": "Create a task named \"Email\" that forwards the input arguments to the SES email address.",
        "text_jp": "SESメールアドレスに入力引数を転送する「Email」という名前のタスクを作成する。"
      },
      {
        "key": "F",
        "text": "Add a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.Runtime\" ] and \"Next\": \"Email\".",
        "text_jp": "すべてのTask、Map、およびParallel状態に「ErrorEquals」として「States.Runtime」を持ち、「Next」を「Email」に設定したCatchフィールドを追加する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ABC (86%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct actions are to create an SNS topic for notifications, ensure Lambda functions can publish to it, and include a Catch field for error handling.",
        "situation_analysis": "The current workflow fails to notify on errors, preventing awareness of nightly retraining issues.",
        "option_analysis": "Option B is crucial as it creates a task to forward the error details to an SNS. Option C, while also correct, does not solve the notification issue directly. Option E further enhances notification but is not sufficient alone.",
        "additional_knowledge": "Exploring different types of notifications helps enhance the resilience of applications.",
        "key_terminology": "Amazon SNS, error handling, AWS Step Functions, Lambda functions.",
        "overall_assessment": "This question highlights the significance of error handling and notification in automated workflows, supporting robust cloud architectures."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい手順は、通知用のSNSトピックを作成し、Lambda関数がこれに発行できるようにし、エラーハンドリングのためにCatchフィールドを含めることです。",
        "situation_analysis": "現在のワークフローはエラーが発生した際に通知が行われず、夜間再訓練の問題に気付くことを防いでいます。",
        "option_analysis": "選択肢Bは、エラー詳細をSNSに転送するタスクを作成するもので、重要です。選択肢Cも正しいですが、直接的に通知の問題を解決しません。選択肢Eは通知をさらに強化しますが、単体では不十分です。",
        "additional_knowledge": "さまざまな通知手法を探求することで、アプリケーションの耐障害性を向上させることができます。",
        "key_terminology": "Amazon SNS、エラーハンドリング、AWS Step Functions、Lambda関数。",
        "overall_assessment": "この質問は自動化されたワークフローにおけるエラーハンドリングと通知の重要性を強調しており、堅牢なクラウドアーキテクチャの構築を支援しています。"
      }
    ],
    "keywords": [
      "Amazon SNS",
      "error handling",
      "AWS Step Functions",
      "Lambda functions"
    ]
  },
  {
    "No": "278",
    "question": "A company plans to deploy a new private intranet service on Amazon EC2 instances inside a VPC. An AWS Site-to-Site VPN connects the VPC to\nthe company's on-premises network. The new service must communicate with existing on-premises services. The on-premises services are\naccessible through the use of hostnames that reside in the company.example DNS zone. This DNS zone is wholly hosted on premises and is\navailable only on the company's private network.\nA solutions architect must ensure that the new service can resolve hostnames on the company.example domain to integrate with existing\nservices.\nWhich solution meets these requirements?",
    "question_jp": "企業は、VPC内のAmazon EC2インスタンス上に新しいプライベートイントラネットサービスを展開する計画を立てています。AWS Site-to-Site VPNがVPCと企業のオンプレミスネットワークを接続しています。新しいサービスは、既存のオンプレミスサービスと通信しなければなりません。オンプレミスサービスは、company.example DNSゾーンに存在するホスト名を使用してアクセスできます。このDNSゾーンは完全にオンプレミスでホストされており、企業のプライベートネットワーク上でのみ利用可能です。ソリューションアーキテクトは、新しいサービスが既存のサービスと統合するために、company.exampleドメインのホスト名を解決できることを確保しなければなりません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an empty private zone in Amazon Route 53 for company.example. Add an additional NS record to the company's on-premises",
        "text_jp": "Amazon Route 53に会社のための空のプライベートゾーンを作成します。追加のNSレコードを会社のオンプレミスに追加します。"
      },
      {
        "key": "B",
        "text": "Turn on DNS hostnames for the VPC. Configure a new outbound endpoint with Amazon Route 53 Resolver. Create a Resolver rule to forward",
        "text_jp": "VPCのDNSホスト名を有効にします。Amazon Route 53 Resolverで新しいアウトバウンドエンドポイントを構成します。Resolverルールを作成して転送します。"
      },
      {
        "key": "C",
        "text": "Turn on DNS hostnames for the VPConfigure a new inbound resolver endpoint with Amazon Route 53 Resolver. Configur&the on-premises",
        "text_jp": "VPCのDNSホスト名を有効にします。Amazon Route 53 Resolverで新しいインバウンドリゾルバーエンドポイントを構成します。オンプレミスに対応します。"
      },
      {
        "key": "D",
        "text": "Use AWS Systems Manager to configure a run document that will install a hosts file that contains any required hostnames. Use an Amazon",
        "text_jp": "AWS Systems Managerを使用して、必要なホスト名を含むhostsファイルをインストールするランドキュメントを構成します。Amazonを使用します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create an empty private zone in Amazon Route 53 for company.example. Add an additional NS record to the company's on-premises.",
        "situation_analysis": "The company needs to resolve hostnames within the company.example DNS zone that is hosted on-premises, using Amazon EC2 instances in a VPC.",
        "option_analysis": "Option A is correct because it enables the use of Route 53 to manage DNS resolution for the private zone while allowing the existing on-premises DNS records to still resolve. Options B, C, and D do not sufficiently meet the requirements as they do not directly address the need to manage the DNS entries from a single place.",
        "additional_knowledge": "Additional configurations may involve ensuring network connectivity and routing are correctly set up to access the DNS zone.",
        "key_terminology": "Amazon Route 53, VPC, private hosted zone, DNS resolution, NS record",
        "overall_assessment": "Option A is the best solution as it directly addresses the need for DNS resolution while allowing for existing on-premises service integration."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA: Amazon Route 53に会社のための空のプライベートゾーンを作成し、会社のオンプレミスに追加のNSレコードを追加します。",
        "situation_analysis": "企業は、VPC内のAmazon EC2インスタンスを使用して、オンプレミスにホストされたcompany.example DNSゾーン内のホスト名を解決する必要があります。",
        "option_analysis": "選択肢Aは正解です。これにより、DNSの解決がRoute 53によって管理され、オンプレミスのDNSレコードも依然として解決されることが可能になります。選択肢B、C、Dは、DNSエントリを一箇所から管理する必要性に十分に対処していません。",
        "additional_knowledge": "追加の構成には、DNSゾーンへのアクセスのためのネットワーク接続とルーティングの適切な設定が含まれる可能性があります。",
        "key_terminology": "Amazon Route 53、VPC、プライベートホストゾーン、DNS解決、NSレコード",
        "overall_assessment": "選択肢Aは、DNS解決の必要性に直接対処しており、既存のオンプレミスサービスとの統合を可能にする最善の解決策です。"
      }
    ],
    "keywords": [
      "Amazon Route 53",
      "VPC",
      "private hosted zone",
      "DNS resolution",
      "NS record"
    ]
  },
  {
    "No": "279",
    "question": "A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached to a transit gateway. Each VPC that sends\ntrafic to the public internet must send the trafic through a shared services VPC. Each subnet within a VPC uses the default VPC route table, and\nthe trafic is routed to the transit gateway. The transit gateway uses its default route table for any VPC attachment.\nA security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate with an EC2 instance that is deployed in any\nof the company's other VPCs. A solutions architect needs to limit the trafic between the VPCs. Each VPC must be able to communicate only with\na predefined, limited set of authorized VPCs.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "企業は、トランジットゲートウェイに接続された複数のVPC内でアプリケーションをデプロイするためにAWS CloudFormationを使用しています。パブリックインターネットにトラフィックを送信する各VPCは、共有サービスVPCを経由してトラフィックを送信する必要があります。各VPC内のサブネットはデフォルトのVPCルートテーブルを使用し、トラフィックはトランジットゲートウェイにルーティングされます。トランジットゲートウェイは、いかなるVPCアタッチメントに対してもデフォルトルートテーブルを使用します。セキュリティ監査により、あるVPC内にデプロイされたAmazon EC2インスタンスが、企業の他の任意のVPCにデプロイされたEC2インスタンスと通信できることが判明しました。ソリューションアーキテクトは、VPC間のトラフィックを制限する必要があります。各VPCは、あらかじめ定義された限られたセットの承認されたVPCとのみ通信できる必要があります。ソリューションアーキテクトはどのようにしてこの要件を満たすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Update the network ACL of each subnet within a VPC to allow outbound trafic only to the authorized VPCs. Remove all deny rules except",
        "text_jp": "各VPC内のサブネットのネットワークACLを更新し、承認されたVPCへのアウトバウンドトラフィックのみを許可します。すべての拒否ルールを削除します。"
      },
      {
        "key": "B",
        "text": "Update all the security groups that are used within a VPC to deny outbound trafic to security groups that are used within the unauthorized",
        "text_jp": "VPC内で使用されているすべてのセキュリティグループを更新し、未承認のVPC内で使用されているセキュリティグループへのアウトバウンドトラフィックを拒否します。"
      },
      {
        "key": "C",
        "text": "Create a dedicated transit gateway route table for each VPC attachment. Route trafic only to the authorized VPCs.",
        "text_jp": "各VPCアタッチメントに対して専用のトランジットゲートウェイルートテーブルを作成します。承認されたVPCへのトラフィックのみをルーティングします。"
      },
      {
        "key": "D",
        "text": "Update the main route table of each VPC to route trafic only to the authorized VPCs through the transit gateway.",
        "text_jp": "各VPCのメインルートテーブルを更新し、トランジットゲートウェイを経由して承認されたVPCへのトラフィックのみをルーティングします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Create a dedicated transit gateway route table for each VPC attachment. This allows for more granular control of traffic routing between VPCs, ensuring that only authorized VPCs can communicate with each other.",
        "situation_analysis": "The company has multiple VPCs connected to a transit gateway, and there is a need to limit traffic to only certain authorized VPCs after a security audit revealed unrestricted communication between EC2 instances in different VPCs.",
        "option_analysis": "Option A discusses modifying network ACLs, which is too coarse for this requirement. Option B mentions updating security groups; however, this approach lacks centralized control and can complicate security management. Option D suggests changing the main route tables, which would not effectively enforce access control.",
        "additional_knowledge": "AWS best practices recommend using the smaller scope of control (like route tables) for enhanced security rather than broader tools like ACLs or security groups.",
        "key_terminology": "AWS Transit Gateway, route table, VPC attachment, traffic control, security audit.",
        "overall_assessment": "The community vote of 100% in favor of option C confirms its correctness. Creating dedicated transit gateway route tables offers a best practice for managing inter-VPC traffic effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです: 各VPCアタッチメントのために専用のトランジットゲートウェイルートテーブルを作成します。これにより、VPC間のトラフィックルーティングをより詳細に制御でき、承認されたVPCのみが相互に通信できることが保証されます。",
        "situation_analysis": "企業はトランジットゲートウェイに接続された複数のVPCを持っており、セキュリティ監査により、異なるVPCのEC2インスタンス間の制限のない通信が明らかになった後、トラフィックを特定の承認されたVPCのみに制限する必要があります。",
        "option_analysis": "選択肢AはネットワークACLの修正について言及していますが、この要件には適していません。選択肢Bはセキュリティグループを更新することについて言及しており、このアプローチは集中管理を欠いており、セキュリティ管理を複雑にする可能性があります。選択肢Dはメインルートテーブルの変更を提案していますが、アクセス制御を効果的に施行できません。",
        "additional_knowledge": "AWSのベストプラクティスでは、セキュリティを強化するために、ACLやセキュリティグループのような広範なツールではなく、ルートテーブルのような狭い制御範囲を使用することを推奨しています。",
        "key_terminology": "AWS Transit Gateway、ルートテーブル、VPCアタッチメント、トラフィック制御、セキュリティ監査。",
        "overall_assessment": "選択肢Cを支持するコミュニティの投票が100％であることは、正しさを確認しています。専用のトランジットゲートウェイルートテーブルを作成することで、VPC間のトラフィックを効果的に管理するためのベストプラクティスが提供されます。"
      }
    ],
    "keywords": [
      "AWS Transit Gateway",
      "route table",
      "VPC attachment",
      "traffic control",
      "security audit"
    ]
  },
  {
    "No": "280",
    "question": "A company has a Windows-based desktop application that is packaged and deployed to the users' Windows machines. The company recently\nacquired another company that has employees who primarily use machines with a Linux operating system. The acquiring company has decided to\nmigrate and rehost the Windows-based desktop application to AWS.\nAll employees must be authenticated before they use the application. The acquiring company uses Active Directory on premises but wants a\nsimplified way to manage access to the application on AWS for all the employees.\nWhich solution will rehost the application on AWS with the LEAST development effort?",
    "question_jp": "ある企業が、ユーザーのWindowsマシンにパッケージ化して展開するWindowsベースのデスクトップアプリケーションを持っています。この企業は最近、主にLinuxオペレーティングシステムを使用するマシンを持つ従業員がいる別の企業を買収しました。買収した企業は、WindowsベースのデスクトップアプリケーションをAWSに移行し、再ホスティングすることを決定しました。すべての従業員はアプリケーションを使用する前に認証を受けなければなりません。買収した企業はオンプレミスのActive Directoryを使用していますが、すべての従業員のAWS上のアプリケーションへのアクセス管理を簡素化したいと考えています。どのソリューションが最も開発コストをかけずにAWSにアプリケーションを再ホスティングしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up and provision an Amazon Workspaces virtual desktop for every employee. Implement authentication by using Amazon Cognito",
        "text_jp": "各従業員に対してAmazon Workspaces仮想デスクトップをセットアップし、Amazon Cognitoを使用して認証を実装する。"
      },
      {
        "key": "B",
        "text": "Create an Auto Scaling group of Windows-based Amazon EC2 instances. Join each EC2 instance to the company's Active Directory domain.",
        "text_jp": "WindowsベースのAmazon EC2インスタンスのAuto Scalingグループを作成し、各EC2インスタンスを企業のActive Directoryドメインに参加させる。"
      },
      {
        "key": "C",
        "text": "Use an Amazon AppStream 2.0 image builder to create an image that includes the application and the required configurations. Provision an",
        "text_jp": "Amazon AppStream 2.0イメージビルダーを使用して、アプリケーションと必要な構成を含むイメージを作成する。"
      },
      {
        "key": "D",
        "text": "Refactor and containerize the application to run as a web-based application. Run the application in Amazon Elastic Container Service",
        "text_jp": "アプリケーションをリファクタリングしてコンテナ化し、Amazon Elastic Container Serviceで実行する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Refactoring and containerizing the application to run as a web-based application allows for a flexible solution that can easily accommodate both Windows and Linux users without requiring extensive development effort.",
        "situation_analysis": "The key requirement is to migrate a Windows-based desktop application to AWS while ensuring that all employees, including those on Linux systems, can access the application efficiently.",
        "option_analysis": "Option D leverages Docker containers to encapsulate the application, allowing for deployment across diverse platforms. Option A would require significant changes to the application, while option B focuses on maintaining a Windows environment, which does not meet the needs of Linux users. Option C is viable but does not provide the level of accessibility and usability as option D.",
        "additional_knowledge": "Utilizing AWS ECS also enhances the ability to manage resources dynamically, optimizing costs and performance.",
        "key_terminology": "AWS ECS, containerization, refactoring, web-based applications, application deployment",
        "overall_assessment": "Option D aligns with modern architectural practices by promoting application availability and scalability, and it's favored for its lower effort in terms of reengineering the application for cross-platform use."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。アプリケーションをウェブベースのアプリケーションとしてリファクタリングしてコンテナ化することで、柔軟なソリューションが提供され、WindowsとLinuxの両方のユーザーが extensiveな開発 effort 없이 利用できるようになる。",
        "situation_analysis": "主な要件は、WindowsベースのデスクトップアプリケーションをAWSに移行し、Linuxシステム上の従業員が効率的にアプリケーションにアクセスできるようにすることです。",
        "option_analysis": "オプションDはDockerコンテナを利用してアプリケーションをカプセル化し、多様なプラットフォームに展開できるようにします。オプションAはアプリケーションに対する大規模な変更を必要とし、オプションBはWindows環境を維持することに焦点を当てているため、Linuxユーザーのニーズには合わない。オプションCは有効であるが、オプションDほどのアクセシビリティとユーザービリティを提供しない。",
        "additional_knowledge": "AWS ECSを活用することで、リソースを動的に管理し、コストとパフォーマンスを最適化する能力も向上する。",
        "key_terminology": "AWS ECS、コンテナ化、リファクタリング、ウェブベースのアプリケーション、アプリケーション展開",
        "overall_assessment": "オプションDは、アプリケーションの可用性とスケーラビリティを促進する現代のアーキテクチャプラクティスに合致しており、クロスプラットフォームの使用のための再エンジニアリングに関する労力が少ないため好まれる。"
      }
    ],
    "keywords": [
      "AWS ECS",
      "containerization",
      "refactoring",
      "web-based applications",
      "application deployment"
    ]
  },
  {
    "No": "281",
    "question": "A company is collecting a large amount of data from a fieet of IoT devices. Data is stored as Optimized Row Columnar (ORC) files in the Hadoop\nDistributed File System (HDFS) on a persistent Amazon EMR cluster. The company's data analytics team queries the data by using SQL in Apache\nPresto deployed on the same EMR cluster. Queries scan large amounts of data, always run for less than 15 minutes, and run only between 5 PM\nand 10 PM.\nThe company is concerned about the high cost associated with the current solution. A solutions architect must propose the most cost-effective\nsolution that will allow SQL data queries.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がIoTデバイスの一群から大量のデータを収集しています。データは、持続的なAmazon EMRクラスター上のHadoop分散ファイルシステム（HDFS）に最適化行列列（ORC）ファイルとして保存されています。この企業のデータ分析チームは、同じEMRクラスター上にデプロイされたApache Prestoを使用してデータをSQLでクエリしています。クエリは大量のデータをスキャンし、常に15分未満で実行され、午後5時から午後10時の間にのみ実行されます。この企業は、現在のソリューションに関連する高コストについて懸念しています。ソリューションアーキテクトは、SQLデータクエリを可能にする最もコスト効果の高いソリューションを提案する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Store data in Amazon S3. Use Amazon Redshift Spectrum to query data.",
        "text_jp": "データをAmazon S3に保存します。Amazon Redshift Spectrumを使用してデータをクエリします。"
      },
      {
        "key": "B",
        "text": "Store data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data.",
        "text_jp": "データをAmazon S3に保存します。AWS Glue Data CatalogとAmazon Athenaを使用してデータをクエリします。"
      },
      {
        "key": "C",
        "text": "Store data in EMR File System (EMRFS). Use Presto in Amazon EMR to query data.",
        "text_jp": "データをEMRファイルシステム（EMRFS）に保存します。Amazon EMRでPrestoを使用してデータをクエリします。"
      },
      {
        "key": "D",
        "text": "Store data in Amazon Redshift. Use Amazon Redshift to query data.",
        "text_jp": "データをAmazon Redshiftに保存します。Amazon Redshiftを使用してデータをクエリします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Store data in Amazon Redshift and query data using Amazon Redshift. This is the best cost-effective solution for querying large datasets.",
        "situation_analysis": "The requirements are to reduce cost while efficiently querying large data sets in a short time frame.",
        "option_analysis": "Option A (Amazon Redshift Spectrum) incurs additional costs for external table querying. Option B (Amazon S3 and Athena) can also be cost-effective but may not match the performance needs. Option C does not effectively address the cost issue. Option D provides a direct way to optimize storage and query performance, justifying its selection.",
        "additional_knowledge": "Understanding the usage patterns during peak hours can further optimize cost through job restructuring.",
        "key_terminology": "AWS, Amazon S3, Amazon Redshift, Amazon EMR, Presto, cost-effective solutions",
        "overall_assessment": "While community votes favor Option B, the analysis supports Option D for its performance and cost optimization in a corporate data querying context."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです：データをAmazon Redshiftに保存し、Amazon Redshiftを使用してデータをクエリします。これは、大規模データセットをクエリするための最もコスト効果の高いソリューションです。",
        "situation_analysis": "要件は、短時間で効率的に大規模データセットをクエリしながら、コストを削減することです。",
        "option_analysis": "オプションA（Amazon Redshift Spectrum）は、外部テーブルクエリに追加のコストがかかります。オプションB（Amazon S3とAthena）もコスト効果が高い場合がありますが、パフォーマンスニーズに一致しない可能性があります。オプションCはコスト問題を効果的に解決していません。オプションDは、ストレージとクエリ性能を最適化するための直接的な方法を提供し、その選択を正当化します。",
        "additional_knowledge": "ピーク時の使用パターンを理解することで、ジョブの再構成を通じてさらにコストを最適化できます。",
        "key_terminology": "AWS、Amazon S3、Amazon Redshift、Amazon EMR、Presto、コスト効果の高いソリューション",
        "overall_assessment": "コミュニティの投票はオプションBに支持されていますが、分析はパフォーマンスとコスト最適化を考慮してオプションDを支持します。"
      }
    ],
    "keywords": [
      "AWS",
      "Amazon S3",
      "Amazon Redshift",
      "Amazon EMR",
      "Presto"
    ]
  },
  {
    "No": "282",
    "question": "A large company recently experienced an unexpected increase in Amazon RDS and Amazon DynamoDB costs. The company needs to increase\nvisibility into details of AWS Billing and Cost Management. There are various accounts associated with AWS Organizations, including many\ndevelopment and production accounts. There is no consistent tagging strategy across the organization, but there are guidelines in place that\nrequire all infrastructure to be deployed using AWS CloudFormation with consistent tagging. Management requires cost center numbers and\nproject ID numbers for all existing and future DynamoDB tables and RDS instances.\nWhich strategy should the solutions architect provide to meet these requirements?",
    "question_jp": "大企業は最近、Amazon RDSおよびAmazon DynamoDBのコストが予期せぬ増加を経験しました。企業はAWS請求およびコスト管理の詳細に対する可視性を高める必要があります。さまざまなアカウントがAWS Organizationsに関連付けられており、多くの開発アカウントと本番アカウントが含まれています。組織全体で一貫したタグ付け戦略はありませんが、すべてのインフラストラクチャはAWS CloudFormationを使用して一貫したタグ付けでデプロイすることが要求されています。管理者は、すべての既存および将来のDynamoDBテーブルとRDSインスタンスに対してコストセンター番号とプロジェクトID番号を必要としています。どの戦略をソリューションアーキテクトが提供して、これらの要件を満たすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to",
        "text_jp": "既存のリソースにタグを付けるためにTag Editorを使用する。コストセンターとプロジェクトIDを定義するコスト配分タグを作成し、タグを適用するのに24時間待つ。"
      },
      {
        "key": "B",
        "text": "Use an AWS Config rule to alert the finance team of untagged resources. Create a centralized AWS Lambda based solution to tag untagged",
        "text_jp": "AWS Configルールを使用して、未タグ付けリソースに対する警告を財務チームに送信する。中央集権的なAWS Lambdaベースのソリューションを作成して、未タグ付けのリソースにタグを追加する。"
      },
      {
        "key": "C",
        "text": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID. Use SCPs to restrict resource",
        "text_jp": "既存のリソースにタグを付けるためにTag Editorを使用する。コストセンターとプロジェクトIDを定義するためのコスト配分タグを作成する。SCPを使用してリソースを制限する。"
      },
      {
        "key": "D",
        "text": "Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources. Update",
        "text_jp": "コストセンターとプロジェクトIDを定義するコスト配分タグを作成し、既存のリソースにタグが伝播するのを24時間待つ。アプリケーションを更新する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (80%) A (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. By using AWS Config rules, the finance team can be alerted on untagged resources, facilitating better cost tracking.",
        "situation_analysis": "The company faces unexpected increases in AWS costs due to lack of visibility into resource usage across multiple accounts.",
        "option_analysis": "Option B directly addresses the need for monitoring untagged resources, which supports the goal of improved cost management by creating a process for added tagging. Options A and D do not provide an alerting mechanism, and option C introduces unnecessary restrictions.",
        "additional_knowledge": "Using cloud-native services to automate and enforce tagging strategy can greatly improve budget compliance.",
        "key_terminology": "AWS Config, tagging strategy, cost allocation tags, centralized tagging solution, AWS Lambda.",
        "overall_assessment": "The community vote heavily favors option C but fails to recognize the proactive approach of option B in preventing untagged resource proliferation. Tagging is essential for cost management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBである。AWS Configルールを使用することで、財務チームは未タグ付けのリソースに対して警告を受けることができ、コスト追跡がより良く行えるようになる。",
        "situation_analysis": "企業は複数のアカウントにわたるリソース使用状況への可視性が不足しているため、AWSのコストが予期しない増加を経験している。",
        "option_analysis": "選択肢Bは、未タグ付けのリソースを監視する必要性に直接対応しており、コスト管理の目標を達成するためのタグ付け追加のプロセスを確立する。選択肢AおよびDは警告メカニズムを提供せず、選択肢Cは不必要な制限を導入する。",
        "additional_knowledge": "クラウドネイティブサービスを使用してタグ付け戦略を自動化および実施することで、予算コンプライアンスが大幅に向上する可能性がある。",
        "key_terminology": "AWS Config、タグ付け戦略、コスト配分タグ、中央集権的タグ付けソリューション、AWS Lambda。",
        "overall_assessment": "コミュニティの投票は選択肢Cに集中しているが、未タグ付けリソースの増加を防ぐBの積極的なアプローチを見逃している。タグ付けはコスト管理において重要である。"
      }
    ],
    "keywords": [
      "AWS Config",
      "tagging strategy",
      "cost allocation tags",
      "centralized tagging solution",
      "AWS Lambda"
    ]
  },
  {
    "No": "283",
    "question": "A company wants to send data from its on-premises systems to Amazon S3 buckets. The company created the S3 buckets in three different\naccounts. The company must send the data privately without the data traveling across the internet. The company has no existing dedicated\nconnectivity to AWS.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "ある企業は、オンプレミスシステムからAmazon S3バケットにデータを送信したいと考えています。この企業は、異なる3つのアカウントにS3バケットを作成しました。企業は、データをインターネットを介さずにプライベートに送信する必要があります。企業はAWSへの専用接続を持っていません。これらの要件を満たすために、ソリューションアーキテクトはどの組み合わせの手順を踏むべきですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect",
        "text_jp": "AWS Cloudにネットワーキングアカウントを確立します。ネットワーキングアカウントにプライベートVPCを作成します。AWS Direct Connectを設定します。"
      },
      {
        "key": "B",
        "text": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect",
        "text_jp": "AWS Cloudにネットワーキングアカウントを確立します。ネットワーキングアカウントにプライベートVPCを作成します。AWS Direct Connectを設定します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon S3 interface endpoint in the networking account.",
        "text_jp": "ネットワーキングアカウントにAmazon S3インターフェイスエンドポイントを作成します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon S3 gateway endpoint in the networking account.",
        "text_jp": "ネットワーキングアカウントにAmazon S3ゲートウェイエンドポイントを作成します。"
      },
      {
        "key": "E",
        "text": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Peer VPCs from the accounts that host",
        "text_jp": "AWS Cloudにネットワーキングアカウントを確立します。ネットワーキングアカウントにプライベートVPCを作成します。S3バケットをホストするアカウントからVPCをピアリングします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "AC (80%) 7% 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are D (Create an Amazon S3 gateway endpoint in the networking account) and A (Establish a networking account in the AWS Cloud and set up AWS Direct Connect).",
        "situation_analysis": "The company requires a private connectivity solution to send data to S3 without traversing the public internet, given that there is no existing dedicated connectivity.",
        "option_analysis": "Creating an S3 gateway endpoint (D) allows access to S3 through private IPs within the VPC and ensures secure data transmission. However, to truly cut off internet access, AWS Direct Connect must be utilized.",
        "additional_knowledge": "While creating a VPC and considering the endpoint is vital, establishing a solid network is fundamental for any successful AWS integration.",
        "key_terminology": "AWS Direct Connect, Amazon S3 gateway endpoint, VPC peering, network architecture, private connectivity",
        "overall_assessment": "The solution is technically sound as it leverages both the gateway endpoint and direct connection to ensure private data transfer to S3. The community votes lean towards a different set of answers, possibly indicating an understanding of misinterpretation of the requirement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はD（ネットワーキングアカウントにAmazon S3ゲートウェイエンドポイントを作成します）とA（AWS Cloudにネットワーキングアカウントを確立し、AWS Direct Connectを設定します）です。",
        "situation_analysis": "企業は、公共インターネットを通過せずにS3にデータを送信するためのプライベート接続ソリューションが必要です。専用の接続がない状況です。",
        "option_analysis": "S3ゲートウェイエンドポイントを作成すること（D）は、VPC内のプライベートIPを介してS3にアクセスできるようにし、安全なデータ転送を保証します。しかし、インターネットアクセスを真正面から遮断するには、AWS Direct Connectを活用する必要があります。",
        "additional_knowledge": "VPCを作成し、エンドポイントを考慮することは重要ですが、堅実なネットワークを構築することは、AWS統合の成功に不可欠です。",
        "key_terminology": "AWS Direct Connect、Amazon S3ゲートウェイエンドポイント、VPCピアリング、ネットワークアーキテクチャ、プライベート接続",
        "overall_assessment": "このソリューションは、プライベートデータ転送を確保するためにゲートウェイエンドポイントと専用接続を組み合わせているため、技術的に健全です。コミュニティの投票は異なる回答を支持する傾向にありますが、これは要求の誤解を示す可能性があります。"
      }
    ],
    "keywords": [
      "AWS Direct Connect",
      "Amazon S3 gateway endpoint",
      "VPC peering"
    ]
  },
  {
    "No": "284",
    "question": "A company operates quick-service restaurants. The restaurants follow a predictable model with high sales trafic for 4 hours daily. Sales trafic is\nlower outside of those peak hours.\nThe point of sale and management platform is deployed in the AWS Cloud and has a backend that is based on Amazon DynamoDB. The database\ntable uses provisioned throughput mode with 100,000 RCUs and 80,000 WCUs to match known peak resource consumption.\nThe company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.\nWhich solution meets these requirements MOST cost-effectively?",
    "question_jp": "ある企業がクイックサービスのレストランを運営しています。レストランは、毎日4時間の高い売上トラフィックを伴う予測可能なモデルに従っています。ピーク時以外では、売上トラフィックは低下します。販売時点管理（POS）システムと管理プラットフォームはAWS Cloudに展開されており、バックエンドはAmazon DynamoDBを基盤としています。データベーステーブルは、既知のピークリソース消費に合わせて、100,000 RCUと80,000 WCUの提供スループットモードを使用しています。企業は、DynamoDBのコストを削減し、ITスタッフの運用オーバーヘッドを最小限に抑えたいと考えています。どのソリューションが、最もコスト効率よくこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Reduce the provisioned RCUs and WCUs.",
        "text_jp": "提供されたRCUとWCUを削減する。"
      },
      {
        "key": "B",
        "text": "Change the DynamoDB table to use on-demand capacity.",
        "text_jp": "DynamoDBテーブルをオンデマンドキャパシティを使用するように変更する。"
      },
      {
        "key": "C",
        "text": "Enable Dynamo DB auto scaling for the table.",
        "text_jp": "テーブルのDynamoDB自動スケーリングを有効にする。"
      },
      {
        "key": "D",
        "text": "Purchase 1-year reserved capacity that is suficient to cover the peak load for 4 hours each day.",
        "text_jp": "毎日4時間のピーク負荷をカバーするのに十分な1年間の予約キャパシティを購入する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (80%) D (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Reduce the provisioned RCUs and WCUs.",
        "situation_analysis": "The company experiences high sales traffic for a limited time each day, allowing them to lower the provisioned capacity outside of peak hours.",
        "option_analysis": "Option A is correct because reducing provisioned RCUs and WCUs directly lowers costs without compromising performance during peak times. Option B would increase unpredictability in costs, option C would still involve operational overhead, and option D is not cost-effective for only peak hours.",
        "additional_knowledge": "",
        "key_terminology": "DynamoDB, provisioned throughput, on-demand capacity, auto scaling, reserved capacity",
        "overall_assessment": "Given that the majority of community votes are on other options, it's worth noting that while automatic scaling and on-demand capacity can be beneficial, for this specific scenario focusing on predictable reductions in RCUs and WCUs is the most cost-effective."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA: 提供されたRCUとWCUを削減することです。",
        "situation_analysis": "企業は毎日限られた時間に高い売上トラフィックを経験するため、ピーク時間外に提供能力を低下させることができます。",
        "option_analysis": "選択肢Aは正しいです。提供されたRCUとWCUを削減することで、ピーク時のパフォーマンスを損なうことなくコストを直接削減できます。選択肢Bはコストの予測可能性を増加させ、選択肢Cは依然として運用オーバーヘッドを伴い、選択肢Dはピーク時間だけに対してコスト効果がありません。",
        "additional_knowledge": "",
        "key_terminology": "DynamoDB, 提供スループット, オンデマンドキャパシティ, 自動スケーリング, 予約キャパシティ",
        "overall_assessment": "コミュニティ投票の大多数が他の選択肢にある点に注意する必要がありますが、予測可能なRCUおよびWCUの低減に焦点を当てることが、特定のシナリオにおいて最もコスト効果的です。"
      }
    ],
    "keywords": [
      "DynamoDB",
      "provisioned throughput",
      "on-demand capacity",
      "auto scaling",
      "reserved capacity"
    ]
  },
  {
    "No": "285",
    "question": "A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application currently\ndoes not use API keys to authorize requests. The API model is as follows:\nGET /posts/{postId}: to get post details\nGET /users/{userId}: to get user details\nGET /comments/{commentId}: to get comments details\nThe company has noticed users are actively discussing topics in the comments section, and the company wants to increase user engagement by\nmaking the comments appear in real time.\nWhich design should be used to reduce comment latency and improve user experience?",
    "question_jp": "ある会社が、Amazon API Gateway、Amazon DynamoDB、およびAWS Lambdaを使用してAWS上でブログポストアプリケーションをホストしています。現在、アプリケーションはリクエストを認証するためのAPIキーを使用していません。APIモデルは次のようになります： \nGET /posts/{postId}: 投稿の詳細を取得するため \nGET /users/{userId}: ユーザーの詳細を取得するため \nGET /comments/{commentId}: コメントの詳細を取得するため \n会社は、ユーザーがコメントセクションでトピックについて活発に議論していることに気づき、コメントをリアルタイムで表示することによってユーザーエンゲージメントを高めたいと考えています。\nどのデザインを使用すれば、コメントの待機時間を短縮し、ユーザーエクスペリエンスを向上させることができるでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use edge-optimized API with Amazon CloudFront to cache API responses.",
        "text_jp": "Amazon CloudFrontを使用してAPIレスポンスをキャッシュするエッジ最適化APIを使用する。"
      },
      {
        "key": "B",
        "text": "Modify the blog application code to request GET/comments/{commentId} every 10 seconds.",
        "text_jp": "ブログアプリケーションのコードを修正して、10秒ごとにGET/comments/{commentId}をリクエストする。"
      },
      {
        "key": "C",
        "text": "Use AWS AppSync and leverage WebSockets to deliver comments.",
        "text_jp": "AWS AppSyncを使用してWebSocketを活用し、コメントを配信する。"
      },
      {
        "key": "D",
        "text": "Change the concurrency limit of the Lambda functions to lower the API response time.",
        "text_jp": "APIレスポンスタイムを短縮するために、Lambda関数の同時実行制限を変更する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using AWS AppSync and WebSockets to deliver comments allows for real-time data synchronization, improving user engagement significantly.",
        "situation_analysis": "The requirement is to display comments in real time to enhance user engagement on the blog post application.",
        "option_analysis": "Option A, while useful for reducing load times, does not provide real-time updates. Option B will cause unnecessary network traffic and latency issues. Option D may help slightly but does not address real-time comment delivery.",
        "additional_knowledge": "Incorporating a subscription model via AWS AppSync will allow users to receive updates as comments are made, which significantly enhances user interaction and experience.",
        "key_terminology": "AWS AppSync, WebSockets, real-time data synchronization",
        "overall_assessment": "The community supports option C strongly, aligning with AWS's best practices for building responsive applications that require real-time data updates."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。AWS AppSyncを使用し、WebSocketを活用してコメントを配信することで、リアルタイムのデータ同期が可能になり、ユーザーエンゲージメントを大幅に向上させることができる。",
        "situation_analysis": "ブログポストアプリケーションにおいてコメントをリアルタイムで表示するという要件がある。",
        "option_analysis": "選択肢Aは、負荷時間を短縮するのには役立つが、リアルタイムの更新を提供しない。選択肢Bは不必要なネットワークトラフィックと待機時間の問題を引き起こす。選択肢Dはわずかに助けるかもしれないが、リアルタイムのコメント配信には対応しない。",
        "additional_knowledge": "AWS AppSyncを介したサブスクリプションモデルの導入により、ユーザーはコメントが追加されるたびに更新を受け取ることができ、ユーザーの対話と体験を大幅に向上させることができる。",
        "key_terminology": "AWS AppSync, WebSocket, リアルタイムデータ同期",
        "overall_assessment": "コミュニティは選択肢Cを強く支持しており、リアルタイムデータ更新を必要とする応答性の高いアプリケーションを構築するためのAWSのベストプラクティスに合致している。"
      }
    ],
    "keywords": [
      "AWS AppSync",
      "WebSockets",
      "real-time data synchronization"
    ]
  },
  {
    "No": "286",
    "question": "A company manages hundreds of AWS accounts centrally in an organization in AWS Organizations. The company recently started to allow product\nteams to create and manage their own S3 access points in their accounts. The S3 access points can be accessed only within VPCs, not on the\ninternet.\nWhat is the MOST operationally eficient way to enforce this requirement?",
    "question_jp": "ある企業は、AWS Organizationsで中央集中管理された数百のAWSアカウントを管理しています。この企業は最近、製品チームが自分たちのアカウント内で独自のS3アクセスパointを作成して管理することを許可しました。このS3アクセスパointは、インターネットではなく、VPC内のみでアクセス可能です。この要件を最も運用効率的に強制する方法は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key",
        "text_jp": "S3アクセスパointのリソースポリシーを設定し、s3:AccessPointNetworkOrigin条件キーがない限りs3:CreateAccessPointアクションを拒否する。"
      },
      {
        "key": "B",
        "text": "Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin",
        "text_jp": "組織のルートレベルでSCPを作成し、s3:AccessPointNetworkOrigin条件キーがない限りs3:CreateAccessPointアクションを拒否する。"
      },
      {
        "key": "C",
        "text": "Use AWS CloudFormation StackSets to create a new IAM policy in each AWS account that allows the s3:CreateAccessPoint action only if",
        "text_jp": "AWS CloudFormation StackSetsを使用して、各AWSアカウントに新しいIAMポリシーを作成し、s3:CreateAccessPointアクションを許可する条件が整った場合のみ。"
      },
      {
        "key": "D",
        "text": "Set the S3 bucket policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
        "text_jp": "S3バケットポリシーを設定し、s3:AccessPointNetworkOrigin条件キーがVPCに評価されない限りs3:CreateAccessPointアクションを拒否する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This approach directly relates to the security of the S3 access points while allowing flexibility for operational teams to create their own access points.",
        "situation_analysis": "The company has granted product teams permission to create S3 access points, and it's critical to ensure those points can only be accessed within a VPC to maintain security.",
        "option_analysis": "Option A provides a direct means of enforcing access rules via resource policies specifically for S3 access points. Option B involves service control policies, which are broader and less specific, while C focuses on IAM policies per account, requiring more operational overhead. Option D would apply to the bucket instead of the access point.",
        "additional_knowledge": "Utilizing resource policies is recommended best practice in scenarios with multiple product teams needing isolated access management.",
        "key_terminology": "S3 Access Points, VPC, resource policy, IAM, SCP",
        "overall_assessment": "Choosing A ensures that the enforcement is easy to manage and maintain directly on the S3 access points without creating unnecessary operational complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。このアプローチは、S3アクセスパointのセキュリティに直接関連しており、運用チームが独自のアクセスパointを作成する柔軟性を提供する。",
        "situation_analysis": "企業は製品チームにS3アクセスパointの作成を許可しており、そのポイントがVPC内でのみアクセスできるようにすることが重要です.",
        "option_analysis": "選択肢Aは、S3アクセスパoint専用のリソースポリシーを介してアクセスルールを施行する直接的な手段を提供する。選択肢Bは、より広範で特定のものでないサービス制御ポリシーに関与し、CはアカウントごとのIAMポリシーに焦点を当て、運用のオーバーヘッドを増やす。選択肢Dは、アクセスパointではなくバケットに適用される。",
        "additional_knowledge": "リソースポリシーの利用は、複数の製品チームが隔離されたアクセス管理を必要とするシナリオにおいて推奨されるベストプラクティスである。",
        "key_terminology": "S3アクセスパント、VPC、リソースポリシー、IAM、SCP",
        "overall_assessment": "選択肢Aを選択することで、アクセスパointに直接施行し、不要な運用の複雑性を生じさせずに管理しやすく、維持しやすい。"
      }
    ],
    "keywords": [
      "S3 Access Points",
      "VPC",
      "resource policy",
      "IAM",
      "SCP"
    ]
  },
  {
    "No": "287",
    "question": "A solutions architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The\nsolutions architect creates an environment that is identical to the existing application environment and deploys the application to the new\nenvironment.\nWhat should be done next to complete the update?",
    "question_jp": "ソリューションアーキテクトは、AWS Elastic Beanstalk内のアプリケーション環境をブルー/グリーンデプロイメント手法を使用して更新する必要があります。ソリューションアーキテクトは、既存のアプリケーション環境と同一の環境を作成し、新しい環境にアプリケーションをデプロイしました。次に、更新を完了するために何をすべきでしょうか。",
    "choices": [
      {
        "key": "A",
        "text": "Redirect to the new environment using Amazon Route 53.",
        "text_jp": "Amazon Route 53を使用して新しい環境にリダイレクトします。"
      },
      {
        "key": "B",
        "text": "Select the Swap Environment URLs option.",
        "text_jp": "環境のURLを入れ替えるオプションを選択します。"
      },
      {
        "key": "C",
        "text": "Replace the Auto Scaling launch configuration.",
        "text_jp": "Auto Scalingの起動設定を置き換えます。"
      },
      {
        "key": "D",
        "text": "Update the DNS records to point to the green environment.",
        "text_jp": "DNSレコードを更新してグリーン環境を指すようにします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Redirecting to the new environment using Amazon Route 53 is a step to complete the blue/green deployment.",
        "situation_analysis": "A blue/green deployment is designed to minimize downtime and risk by running two identical production environments. After deploying the application to the new environment, the next step involves directing traffic to the newly created environment.",
        "option_analysis": "Option A is correct, as it facilitates the redirection of users to the new version. Option B, swapping environment URLs, is typically an alternative method but not specifically applicable in this step. Option C is not necessary here as the environment swap does not require changing Auto Scaling options. Option D, while it might work, is not as direct as option A in terms of leveraging Route 53 features.",
        "additional_knowledge": "Understanding the blue/green deployment process is crucial for minimizing downtime during application updates.",
        "key_terminology": "Blue/Green Deployment, AWS Elastic Beanstalk, Amazon Route 53, DNS Management, Traffic Routing.",
        "overall_assessment": "This question is relevant for AWS Solutions Architect exams, focusing on deployment strategies in AWS."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。Amazon Route 53を使用して新しい環境にリダイレクトすることは、ブルー/グリーンデプロイメントを完了するためのステップです。",
        "situation_analysis": "ブルー/グリーンデプロイメントは、2つの同一の本番環境を実行することでダウンタイムとリスクを最小限に抑えることを目的としています。アプリケーションを新しい環境にデプロイした後、次のステップは、トラフィックを新しく作成された環境に向けることです。",
        "option_analysis": "選択肢Aは正しいです。これは、ユーザーを新しいバージョンにリダイレクトすることを促進します。選択肢Bの環境URLを入れ替えることは、通常の代替手段ですが、特にこのステップでは適用されません。選択肢Cは、環境の交換にはAuto Scalingオプションを変更する必要がないため、必要ありません。選択肢Dは機能するかもしれませんが、Route 53機能を活用する観点からは選択肢Aほど直接的ではありません。",
        "additional_knowledge": "アプリケーション更新中のダウンタイムを最小限に抑えるためには、ブルー/グリーンデプロイメントプロセスを理解することが重要です。",
        "key_terminology": "ブルー/グリーンデプロイメント、AWS Elastic Beanstalk、Amazon Route 53、DNS管理、トラフィックルーティング。",
        "overall_assessment": "この質問はAWSソリューションアーキテクト試験に関連しており、AWSにおけるデプロイ戦略に焦点を当てています。"
      }
    ],
    "keywords": [
      "Blue/Green Deployment",
      "AWS Elastic Beanstalk",
      "Amazon Route 53",
      "DNS Management",
      "Traffic Routing"
    ]
  },
  {
    "No": "288",
    "question": "A company is building an image service on the web that will allow users to upload and search random photos. At peak usage, up to 10,000 users\nworldwide will upload their images. The will then overlay text on the uploaded images, which will then be published on the company website.\nWhich design should a solutions architect implement?",
    "question_jp": "ある企業が、ユーザーがランダムな写真をアップロードして検索できるウェブ上の画像サービスを構築しています。ピーク時には、世界中で最大10,000人のユーザーが画像をアップロードします。その後、アップロードされた画像にテキストをオーバーレイし、これを企業のウェブサイトに公開します。この場合、ソリューションアーキテクトはどのような設計を実装すべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Store the uploaded images in Amazon Elastic File System (Amazon EFS). Send application log information about each image to Amazon",
        "text_jp": "アップロードされた画像をAmazon Elastic File System (Amazon EFS)に保存します。各画像に関するアプリケーションログ情報をAmazonに送信します。"
      },
      {
        "key": "B",
        "text": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to Amazon Simple",
        "text_jp": "アップロードされた画像をAmazon S3バケットに保存し、S3バケットイベント通知を構成してAmazon Simpleにメッセージを送信します。"
      },
      {
        "key": "C",
        "text": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to the Amazon Simple",
        "text_jp": "アップロードされた画像をAmazon S3バケットに保存し、S3バケットイベント通知を構成してAmazon Simpleにメッセージを送信します。"
      },
      {
        "key": "D",
        "text": "Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume mounted to a fieet of Amazon EC2 Spot",
        "text_jp": "アップロードされた画像を共有のAmazon Elastic Block Store (Amazon EBS)ボリュームに保存し、Amazon EC2スポットインスタンスのフィートにマウントします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Storing images on Amazon EBS allows for low-latency access and easy integration with EC2 for processing the uploaded images.",
        "situation_analysis": "The requirement is to handle up to 10,000 simultaneous uploads while efficiently processing and storing images.",
        "option_analysis": "Option A is not optimal for scaling due to EFS latency. B and C use S3, but they don't offer the necessary integration for real-time processing on EC2.",
        "additional_knowledge": "Implementing S3 for storage would require additional services for processing. EBS allows direct access from EC2 instances.",
        "key_terminology": "Amazon EBS, Amazon EC2 Spot Instances, low-latency access",
        "overall_assessment": "Despite community support for option C, the architecture needs to prioritize performance and integration for immediate processing, making D the best choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。画像をAmazon EBSに保存することで、低遅延アクセスを提供し、アップロードされた画像の処理のためにEC2との統合が容易になります。",
        "situation_analysis": "最大10,000件の同時アップロードを処理しながら、効率的に画像を処理・保存する必要があります。",
        "option_analysis": "選択肢AはEFSのレイテンシーのためにスケーリングに最適ではありません。選択肢BとCはS3を使用していますが、EC2でのリアルタイム処理のための必要な統合を提供していません。",
        "additional_knowledge": "S3をストレージに使用した場合、処理のために追加のサービスが必要になります。EBSはEC2インスタンスからの直接アクセスを可能にします。",
        "key_terminology": "Amazon EBS, Amazon EC2スポットインスタンス, 低遅延アクセス",
        "overall_assessment": "コミュニティが選択肢Cを支持しているにもかかわらず、アーキテクチャはパフォーマンスと統合を優先する必要があり、Dが最も適した選択肢になります。"
      }
    ],
    "keywords": [
      "Amazon EBS",
      "Amazon EC2",
      "AWS Architecture"
    ]
  },
  {
    "No": "289",
    "question": "A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data\navailable to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not\ntolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of\ncustomers need to see updates from the other group in real time.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、us-east-1リージョンにAmazon RDS for MySQL DBインスタンスをデプロイしました。この企業は自社のデータをヨーロッパの顧客に提供する必要があります。ヨーロッパの顧客は、米国の顧客と同じデータにアクセスする必要があり、高いアプリケーション遅延や古いデータを容認しません。ヨーロッパの顧客と米国の顧客は、両方ともデータベースに書き込む必要があります。両グループの顧客は、リアルタイムで他のグループからの更新を確認する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the",
        "text_jp": "Amazon Aurora MySQLのレプリカをRDS for MySQL DBインスタンスの作成。アプリケーションの書き込みをRDS DBインスタンスで一時停止します。"
      },
      {
        "key": "B",
        "text": "Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the",
        "text_jp": "RDS for MySQL DBインスタンスのためにeu-west-1にクロスリージョンレプリカを追加します。レプリカの設定を行い、書き込みクエリをDBインスタンスに戻します。"
      },
      {
        "key": "C",
        "text": "Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1",
        "text_jp": "RDS for MySQL DBインスタンスから最新のスナップショットをeu-west-1にコピーします。eu-west-1に新しいRDS for MySQL DBインスタンスを作成します。"
      },
      {
        "key": "D",
        "text": "Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster.",
        "text_jp": "RDS for MySQL DBインスタンスをAmazon Aurora MySQL DBクラスタに変換します。DBクラスタにeu-west-1をセカンダリリージョンとして追加します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (58%) D (42%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Adding a cross-Region replica in eu-west-1 allows for write operations to be replicated back to the primary RDS instance in us-east-1, ensuring that both US and European customers have access to the most current data in real time.",
        "situation_analysis": "The requirement is for low latency and real-time synchronization of data between customers in the US and Europe. Both sets of customers need to write to the database and view each other's updates instantly.",
        "option_analysis": "Option A does not fulfill the real-time requirement as it involves pausing write operations. Option C provides a one-time snapshot and does not allow for ongoing updates. Option D would not provide the necessary cross-region write capabilities.",
        "additional_knowledge": "Cross-Region replication can also help in disaster recovery scenarios, by having up-to-date data in a different region.",
        "key_terminology": "Amazon RDS, cross-Region replica, real-time data synchronization, write operations, latency.",
        "overall_assessment": "Option B aligns with best practices for maintaining data consistency and availability across regions, specifically designed for applications requiring low latency and real-time data access."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBである。eu-west-1にクロスリージョンレプリカを追加すると、書き込み操作がus-east-1のプライマリRDSインスタンスに戻されるようになり、米国とヨーロッパの顧客がリアルタイムで最新のデータにアクセスできる。",
        "situation_analysis": "要件は、米国とヨーロッパの顧客間でのデータの低遅延およびリアルタイム同期である。両方の顧客グループがデータベースに書き込み、互いの更新を即座に確認する必要がある。",
        "option_analysis": "選択肢Aは、操作を一時停止するため、リアルタイム要件を満たさない。選択肢Cは、一度限りのスナップショットを提供し、継続的な更新を可能にしない。選択肢Dは、必要なクロスリージョン書き込み機能を提供しない。",
        "additional_knowledge": "クロスリージョンレプリケーションは、異なるリージョンに最新のデータを持つことで災害復旧シナリオにも役立つ。",
        "key_terminology": "Amazon RDS、クロスリージョンレプリカ、リアルタイムデータ同期、書き込み操作、遅延。",
        "overall_assessment": "選択肢Bは、地域を超えたデータの整合性と可用性を維持するためのベストプラクティスに沿っており、特に低遅延でリアルタイムデータアクセスを必要とするアプリケーション向けに設計されている。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "cross-Region replica",
      "real-time data synchronization",
      "write operations",
      "latency"
    ]
  },
  {
    "No": "290",
    "question": "A company is serving files to its customers through an SFTP server that is accessible over the internet. The SFTP server is running on a single\nAmazon EC2 instance with an Elastic IP address attached. Customers connect to the SFTP server through its Elastic IP address and use SSH for\nauthentication. The EC2 instance also has an attached security group that allows access from all customer IP addresses.\nA solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the\ndisruption to customers who access files. The solution must not change the way customers connect.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、インターネット経由でアクセス可能なSFTPサーバーを通じて顧客にファイルを提供している。このSFTPサーバーは、Elastic IPアドレスが割り当てられた単一のAmazon EC2インスタンス上で稼働している。顧客はSFTPサーバーにElastic IPアドレスを通じて接続し、SSHを利用して認証を行っている。EC2インスタンスには、すべての顧客IPアドレスからのアクセスを許可するセキュリティグループも付与されている。ソリューションアーキテクトは、可用性を向上させ、インフラ管理の複雑さを最小限に抑え、ファイルにアクセスする顧客への中断を最小限に抑えるソリューションを実装する必要がある。このソリューションは、顧客の接続方法を変更してはならない。どのソリューションがこれらの要件を満たすか？",
    "choices": [
      {
        "key": "A",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS",
        "text_jp": "Elastic IPアドレスをEC2インスタンスから外す。SFTPファイルホスティング用にAmazon S3バケットを作成する。AWSを作成する"
      },
      {
        "key": "B",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS",
        "text_jp": "Elastic IPアドレスをEC2インスタンスから外す。SFTPファイルホスティング用にAmazon S3バケットを作成する。AWSを作成する"
      },
      {
        "key": "C",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create a new Amazon Elastic File System (Amazon EFS) file system to be used",
        "text_jp": "Elastic IPアドレスをEC2インスタンスから外す。使用される新しいAmazon Elastic File System（Amazon EFS）ファイルシステムを作成する"
      },
      {
        "key": "D",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create a multi-attach Amazon Elastic Block Store (Amazon EBS) volume to be",
        "text_jp": "Elastic IPアドレスをEC2インスタンスから外す。マルチアタッチAmazon Elastic Block Store（Amazon EBS）ボリュームを作成する"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (88%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating an Amazon Elastic File System (EFS) will improve availability by providing a managed file storage solution that can be accessed by multiple EC2 instances.",
        "situation_analysis": "The company requires a solution to improve availability of the SFTP service without changing how customers connect. The current setup using a single EC2 instance introduces a single point of failure.",
        "option_analysis": "Option A and B involve using Amazon S3 for SFTP which does not fit the requirement of SFTP for file transfer. Option D's multi-attach EBS is not suited for high availability as it still relies on a single EC2 instance, making option C the best choice.",
        "additional_knowledge": "Using EFS, the clients can still connect via SFTP without any changes in their experience.",
        "key_terminology": "Amazon EFS, Amazon EC2, Elastic IP, SFTP, availability",
        "overall_assessment": "Answer C is validated as it directly addresses the problem of availability while meeting the customer's requirement for SFTP access. Community votes show a noticeable preference towards option B but do not correctly address the use case."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。Amazon Elastic File System（EFS）を作成することで、複数のEC2インスタンスがアクセス可能な管理されたファイルストレージソリューションが提供され、可用性が向上する。",
        "situation_analysis": "企業は、顧客の接続方法を変更することなくSFTPサービスの可用性を向上させるソリューションを必要としている。現在の単一のEC2インスタンスの設定は、単一障害点を生じさせている。",
        "option_analysis": "選択肢A及びBはSFTP用のAmazon S3利用を含んでおり、ファイル転送の要件を満たさない。選択肢DのマルチアタッチEBSも単一インスタンスに依存しており、高可用性に適さないため、選択肢Cが最良の選択となる。",
        "additional_knowledge": "EFSを使用することで、クライアントはその体験に変更を加えることなくSFTP経由で接続し続けることができる。",
        "key_terminology": "Amazon EFS, Amazon EC2, Elastic IP, SFTP, 可用性",
        "overall_assessment": "答えCは、可用性の問題に直接対処し、顧客のSFTPアクセス要件を満たすために認証されている。コミュニティの投票では選択肢Bに対して著しい支持が示されているが、ユースケースに適切に対処していない。"
      }
    ],
    "keywords": [
      "Amazon EFS",
      "Amazon EC2",
      "Elastic IP",
      "SFTP",
      "availability"
    ]
  },
  {
    "No": "291",
    "question": "A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics takes 4\nhours to complete. The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run\nfails.\nThe current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations. These EC2 instances run full time to ingest and\nstore the streaming data in attached Amazon Elastic Block Store (Amazon EBS) volumes. A scheduled script launches EC2 On-Demand Instances\neach night to perform the nightly processing. The instances access the stored data from NFS shares on the ingestion servers. The script\nterminates the instances when the processing is complete.\nThe Reserved Instance reservations are expiring. The company needs to determine whether to purchase new reservations or implement a new\ndesign.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "会社はストリーミング市場データを取り込み、処理しています。データレートは一定です。集計統計を計算する夜間プロセスは完了するのに4時間かかります。この統計分析はビジネスにとって重要ではなく、特定の実行が失敗した場合、データポイントは次の繰り返し中に処理されます。現在のアーキテクチャは、1年間の予約があるAmazon EC2リザーブドインスタンスのプールを使用しています。これらのEC2インスタンスは、ストリーミングデータを取り込み、接続されたAmazon Elastic Block Store（Amazon EBS）ボリュームに保存するために常時稼働しています。スケジュールされたスクリプトは、毎晩EC2オンデマンドインスタンスを起動して夜間処理を実行します。インスタンスは、取り込みサーバー上のNFSシェアから保存されたデータにアクセスします。スクリプトは、処理が完了するとインスタンスを終了します。リザーブドインスタンスの予約が期限切れになっています。会社は新しい予約を購入するか、新しい設計を実装するかを決定する必要があります。どのソリューションがこれらの要件を最もコスト効率よく満たすことができますか？",
    "choices": [
      {
        "key": "A",
        "text": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a scheduled script to launch a fieet of",
        "text_jp": "取り込みプロセスを更新して、Amazon Kinesis Data Firehoseを使用してデータをAmazon S3に保存します。スケジュールされたスクリプトを使用して一群の"
      },
      {
        "key": "B",
        "text": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to",
        "text_jp": "取り込みプロセスを更新して、Amazon Kinesis Data Firehoseを使用してデータをAmazon S3に保存します。AWS Batchを使用してスポットインスタンスで"
      },
      {
        "key": "C",
        "text": "Update the ingestion process to use a fieet of EC2 Reserved Instances with 3-year reservations behind a Network LoadBalancer. Use AWS",
        "text_jp": "取り込みプロセスを更新して、Network Load Balancerの背後に3年間の予約済みインスタンスの群を使用します。AWSを使用して"
      },
      {
        "key": "D",
        "text": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use Amazon EventBridge to schedule",
        "text_jp": "取り込みプロセスを更新して、Amazon Kinesis Data Firehoseを使用してデータをAmazon Redshiftに保存します。Amazon EventBridgeを使用してスケジュールします"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Leveraging Amazon Kinesis Data Firehose to save streaming data to Amazon S3 allows for efficient data ingestion without the need for reserved capacity. This reduces costs associated with idle instances.",
        "situation_analysis": "The company processes streaming data in a way that does not require real-time processing. As such, optimizing the data ingestion pipeline for flexibility and cost-effectiveness is crucial during the transition away from expiring Reserved Instances.",
        "option_analysis": "Option A effectively addresses the required ingestion and storage, enabling cost savings. Option B introduces AWS Batch, which may require additional configuration and does not guarantee cost-effectiveness compared to option A. Option C, while offering reserved instances, does not provide flexibility or potentially leads to higher costs in the long run. Option D introduces Amazon Redshift, which is unnecessary for the current requirement and could incur higher costs.",
        "additional_knowledge": "",
        "key_terminology": "Amazon Kinesis Data Firehose, Amazon S3, Cost Efficiency, Serverless Architecture, AWS Batch, Spot Instances, Amazon Redshift",
        "overall_assessment": "Given the conditions described, option A is the most viable solution for cost-effective stream data ingestion, despite community voting indicating option B. This discrepancy may arise from the perceived benefits of AWS Batch; however, it does not align with the overarching need for flexibility without incurring costs from long-term reservations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAである。Amazon Kinesis Data Firehoseを使用してストリーミングデータをAmazon S3に保存することで、リザーブドキャパシティを必要とせずに効率的なデータ取り込みが可能となり、アイドルインスタンスに関連するコストを削減できる。",
        "situation_analysis": "会社はリアルタイム処理を必要としない方法でストリーミングデータを処理している。そのため、リザーブドインスタンスの期限切れに伴う移行期間中に、取り込みパイプラインを柔軟性とコスト効率の観点から最適化することが重要である。",
        "option_analysis": "オプションAは、必要な取り込みと保存を確実に解決し、コスト削減を実現できる。オプションBはAWS Batchを導入し、追加の設定が必要になる可能性があり、Aに比べてコスト効率が良いとは言えない。オプションCは、リザーブドインスタンスを提供するが、柔軟性が欠け、長期的に見ると高いコストが発生する可能性がある。オプションDはAmazon Redshiftを導入し、現在の要件には不要であり、より高いコストが発生する可能性がある。",
        "additional_knowledge": "",
        "key_terminology": "Amazon Kinesis Data Firehose, Amazon S3, コスト効率, サーバーレスアーキテクチャ, AWS Batch, スポットインスタンス, Amazon Redshift",
        "overall_assessment": "記載された条件に対して、オプションAがストリーミングデータのコスト効率的な取り込みを実現する最も有効なソリューションであり、コミュニティの投票がBを示しているにもかかわらず、このディスクリパンシーはAWS Batchの潜在的な利点によるものである可能性があるが、長期的な予約コストを回避するという全体的なニーズには合致しない。"
      }
    ],
    "keywords": [
      "Amazon Kinesis Data Firehose",
      "Amazon S3",
      "Cost Efficiency",
      "Serverless Architecture",
      "AWS Batch",
      "Spot Instances",
      "Amazon Redshift"
    ]
  },
  {
    "No": "292",
    "question": "A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a Linux VM. Uploaded files are made available to\ndownstream applications through an NFS share.\nAs part of the migration to AWS, a solutions architect must implement high availability. The solution must provide external vendors with a set of\nstatic public IP addresses that the vendors can allow. The company has set up an AWS Direct Connect connection between its on-premises data\ncenter and its VPC.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業は、オンプレミスのSFTPサイトをAWSに移行する必要があります。現在、SFTPサイトはLinux VM上で動作しています。アップロードされたファイルは、NFS共有を介して下流のアプリケーションに提供されています。AWSへの移行の一環として、ソリューションアーキテクトは高可用性を実装する必要があります。このソリューションは、外部のベンダーに対して、ベンダーが許可できる一連の静的なパブリックIPアドレスを提供しなければなりません。企業は、オンプレミスのデータセンターとVPC間にAWS Direct Connect接続を設定しています。最小限の運用オーバーヘッドでこれらの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Transfer Family server. Configure an internet-facing VPC endpoint for the Transfer Family server. Specify an Elastic IP",
        "text_jp": "AWS Transfer Familyサーバーを作成する。Transfer Familyサーバーのためにインターネット向けのVPCエンドポイントを構成する。Elastic IPを指定する。"
      },
      {
        "key": "B",
        "text": "Create an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family",
        "text_jp": "AWS Transfer Familyサーバーを作成する。Transfer Familyサーバーのためにパブリックにアクセス可能なエンドポイントを設定する。Transfer Familyを構成する。"
      },
      {
        "key": "C",
        "text": "Use AWS Application Migration Service to migrate the existing Linux VM to an Amazon EC2 instance. Assign an Elastic IP address to the",
        "text_jp": "AWSアプリケーションマイグレーションサービスを使用して、既存のLinux VMをAmazon EC2インスタンスに移行する。Elastic IPアドレスを割り当てる。"
      },
      {
        "key": "D",
        "text": "Use AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Family server. Configure a publicly accessible",
        "text_jp": "AWSアプリケーションマイグレーションサービスを使用して、既存のLinux VMをAWS Transfer Familyサーバーに移行する。パブリックにアクセス可能なエンドポイントを設定する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Create an AWS Transfer Family server with a publicly accessible endpoint and configure it accordingly.",
        "situation_analysis": "The company requires high availability for their SFTP service and needs to provide static public IP addresses to external vendors. Additionally, they already have an AWS Direct Connect connection established.",
        "option_analysis": "Option B provides a managed service that inherently supports high availability and allows public access, fulfilling the requirements efficiently. Option A creates an unnecessary VPC endpoint, while Options C and D involve more operational complexity and may not directly provide static IPs as needed.",
        "additional_knowledge": "Using AWS Transfer Family also ensures that the setup is easily maintained and managed by AWS.",
        "key_terminology": "AWS Transfer Family, Elastic IP, public endpoint, high availability, SFTP, AWS Direct Connect",
        "overall_assessment": "The choice B aligns optimally with the requirements of high availability and low operational overhead. While community voting favors other options, they may be misinformed regarding the operational ease of a managed service."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはB：AWS Transfer Familyサーバーを作成し、パブリックにアクセス可能なエンドポイントを設定することである。",
        "situation_analysis": "企業はSFTPサービスの高可用性を必要としており、外部ベンダーに静的なパブリックIPアドレスを提供する必要があります。さらに、AWS Direct Connect接続が設定済みです。",
        "option_analysis": "選択肢Bは、高可用性をサポートし、パブリックアクセスを可能にするマネージドサービスを提供し、要件を効率的に満たします。一方、選択肢Aは不要なVPCエンドポイントを作成し、選択肢CとDは運用上の複雑さが増し、必要な静的IPを直接提供しない可能性があります。",
        "additional_knowledge": "AWS Transfer Familyを使用すると、設定がAWSによって簡単に管理・運用されることが保証されます。",
        "key_terminology": "AWS Transfer Family、Elastic IP、パブリックエンドポイント、高可用性、SFTP、AWS Direct Connect",
        "overall_assessment": "選択肢Bは高可用性と低い運用オーバーヘッドの要件に最適に一致しています。コミュニティの投票が他の選択肢を優先していますが、マネージドサービスの容易さについて誤解している可能性があります。"
      }
    ],
    "keywords": [
      "AWS Transfer Family",
      "Elastic IP",
      "public endpoint",
      "high availability",
      "SFTP",
      "AWS Direct Connect"
    ]
  },
  {
    "No": "293",
    "question": "A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two\nAvailability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and\nconnectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as\nfollows:\nVPC CIDR: 10.0.0.0/23 -\nAZ1 subnet CIDR: 10.0.0.0/24 -\nAZ2 subnet CIDR: 10.0.1.0/24 -\nSince deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional\nIPv4 address space and without service downtime. Which solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、Auto Scalingグループ内のAmazon EC2インスタンスにデプロイされた運用ワークロードを持っています。VPCアーキテクチャは、Auto Scalingグループがターゲットにしている各サブネットを持つ2つのアベイラビリティゾーン（AZ）にまたがっています。VPCはオンプレミス環境に接続されており、接続を中断することはできません。Auto Scalingグループの最大サイズは、サービス中のインスタンスが20です。VPCのIPv4アドレッシングは次のとおりです：\nVPC CIDR: 10.0.0.0/23 -\nAZ1サブネットCIDR: 10.0.0.0/24 -\nAZ2サブネットCIDR: 10.0.1.0/24 -\nデプロイ以来、リージョンに新しいAZが利用可能になりました。ソリューションアーキテクトは、追加のIPv4アドレス空間を追加することなく、新しいAZをサービスのダウンタイムなしに採用したいと考えています。どの解決策がこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space.",
        "text_jp": "Auto ScalingグループをAZ2サブネットのみを使用するように更新します。AZ1サブネットを削除して再作成し、以前のアドレス空間の半分を使用します。"
      },
      {
        "key": "B",
        "text": "Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling",
        "text_jp": "AZ1サブネット内のEC2インスタンスを終了します。AZ1サブネットを削除して再作成し、アドレス空間の半分を使用します。Auto Scalingを更新します。"
      },
      {
        "key": "C",
        "text": "Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling",
        "text_jp": "同じIPv4アドレス空間を持つ新しいVPCを作成し、各AZ用の1つのサブネットを定義します。既存のAuto Scalingを更新します。"
      },
      {
        "key": "D",
        "text": "Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have half the previous address space. Adjust the Auto",
        "text_jp": "Auto ScalingグループをAZ2サブネットのみを使用するように更新します。AZ1サブネットを以前のアドレス空間の半分に更新します。Autoを調整します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (82%) D (18%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This option allows for the adoption of the new AZ without adding additional IPv4 address space and without causing service downtime.",
        "situation_analysis": "The requirements are to leverage a new AZ while ensuring existing connectivity remains uninterrupted, and the address space is not expanded.",
        "option_analysis": "Option A cannot be selected because it fails to retain AZ1's instances and leads to downtime. Option B is also not feasible since it requires terminating instances. Option C suggests creating a new VPC, which would cause service disruption and complexity.",
        "additional_knowledge": "Using existing resources without downtime aligns with best practices in cloud architecture.",
        "key_terminology": "Auto Scaling, Availability Zone, EC2, Subnet, VPC",
        "overall_assessment": "This question tests understanding of AWS networking concepts and the ability to manage resource allocation effectively across Availability Zones. The community's preference for option A may indicate a misunderstanding of the requirement for continuous service availability."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。この選択肢は、新しいAZを採用しながら、追加のIPv4アドレス空間を追加せず、サービスのダウンタイムを引き起こさずに済む。",
        "situation_analysis": "既存の接続が中断されないことを保証しつつ、新しいAZを活用することが求められている。アドレス空間は拡張しないこと。",
        "option_analysis": "選択肢Aは、AZ1のインスタンスを維持せず、ダウンタイムを引き起こすため選べない。選択肢Bも、インスタンスを終了する必要があるため実現不可能である。選択肢Cは新しいVPCの作成を提案しており、サービスの中断と複雑さを引き起こす。",
        "additional_knowledge": "ダウンタイムなしで既存のリソースを使用することは、クラウドアーキテクチャのベストプラクティスに沿っている。",
        "key_terminology": "Auto Scaling、アベイラビリティゾーン、EC2、サブネット、VPC",
        "overall_assessment": "この質問は、AWSのネットワーク概念の理解と、アベイラビリティゾーン間でのリソース配分を効果的に管理する能力をテストしている。選択肢Aへのコミュニティの支持は、継続的なサービスの可用性に関する理解の誤りを示すかもしれない。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "Availability Zone",
      "EC2",
      "Subnet",
      "VPC"
    ]
  },
  {
    "No": "294",
    "question": "A company uses an organization in AWS Organizations to manage the company's AWS accounts. The company uses AWS CloudFormation to\ndeploy all infrastructure. A finance team wants to build a chargeback model. The finance team asked each business unit to tag resources by using\na predefined list of project values.\nWhen the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based on project, the team noticed noncompliant\nproject values. The company wants to enforce the use of project tags for new resources.\nWhich solution will meet these requirements with the LEAST effort?",
    "question_jp": "企業はAWS Organizationsを使用して、企業のAWSアカウントを管理しています。企業はすべてのインフラをデプロイするためにAWS CloudFormationを使用しています。財務チームは課金モデルを構築したいと考えています。財務チームは、各ビジネスユニットに事前定義されたプロジェクト値のリストを使用してリソースにタグを付けるように依頼しました。財務チームはAWS Cost ExplorerのAWS Cost and Usage Reportを使用し、プロジェクトごとにフィルタリングしたところ、非準拠のプロジェクト値に気付きました。企業は新しいリソースに対してプロジェクトタグの使用を強制したいと考えています。この要件を最小限の労力で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a tag policy that contains the allowed project tag values in the organization's management account. Create an SCP that denies the",
        "text_jp": "組織の管理アカウントに許可されたプロジェクトタグ値を含むタグポリシーを作成します。SCPを作成してこれを拒否します。"
      },
      {
        "key": "B",
        "text": "Create a tag policy that contains the allowed project tag values in each OU. Create an SCP that denies the cloudformation:CreateStack API",
        "text_jp": "各OUに許可されたプロジェクトタグ値を含むタグポリシーを作成します。SCPを作成してcloudformation:CreateStack APIを拒否します。"
      },
      {
        "key": "C",
        "text": "Create a tag policy that contains the allowed project tag values in the AWS management account. Create an IAM policy that denies the",
        "text_jp": "AWS管理アカウントに許可されたプロジェクトタグ値を含むタグポリシーを作成します。IAMポリシーを作成してそれを拒否します。"
      },
      {
        "key": "D",
        "text": "Use AWS Service Catalog to manage the CloudFormation stacks as products. Use a TagOptions library to control project tag values. Share",
        "text_jp": "AWS Service Catalogを使用してCloudFormationスタックを製品として管理します。TagOptionsライブラリを使用してプロジェクトタグ値を制御します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Creating a tag policy that contains the allowed project tag values in the AWS management account and an IAM policy that denies noncompliance ensures that project tags are enforced with minimal effort.",
        "situation_analysis": "The company wants to enforce the use of project tags for new resources. The finance team found noncompliant project values when filtering cost reports. It is crucial to enforce tagging consistently.",
        "option_analysis": "Option A only suggests a tag policy but lacks detail on enforcement for existing stacks. Option B introduces complexity with per-OU tag policies. Option C effectively combines a tag policy with IAM enforcement. Option D, while a valid strategy for managing resources, introduces additional complexity without directly addressing the tagging issue.",
        "additional_knowledge": "Focusing on IAM policies in the AWS management account can centralize control over tagging compliance.",
        "key_terminology": "AWS Organizations, AWS CloudFormation, IAM policy, tag policy, cost management",
        "overall_assessment": "Answer C is optimal in terms of both effort and effectiveness, ensuring compliance with tagging standards without introducing unnecessary complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです：AWS管理アカウントに許可されたプロジェクトタグ値を含むタグポリシーを作成し、非準拠を拒否するIAMポリシーを作成することで、最小限の労力でプロジェクトタグを強制できます。",
        "situation_analysis": "企業は新しいリソースに対してプロジェクトタグの使用を強制したいと考えています。財務チームはコストレポートをフィルタリングした際に非準拠のプロジェクト値を発見しました。タグ付けを一貫して強制することが重要です。",
        "option_analysis": "選択肢Aはタグポリシーの提案のみですが、既存スタックへの強制に関する詳細が不足しています。選択肢BはOUごとのタグポリシーという複雑さを導入します。選択肢CはタグポリシーとIAMによる強制を効果的に組み合わせています。選択肢Dはリソース管理のための有効な戦略ですが、タグ付けの問題を直接解決することなく、追加の複雑さを導入します。",
        "additional_knowledge": "AWS管理アカウント内のIAMポリシーに焦点を当てることで、タグ付け遵守に対する管理を集中化できます。",
        "key_terminology": "AWS Organizations、AWS CloudFormation、IAMポリシー、タグポリシー、コスト管理",
        "overall_assessment": "答えCは、労力と効果の両方において最適であり、不要な複雑さを導入することなくタグ付け基準への遵守を確保します。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS CloudFormation",
      "IAM Policy",
      "Tag Policy",
      "Cost Management"
    ]
  },
  {
    "No": "295",
    "question": "An application is deployed on Amazon EC2 instances that run in an Auto Scaling group. The Auto Scaling group configuration uses only one type\nof instance.\nCPU and memory utilization metrics show that the instances are underutilized. A solutions architect needs to implement a solution to permanently\nreduce the EC2 cost and increase the utilization.\nWhich solution will meet these requirements with the LEAST number of configuration changes in the future?",
    "question_jp": "アプリケーションは、Auto Scaling グループで実行される Amazon EC2 インスタンスにデプロイされています。Auto Scaling グループの構成では、1 種類のインスタンスのみが使用されています。CPU とメモリの利用状況のメトリクスは、インスタンスが過少利用されていることを示しています。ソリューションアーキテクトは、EC2 コストを恒久的に削減し、利用率を向上させるためのソリューションを実装する必要があります。将来的に最も少ない構成変更でこの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "List instance types that have properties that are similar to the properties that the current instances have. Modify the Auto Scaling group's",
        "text_jp": "現在のインスタンスが持つプロパティに類似したプロパティを持つインスタンスタイプのリストを作成します。Auto Scaling グループを変更します。"
      },
      {
        "key": "B",
        "text": "Use the information about the application's CPU and memory utilization to select an instance type that matches the requirements. Modify",
        "text_jp": "アプリケーションの CPU とメモリの利用状況に関する情報を使用して、要件に合ったインスタンスタイプを選択します。変更します。"
      },
      {
        "key": "C",
        "text": "Use the information about the application's CPU and memory utilization to specify CPU and memory requirements in a new revision of the",
        "text_jp": "アプリケーションの CPU とメモリの利用状況に関する情報を使用して、新しいリビジョンの CPU とメモリの要件を指定します。"
      },
      {
        "key": "D",
        "text": "Create a script that selects the appropriate instance types from the AWS Price List Bulk API. Use the selected instance types to create a",
        "text_jp": "AWS Price List Bulk API から適切なインスタンスタイプを選択するスクリプトを作成します。選択されたインスタンスタイプを使用して作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (70%) B (30%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct answer is B: Use the information about the application's CPU and memory utilization to select an instance type that matches the requirements.",
        "situation_analysis": "Current instances are underutilized, indicating that lower-cost instances may be sufficient to handle the workload while still maintaining performance.",
        "option_analysis": "Option B allows for matching instance types to the actual utilization metrics, effectively reducing costs while potentially increasing performance due to optimization of resource usage. Other options may not lead to as efficient cost management.",
        "additional_knowledge": "It's crucial to regularly analyze resource usage and adjust instance types accordingly.",
        "key_terminology": "Auto Scaling, EC2, instance types, CPU utilization, memory utilization.",
        "overall_assessment": "B is the best choice as it directly addresses the need for cost reduction with minimal future configuration changes based on actual utilization metrics. Other options are either less specific or involve more complex modification processes."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は B：アプリケーションの CPU とメモリの利用状況に関する情報を使用して、要件に合ったインスタンスタイプを選択します。",
        "situation_analysis": "現在のインスタンスは過少利用されており、コストの低いインスタンスであってもワークロードを処理できる可能性があります。これにより、パフォーマンスを維持しつつコストを削減できます。",
        "option_analysis": "選択肢 B は実際の利用状況の指標に基づいてインスタンスタイプを見直すことができ、コスト削減を実現しつつ、リソース使用の最適化を図る可能性があります。その他の選択肢は、コスト管理の効率が悪い可能性があります。",
        "additional_knowledge": "リソース利用状況を定期的に分析し、それに応じてインスタンスタイプを調整することが重要です。",
        "key_terminology": "Auto Scaling、EC2、インスタンスタイプ、CPU 利用率、メモリ利用率。",
        "overall_assessment": "B は、実際の利用状況に基づく最小限の将来的な構成変更でコスト削減が必要なニーズに直接対応しているため、最良の選択肢です。その他の選択肢は、より複雑な変更プロセスを伴うため望ましくありません。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "EC2",
      "instance types",
      "CPU utilization",
      "memory utilization"
    ]
  },
  {
    "No": "296",
    "question": "A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway The\napplication data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning\nby using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.\nA solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある会社が Amazon Elastic Container Service (Amazon ECS) と Amazon API Gateway を使用してコンテナ化されたアプリケーションを実装しています。このアプリケーションデータは Amazon Aurora データベースと Amazon DynamoDB データベースに保存されています。会社は AWS CloudFormation を使用してインフラのプロビジョニングを自動化しています。また、アプリケーションのデプロイは AWS CodePipeline を使用して自動化されています。ソリューションアーキテクトは、RPO（復旧ポイント目標）が2時間、RTO（復旧時間目標）が4時間の要件を満たす災害復旧（DR）戦略を実装する必要があります。どのソリューションが最もコスト効果的にこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary",
        "text_jp": "Aurora グローバルデータベースと DynamoDB グローバルテーブルを設定し、データベースをセカンダリの AWS リージョンにレプリケートします。プライマリで"
      },
      {
        "key": "B",
        "text": "Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS Lambda to replicate the Aurora databases to a secondary",
        "text_jp": "AWS Database Migration Service (AWS DMS)、Amazon EventBridge、および AWS Lambda を使用して Aurora データベースをセカンダリにレプリケートします"
      },
      {
        "key": "C",
        "text": "Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary",
        "text_jp": "AWS Backup を使用して Aurora データベースと DynamoDB データベースのバックアップをセカンダリの AWS リージョンに作成します。プライマリで"
      },
      {
        "key": "D",
        "text": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary",
        "text_jp": "Aurora グローバルデータベースと DynamoDB グローバルテーブルを設定し、データベースをセカンダリの AWS リージョンにレプリケートします。プライマリで"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (57%) D (39%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Setting up an Aurora global database and DynamoDB global tables allows for near real-time replication of data across regions, which meets the required RPO and RTO.",
        "situation_analysis": "The company's requirements for disaster recovery necessitate a solution that minimizes downtime and data loss. An RPO of 2 hours indicates that data should not be older than this duration, while an RTO of 4 hours signifies that services must be restored within this timeframe.",
        "option_analysis": "Option A leverages built-in AWS services to provide robust replication, while option B introduces unnecessary complexity with AWS DMS. Option C does provide backups but is not as effective for maintaining RPO and RTO compared to real-time replication. The duplicated statement in D is redundant as it repeats A.",
        "additional_knowledge": "Leveraging AWS services like CloudFormation and CodePipeline can also enhance automation during DR events.",
        "key_terminology": "Disaster Recovery, RPO, RTO, Aurora Global Database, DynamoDB Global Tables",
        "overall_assessment": "Option A was chosen based on its cost-effectiveness and alignment with disaster recovery objectives. The community's support of options C and D may suggest confusion about the necessity of data replication vs. backup strategies in DR planning."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは A である。Aurora グローバルデータベースと DynamoDB グローバルテーブルを設定することで、地域間でのデータのほぼリアルタイムレプリケーションが可能となり、必要な RPO および RTO を満たす。",
        "situation_analysis": "会社の災害復旧要件は、ダウンタイムとデータ損失を最小限に抑えるソリューションを必要とする。RPO が 2 時間ということは、この時間内より古いデータを持つべきではなく、RTO が 4 時間ということは、サービスをこの時間内に復旧させる必要があることを示す。",
        "option_analysis": "選択肢 A は、AWS の組み込みサービスを利用して堅牢なレプリケーションを提供し、選択肢 B は AWS DMS による不必要な複雑さを導入する。選択肢 C はバックアップを提供するが、リアルタイムレプリケーションに比べて RPO および RTO を維持するには効果的ではない。選択肢 D は、A と重複した内容のため冗長である。",
        "additional_knowledge": "AWS のサービスとして、CloudFormation と CodePipeline を利用することで、DR イベント中の自動化をさらに強化できる。",
        "key_terminology": "災害復旧, RPO, RTO, Aurora グローバルデータベース, DynamoDB グローバルテーブル",
        "overall_assessment": "コスト効果及び災害復旧の目標に整合していることから、選択肢 A が選ばれた。コミュニティの選択肢 C と D の支持は、DR 計画におけるデータレプリケーションとバックアップ戦略の必要性についての混乱を示唆しているかもしれない。"
      }
    ],
    "keywords": [
      "Disaster Recovery",
      "RPO",
      "RTO",
      "Aurora Global Database",
      "DynamoDB Global Tables"
    ]
  },
  {
    "No": "297",
    "question": "A company has a complex web application that leverages Amazon CloudFront for global scalability and performance. Over time, users report that\nthe web application is slowing down.\nThe company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. The cache metrics report indicates that\nquery strings on some URLs are inconsistently ordered and are specified sometimes in mixed-case letters and sometimes in lowercase letters.\nWhich set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?",
    "question_jp": "ある企業が、Amazon CloudFrontを利用してグローバルにスケーラブルなパフォーマンスを実現している複雑なウェブアプリケーションを持っています。時間が経つにつれ、ユーザーからウェブアプリケーションの動作が遅くなっていると報告されています。企業のオペレーションチームは、CloudFrontのキャッシュヒット率が着実に低下していることを報告しています。キャッシュメトリックの報告によると、いくつかのURLのクエリ文字列が不整合に並べ替えられ、時には大文字および小文字が混在して指定されていることが示されています。ソリューションアーキテクトがキャッシュヒット率を迅速に向上させるために取るべき一連のアクションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger",
        "text_jp": "Lambda@Edge関数をデプロイして、パラメータを名前でソートし、小文字に強制変換します。CloudFrontビューワーリクエストトリガーを選択します。"
      },
      {
        "key": "B",
        "text": "Update the CloudFront distribution to disable caching based on query string parameters.",
        "text_jp": "クエリ文字列パラメータに基づくキャッシュを無効にするようCloudFrontディストリビューションを更新します。"
      },
      {
        "key": "C",
        "text": "Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase.",
        "text_jp": "アプリケーションで出力されるURLを後処理するために、ロードバランサーの後にリバースプロキシをデプロイし、URL文字列を小文字に強制変換します。"
      },
      {
        "key": "D",
        "text": "Update the CloudFront distribution to specify casing-insensitive query string processing.",
        "text_jp": "CloudFrontディストリビューションを更新して、大文字小文字を区別しないクエリ文字列処理を指定します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, which involves deploying a Lambda@Edge function to sort query parameters by name and converting them to lowercase to ensure consistent cache hits.",
        "situation_analysis": "The company is experiencing a decline in CloudFront cache hit ratio due to inconsistent ordering and casing of query string parameters. This leads to cache misses as different parameter appearances create separate cache objects.",
        "option_analysis": "Option A effectively addresses the issue by modifying the query string parameters before caching occurs. Option B eliminates query string caching, which is detrimental to performance. Option C is less efficient as it involves additional processing after load balancing. Option D, while offering a solution, is not a part of CloudFront capabilities at this moment.",
        "additional_knowledge": "Applying transformations via Lambda@Edge not only improves cache efficiencies but also assists with SEO and uniformity in application requests.",
        "key_terminology": "CloudFront, Lambda@Edge, cache hit ratio, query string parameters, caching strategy",
        "overall_assessment": "The question effectively evaluates the understanding of caching strategies and the use of Lambda@Edge within CloudFront distributions. Community support overwhelmingly verifies option A as the preferred solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAであり、Lambda@Edge関数をデプロイしてクエリパラメータを名前でソートし、小文字に変換することで、一貫したキャッシュヒットを確保します。",
        "situation_analysis": "企業はクエリ文字列パラメータの不整合な順序と大文字小文字によるCloudFrontキャッシュヒット率の低下を経験しています。これにより、異なるパラメータの出現が別々のキャッシュオブジェクトを作成し、キャッシュミスを引き起こしています。",
        "option_analysis": "オプションAは、キャッシュが行われる前にクエリ文字列パラメータを修正することで、問題に効果的に対処します。オプションBはクエリ文字列キャッシュを排除するため、パフォーマンスに悪影響を及ぼします。オプションCは、ロードバランシングの後に追加処理が必要であり、効率が低下します。オプションDは解決策を提供しますが、現在のところCloudFrontの機能には含まれていません。",
        "additional_knowledge": "Lambda@Edgeを介て変換を適用することにより、キャッシュの効率が向上するだけでなく、アプリケーションリクエストの均一性にも役立ちます。",
        "key_terminology": "CloudFront, Lambda@Edge, キャッシュヒット率, クエリ文字列パラメータ, キャッシング戦略",
        "overall_assessment": "この質問は、キャッシング戦略とCloudFrontディストリビューション内でのLambda@Edgeの使用を理解する能力を効果的に評価しています。コミュニティは圧倒的にオプションAを優先的な解決策として支持しています。"
      }
    ],
    "keywords": [
      "CloudFront",
      "Lambda@Edge",
      "cache hit ratio",
      "query string parameters",
      "caching strategy"
    ]
  },
  {
    "No": "298",
    "question": "A company runs an ecommerce application in a single AWS Region. The application uses a five-node Amazon Aurora MySQL DB cluster to store\ninformation about customers and their recent orders. The DB cluster experiences a large number of write transactions throughout the day.\nThe company needs to replicate the data in the Aurora database to another Region to meet disaster recovery requirements. The company has an\nRPO of 1 hour.\nWhich solution will meet these requirements with the LOWEST cost?",
    "question_jp": "ある企業が一つのAWSリージョンでeコマースアプリケーションを運営しています。このアプリケーションは、顧客およびその最近の注文に関する情報を格納するために、5ノードのAmazon Aurora MySQL DBクラスターを使用しています。DBクラスターは、1日の間に多くの書き込みトランザクションを経験しています。企業は、災害復旧要件を満たすために、Auroraデータベースのデータを別のリージョンに複製する必要があります。企業には1時間のRPOがあります。この要件を最も低コストで満たす解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Modify the Aurora database to be an Aurora global database. Create a second Aurora database in another Region.",
        "text_jp": "AuroraデータベースをAuroraグローバルデータベースに変更する。別のリージョンに2番目のAuroraデータベースを作成する。"
      },
      {
        "key": "B",
        "text": "Enable the Backtrack feature for the Aurora database. Create an AWS Lambda function that runs daily to copy the snapshots of the",
        "text_jp": "AuroraデータベースのBacktrack機能を有効にする。毎日スナップショットをコピーするAWS Lambda関数を作成する。"
      },
      {
        "key": "C",
        "text": "Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from",
        "text_jp": "AWSデータベース移行サービス（AWS DMS）を使用する。DMSの変更データキャプチャ（CDC）タスクを作成し、"
      },
      {
        "key": "D",
        "text": "Turn off automated Aurora backups. Configure Aurora backups with a backup frequency of 1 hour. Specify another Region as the",
        "text_jp": "自動Auroraバックアップをオフにする。バックアップの頻度を1時間に設定して、別のリージョンを指定する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (77%) A (23%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which utilizes AWS Database Migration Service (AWS DMS) for ongoing data replication with a focus on cost-efficiency.",
        "situation_analysis": "The company requires data replication to another region to comply with disaster recovery policies, with a recovery point objective (RPO) of 1 hour.",
        "option_analysis": "Option C provides continuous data replication while minimizing costs. Other options either involve higher costs or do not meet the required RPO effectively.",
        "additional_knowledge": "AWS DMS is a managed service that simplifies the migration and replication process significantly.",
        "key_terminology": "AWS Database Migration Service, Change Data Capture, Aurora, disaster recovery, RPO",
        "overall_assessment": "C is the most suitable option considering both cost and RPO requirements, as community votes reflect a strong preference for this choice with 77%."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCであり、AWSデータベース移行サービス（AWS DMS）を利用して、コスト効率を重視した継続的なデータ複製を実施します。",
        "situation_analysis": "企業は、災害復旧ポリシーに準拠するために、1時間の回復時点目標（RPO）を持つ別のリージョンへのデータ複製を必要としています。",
        "option_analysis": "選択肢Cは、コストを抑えながら継続的なデータ複製を提供します。他の選択肢は、コストが高いか、必要なRPOを効果的に満たしていない可能性があります。",
        "additional_knowledge": "AWS DMSは、移行および複製プロセスを大幅に簡素化するマネージドサービスです。",
        "key_terminology": "AWSデータベース移行サービス, 変更データキャプチャ, Aurora, 災害復旧, RPO",
        "overall_assessment": "Cは、コストとRPO要件を考慮した最も適切な選択肢であり、コミュニティの投票結果も77%でこの選択肢に強く支持されています。"
      }
    ],
    "keywords": [
      "AWS Database Migration Service",
      "Change Data Capture",
      "Aurora",
      "disaster recovery",
      "RPO"
    ]
  }
]