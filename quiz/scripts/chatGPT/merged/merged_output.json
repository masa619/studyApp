[
  {
    "No": "1",
    "question": "A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain\ncloud.example.com for the resources stored within VPCs.\nThe company has the following DNS resolution requirements:\nOn-premises systems should be able to resolve and connect to cloud.example.com.\nAll VPCs should be able to resolve cloud.example.com.\nThere is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.\nWhich architecture should the company use to meet these requirements with the HIGHEST performance?",
    "question_jp": "会社はハイブリッドDNSソリューションを設計する必要がある。このソリューションは、VPC内に格納されているリソースのために、ドメインcloud.example.comに対してAmazon Route 53プライベートホステッドゾーンを使用する。この会社には以下のDNS解決要件がある：オンプレミスシステムはcloud.example.comを解決し、接続できる必要がある。すべてのVPCはcloud.example.comを解決できる必要がある。すでにオンプレミスの企業ネットワークとAWS Transit Gatewayとの間にAWS Direct Connect接続がある。最高のパフォーマンスでこれらの要件を満たすために、会社はどのアーキテクチャを使用すべきか？",
    "choices": [
      {
        "key": "A",
        "text": "Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.",
        "text_jp": "プライベートホステッドゾーンをすべてのVPCに関連付ける。共有サービスVPCにRoute 53インバウンドリゾルバーを作成する。すべてのVPCをトランジットゲートウェイに接続し、オンプレミスDNSサーバーにcloud.example.comのための転送ルールを作成し、インバウンドリゾルバーを指す。"
      },
      {
        "key": "B",
        "text": "Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder.",
        "text_jp": "プライベートホステッドゾーンをすべてのVPCに関連付ける。共有サービスVPCにAmazon EC2条件付きフォワーダーをデプロイする。すべてのVPCをトランジットゲートウェイに接続し、オンプレミスDNSサーバーにcloud.example.comのための転送ルールを作成し、条件付きフォワーダーを指す。"
      },
      {
        "key": "C",
        "text": "Associate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver.",
        "text_jp": "プライベートホステッドゾーンを共有サービスVPCに関連付ける。共有サービスVPCにRoute 53アウトバウンドリゾルバーを作成する。すべてのVPCをトランジットゲートウェイに接続し、オンプレミスDNSサーバーにcloud.example.comのための転送ルールを作成し、アウトバウンドリゾルバーを指す。"
      },
      {
        "key": "D",
        "text": "Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.",
        "text_jp": "プライベートホステッドゾーンを共有サービスVPCに関連付ける。共有サービスVPCにRoute 53インバウンドリゾルバーを作成する。共有サービスVPCをトランジットゲートウェイに接続し、オンプレミスDNSサーバーにcloud.example.comのための転送ルールを作成し、インバウンドリゾルバーを指す。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (74%) D (26%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, which allows the on-premises system to resolve the domain efficiently via the inbound resolver.",
        "situation_analysis": "The company needs to ensure that both on-premises systems and all VPCs can resolve the domain cloud.example.com. An existing AWS Direct Connect connection enhances connectivity and reduces latency.",
        "option_analysis": "Option A focuses on using an inbound resolver, which is essential for this scenario, making it a solid option. However, it doesn't necessarily state its interaction with the VPCs. Option B introduces a conditional forwarder which is less optimal. Option C utilizes an outbound resolver, which is generally used for resolving external domains, not suitable here. Option D correctly establishes a direct link through the inbound resolver for the shared services VPC, which is optimal.",
        "additional_knowledge": "",
        "key_terminology": "Amazon Route 53, inbound resolver, Direct Connect, Transit Gateway, private hosted zone.",
        "overall_assessment": "Considering the high-performance requirement and the existing infrastructure, option D clearly provides the best architecture for the scenario presented."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDであり、オンプレミスシステムがインバウンドリゾルバーを介してドメインを効率的に解決できるようにする。",
        "situation_analysis": "会社は、オンプレミスシステムとすべてのVPCがcloud.example.comというドメインを解決できるようにする必要がある。既存のAWS Direct Connect接続は、接続性を向上させ、レイテンシを低減する。",
        "option_analysis": "選択肢Aは、インバウンドリゾルバーを使用することに注目しており、このシナリオに対して必要不可欠であるため、良い選択肢である。しかし、VPCとの相互作用が必ずしも明記されていない。選択肢Bは条件付きフォワーダーを導入しており、最適ではない。選択肢Cはアウトバウンドリゾルバーを使用しており、通常は外部ドメインの解決に使用されるため、このケースには適していない。選択肢Dは、共有サービスVPCのためにインバウンドリゾルバーを介して直接接続を確立し、最適である。",
        "additional_knowledge": "",
        "key_terminology": "Amazon Route 53, インバウンドリゾルバー, Direct Connect, トランジットゲートウェイ, プライベートホステッドゾーン。",
        "overall_assessment": "高パフォーマンスの要件と既存のインフラストラクチャを考慮すると、選択肢Dが提示されたシナリオに対して明らかに最良のアーキテクチャを提供する。"
      }
    ],
    "keywords": [
      "Amazon Route 53",
      "inbound resolver",
      "Direct Connect",
      "Transit Gateway",
      "private hosted zone"
    ]
  },
  {
    "No": "2",
    "question": "A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated\nwith different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of\nweather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the\nability to fail over to a different AWS Region.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、複数の顧客に対してRESTベースのAPIを通じて気象データを提供しています。このAPIはAmazon API Gatewayによってホストされ、各API操作のために異なるAWS Lambda関数と統合されています。企業はAmazon Route 53を使用してDNSを管理し、weather.example.comというリソースレコードを作成しています。企業はAPIのデータをAmazon DynamoDBテーブルに保存しています。企業は、APIが別のAWSリージョンにフェイルオーバーできるソリューションを必要としています。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.",
        "text_jp": "新しいリージョンに新しいセットのLambda関数をデプロイします。API Gateway APIをエッジ最適化APIエンドポイントに更新し、両方のリージョンからのLambda関数をターゲットとして使用します。DynamoDBテーブルをグローバルテーブルに変換します。"
      },
      {
        "key": "B",
        "text": "Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.",
        "text_jp": "別のリージョンに新しいAPI Gateway APIとLambda関数をデプロイします。Route 53のDNSレコードをマルチバリュー応答に変更します。両方のAPI Gateway APIを応答に追加します。ターゲットのヘルスモニタリングを有効にします。DynamoDBテーブルをグローバルテーブルに変換します。"
      },
      {
        "key": "C",
        "text": "Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.",
        "text_jp": "別のリージョンに新しいAPI Gateway APIとLambda関数をデプロイします。Route 53のDNSレコードをフェイルオーバーレコードに変更します。ターゲットのヘルスモニタリングを有効にします。DynamoDBテーブルをグローバルテーブルに変換します。"
      },
      {
        "key": "D",
        "text": "Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.",
        "text_jp": "新しいリージョンに新しいAPI Gateway APIをデプロイします。Lambda関数をグローバル関数に変更します。Route 53のDNSレコードをマルチバリュー応答に変更します。両方のAPI Gateway APIを応答に追加します。ターゲットのヘルスモニタリングを有効にします。DynamoDBテーブルをグローバルテーブルに変換します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, as it focuses on deploying a new API Gateway API and Lambda functions in another region and changing the Route 53 DNS record to a failover record which ensures high availability.",
        "situation_analysis": "The company needs to ensure that if the primary API hosted in one AWS Region goes down, the API can failover to another Region seamlessly, thus ensuring availability.",
        "option_analysis": "Option A is incorrect because using an edge-optimized API endpoint with Lambda functions from both regions complicates the architecture. Option B suggests a multivalue answer but does not specifically address the failover requirement as well as option C. Option D involves unnecessary changes to Lambda functions as global functions.",
        "additional_knowledge": "Utilizing health checks and ensuring global tables for DynamoDB is essential for protecting against data loss and maintaining service reliability.",
        "key_terminology": "AWS Lambda, Amazon API Gateway, Amazon Route 53, DynamoDB global tables, failover, high availability.",
        "overall_assessment": "The solution provided in option C is simple yet effective for ensuring high availability of the API across regions. The community vote overwhelmingly supports this option."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。これは、新しいリージョンに新しいAPI Gateway APIとLambda関数をデプロイし、Route 53のDNSレコードをフェイルオーバーレコードに変更することに重点を置くため、高可用性を確保します。",
        "situation_analysis": "この企業は、1つのAWSリージョンでホストされている主要なAPIがダウンした場合に、APIが別のリージョンにシームレスにフェイルオーバーできることを確保する必要があります。これにより、可用性が維持されます。",
        "option_analysis": "オプションAは、エッジ最適化APIエンドポイントを使用し、両方のリージョンのLambda関数をターゲットとすることでアーキテクチャが複雑化するため不正解です。オプションBはマルチバリュー応答を提案しますが、Cと同様にフェイルオーバー要件を特に満たしていません。オプションDは、グローバル関数としてのLambda関数への不必要な変更が含まれています。",
        "additional_knowledge": "ヘルスチェックを利用し、DynamoDBのグローバルテーブルを確保することが、データ損失から保護し、サービスの信頼性を維持するために不可欠です。",
        "key_terminology": "AWS Lambda, Amazon API Gateway, Amazon Route 53, DynamoDB グローバルテーブル, フェイルオーバー, 高可用性。",
        "overall_assessment": "Cのオプションが提供するソリューションは、リージョン間でのAPIの高可用性を確保するためにシンプルで効果的です。コミュニティ投票は圧倒的にこのオプションを支持しています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon API Gateway",
      "Amazon Route 53",
      "DynamoDB global tables",
      "failover",
      "high availability"
    ]
  },
  {
    "No": "3",
    "question": "A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the\nProduction OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.\nThe company recently acquired a new business unit and invited the new unit's existing AWS account to the organization. Once onboarded, the\nadministrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company's policies.\nWhich option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term\nmaintenance?",
    "question_jp": "A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services. The company recently acquired a new business unit and invited the new unit's existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company's policies. Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?",
    "choices": [
      {
        "key": "A",
        "text": "Remove the organization's root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company's standard AWS Config rules and deploy them throughout the organization, including the new account.",
        "text_jp": "Remove the organization's root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company's standard AWS Config rules and deploy them throughout the organization, including the new account."
      },
      {
        "key": "B",
        "text": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.",
        "text_jp": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete."
      },
      {
        "key": "C",
        "text": "Convert the organization's root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization's root that allows AWS Config actions for principals only in the new account.",
        "text_jp": "Convert the organization's root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization's root that allows AWS Config actions for principals only in the new account."
      },
      {
        "key": "D",
        "text": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization's root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.",
        "text_jp": "Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization's root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete."
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (86%) 14%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating a temporary Organizational Unit (OU) allows targeted policy changes without affecting the entire organization.",
        "situation_analysis": "The new business unit's AWS account is part of the larger organization's structure, which is controlled by Service Control Policies (SCPs). The existing SCPs restrict access to AWS Config, preventing necessary updates.",
        "option_analysis": "Option B effectively allows the new account to temporarily modify AWS Config settings without disrupting the existing governance of the Production OU. The other options either remove necessary restrictions or complicate the management of organizational policies.",
        "additional_knowledge": "Proper understanding of AWS Organizations, SCPs, and Config management is crucial for cloud governance.",
        "key_terminology": "Service Control Policies (SCPs), Organizational Units (OUs), AWS Config, AWS Organizations.",
        "overall_assessment": "Option B provides a feasible and efficient method to resolve the policy conflict while maintaining long-term management practices. However, community voting suggests a popular but potentially flawed opinion towards option D, indicating a need for well-informed choices based on AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBである。一時的な組織単位（OU）を作成することにより、全体の組織に影響を与えることなく、特定のポリシー変更を行うことができる。",
        "situation_analysis": "新しいビジネスユニットのAWSアカウントは、サービスコントロールポリシー（SCP）によって管理されている大規模な組織構造の一部である。既存のSCPがAWS Configへのアクセスを制限しているため、必要な更新が行えない。",
        "option_analysis": "選択肢Bは、新しいアカウントが既存の生産OUのガバナンスを混乱させることなく、AWS Config設定を一時的に変更できる適切な方法である。他の選択肢は必要な制限を取り除くか、組織政策の管理を複雑化する。",
        "additional_knowledge": "AWS Organizations、SCP、Config管理についての適切な理解は、クラウドガバナンスにとって重要である。",
        "key_terminology": "サービスコントロールポリシー（SCP）、組織単位（OU）、AWS Config、AWS Organizations。",
        "overall_assessment": "選択肢Bは、ポリシーの対立を解決する実行可能かつ効率的な方法を提供し、長期的な管理慣行を維持する。しかし、コミュニティ投票は選択肢Dへの支持が多数を占めており、AWSのベストプラクティスに基づく十分な知識が必要であることを示している。"
      }
    ],
    "keywords": [
      "Service Control Policies",
      "Organizational Units",
      "AWS Config",
      "AWS Organizations"
    ]
  },
  {
    "No": "4",
    "question": "A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a\nstateful application. The application connects to a PostgreSQL database running on a separate server. The application's user base is expected to\ngrow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon\nEC2 Auto Scaling, and Elastic Load Balancing.\nWhich solution will provide a consistent user experience that will allow the application and database tiers to scale?",
    "question_jp": "ある企業がオンプレミスのデータセンターで二層ウェブベースのアプリケーションを運用しています。アプリケーション層は、ステートフルなアプリケーションを実行する単一サーバーで構成されています。アプリケーションは、別のサーバーで実行されているPostgreSQLデータベースに接続します。アプリケーションのユーザーベースは大幅に増加することが予想されるため、企業はアプリケーションとデータベースをAWSに移行します。このソリューションは、Amazon Aurora PostgreSQL、Amazon EC2 Auto Scaling、およびElastic Load Balancingを使用します。どのソリューションが、一貫したユーザーエクスペリエンスを提供し、アプリケーションとデータベースの層をスケーリング可能にするでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.",
        "text_jp": "AuroraレプリカのためにAurora自動スケーリングを有効にします。残りのリクエスト数が最も少ないルーティングアルゴリズムを使用したネットワークロードバランサーを使用し、スティッキーセッションを有効にします。"
      },
      {
        "key": "B",
        "text": "Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled.",
        "text_jp": "AuroraライターのためにAurora自動スケーリングを有効にします。ラウンドロビンルーティングアルゴリズムを使用したアプリケーションロードバランサーを使用し、スティッキーセッションを有効にします。"
      },
      {
        "key": "C",
        "text": "Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.",
        "text_jp": "AuroraレプリカのためにAurora自動スケーリングを有効にします。ラウンドロビンルーティングアルゴリズムを使用したアプリケーションロードバランサーを使用し、スティッキーセッションを有効にします。"
      },
      {
        "key": "D",
        "text": "Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.",
        "text_jp": "AuroraライターのためにAurora自動スケーリングを有効にします。残りのリクエスト数が最も少ないルーティングアルゴリズムを使用したネットワークロードバランサーを使用し、スティッキーセッションを有効にします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, enabling Aurora Auto Scaling for Aurora Replicas while using an Application Load Balancer with round robin routing and sticky sessions enabled.",
        "situation_analysis": "The application is stateful and requires scaling for both the application and database layers. As user traffic increases, this solution must maintain session affinity and distribute load evenly among Aurora replicas.",
        "option_analysis": "Option A uses a Network Load Balancer, which is better suited for TCP traffic but does not provide sticky sessions effectively for HTTP. Option B focuses on Aurora writers rather than replicas, which is less optimal for read scaling. Option D could potentially overload the writer, causing performance issues.",
        "additional_knowledge": "An understanding of how Aurora manages replicas and scaling can greatly improve overall application performance.",
        "key_terminology": "Aurora Auto Scaling, Application Load Balancer, sticky sessions, stateful application, scaling",
        "overall_assessment": "This question effectively tests the knowledge of AWS services and their configuration to manage scaling for stateful applications. The community overwhelmingly supports option C, aligning with best practices for maintaining a consistent user experience."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCであり、AuroraレプリカのためのAurora自動スケーリングを有効にし、アプリケーションロードバランサーを使用してラウンドロビンルーティングとスティッキーセッションを有効にします。",
        "situation_analysis": "アプリケーションはステートフルであり、アプリケーション層とデータベース層の両方をスケーリングする必要があります。ユーザートラフィックが増加するにつれ、このソリューションはセッションアフィニティを維持し、Auroraレプリカ間で負荷を均等に分散する必要があります。",
        "option_analysis": "選択肢Aはネットワークロードバランサーを使用しており、TCPトラフィックに適していますが、HTTPのスティッキーセッションを効果的に提供しません。選択肢BはレプリカではなくAuroraライターに焦点を当てており、読み込みのスケーリングには適しません。選択肢Dはライターに過負荷がかかり、パフォーマンスの問題を引き起こす可能性があります。",
        "additional_knowledge": "Auroraがレプリカとスケーリングをどのように管理しているかを理解することで、全体的なアプリケーションパフォーマンスが大きく向上します。",
        "key_terminology": "Aurora自動スケーリング、アプリケーションロードバランサー、スティッキーセッション、ステートフルアプリケーション、スケーリング",
        "overall_assessment": "この質問は、AWSサービスとそれらの構成についての知識を効果的にテストします。コミュニティは選択肢Cを圧倒的に支持しており、一貫したユーザーエクスペリエンスを維持するためのベストプラクティスと一致しています。"
      }
    ],
    "keywords": [
      "Aurora Auto Scaling",
      "Application Load Balancer",
      "sticky sessions",
      "stateful application",
      "scaling"
    ]
  },
  {
    "No": "5",
    "question": "A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and\ninternet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are\npresent in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to\nolder devices, which the company identified by the User-Agent headers.\nThe company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company\nhas already migrated the applications into a set of AWS Lambda functions.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、自社がオンプレミスでホストするアプリケーションからメタデータを収集するサービスを使用しています。テレビやインターネットラジオなどの消費者デバイスがアプリケーションにアクセスします。多くの古いデバイスは特定のHTTPヘッダーをサポートしておらず、これらのヘッダーがレスポンスに含まれるとエラーが発生します。企業は、ユーザーエージェントヘッダーによって特定した古いデバイスへのレスポンスからサポートされていないヘッダーを削除するように、オンプレミスのロードバランサーを構成しました。企業は、このサービスをAWSに移行し、サーバーレス技術を採用し、古いデバイスをサポートする機能を保持したいと考えています。企業はすでにアプリケーションをAWS Lambda関数のセットに移行しています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header.",
        "text_jp": "メタデータサービスのためにAmazon CloudFrontディストリビューションを作成します。アプリケーションロードバランサー(ALB)を作成します。CloudFrontディストリビューションを構成してリクエストをALBに転送します。ALBを構成して、リクエストの種類ごとに正しいLambda関数を呼び出します。ユーザーエージェントヘッダーの値に基づいて問題のあるヘッダーを削除するためのCloudFront関数を作成します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the User-Agent header.",
        "text_jp": "メタデータサービスのためにAmazon API Gateway REST APIを作成します。API Gatewayを構成して、リクエストの種類ごとに正しいLambda関数を呼び出します。デフォルトのゲートウェイレスポンスを修正し、ユーザーエージェントヘッダーの値に基づいて問題のあるヘッダーを削除します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the User-Agent. Associate the response data mapping with the HTTP API.",
        "text_jp": "メタデータサービスのためにAmazon API Gateway HTTP APIを作成します。API Gatewayを構成して、リクエストの種類ごとに正しいLambda関数を呼び出します。ユーザーエージェントに基づいて問題のあるヘッダーを削除するためのレスポンスマッピングテンプレートを作成します。レスポンスデータのマッピングをHTTP APIに関連付けます。"
      },
      {
        "key": "D",
        "text": "Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header.",
        "text_jp": "メタデータサービスのためにAmazon CloudFrontディストリビューションを作成します。アプリケーションロードバランサー(ALB)を作成します。CloudFrontディストリビューションを構成してリクエストをALBに転送します。ALBを構成して、リクエストの種類ごとに正しいLambda関数を呼び出します。ユーザーエージェントヘッダーの値に基づいて、ビューアリクエストに応じて問題のあるヘッダーを削除するLambda@Edge関数を作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (44%) D (22%) B (19%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves creating an Amazon API Gateway REST API that can invoke the appropriate Lambda functions while filtering out unsupported HTTP headers based on the User-Agent.",
        "situation_analysis": "The company needs a solution to migrate its on-premises metadata service to AWS while addressing the limitations of older consumer devices that cannot handle certain HTTP headers. This is critical to ensure seamless functionality for users accessing the applications.",
        "option_analysis": "Option B effectively implements the backend API using AWS services while enabling customization of responses to meet the needs of older devices. Other options, while capable of serving the API, do not address the requirement to modify the headers as effectively as B does. Options A and D use CloudFront but do not modify API Gateway responses, and option C, although using API Gateway, does not use REST API which is more straightforward for this scenario.",
        "additional_knowledge": "API Gateway also supports different types of APIs, and REST APIs are ideal for crafting fine-tuned control over headers and responses.",
        "key_terminology": "Amazon API Gateway, AWS Lambda, HTTP headers, User-Agent, serverless architecture.",
        "overall_assessment": "Given the community vote distribution, there is some support for options A and D, but option B is more aligned with the requirements for modifying HTTP headers and hence is the best answer."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、Amazon API Gateway REST APIを作成して、適切なLambda関数を呼び出しつつ、ユーザーエージェントに基づいてサポートされていないHTTPヘッダーをフィルタリングします。",
        "situation_analysis": "企業は、オンプレミスのメタデータサービスをAWSに移行する必要がありますが、特定のHTTPヘッダーに対応できない古い消費者デバイスの制限に対処する必要があります。これは、アプリケーションにアクセスするユーザーにとってシームレスな機能性を確保するために重要です。",
        "option_analysis": "オプションBは、AWSサービスを使用してバックエンドAPIを効果的に実装し、古いデバイスのニーズを満たすためにレスポンスをカスタマイズできるようにします。他のオプションもAPIを提供できますが、Bほど効果的にヘッダーを修正する要件に対処していません。オプションAとDはCloudFrontを使用しますが、API Gatewayのレスポンスを修正せず、オプションCはAPI Gatewayを使用していますが、このシナリオにはより簡単なREST APIではありません。",
        "additional_knowledge": "API Gatewayは異なるタイプのAPIもサポートしており、REST APIはヘッダーやレスポンスに対する細かな制御を行うのに理想的です。",
        "key_terminology": "Amazon API Gateway, AWS Lambda, HTTPヘッダー, ユーザーエージェント, サーバーレスアーキテクチャ。",
        "overall_assessment": "コミュニティの投票分布を考慮すると、オプションAとDには一定の支持がありますが、オプションBはHTTPヘッダーを変更する要件により一層一致しており、最適な回答です。"
      }
    ],
    "keywords": [
      "Amazon API Gateway",
      "AWS Lambda",
      "HTTP headers",
      "User-Agent",
      "serverless architecture"
    ]
  },
  {
    "No": "6",
    "question": "A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3\nbucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to\naccess the files from its own AWS account (Account B).\nWhich combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)",
    "question_jp": "小売会社は、ビジネスパートナー会社に一連のデータファイルを提供する必要があります。これらのファイルは、Amazon S3バケットのアカウントAに保存されています。このビジネスパートナー会社は、そのAWSアカウント（アカウントB）からIAMユーザーであるUser_DataProcessorがファイルにアクセスできるようにしたいと考えています。User_DataProcessorがS3バケットに正常にアクセスできるようにするために、会社はどの組み合わせの手順を踏む必要がありますか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A.",
        "text_jp": "アカウントAのS3バケットのクロスオリジンリソースシェアリング（CORS）機能を有効にします。"
      },
      {
        "key": "B",
        "text": "In Account A, set the S3 bucket policy to the following: [image_4_0] [image_4_1] [image_4_2]",
        "text_jp": "アカウントAで、S3バケットポリシーを次のように設定します：[image_4_0] [image_4_1] [image_4_2]"
      },
      {
        "key": "C",
        "text": "In Account A, set the S3 bucket policy to the following: [image_4_0] [image_4_1] [image_4_2]",
        "text_jp": "アカウントAで、S3バケットポリシーを次のように設定します：[image_4_0] [image_4_1] [image_4_2]"
      },
      {
        "key": "D",
        "text": "In Account B, set the permissions of User_DataProcessor to the following: [image_4_0] [image_4_1] [image_4_2]",
        "text_jp": "アカウントBで、User_DataProcessorの権限を次のように設定します：[image_4_0] [image_4_1] [image_4_2]"
      },
      {
        "key": "E",
        "text": "In Account B, set the permissions of User_DataProcessor to the following: [image_4_0] [image_4_1] [image_4_2]",
        "text_jp": "アカウントBで、User_DataProcessorの権限を次のように設定します：[image_4_0] [image_4_1] [image_4_2]"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (73%) D (23%)",
    "page_images": [
      "image_4_0.png",
      "image_4_1.png",
      "image_4_2.png",
      "image_4_3.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is option D and one additional step must also be taken to enable User_DataProcessor to access the S3 bucket.",
        "situation_analysis": "The retail company needs to share files stored in an S3 bucket with a business partner's IAM user. This scenario requires cross-account access configuration.",
        "option_analysis": "Option D is required because it clearly involves granting User_DataProcessor the necessary permissions in Account B. Other options may involve settings in Account A but do not address the direct permission aspect for the IAM user involved.",
        "additional_knowledge": "It's crucial to verify that the IAM policy in Account B corresponds correctly with the permissions required to access the S3 bucket from Account A.",
        "key_terminology": "IAM, S3 Bucket Policy, Cross-Account Access, Permissions",
        "overall_assessment": "While community votes showed a majority leaning towards option C, option D is critical for granting access to User_DataProcessor in Account B which is essential for file access."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはオプションDであり、User_DataProcessorがS3バケットにアクセスできるようにするためにもう1つの手順が必要です。",
        "situation_analysis": "小売会社は、S3バケットに保存されているファイルをビジネスパートナーのIAMユーザーと共有する必要があります。このシナリオでは、クロスアカウントアクセスの構成が要求されます。",
        "option_analysis": "オプションDは、アカウントBでUser_DataProcessorに必要な権限を付与するため、必須です。他のオプションはアカウントAの設定に関連していますが、関与するIAMユーザーの直接的な権限の側面には触れていません。",
        "additional_knowledge": "アカウントBのIAMポリシーが、アカウントAのS3バケットにアクセスするために必要な権限と正しく対応していることを確認することが重要です。",
        "key_terminology": "IAM, S3バケットポリシー, クロスアカウントアクセス, 権限",
        "overall_assessment": "コミュニティ投票ではオプションCに対する大多数の支持が見られましたが、オプションDはアカウントBでUser_DataProcessorにアクセス権を付与するために重要です。これはファイルアクセスには不可欠です。"
      }
    ],
    "keywords": [
      "IAM",
      "S3 Bucket Policy",
      "Cross-Account Access",
      "Permissions"
    ]
  },
  {
    "No": "7",
    "question": "A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices\nthat run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is\nvariable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless\narchitecture that minimizes operational complexity.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業は、従来のウェブアプリケーションを Amazon EC2 インスタンスで運用しています。この企業は、アプリケーションをコンテナで稼働するマイクロサービスとして再構築する必要があります。アプリケーションの異なるバージョンは本番環境とテスト環境の2つの異なる環境で存在します。アプリケーションの負荷は変動しますが、最小負荷と最大負荷は把握されています。ソリューションアーキテクトは、運用の複雑さを最小限に抑えたサーバーレスアーキテクチャで更新されたアプリケーションを設計する必要があります。この要件を最もコスト効果的に満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle the expected peak load. Configure two separate Lambda integrations within Amazon API Gateway: one for production and one for testing.",
        "text_jp": "コンテナイメージを AWS Lambda に関数としてアップロードします。予想されるピーク負荷を処理するために、関連する Lambda 関数の同時実行制限を設定します。本番環境向けとテスト環境向けの 2 つの別々の Lambda 統合を Amazon API Gateway に構成します。"
      },
      {
        "key": "B",
        "text": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct trafic to the ECS clusters.",
        "text_jp": "コンテナイメージを Amazon Elastic Container Registry (Amazon ECR) にアップロードします。予想される負荷を処理するために、Fargate 起動タイプの自動スケール Amazon Elastic Container Service (Amazon ECS) クラスターを2つ構成します。ECR イメージからタスクをデプロイします。ECS クラスターにトラフィックを誘導するために、2つの別々のアプリケーションクラウドバランサーを構成します。"
      },
      {
        "key": "C",
        "text": "Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct trafic to the EKS clusters.",
        "text_jp": "コンテナイメージを Amazon Elastic Container Registry (Amazon ECR) にアップロードします。予想される負荷を処理するために、Fargate 起動タイプの自動スケール Amazon Elastic Kubernetes Service (Amazon EKS) クラスターを2つ構成します。ECR イメージからタスクをデプロイします。EKS クラスターにトラフィックを誘導するために、2つの別々のアプリケーションクラウドバランサーを構成します。"
      },
      {
        "key": "D",
        "text": "Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct trafic to the Elastic Beanstalk deployments.",
        "text_jp": "コンテナイメージを AWS Elastic Beanstalk にアップロードします。Elastic Beanstalk で、本番環境とテスト環境のために別々の環境とデプロイメントを作成します。Elastic Beanstalk のデプロイメントにトラフィックを誘導するために、2つの別々のアプリケーションクラウドバランサーを構成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (81%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Upload the container images to Amazon Elastic Container Registry (Amazon ECR) and configure two auto-scaled Amazon ECS clusters with the Fargate launch type. This approach allows for handling variable loads effectively with the benefits of container orchestration and scaling.",
        "situation_analysis": "The company needs a cost-effective solution to implement a serverless architecture while transitioning to microservices on containers. The need for separate environments for production and testing, combined with variable loads, makes the elasticity and management provided by Amazon ECS with Fargate an excellent choice.",
        "option_analysis": "Option A does not utilize a container service effectively and can lead to limitations on scaling and performance. Option C, while viable, may introduce unnecessary complexity compared to ECS for users unfamiliar with Kubernetes. Option D introduces additional management overhead with Elastic Beanstalk, which may not be necessary for this scenario.",
        "additional_knowledge": "It's important to understand the distinction between ECS and EKS, especially in terms of complexity, management, and best practices for different application use cases.",
        "key_terminology": "Amazon ECR, Amazon ECS, Fargate, Application Load Balancer, serverless architecture",
        "overall_assessment": "The quality of the question is good, effectively assessing knowledge of AWS services and cost-effective solutions. The community vote support for option B indicates strong alignment with the correct answer."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは B です: コンテナイメージを Amazon Elastic Container Registry (Amazon ECR) にアップロードし、Fargate 起動タイプの自動スケール Amazon ECS クラスターを 2 つ構成します。このアプローチは、コンテナオーケストレーションとスケーリングの利点を活かしつつ、変動する負荷を効果的に処理できることを可能にします。",
        "situation_analysis": "企業は、マイクロサービスをコンテナで実装する際に、コスト効果の高いソリューションを必要としています。本番環境とテスト環境用に別々の環境が必要であり、変動する負荷に対する弾力性と管理を提供する Amazon ECS と Fargate が優れた選択肢となります。",
        "option_analysis": "選択肢 A は、コンテナサービスを効果的に活用せず、スケーリングやパフォーマンスに制限をもたらす可能性があります。選択肢 C は適切ですが、EKS は Kubernetes に不慣れなユーザーには不必要な複雑さをもたらす可能性があります。選択肢 D は、Elastic Beanstalk の追加管理オーバーヘッドを導入し、このシナリオには必要ないかもしれません。",
        "additional_knowledge": "特に、アプリケーションの使用ケースに応じた最適な管理とベストプラクティスの点で、ECS と EKS の違いを理解することが重要です。",
        "key_terminology": "Amazon ECR, Amazon ECS, Fargate, Application Load Balancer, サーバーレスアーキテクチャ",
        "overall_assessment": "この質問の質は良好で、AWS サービスとコスト効果の高いソリューションの知識を効果的に評価しています。選択肢 B に対するコミュニティ投票の支持は、正しい答えとの強い整合性を示しています。"
      }
    ],
    "keywords": [
      "Amazon ECR",
      "Amazon ECS",
      "Fargate",
      "Application Load Balancer",
      "serverless architecture"
    ]
  },
  {
    "No": "8",
    "question": "A company has a multi-tier web application that runs on a fieet of Amazon EC2 instances behind an Application Load Balancer (ALB). The\ninstances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the\nmaximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application's data. The DB instance\nhas a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.\nThe company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region.\nThe company does not have a large enough budget for an active-active strategy.\nWhat should a solutions architect recommend to meet these requirements?",
    "question_jp": "ある企業は、Amazon EC2インスタンスの群を含むマルチティアWebアプリケーションを運用しており、Application Load Balancer (ALB)の背後で動作しています。これらのインスタンスはAuto Scalingグループ内にあり、ALBとAuto ScalingグループはバックアップAWSリージョンに複製されています。Auto Scalingグループの最小値と最大値はゼロに設定されており、アプリケーションのデータはAmazon RDS Multi-AZ DBインスタンスに保存されています。このDBインスタンスにはバックアップリージョンにリードレプリカがあります。アプリケーションはAmazon Route 53レコードを使用してエンドユーザーにエンドポイントを提示しています。企業は、アプリケーションにバックアップリージョンに自動的にフェイルオーバーする能力を与えることで、RTOを15分未満に減少させる必要があります。企業にはアクティブ-アクティブ戦略に十分な予算がありません。これらの要件を満たすためにソリューションアーキテクトは何を推奨すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Reconfigure the application's Route 53 record with a latency-based routing policy that load balances trafic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.",
        "text_jp": "アプリケーションのRoute 53レコードを、2つのALB間でトラフィックを負荷分散するレイテンシーベースのルーティングポリシーで再構成します。バックアップリージョンにAWS Lambda関数を作成し、リードレプリカを昇格させ、Auto Scalingグループの値を修正します。CloudWatchアラームを、プライマリーリージョンのALBのHTTPCode_Target_5XX_Countメトリクスに基づいて設定します。CloudWatchアラームがLambda関数を呼び出すように設定します。"
      },
      {
        "key": "B",
        "text": "Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application's Route 53 record with a failover policy that routes trafic to the ALB in the backup Region when a health check failure occurs.",
        "text_jp": "バックアップリージョンにAWS Lambda関数を作成し、リードレプリカを昇格させ、Auto Scalingグループの値を修正します。Route 53を設定し、Webアプリケーションを監視するヘルスチェックを行い、ヘルスチェックステータスが不健康な場合にAmazon Simple Notification Service (Amazon SNS)通知をLambda関数に送ります。不健康なヘルスチェックが発生した場合、アプリケーションのRoute 53レコードをバックアップリージョンのALBにトラフィックをルーティングするフェイルオーバーポリシーで更新します。"
      },
      {
        "key": "C",
        "text": "Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application's Route 53 record with a latency-based routing policy that load balances trafic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3.",
        "text_jp": "バックアップリージョンのAuto ScalingグループをプライマリーリージョンのAuto Scalingグループと同じ値に設定します。アプリケーションのRoute 53レコードを、2つのALB間でトラフィックを負荷分散するレイテンシーベースのルーティングポリシーで再構成します。リードレプリカを削除します。リードレプリカをスタンドアロンのRDS DBインスタンスに置き換えます。スナップショットとAmazon S3を使用してRDS DBインスタンス間でクロスリージョンレプリケーションを設定します。"
      },
      {
        "key": "D",
        "text": "Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.",
        "text_jp": "AWS Global Acceleratorで、2つのALBを同等の重み付けターゲットとしてエンドポイントを構成します。バックアップリージョンにAWS Lambda関数を作成し、リードレプリカを昇格させ、Auto Scalingグループの値を修正します。CloudWatchアラームを、プライマリーリージョンのALBのHTTPCode_Target_5XX_Countメトリクスに基づいて作成します。CloudWatchアラームがLambda関数を呼び出すように設定します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This solution ensures automatic failover to the backup region when the health check detects an issue with the primary application.",
        "situation_analysis": "The company needs to lower its RTO to under 15 minutes and cannot afford an active-active setup. Thus, it requires a solution that promotes automatic recovery in a secondary region when the primary fails.",
        "option_analysis": "Option B creates a Lambda function to promote the read replica and facilitate changes in the Auto Scaling group values if the Route 53 health checks fail. This approach allows for timely failover without manual intervention. Other options either involve costly changes or do not adequately address failover within the required time.",
        "additional_knowledge": "AWS services used are designed to provide high availability and disaster recovery, crucial in minimizing downtime.",
        "key_terminology": "Route 53, Amazon RDS, AWS Lambda, Auto Scaling, Amazon SNS",
        "overall_assessment": "Option B aligns well with the need for automatic failover while keeping costs manageable. The predicted workflow is efficient and utilizes AWS services effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。このソリューションは、ヘルスチェックがプライマリアプリケーションに問題を検出した場合のバックアップリージョンへの自動フェイルオーバーを確実にする。",
        "situation_analysis": "企業はRTOを15分未満に下げる必要があり、アクティブ-アクティブ構成を予算的に利用できない。このため、プライマリが失敗した場合に、セカンダリリージョンで自動的に回復するソリューションが必要である。",
        "option_analysis": "選択肢Bは、ヘルスチェックが失敗した場合にリードレプリカを昇格させ、Auto Scalingグループの値を変更するためのLambda関数を作成する。このアプローチは、手動介入なしでタイムリーなフェイルオーバーを可能にする。他の選択肢は、コストがかかる変更を含むか、必要な時間内にフェイルオーバーを適切に対処していない。",
        "additional_knowledge": "使用されるAWSサービスは、高可用性と災害復旧を提供するように設計されており、ダウンタイムを最小限に抑える上で重要である。",
        "key_terminology": "Route 53, Amazon RDS, AWS Lambda, Auto Scaling, Amazon SNS",
        "overall_assessment": "選択肢Bは、自動フェイルオーバーの必要性に適切に準拠しており、コストを管理しながら効率的なワークフローを予測し、AWSサービスを効果的に活用している。"
      }
    ],
    "keywords": [
      "Route 53",
      "Amazon RDS",
      "AWS Lambda",
      "Auto Scaling",
      "Amazon SNS"
    ]
  },
  {
    "No": "9",
    "question": "A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node\ncluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application\nto function, each piece of the infrastructure must be healthy and must be in an active state.\nA solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least\npossible downtime.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "企業は、単一のAmazon EC2インスタンス上で重要なアプリケーションをホストしています。このアプリケーションは、メモリ内データストアとして、Amazon ElastiCache for Redisの単一ノードクラスターを使用しています。アプリケーションは、リレーショナルデータベースとしてAmazon RDS for MariaDB DBインスタンスを使用しています。アプリケーションが機能するためには、インフラストラクチャの各部分が正常であり、アクティブな状態である必要があります。ソリューションアーキテクトは、ダウンタイムを最小限に抑えながら、インフラストラクチャが故障から自動的に復旧できるようにアプリケーションのアーキテクチャを改善する必要があります。どの組み合わせの手順がこれらの要件を満たしますか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Use an Elastic Load Balancer to distribute trafic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances.",
        "text_jp": "複数のEC2インスタンスにトラフィックを分散させるためにElastic Load Balancerを使用する。EC2インスタンスは、最小容量が2つのインスタンスを持つAuto Scalingグループの一部であることを確認する。"
      },
      {
        "key": "B",
        "text": "Use an Elastic Load Balancer to distribute trafic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited mode.",
        "text_jp": "複数のEC2インスタンスにトラフィックを分散させるためにElastic Load Balancerを使用する。EC2インスタンスは、無制限モードで構成されていることを確認する。"
      },
      {
        "key": "C",
        "text": "Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in failure scenarios.",
        "text_jp": "DBインスタンスを変更して、同じアベイラビリティゾーンにリードレプリカを作成する。障害シナリオでリードレプリカをプライマリDBインスタンスに昇格させる。"
      },
      {
        "key": "D",
        "text": "Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones.",
        "text_jp": "DBインスタンスを変更して、2つのアベイラビリティゾーンにまたがるMulti-AZデプロイメントを作成する。"
      },
      {
        "key": "E",
        "text": "Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances.",
        "text_jp": "ElastiCache for Redisクラスタのレプリケーショングループを作成する。クラスタが最小容量2つのインスタンスを持つAuto Scalingグループを使用するように構成する。"
      },
      {
        "key": "F",
        "text": "Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster.",
        "text_jp": "ElastiCache for Redisクラスタのレプリケーショングループを作成する。クラスタでMulti-AZを有効にする。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ADF (96%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using an Elastic Load Balancer with an Auto Scaling group enhances application availability and resilience.",
        "situation_analysis": "The application requires high availability, as it serves critical business functions. It must be able to recover from failures without significant downtime.",
        "option_analysis": "Option A provides a robust solution by distributing incoming traffic across multiple EC2 instances, mitigating the impact of a single instance failure. Option B's mention of unlimited mode does not contribute to fault tolerance. Option C adds a read replica but does not specify a failover mechanism. Option D improves redundancy but does not address the EC2 instance availability. Options E and F support caching but do not protect the EC2 application layers.",
        "additional_knowledge": "Implementing Multi-AZ deployments for RDS and replication for ElastiCache would further enhance data availability.",
        "key_terminology": "Elastic Load Balancer, Auto Scaling, Multi-AZ, ElastiCache, read replica",
        "overall_assessment": "The question addresses important aspects of high availability architecture. The community's support for option A indicates its recognition of the best practices in building resilient applications."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。Elastic Load BalancerとAuto Scalingグループを使用することで、アプリケーションの可用性と耐障害性が向上する。",
        "situation_analysis": "このアプリケーションは重要なビジネス機能を提供するため、高可用性を必要とする。重大なダウンタイムなしで故障から回復できる必要がある。",
        "option_analysis": "選択肢Aは、複数のEC2インスタンスに受信トラフィックを分散させることで、単一のインスタンス故障の影響を軽減するという強力なソリューションを提供する。選択肢Bの無制限モードは耐障害性に寄与しない。選択肢Cはリードレプリカを追加するが、フェイルオーバー機構が明示されていない。選択肢Dは冗長性を向上させるが、EC2インスタンスの可用性には対処していない。選択肢EとFはキャッシュを支援するが、EC2アプリケーションレイヤーを保護するものではない。",
        "additional_knowledge": "RDSのMulti-AZデプロイメントとElastiCacheのレプリケーションを実装することで、データの可用性がさらに向上する。",
        "key_terminology": "Elastic Load Balancer, Auto Scaling, Multi-AZ, ElastiCache, リードレプリカ",
        "overall_assessment": "この質問は高可用性アーキテクチャの重要な側面に関するものである。コミュニティが選択肢Aを支持していることから、その認識は耐障害性のあるアプリケーション構築におけるベストプラクティスに反映されているといえる。"
      }
    ],
    "keywords": [
      "Elastic Load Balancer",
      "Auto Scaling",
      "Multi-AZ",
      "ElastiCache",
      "read replica"
    ]
  },
  {
    "No": "10",
    "question": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load\nBalancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that\npoints to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.\nAfter an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP\nheaders that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the\nerror occurs.\nWhile the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page\nto visitors.\nWhich combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)",
    "question_jp": "小売会社はAWS上でeコマースアプリケーションを運営しています。アプリケーションはAmazon EC2インスタンス上で実行されており、Application Load Balancer (ALB)の背後に配置されています。会社はデータベースバックエンドとしてAmazon RDS DBインスタンスを使用しています。静的コンテンツはキャッシュされており、Amazon CloudFrontはALBを指す1つのオリジンで構成されています。すべてのパブリックゾーンはAmazon Route 53でホストされています。アプリケーションの更新後、ALBは時折502ステータスコード（Bad Gateway）エラーを返します。根本的な原因は、ALBに返されるHTTPヘッダーが不正であることです。エラーが発生した直後にソリューションアーキテクトがウェブページを再読み込みすると、ウェブページは正常に返されます。会社が問題に取り組んでいる間、ソリューションアーキテクトは、訪問者に標準のALBエラーページではなくカスタムエラーページを提供する必要があります。この要件を最小限の運用コストで満たすための手順の組み合わせはどれですか？（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.",
        "text_jp": "Amazon S3バケットを作成し、静的ウェブページをホストするように構成します。カスタムエラーページをAmazon S3にアップロードします。"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.",
        "text_jp": "ALBのヘルスチェック応答Target.FailedHealthChecksが0より大きい場合にAWS Lambda関数を呼び出すAmazon CloudWatchアラームを作成します。Lambda関数を構成して、ALBの転送ルールを公開されているウェブサーバーを指すように変更します。"
      },
      {
        "key": "C",
        "text": "Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.",
        "text_jp": "既存のAmazon Route 53レコードを変更してヘルスチェックを追加します。ヘルスチェックが失敗した場合にフォールバックターゲットを構成します。DNSレコードを変更して公開されているウェブページを指します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than",
        "text_jp": "ALBのヘルスチェック応答Elb.InternalErrorが大きい場合にAWS Lambda関数を呼び出すAmazon CloudWatchアラームを作成します。"
      },
      {
        "key": "0",
        "text": "Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.",
        "text_jp": "Lambda関数を構成して、ALBの転送ルールを公開されているウェブサーバーを指すように変更します。"
      },
      {
        "key": "E",
        "text": "Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.",
        "text_jp": "CloudFrontカスタムエラーページを設定してカスタムエラーレスポンスを追加します。DNSレコードを変更して公開されているウェブページを指します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "AE (96%)",
    "page_images": [],
    "community_vote_distribution_jp": "AE (96%)",
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are C and E. Option C deals directly with providing a fallback for the DNS in case specific health checks fail, while E allows for CloudFront to serve custom error pages efficiently.",
        "situation_analysis": "The application is experiencing 502 errors due to malformed HTTP headers, requiring a solution for custom error handling in a way that minimally impacts operational overhead.",
        "option_analysis": "Option C effectively allows direct control over DNS failover. Option A, while useful for custom error pages, does not provide real-time switching based on health checks. Option B and D involve using Lambda but add unnecessary complexity, and E requires additional configuration steps and management.",
        "additional_knowledge": "Using CloudFront for caching and reduced latency, combined with Route 53 for health checks, is a best practice in maintaining high availability applications.",
        "key_terminology": "Route 53, ALB, health check, DNS failover, CloudFront",
        "overall_assessment": "Despite community support for options A and E, option C provides a more direct and efficient means of handling the issue at hand. Implementing Route 53 health checks ensures that users are routed to available resources without the need for complex Lambda functions. The community's majority preference does not always align with the best solution in this context."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCとEである。選択肢CはDNSのフォールバックを提供し、特定のヘルスチェックが失敗した場合の対策として直接的に機能する。一方、EはCloudFrontが効率的にカスタムエラーページを提供できるようにする。",
        "situation_analysis": "アプリケーションは不正なHTTPヘッダーのために502エラーを経験しており、運用オーバーヘッドを最小限に抑えたカスタムエラーハンドリングの解決策が必要である。",
        "option_analysis": "選択肢CはDNSのフォールオーバーを直接制御できる。選択肢Aはカスタムエラーページに役立つが、ヘルスチェックに基づくリアルタイムでの切り替えは提供しない。選択肢BおよびDはLambdaを利用するが、不必要な複雑さを加える。選択肢Eは追加の設定ステップと管理が必要である。",
        "additional_knowledge": "CloudFrontを利用したキャッシングとレイテンシーの削減を実現し、Route 53のヘルスチェックと組み合わせることで、高可用性のアプリケーションを維持するためのベストプラクティスである。",
        "key_terminology": "Route 53, ALB, ヘルスチェック, DNSフォールバック, CloudFront",
        "overall_assessment": "コミュニティが選択肢AとEに支持を寄せているにもかかわらず、選択肢Cは問題を処理するためのより迅速かつ効率的な手段を提供する。Route 53のヘルスチェックを実装することにより、ユーザーは複雑なLambda関数を必要とせず、利用可能なリソースにルーティングされる。コミュニティの選好は必ずしもこの文脈での最良の解決策と一致しない。"
      }
    ],
    "keywords": [
      "Route 53",
      "ALB",
      "health check",
      "DNS failover",
      "CloudFront"
    ]
  },
  {
    "No": "11",
    "question": "A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the\ncompany can use to share a common network across multiple accounts.\nThe company's infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to\nmanage the network. Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to\ncreate AWS resources within subnets.\nWhich combination of actions should the solutions architect perform to meet these requirements? (Choose two.)",
    "question_jp": "ある企業は多数のAWSアカウントを持ち、AWS Organizationsを使用してすべてを管理しています。ソリューションアーキテクトは、企業が複数のアカウント間で共通のネットワークを共有するために使用できるソリューションを実装する必要があります。企業のインフラチームは、VPCを持つ専用のインフラアカウントを保持しています。インフラチームはこのアカウントを使用してネットワークを管理する必要があります。個々のアカウントは、独自のネットワークを管理する能力を持つことはできません。しかし、個々のアカウントはサブネット内でAWSリソースを作成できる必要があります。これらの要件を満たすために、ソリューションアーキテクトはどの組み合わせのアクションを実行すべきですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a transit gateway in the infrastructure account.",
        "text_jp": "インフラアカウントにトランジットゲートウェイを作成する。"
      },
      {
        "key": "B",
        "text": "Enable resource sharing from the AWS Organizations management account.",
        "text_jp": "AWS Organizationsの管理アカウントからリソース共有を有効にする。"
      },
      {
        "key": "C",
        "text": "Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs in each individual account with the VPC in the infrastructure account.",
        "text_jp": "AWS Organizations内の各AWSアカウントにVPCを作成する。VPCをインフラアカウントのVPCと同じCIDR範囲とサブネットを共有するように構成する。各個別アカウントのVPCをインフラアカウントのVPCとピア接続させる。"
      },
      {
        "key": "D",
        "text": "Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share.",
        "text_jp": "インフラアカウントでAWSリソースアクセスマネージャーにリソース共有を作成する。共有ネットワークを使用する特定のAWS Organizations OUを選択する。リソース共有に関連付ける各サブネットを選択する。"
      },
      {
        "key": "E",
        "text": "Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each prefix list to associate with the resource share.",
        "text_jp": "インフラアカウントでAWSリソースアクセスマネージャーにリソース共有を作成する。共有ネットワークを使用する特定のAWS Organizations OUを選択する。リソース共有に関連付ける各プレフィックスリストを選択する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BD (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create a transit gateway in the infrastructure account. This allows the infrastructure team to manage the network while still enabling other accounts to create resources within subnets. The dual action that complements it is D: Create a resource share in AWS Resource Access Manager.",
        "situation_analysis": "The company requires a centralized network management system where only the infrastructure account has the capability to manage the VPC and network settings.",
        "option_analysis": "Option A allows an efficient network routing solution through the transit gateway. Option B is not required since resource sharing does not depend solely on enabling it at the management account level. Option C contradicts the requirement that individual accounts should not manage their own networks. Option D appropriately allows shared resource access but is secondary to establishing the transit method in A which is essential to meet requirements.",
        "additional_knowledge": "Understanding how services interconnect in AWS is crucial for organizing infrastructure and optimizing workloads.",
        "key_terminology": "AWS Organizations, Transit Gateway, VPC, Resource Access Manager, Cloud Networking",
        "overall_assessment": "The question effectively assesses knowledge of AWS networking strategies across multiple accounts. Although community votes suggest alternatives, A is warranted due to its formal role in managing shared networking efficiently."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA: インフラアカウントにトランジットゲートウェイを作成することである。この方法により、インフラチームはネットワークを管理しつつ、他のアカウントがサブネット内でリソースを作成することができる。これに補完的なアクションとしてD: インフラアカウントでAWSリソースアクセスマネージャーにリソース共有を作成することが挙げられる。",
        "situation_analysis": "企業は、インフラアカウントのみがVPCおよびネットワーク設定を管理する能力を持つ集中化されたネットワーク管理システムを必要としている。",
        "option_analysis": "選択肢Aは、トランジットゲートウェイを介して効率的なネットワークルーティングソリューションを提供する。選択肢Bは、リソース共有が管理アカウントレベルで有効にされているだけでは必要ではない。選択肢Cは、個別アカウントが独自のネットワークを管理しないという要件に矛盾している。選択肢Dはリソースアクセスを共有するのに適切であるが、Aのトランジット手段を確立することが要件を満たすために本質的であるため、補助的である。",
        "additional_knowledge": "AWS内でサービスがどのように相互接続するかを理解することは、インフラを整理し、ワークロードを最適化する上で非常に重要である。",
        "key_terminology": "AWS Organizations、トランジットゲートウェイ、VPC、リソースアクセスマネージャー、クラウドネットワーキング",
        "overall_assessment": "この問題は、複数のアカウント間のAWSネットワーキング戦略に対する知識を効果的に評価している。コミュニティ投票が代替案を示唆しているが、Aは共有ネットワークを効率的に管理する役割があるため正当化される。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Transit Gateway",
      "VPC",
      "Resource Access Manager",
      "Cloud Networking"
    ]
  },
  {
    "No": "12",
    "question": "A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API\ncalls. The third-party SaaS application also runs on AWS inside a VPC.\nThe company will consume the third-party SaaS application from inside a VPC. The company has internal security policies that mandate the use of\nprivate connectivity that does not traverse the internet. No resources that run in the company VPC are allowed to be accessed from outside the\ncompany's VPC. All permissions must conform to the principles of least privilege.\nWhich solution meets these requirements?",
    "question_jp": "ある企業がサードパーティのソフトウェア・アズ・ア・サービス (SaaS) アプリケーションを利用したいと考えています。このサードパーティのSaaSアプリケーションは、いくつかのAPIコールを通じて使用されます。サードパーティのSaaSアプリケーションは、AWSのVPC内で実行されています。企業は、VPC内からサードパーティのSaaSアプリケーションを利用します。企業には、インターネットを通過しないプライベート接続を使用することを義務付ける内部セキュリティポリシーがあります。企業のVPC内で実行されているリソースは、企業のVPCの外部からアクセスされることは許可されていません。すべての権限は最小特権の原則に従う必要があります。これらの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint.",
        "text_jp": "AWS PrivateLinkインターフェイスVPCエンドポイントを作成します。このエンドポイントをサードパーティのSaaSアプリケーションが提供するエンドポイントサービスに接続します。アクセスを制限するためのセキュリティグループを作成します。エンドポイントにセキュリティグループを関連付けます。"
      },
      {
        "key": "B",
        "text": "Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. Configure network ACLs to limit access across the VPN tunnels.",
        "text_jp": "サードパーティのSaaSアプリケーションと企業VPCの間にAWS Site-to-Site VPN接続を作成します。VPNトンネル間のアクセスを制限するためにネットワークACLを構成します。"
      },
      {
        "key": "C",
        "text": "Create a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed routes for the peering connection.",
        "text_jp": "サードパーティのSaaSアプリケーションと企業VPC間にVPCピアリング接続を作成します。ピアリング接続のために必要なルートを追加してルートテーブルを更新します。"
      },
      {
        "key": "D",
        "text": "Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service. Grant permissions for the endpoint service to the specific account of the third-party SaaS provider.",
        "text_jp": "AWS PrivateLinkエンドポイントサービスを作成します。サードパーティのSaaSプロバイダーにこのエンドポイントサービスのためのインターフェイスVPCエンドポイントを作成するように依頼します。特定のサードパーティのSaaSプロバイダーのアカウントに対してエンドポイントサービスの権限を付与します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. The solution involves creating an AWS PrivateLink interface VPC endpoint to securely connect to the third-party SaaS application without traversing the internet.",
        "situation_analysis": "The company requires a private connection, meaning that the solution should not allow traffic over the internet. Since the third-party SaaS is hosted within a VPC in AWS, leveraging AWS PrivateLink is a suitable approach.",
        "option_analysis": "Option A securely connects to the third-party service while adhering to the company's internal policy. Option B introduces a VPN, which is unnecessary in this context and may complicate connectivity. Option C doesn't enforce the privacy requirement effectively, and Option D, while valid, requires additional steps from a third-party provider.",
        "additional_knowledge": "AWS PrivateLink provides a connection between VPCs within the AWS environment, ensuring secure access to services hosted in a different VPC.",
        "key_terminology": "AWS PrivateLink, VPC, API calls, security groups, least privilege",
        "overall_assessment": "Option A aligns perfectly with the requirements; the community supports this solution strongly. The high voting percentage indicates broad acceptance within the AWS community."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。このソリューションは、AWS PrivateLinkインターフェイスVPCエンドポイントを作成し、インターネットを経由することなくサードパーティのSaaSアプリケーションに安全に接続することを含む。",
        "situation_analysis": "企業はプライベート接続を必要としているため、ソリューションはインターネット経由のトラフィックを許可してはならない。サードパーティのSaaSがAWSのVPC内でホストされているため、AWS PrivateLinkを利用することが適切なアプローチである。",
        "option_analysis": "選択肢Aは、企業の内部ポリシーに従いながら、サードパーティサービスに安全に接続する。選択肢BはVPNを導入しており、この文脈では不必要で接続を複雑にするかもしれない。選択肢Cはプライバシー要件を効果的に遵守できず、選択肢Dは有効ではあるが、サードパーティプロバイダーによる追加手順を必要とする。",
        "additional_knowledge": "AWS PrivateLinkは、AWS環境内でのVPC間の接続を提供し、別のVPCでホストされているサービスへの安全なアクセスを実現する。",
        "key_terminology": "AWS PrivateLink, VPC, APIコール, セキュリティグループ, 最小特権",
        "overall_assessment": "選択肢Aは要件に完全に合致しており、コミュニティはこのソリューションを強く支持している。高い投票率はAWSコミュニティ内での広範な支持を示している。"
      }
    ],
    "keywords": [
      "AWS PrivateLink",
      "VPC",
      "API calls",
      "security groups",
      "least privilege"
    ]
  },
  {
    "No": "13",
    "question": "A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to\nperform patching. Management requires a single report showing the patch status of all the servers and instances.\nWhich set of actions should a solutions architect take to meet these requirements?",
    "question_jp": "企業はサーバーのパッチ処理プロセスを実装する必要があります。オンプレミスのサーバーとAmazon EC2インスタンスは、さまざまなツールを使用してパッチ処理を行っています。経営陣は、すべてのサーバーとインスタンスのパッチ状況を示す単一のレポートを要求しています。これらの要件を満たすために、ソリューションアーキテクトはどのアクションセットを取るべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports.",
        "text_jp": "AWS Systems Managerを使用して、オンプレミスのサーバーとEC2インスタンスのパッチを管理します。Systems Managerを使用して、パッチコンプライアンスレポートを生成します。"
      },
      {
        "key": "B",
        "text": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks to generate patch compliance reports.",
        "text_jp": "AWS OpsWorksを使用して、オンプレミスのサーバーとEC2インスタンスのパッチを管理します。OpsWorksとのAmazon QuickSight統合を使用して、パッチコンプライアンスレポートを生成します。"
      },
      {
        "key": "C",
        "text": "Use an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to generate patch compliance reports.",
        "text_jp": "Amazon EventBridgeルールを使用して、AWS Systems Managerパッチ修復ジョブをスケジュールしてパッチを適用します。Amazon Inspectorを使用して、パッチコンプライアンスレポートを生成します。"
      },
      {
        "key": "D",
        "text": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports.",
        "text_jp": "AWS OpsWorksを使用して、オンプレミスのサーバーとEC2インスタンスのパッチを管理します。AWS X-Rayを使用して、パッチ状況をAWS Systems Manager OpsCenterに投稿し、パッチコンプライアンスレポートを生成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using AWS Systems Manager provides a centralized way to manage patches across both on-premises servers and EC2 instances, and it includes the ability to generate compliance reports directly within the service.",
        "situation_analysis": "The company requires visibility into patch compliance for all servers and instances, necessitating a unified solution. AWS Systems Manager is designed for this purpose.",
        "option_analysis": "Option A is correct because it meets the requirements for both patch management and reporting. Option B introduces unnecessary complexity by adding OpsWorks and QuickSight, which is not needed for basic compliance reporting. Option C does not provide a direct method for patch compliance reporting. Option D also complicates the process with AWS X-Ray, which is not suitable for this requirement.",
        "additional_knowledge": "SysOps and DevOps practices benefit significantly from using AWS Systems Manager for comprehensive patch management.",
        "key_terminology": "AWS Systems Manager, Patch Manager, compliance reports, automation.",
        "overall_assessment": "This question effectively tests knowledge of AWS services concerning patch management. The community supports option A unanimously, indicating strong alignment with best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。AWS Systems Managerは、オンプレミスのサーバーとEC2インスタンス全体のパッチを中央集中的に管理する方法を提供し、サービス内で直接コンプライアンスレポートを生成する能力を含んでいます。",
        "situation_analysis": "企業は、すべてのサーバーとインスタンスのパッチコンプライアンスの可視性を必要としており、統一されたソリューションが必要です。AWS Systems Managerはこの目的のために設計されています。",
        "option_analysis": "選択肢Aは、パッチ管理と報告の要件を満たすため正しいです。選択肢Bは、OpsWorksとQuickSightを追加することで不必要な複雑さを導入しており、基本的なコンプライアンスレポートには必要ありません。選択肢Cは、パッチコンプライアンスレポートを生成する直接的な方法を提供していません。選択肢DはAWS X-Rayを使ってプロセスを複雑にしていますが、この要件には適していません。",
        "additional_knowledge": "SysOpsやDevOpsの実践は、包括的なパッチ管理のためにAWS Systems Managerを使用することから大きな利益を得られます。",
        "key_terminology": "AWS Systems Manager、Patch Manager、コンプライアンスレポート、自動化。",
        "overall_assessment": "この質問は、パッチ管理に関するAWSサービスの知識を効果的にテストしています。コミュニティは選択肢Aを無条件に支持しており、ベストプラクティスとの強い整合性を示しています。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "Patch Manager",
      "compliance reports",
      "automation"
    ]
  },
  {
    "No": "14",
    "question": "A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on\nthe application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied\nto a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2\ninstances.\nWhich set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?",
    "question_jp": "ある企業が、アプリケーションを複数のAmazon EC2インスタンスで実行しており、それはApplication Load Balancerの背後にあるAuto Scalingグループに配置されています。アプリケーションへの負荷は時間帯によって変動し、EC2インスタンスは定期的にスケールインおよびスケールアウトされています。EC2インスタンスからのログファイルは15分ごとに中央のAmazon S3バケットにコピーされています。しかし、セキュリティチームは、一部の停止されたEC2インスタンスからログファイルが欠落していることを発見しました。どのアクションのセットが、停止されたEC2インスタンスから中央のS3バケットにログファイルがコピーされることを確実にするでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group to prevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK.",
        "text_jp": "Amazon S3にログファイルをコピーするスクリプトを作成し、そのスクリプトをEC2インスタンス上のファイルに保存します。Auto Scalingライフサイクルフックを作成し、オートスケーリンググループからのライフサイクルイベントを検出するAmazon EventBridgeルールを作成します。autoscaling:EC2_INSTANCE_TERMINATINGの遷移時にAWS Lambda関数を呼び出し、オートスケーリンググループに対してABANDONを送信して、停止を防ぎ、スクリプトを実行してログファイルをコピーし、AWS SDKを使用してインスタンスを停止します。"
      },
      {
        "key": "B",
        "text": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance.",
        "text_jp": "Amazon S3にログファイルをコピーするスクリプトを含むAWS Systems Managerドキュメントを作成します。Auto Scalingライフサイクルフックを作成し、オートスケーリンググループからのライフサイクルイベントを検出するAmazon EventBridgeルールを作成します。autoscaling:EC2_INSTANCE_TERMINATINGの遷移時にAWS Lambda関数を呼び出し、AWS Systems Manager APIのSendCommand操作を呼び出して、ドキュメントを実行してログファイルをコピーし、Auto ScalingグループにCONTINUEを送信してインスタンスを停止します。"
      },
      {
        "key": "C",
        "text": "Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data. Create an Amazon EventBridge rule to detect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge rule that uses the AWS CLI to run the user-data script to copy the log files and terminate the instance.",
        "text_jp": "ログ配信レートを5分ごとに変更します。ログファイルをAmazon S3にコピーするスクリプトを作成し、そのスクリプトをEC2インスタンスのユーザーデータに追加します。EC2インスタンスの停止を検出するAmazon EventBridgeルールを作成します。EventBridgeルールからAWS Lambda関数を呼び出し、AWS CLIを使用してユーザーデータスクリプトを実行してログファイルをコピーし、インスタンスを停止します。"
      },
      {
        "key": "D",
        "text": "Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (Amazon SNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send ABANDON to the Auto Scaling group to terminate the instance.",
        "text_jp": "Amazon S3にログファイルをコピーするスクリプトを含むAWS Systems Managerドキュメントを作成します。Amazon SNSトピックにメッセージを発行するAuto Scalingライフサイクルフックを作成します。SNS通知からAWS Systems Manager APIのSendCommand操作を呼び出してドキュメントを実行し、ログファイルをコピーし、Auto ScalingグループにABANDONを送信してインスタンスを停止します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This approach uses AWS Systems Manager to create a scalable solution that triggers log file copying when instances are terminating.",
        "situation_analysis": "Log files are not consistently copied from terminated EC2 instances, indicating a need for a robust mechanism to ensure file transfer before termination.",
        "option_analysis": "Option B allows pre-termination execution of commands managed by AWS Systems Manager, ensuring that log files are copied before the instance is terminated safely.",
        "additional_knowledge": "By using AWS Systems Manager, commands can also be executed asynchronously, ensuring operations do not block instance termination.",
        "key_terminology": "AWS Systems Manager, SendCommand, Auto Scaling, EventBridge, Lambda, S3",
        "overall_assessment": "This question tests understanding of the interaction between Auto Scaling and AWS Systems Manager. The community heavily supports option B, indicating it is viewed as the most effective method."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。このアプローチは、AWS Systems Managerを使用して、インスタンスが終了する際にログファイルのコピーをトリガーするスケーラブルなソリューションを提供する。",
        "situation_analysis": "ログファイルが停止されたEC2インスタンスから一貫してコピーされていないことは、終了前にファイル転送を確実にするための堅牢なメカニズムが必要であることを示している。",
        "option_analysis": "選択肢Bでは、AWS Systems Managerによって管理されるコマンドの事前終了時の実行が可能であり、そのためログファイルがインスタンスが安全に終了される前にコピーされることが保証される。",
        "additional_knowledge": "AWS Systems Managerを使用することで、コマンドを非同期に実行することも可能であり、処理がインスタンスの終了をブロックすることがない。",
        "key_terminology": "AWS Systems Manager、SendCommand、Auto Scaling、EventBridge、Lambda、S3",
        "overall_assessment": "この質問は、Auto ScalingとAWS Systems Managerの相互作用に対する理解を試すものである。コミュニティは選択肢Bを強く支持しており、最も効果的な方法として見なされていることを示している。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "SendCommand",
      "Auto Scaling",
      "EventBridge",
      "Lambda",
      "S3"
    ]
  },
  {
    "No": "15",
    "question": "A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The\ncompany's applications and databases are running in Account B.\nA solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the\nAmazon RDS endpoint was created in a private hosted zone for Amazon Route 53.\nDuring deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance.\nThe solutions architect confirmed that the record set was created correctly in Route 53.\nWhich combination of steps should the solutions architect take to resolve this issue? (Choose two.)",
    "question_jp": "ある企業が複数のAWSアカウントを使用しています。DNSレコードはアカウントAのAmazon Route 53のプライベートホステッドゾーンに保存されています。\n企業のアプリケーションとデータベースはアカウントBで実行されています。\nソリューションアーキテクトは、新しいVPCに二層アプリケーションを展開する予定です。設定を簡素化するために、Amazon RDSエンドポイントのためのdb.example.com CNAMEレコードセットが、Amazon Route 53のプライベートホステッドゾーンに作成されました。\n展開中に、アプリケーションは起動に失敗しました。トラブルシューティングの結果、db.example.comがAmazon EC2インスタンス上で解決できないことが判明しました。\nソリューションアーキテクトは、Route 53にレコードセットが正しく作成されていることを確認しました。\nこの問題を解決するために、ソリューションアーキテクトはどの組み合わせの手順を取るべきですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance's private IP in the private hosted zone.",
        "text_jp": "データベースを新しいVPC内の別のEC2インスタンスに展開する。インスタンスのプライベートIPに対するレコードセットをプライベートホステッドゾーンに作成する。"
      },
      {
        "key": "B",
        "text": "Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.",
        "text_jp": "SSHを使用してアプリケーション層のEC2インスタンスに接続する。/etc/resolv.confファイルにRDSエンドポイントIPアドレスを追加する。"
      },
      {
        "key": "C",
        "text": "Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.",
        "text_jp": "アカウントAのプライベートホステッドゾーンをアカウントBの新しいVPCに関連付けるための認可を作成する。"
      },
      {
        "key": "D",
        "text": "Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts.",
        "text_jp": "アカウントBにexample.comドメイン用のプライベートホステッドゾーンを作成する。AWSアカウント間でのRoute 53のレプリケーションを設定する。"
      },
      {
        "key": "E",
        "text": "Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.",
        "text_jp": "アカウントBの新しいVPCをアカウントAのホステッドゾーンに関連付ける。アカウントAの認可を削除する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is to connect to the application EC2 instance using SSH and add the RDS endpoint IP address to the /etc/resolv.conf file. This resolves name resolution issues.",
        "situation_analysis": "The application is deployed but cannot resolve db.example.com, indicating a DNS configuration issue between the accounts.",
        "option_analysis": "Option B fixes the immediate DNS resolution problem. Option C would be necessary to set up traffic correctly, but without immediate fixes to /etc/resolv.conf, it won't help the application start.",
        "additional_knowledge": "It’s crucial to ensure proper associations and authorizations exist for core services like RDS across multiple accounts.",
        "key_terminology": "Route 53, VPC Peering, private hosted zone, DNS resolution, RDS endpoint",
        "overall_assessment": "This scenario highlights the need for careful consideration of DNS setup in multi-account AWS environments. The community supports this because it directly targets the involved issue."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は、SSHを使用してアプリケーションEC2インスタンスに接続し、/etc/resolv.confファイルにRDSエンドポイントのIPアドレスを追加することである。これにより、名前解決の問題が解決される。",
        "situation_analysis": "アプリケーションは展開されているが、db.example.comを解決できないため、アカウント間のDNS構成に問題があることを示している。",
        "option_analysis": "選択肢Bは、即座にDNS解決の問題を修正する。選択肢Cはトラフィックを正しく設定するために必要だが、/etc/resolv.confに対する即時の修正なしではアプリケーションの起動を助けない。",
        "additional_knowledge": "RDSなどのコアサービスを複数のアカウント間で適切に関連付け、承認が存在することを確認することが重要である。",
        "key_terminology": "Route 53、VPCピアリング、プライベートホステッドゾーン、DNS解決、RDSエンドポイント",
        "overall_assessment": "このシナリオは、マルチアカウントAWS環境でのDNS設計に対する慎重な考慮の必要性を強調している。コミュニティは、関与する問題に直接アプローチするため、この選択肢を支持している。"
      }
    ],
    "keywords": [
      "Route 53",
      "VPC Peering",
      "private hosted zone",
      "DNS resolution",
      "RDS endpoint"
    ]
  },
  {
    "No": "16",
    "question": "A company used Amazon EC2 instances to deploy a web fieet to host a blog site. The EC2 instances are behind an Application Load Balancer\n(ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.\nThe company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user trafic. At peak times of day,\nusers report buffering and timeout issues while attempting to reach the site or watch videos.\nWhich is the MOST cost-eficient and scalable deployment that will resolve the issues for users?",
    "question_jp": "ある企業が、ブログサイトをホストするためにAmazon EC2インスタンスを使用してWebファーストを展開しました。EC2インスタンスはアプリケーションロードバランサー（ALB）の背後にあり、オートスケーリンググループに構成されています。Webアプリケーションは、すべてのブログコンテンツをAmazon EFSボリュームに保存しています。この企業は最近、ブロガーが投稿にビデオを追加できる機能を追加し、以前の10倍のユーザートラフィックを引き寄せました。ピーク時に、ユーザーはサイトにアクセスしたり、ビデオを視聴したりしようとするときにバッファリングやタイムアウトの問題を報告しています。ユーザーの問題を解決するための最もコスト効率的かつスケーラブルな展開はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Reconfigure Amazon EFS to enable maximum I/O.",
        "text_jp": "Amazon EFSを再構成して最大I/Oを有効にする。"
      },
      {
        "key": "B",
        "text": "Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown.",
        "text_jp": "ブログサイトをインスタンスストレージボリュームを使用するように更新する。起動時にサイトコンテンツをボリュームにコピーし、シャットダウン時にAmazon S3にコピーする。"
      },
      {
        "key": "C",
        "text": "Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.",
        "text_jp": "Amazon CloudFrontディストリビューションを構成する。ディストリビューションをS3バケットにポイントし、ビデオをEFSからAmazon S3に移行する。"
      },
      {
        "key": "D",
        "text": "Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB.",
        "text_jp": "すべてのサイトコンテンツについてAmazon CloudFrontディストリビューションを設定し、ディストリビューションをALBにポイントする。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (98%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which involves configuring an Amazon CloudFront distribution to enhance content delivery performance and migrating videos to Amazon S3 to reduce latency and buffering issues.",
        "situation_analysis": "The company is experiencing significant user traffic due to the new video feature, causing performance issues with EC2 instances and Amazon EFS. The primary goal is to efficiently handle the increased load during peak times.",
        "option_analysis": "Option A does not effectively address the root cause of high latency when delivering content. Option B introduces unnecessary complexity and does not scale well with increased traffic. Option D would optimize only the website's static resources but still relies on the EFS, which is experiencing performance issues. Option C effectively leverages CloudFront for content caching and reduces load on EFS by moving video content to S3.",
        "additional_knowledge": "Using CloudFront also improves the overall user experience by providing faster access to videos and reducing load on back-end servers.",
        "key_terminology": "Amazon EFS, Amazon S3, Amazon CloudFront, Auto Scaling, latency.",
        "overall_assessment": "The community overwhelmingly supports answer C (98% votes). This option meets the requirements of cost efficiency, scalability, and addressing performance issues during peak traffic."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCであり、Amazon CloudFrontディストリビューションを構成してコンテンツ配信のパフォーマンスを向上させ、ビデオをAmazon S3に移行してバッファリングや遅延問題を軽減します。",
        "situation_analysis": "企業は新しいビデオ機能により、ユーザートラフィックが急増し、EC2インスタンスとAmazon EFSのパフォーマンスに問題が生じています。主な目標は、ピーク時に増加した負荷を効果的に処理することです。",
        "option_analysis": "選択肢Aは、コンテンツ配信の遅延を改善する根本的な原因には対処していません。選択肢Bは不要な複雑さを導入し、トラフィックの増加にはうまくスケーリングしません。選択肢Dは、Webサイトの静的リソースのみを最適化しますが、EFSへの依存は残ります。選択肢Cは、CloudFrontを活用してコンテンツをキャッシュし、ビデオコンテンツをS3に移動することによりEFSへの負担を軽減します。",
        "additional_knowledge": "CloudFrontを使用することで、ビデオへのアクセスが迅速になり、バックエンドサーバーへの負担が軽減されるため、全体的なユーザー体験が向上します。",
        "key_terminology": "Amazon EFS, Amazon S3, Amazon CloudFront, オートスケーリング, 遅延。",
        "overall_assessment": "コミュニティは圧倒的にCの答えを支持しています（98％の投票）。この選択肢は、コスト効率、スケーラビリティ、およびピークトラフィック中のパフォーマンス問題の解決に適しています。"
      }
    ],
    "keywords": [
      "Amazon EFS",
      "Amazon S3",
      "Amazon CloudFront",
      "Auto Scaling",
      "latency"
    ]
  },
  {
    "No": "17",
    "question": "A company with global ofices has a single 1 Gbps AWS Direct Connect connection to a single AWS Region. The company's on-premises network\nuses the connection to communicate with the company's resources in the AWS Cloud. The connection has a single private virtual interface that\nconnects to a single VPC.\nA solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. The solution also must\nprovide connectivity to other Regions through the same pair of Direct Connect connections as the company expands into other Regions.\nWhich solution meets these requirements?",
    "question_jp": "グローバルにオフィスを持つ企業には、単一の1 Gbps AWS Direct Connect接続があり、単一のAWSリージョンに接続されています。この企業のオンプレミスネットワークは、AWSクラウド内のリソースと通信するためにこの接続を使用しています。この接続には、単一のVPCに接続された単一のプライベート仮想インターフェイスがあります。ソリューションアーキテクトは、同じリージョンに冗長のDirect Connect接続を追加するソリューションを実装しなければなりません。このソリューションは、企業が他のリージョンに拡大するにあたり、同じペアのDirect Connect接続を介して他のリージョンへの接続も提供する必要があります。どのソリューションがこれらの要件を満たしていますか？",
    "choices": [
      {
        "key": "A",
        "text": "Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC.",
        "text_jp": "Direct Connectゲートウェイをプロビジョニングします。既存の接続から既存のプライベート仮想インターフェイスを削除します。2番目のDirect Connect接続を作成します。それぞれの接続に新しいプライベート仮想インターフェイスを作成し、両方のプライベート仮想インターフェイスをDirect Connectゲートウェイに接続します。Direct Connectゲートウェイを単一のVPCに接続します。"
      },
      {
        "key": "B",
        "text": "Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new private virtual interface on the new connection, and connect the new private virtual interface to the single VPC.",
        "text_jp": "既存のプライベート仮想インターフェイスを維持します。2番目のDirect Connect接続を作成します。新しい接続に新しいプライベート仮想インターフェイスを作成し、新しいプライベート仮想インターフェイスを単一のVPCに接続します。"
      },
      {
        "key": "C",
        "text": "Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new public virtual interface on the new connection, and connect the new public virtual interface to the single VPC.",
        "text_jp": "既存のプライベート仮想インターフェイスを維持します。2番目のDirect Connect接続を作成します。新しい接続に新しいパブリック仮想インターフェイスを作成し、新しいパブリック仮想インターフェイスを単一のVPCに接続します。"
      },
      {
        "key": "D",
        "text": "Provision a transit gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the transit gateway. Associate the transit gateway with the single VPC.",
        "text_jp": "トランジットゲートウェイをプロビジョニングします。既存の接続から既存のプライベート仮想インターフェイスを削除します。2番目のDirect Connect接続を作成します。それぞれの接続に新しいプライベート仮想インターフェイスを作成し、両方のプライベート仮想インターフェイスをトランジットゲートウェイに接続します。トランジットゲートウェイを単一のVPCに関連付けます。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, which provides the required redundancy and allows connectivity to other regions through a Direct Connect gateway.",
        "situation_analysis": "The company requires redundancy in its Direct Connect connection and wants to expand connectivity to additional AWS regions.",
        "option_analysis": "Option A meets the redundancy requirement by provisioning a Direct Connect gateway, allowing multiple virtual interfaces and connections. Option B does not provide redundancy. Option C wrongly suggests using a public virtual interface, which does not satisfy VPC connectivity needs. Option D suggests a transit gateway which is not necessary in this context.",
        "additional_knowledge": "Direct Connect connections are physical connections to AWS data centers, and planning for redundancy is crucial for maintaining uptime.",
        "key_terminology": "Direct Connect, Direct Connect gateway, VPC, redundancy, transit gateway",
        "overall_assessment": "The question is well structured and tests knowledge of AWS network architecture effectively. The community supports Option A unanimously, indicating a strong agreement with its correctness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAであり、要求される冗長性を提供し、Direct Connectゲートウェイを介して他のリージョンへの接続を可能にします。",
        "situation_analysis": "この企業はDirect Connect接続の冗長性を必要としており、追加のAWSリージョンへの接続を拡張したいと考えています。",
        "option_analysis": "選択肢AはDirect Connectゲートウェイをプロビジョニングすることによって冗長性要件を満たしており、複数の仮想インターフェイスと接続を可能にします。選択肢Bは冗長性を提供しません。選択肢Cはパブリック仮想インターフェイスの使用を誤って提案しており、VPC接続の要件を満たしません。選択肢Dは不必要なトランジットゲートウェイを提案しています。",
        "additional_knowledge": "Direct Connect接続はAWSデータセンターへの物理接続であり、冗長性の計画はアップタイムを維持するために重要です。",
        "key_terminology": "Direct Connect, Direct Connectゲートウェイ, VPC, 冗長性, トランジットゲートウェイ",
        "overall_assessment": "この質問は構造が良く、AWSネットワークアーキテクチャの知識を効果的にテストします。コミュニティは選択肢Aを全会一致で支持しており、その正当性に強い合意が示されています。"
      }
    ],
    "keywords": [
      "Direct Connect",
      "Direct Connect gateway",
      "VPC",
      "redundancy",
      "transit gateway"
    ]
  },
  {
    "No": "18",
    "question": "A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by\ncustom recognition software for categorization.\nThe website contains static content that has variable trafic with peaks in certain months. The architecture consists of Amazon EC2 instances\nrunning in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue.\nThe company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove\ndependencies on third-party software.\nWhich solution meets these requirements?",
    "question_jp": "ある企業は、ユーザーが短いビデオをアップロードできるウェブアプリケーションを持っています。これらのビデオはAmazon EBSボリュームに保存され、カスタム認識ソフトウェアによってカテゴリ分けのために分析されます。ウェブサイトには、特定の月にトラフィックが変動する静的コンテンツがあります。このアーキテクチャは、ウェブアプリケーション用のAuto Scalingグループで実行されるAmazon EC2インスタンスと、Amazon SQSキューを処理するためのAuto Scalingグループで実行されるEC2インスタンスで構成されています。企業は、可能な限りAWSのマネージドサービスを活用し、サードパーティのソフトウェアへの依存を排除することで、アプリケーションの運用オーバーヘッドを削減したいと考えています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.",
        "text_jp": "ウェブアプリケーション用にAmazon ECSコンテナを使用し、SQSキューを処理するAuto ScalingグループにSpotインスタンスを使用します。カスタムソフトウェアをAmazon Rekognitionに置き換えて、ビデオをカテゴリ分けします。"
      },
      {
        "key": "B",
        "text": "Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
        "text_jp": "アップロードされたビデオをAmazon EFSに保存し、ファイルシステムをウェブアプリケーション用のEC2インスタンスにマウントします。SQSキューを処理するためにAWS Lambda関数を使用してAmazon Rekognition APIを呼び出し、ビデオをカテゴリ分けします。"
      },
      {
        "key": "C",
        "text": "Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
        "text_jp": "ウェブアプリケーションをAmazon S3でホストし、アップロードされたビデオをAmazon S3に保存します。S3イベント通知を使用してSQSキューにイベントを発行し、AWS Lambda関数を使用してAmazon Rekognition APIを呼び出してビデオをカテゴリ分けします。"
      },
      {
        "key": "D",
        "text": "Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.",
        "text_jp": "AWS Elastic Beanstalkを使用してウェブアプリケーション用のAuto ScalingグループにEC2インスタンスを起動し、SQSキューを処理するためにワーカー環境を起動します。カスタムソフトウェアをAmazon Rekognitionに置き換えて、ビデオをカテゴリ分けします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (85%) D (15%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using AWS Elastic Beanstalk to manage the infrastructure simplifies deployment and scaling, while leveraging Amazon Rekognition streamlines the categorization process.",
        "situation_analysis": "The company is aiming to reduce operational overhead by adopting AWS managed services and eliminating third-party dependencies. It requires a scalable and efficient way to handle video uploads and analysis.",
        "option_analysis": "Option D provides the best approach by utilizing AWS Elastic Beanstalk, which automates the provisioning of EC2 instances and scaling operations. It directly addresses operational overhead with a managed service solution. Other options either require more management overhead or do not fully leverage AWS managed services for optimal performance.",
        "additional_knowledge": "",
        "key_terminology": "AWS Elastic Beanstalk, Amazon Rekognition, Auto Scaling, SQS, managed services",
        "overall_assessment": "Although community vote distribution shows a strong preference for option C, option D aligns best with the company's goal to reduce operational overhead by using managed services."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。AWS Elastic Beanstalkを使用してインフラストラクチャを管理すると、デプロイメントとスケーリングが簡素化され、Amazon Rekognitionを活用することでカテゴリ分けプロセスが効率化される。",
        "situation_analysis": "企業はAWSのマネージドサービスを採用し、サードパーティ依存を排除することで運用コストを削減することを目指している。ビデオのアップロードと分析を処理するためのスケーラブルで効率的な方法が必要である。",
        "option_analysis": "Dの選択肢は、EC2インスタンスのプロビジョニングとスケーリング操作を自動化するAWS Elastic Beanstalkを利用することで、運用コストを最適化する点で最も適切なアプローチを提案している。他の選択肢は、より多くの管理オーバーヘッドを必要とするか、完全にAWSのマネージドサービスを活用していない。",
        "additional_knowledge": "",
        "key_terminology": "AWS Elastic Beanstalk, Amazon Rekognition, Auto Scaling, SQS, マネージドサービス",
        "overall_assessment": "コミュニティの投票分布ではCの選択肢が強い支持を得ているが、運用オーバーヘッドを削減するという企業の目標に最も合致しているのはDの選択肢である。"
      }
    ],
    "keywords": [
      "AWS Elastic Beanstalk",
      "Amazon Rekognition",
      "Auto Scaling",
      "SQS",
      "managed services"
    ]
  },
  {
    "No": "19",
    "question": "A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current\ndeployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the\nnew function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to\ndecrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert\nwhen errors are identified.\nHow can this be accomplished?",
    "question_jp": "ある企業は、Amazon CloudFront、Amazon API Gateway、およびAWS Lambda関数で構成されるサーバーレスアプリケーションを持っている。このアプリケーションコードの現在のデプロイメントプロセスは、Lambda関数の新しいバージョン番号を作成し、AWS CLIスクリプトを実行して更新することである。新しい関数バージョンにエラーが発生した場合、別のCLIスクリプトを実行して、以前の動作していたバージョンの関数をデプロイする。企業は、Lambda関数によって提供されるアプリケーションロジックの新しいバージョンをデプロイする時間を短縮し、エラーが識別されたときのリバートにかかる時間も短縮したいと考えている。これをどのように達成できるか？",
    "choices": [
      {
        "key": "A",
        "text": "Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version.",
        "text_jp": "親スタックがAWS CloudFrontディストリビューションとAPI Gatewayで構成され、子スタックがLambda関数を含むネストされたAWS CloudFormationスタックを作成およびデプロイする。Lambdaの変更がある場合、AWS CloudFormationチェンジセットを作成してデプロイし、エラーが発生した場合は、AWS CloudFormationチェンジセットを以前のバージョンにリバートする。"
      },
      {
        "key": "B",
        "text": "Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift trafic to the new version, and use pre-trafic and post-trafic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.",
        "text_jp": "AWS SAMと組み込みのAWS CodeDeployを使用して新しいLambdaバージョンをデプロイし、徐々にトラフィックを新しいバージョンにシフトさせ、事前トラフィックおよび事後トラフィックのテスト関数を使用してコードを検証する。Amazon CloudWatchアラームがトリガーされた場合はロールバックする。"
      },
      {
        "key": "C",
        "text": "Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version.",
        "text_jp": "AWS CLIスクリプトを単一のスクリプトにリファクタリングして、新しいLambdaバージョンをデプロイする。デプロイが完了すると、スクリプトのテストを実行する。エラーが検出された場合、以前のLambdaバージョンにリバートする。"
      },
      {
        "key": "D",
        "text": "Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint.",
        "text_jp": "新しいAPI Gatewayエンドポイントを含むAWS CloudFormationスタックを作成およびデプロイし、新しいLambdaバージョンを参照する。CloudFrontオリジンを新しいAPI Gatewayエンドポイントに変更し、エラーを監視し、検出された場合はAWS CloudFrontオリジンを以前のAPI Gatewayエンドポイントに変更する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. AWS SAM in conjunction with AWS CodeDeploy enables easier deployment and rollback of Lambda function versions.",
        "situation_analysis": "The company is looking to streamline their deployment process and improve error detection and reversal times, which can be achieved through modern deployment practices.",
        "option_analysis": "Option B facilitates a controlled traffic shift and verifies functionality before fully deploying the new version, whereas other options lack this structured approach.",
        "additional_knowledge": "Using traffic shifting allows for progressive exposure of changes, reducing impact from potential errors.",
        "key_terminology": "AWS SAM, AWS CodeDeploy, blue/green deployment, rollback, CloudWatch alarms.",
        "overall_assessment": "Given the community vote confirms option B as the best practice, its implementation aligns with AWS's recommended strategies for serverless architectures."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。AWS SAMとAWS CodeDeployを組み合わせることで、Lambda関数のバージョンのデプロイとロールバックを容易にすることができる。",
        "situation_analysis": "企業はデプロイメントプロセスを合理化し、エラー検出とリバートの時間を改善したいと考えており、これは最新のデプロイメント手法を通じて達成可能である。",
        "option_analysis": "オプションBは、制御されたトラフィックシフトを可能にし、新しいバージョンを完全にデプロイする前に機能を検証する。一方、他のオプションはこの構造化されたアプローチを欠いている。",
        "additional_knowledge": "トラフィックシフトを使用することで、変更の段階的な露出が可能になり、潜在的なエラーからの影響を軽減する。",
        "key_terminology": "AWS SAM、AWS CodeDeploy、ブルー・グリーンデプロイメント、ロールバック、CloudWatchアラーム。",
        "overall_assessment": "コミュニティの投票がオプションBを最良の実践として確認していることから、その実装はサーバーレスアーキテクチャに対するAWSの推奨戦略と一致している。"
      }
    ],
    "keywords": [
      "AWS SAM",
      "AWS CodeDeploy",
      "blue/green deployment",
      "rollback",
      "CloudWatch alarms"
    ]
  },
  {
    "No": "20",
    "question": "A company is planning to store a large number of archived documents and make the documents available to employees through the corporate\nintranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible\nto the public.\nThe documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low.\nAvailability and speed of retrieval are not concerns of the company.\nWhich solution will meet these requirements at the LOWEST cost?",
    "question_jp": "ある企業が多数のアーカイブ文書を保存し、企業のイントラネットを通じて従業員に文書を提供する計画を立てています。従業員は、VPCに接続されたクライアントVPNサービスを介してシステムにアクセスします。データは公共にはアクセスできない必要があります。企業が保存する文書は、他の場所に物理メディアとして保持されているデータのコピーです。リクエストの数は少なくなります。可用性と取得速度は企業の懸念事項ではありません。これらの要件を最低コストで満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.",
        "text_jp": "Amazon S3バケットを作成します。S3バケットのデフォルトとしてS3 One Zone-Infrequent Access (S3 One Zone-IA)ストレージクラスを使用するように構成します。S3バケットをウェブサイトホスティング用に構成します。S3インターフェースエンドポイントを作成します。S3バケットがそのエンドポイントを通じてのみアクセスを許可するように構成します。"
      },
      {
        "key": "B",
        "text": "Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class Configure the instance security groups to allow access only from private networks.",
        "text_jp": "ウェブサーバーを実行するAmazon EC2インスタンスを起動します。アーカイブデータをEFS One Zone-Infrequent Access (EFS One Zone-IA)ストレージクラスに保存するために、Amazon Elastic File System (Amazon EFS)ファイルシステムをアタッチします。インスタンスのセキュリティグループを構成して、プライベートネットワークからのみアクセスを許可します。"
      },
      {
        "key": "C",
        "text": "Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instance security groups to allow access only from private networks.",
        "text_jp": "ウェブサーバーを実行するAmazon EC2インスタンスを起動します。アーカイブデータを保存するために、Amazon Elastic Block Store (Amazon EBS)ボリュームをアタッチします。Cold HDD (sc1)ボリュームタイプを使用します。インスタンスのセキュリティグループを構成して、プライベートネットワークからのみアクセスを許可します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.",
        "text_jp": "Amazon S3バケットを作成します。S3バケットのデフォルトとしてS3 Glacier Deep Archiveストレージクラスを使用するように構成します。S3バケットをウェブサイトホスティング用に構成します。S3インターフェースエンドポイントを作成します。S3バケットがそのエンドポイントを通じてのみアクセスを許可するように構成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (65%) D (34%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. The S3 Glacier Deep Archive storage class is specifically designed for long-term data archiving at the lowest cost.",
        "situation_analysis": "The company needs a cost-effective solution for storing archived documents that are not frequently accessed, and they must ensure this data is not publicly accessible.",
        "option_analysis": "Option D meets the requirements by using S3 Glacier Deep Archive for low-cost storage. Options A and B are more expensive due to the storage classes they utilize. Option C is not as cost-effective for archiving purposes due to the type of EBS used.",
        "additional_knowledge": "Organizations often use S3 Glacier Deep Archive for compliance and regulatory data retention due to its cost-effectiveness and security features.",
        "key_terminology": "Amazon S3, S3 Glacier Deep Archive, interface endpoint, VPN, private network",
        "overall_assessment": "The voting shows a significant preference for option A, but option D is the most cost-effective solution for the given requirements. Users may not have fully considered the very low access frequency and long-term storage needs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。S3 Glacier Deep Archiveストレージクラスは、最低コストでの長期データアーカイブ専用に設計されています。",
        "situation_analysis": "企業は頻繁にアクセスされないアーカイブ文書をコスト効果の高い方法で保存する必要があり、そのデータは公共からアクセスできないようにする必要があります。",
        "option_analysis": "選択肢Dは、低コストのストレージのためにS3 Glacier Deep Archiveを使用することで要件に適合します。選択肢AとBは使用するストレージクラスが高価なためです。選択肢Cは、使用するEBSタイプによりアーカイブ目的にはあまりコスト効果が高くありません。",
        "additional_knowledge": "組織は、コスト効果とセキュリティ機能のために、コンプライアンスや規制のデータ保持にS3 Glacier Deep Archiveを使用することがよくあります。",
        "key_terminology": "Amazon S3、S3 Glacier Deep Archive、インターフェースエンドポイント、VPN、プライベートネットワーク",
        "overall_assessment": "投票では選択肢Aへの顕著な支持が示されていますが、選択肢Dは与えられた要件に対して最もコスト効果の高い解決策です。ユーザーは非常に低いアクセス頻度と長期ストレージのニーズを十分に考慮していない可能性があります。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "interface endpoint",
      "VPN",
      "private network"
    ]
  },
  {
    "No": "21",
    "question": "A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to\nsign in to the company's AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-\npremises environment and all the company's AWS accounts.\nThe company's security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a\nsingle location.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がオンプレミスのActive Directoryサービスをユーザー認証に利用しています。この企業は、AWS Organizationsを使用しているAWSアカウントにサインインするために、同じ認証サービスを利用したいと考えています。企業のオンプレミス環境とすべてのAWSアカウントの間にはすでにAWS Site-to-Site VPN接続が存在しています。企業のセキュリティポリシーにより、ユーザーグループおよびロールに基づいてアカウントへの条件付きアクセスが要求されます。ユーザーIDは単一の場所で管理する必要があります。これらの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using attribute-based access controls (ABACs).",
        "text_jp": "AWS IAM Identity Center (AWS Single Sign-On) を設定し、SAML 2.0を使用してActive Directoryに接続します。System for Cross-domain Identity Management (SCIM) v2.0プロトコルを使用して自動プロビジョニングを有効にします。属性ベースのアクセス制御 (ABAC) を使用してAWSアカウントへのアクセスを許可します。"
      },
      {
        "key": "B",
        "text": "Configure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using IAM Identity Center permission sets.",
        "text_jp": "AWS IAM Identity Center (AWS Single Sign-On) を設定し、IAM Identity Centerをアイデンティティソースとして使用します。System for Cross-domain Identity Management (SCIM) v2.0プロトコルを使用して自動プロビジョニングを有効にします。IAM Identity Centerの権限セットを使用してAWSアカウントへのアクセスを許可します。"
      },
      {
        "key": "C",
        "text": "In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provider. Provision IAM users that are mapped to the federated users. Grant access that corresponds to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM users.",
        "text_jp": "企業のAWSアカウントの1つで、SAML 2.0アイデンティティプロバイダーを使用するようにAWS Identity and Access Management (IAM) を設定します。フェデレーテッドユーザーにマッピングされたIAMユーザーをプロビジョニングします。Active Directory内の適切なグループに対応するアクセスを許可します。必要なAWSアカウントへのアクセスをcross-account IAMユーザーを使用して許可します。"
      },
      {
        "key": "D",
        "text": "In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity provider. Provision IAM roles that grant access to the AWS account for the federated users that correspond to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM roles.",
        "text_jp": "企業のAWSアカウントの1つで、OpenID Connect (OIDC) アイデンティティプロバイダーを使用するようにAWS Identity and Access Management (IAM) を設定します。Active Directory内の適切なグループに対応するフェデレーテッドユーザー向けに、AWSアカウントへのアクセスを許可するIAMロールをプロビジョニングします。必要なAWSアカウントへのアクセスをcross-account IAMロールを使用して許可します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (82%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This option details a configuration that allows the use of an OIDC identity provider, which can facilitate the integration of Active Directory with AWS accounts.",
        "situation_analysis": "The company requires an authentication method that allows users from on-premises Active Directory to access AWS accounts with appropriate controls based on groups and roles.",
        "option_analysis": "Option D uses OIDC, which is suitable for federated authentication, allowing users in Active Directory to authenticate seamlessly while managing permissions through IAM roles. Other options either do not fully meet the conditional access requirements or involve unnecessary complexity.",
        "additional_knowledge": "Using IAM roles for cross-account access minimizes security risks, as it avoids the direct federation of IAM users across accounts.",
        "key_terminology": "OpenID Connect, OIDC, AWS IAM, federated authentication, cross-account IAM roles",
        "overall_assessment": "Despite community voting heavily favoring option A, option D is preferable due to the security policies requiring group-based access. The community may support option A out of familiarity with AWS SSO, but it does not match the strict conditional access requirements as effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。この選択肢は、Active DirectoryとAWSアカウントとの統合を可能にするOIDCアイデンティティプロバイダーの利用を詳細に説明している。",
        "situation_analysis": "企業は、オンプレミスのActive DirectoryからAWSアカウントへの適切な制御に基づいたユーザーのアクセスを許可する認証方法を必要としている。",
        "option_analysis": "選択肢DはOIDCを使用しており、フェデレーテッド認証に適しており、Active Directoryのユーザーがシームレスに認証され、IAMロールを通じて権限が管理されることを可能にする。他の選択肢は、条件付きアクセス要件を完全には満たしていないか、不必要な複雑さを伴う。",
        "additional_knowledge": "cross-accountアクセスのためにIAMロールを使用することは、セキュリティリスクを最小限に抑えるため、アカウント間でのIAMユーザーの直接フェデレーションを避ける。",
        "key_terminology": "OpenID Connect, OIDC, AWS IAM, フェデレーテッド認証, クロスアカウントIAMロール",
        "overall_assessment": "コミュニティ投票が選択肢Aを大いに支持しているにもかかわらず、選択肢Dはユーザーグループに基づくアクセスを必要とするセキュリティポリシーにより、より好ましい。コミュニティはAWS SSOに対する親しみから選択肢Aを選んでいる可能性があるが、それは厳格な条件付きアクセス要件をそれほど効果的に満たしていない。"
      }
    ],
    "keywords": [
      "OpenID Connect",
      "OIDC",
      "AWS IAM",
      "federated authentication",
      "cross-account IAM roles"
    ]
  },
  {
    "No": "22",
    "question": "A software company has deployed an application that consumes a REST API by using Amazon API Gateway, AWS Lambda functions, and an\nAmazon DynamoDB table. The application is showing an increase in the number of errors during PUT requests. Most of the PUT calls come from a\nsmall number of clients that are authenticated with specific API keys.\nA solutions architect has identified that a large number of the PUT requests originate from one client. The API is noncritical, and clients can\ntolerate retries of unsuccessful calls. However, the errors are displayed to customers and are causing damage to the API's reputation.\nWhat should the solutions architect recommend to improve the customer experience?",
    "question_jp": "あるソフトウェア会社が、Amazon API Gateway、AWS Lambda 関数、および Amazon DynamoDB テーブルを使用して REST API を消費するアプリケーションを展開しました。このアプリケーションでは、PUT リクエスト中にエラーが増加しています。ほとんどの PUT 呼び出しは、特定の API キーで認証されている少数のクライアントから行われています。ソリューションアーキテクトは、数多くの PUT リクエストが1つのクライアントから発生していることを特定しました。API は非重要であり、クライアントは不成功の呼び出しの再試行を許容できます。しかし、エラーは顧客に表示されており、API の評判に損害を与えています。ソリューションアーキテクトは、顧客体験を向上させるために何を推奨すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Implement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and handled with descriptive error messages.",
        "text_jp": "クライアントアプリケーションに指数バックオフと不規則なバリエーションを持つ再試行ロジックを実装する。エラーをキャッチし、説明的なエラーメッセージを用いて処理されることを確認する。"
      },
      {
        "key": "B",
        "text": "Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.",
        "text_jp": "API Gateway レベルで使用プランを介して API スロットリングを実装する。クライアントアプリケーションが 429 の応答コードをエラーなしで処理できることを確認する。"
      },
      {
        "key": "C",
        "text": "Turn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is appropriate for the workload.",
        "text_jp": "API キャッシングを有効にし、製品ステージの応答性を向上させる。10 分間の負荷テストを実行する。キャッシュ容量が作業負荷に適しているか確認する。"
      },
      {
        "key": "D",
        "text": "Implement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in trafic.",
        "text_jp": "Lambda 関数レベルでの予約された同時実行を実装し、トラフィックの急増時に必要なリソースを提供する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (68%) A (31%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Implementing API throttling through a usage plan at the API Gateway level can help manage the excessive number of PUT requests originating from the problematic client. By enforcing limits on usage, it can help reduce the error rates and improve the overall customer experience.",
        "situation_analysis": "The application has a notable increase in PUT request errors, primarily from a single client. This points towards either misuse or abuse of the API by that client, potentially leading to service degradation.",
        "option_analysis": "Option A suggests implementing retry logic, which could help with transient errors but does not address the underlying cause of excessive requests. Option C focuses on caching, which is beneficial but not directly linked to error reduction. Option D suggests increasing Lambda resources but does not manage the client requests effectively.",
        "additional_knowledge": "Monitoring and analytics tools can also be employed to track API usage and identify further improvement areas.",
        "key_terminology": "API Gateway, Throttling, Usage Plan, 429 Error, Client Management",
        "overall_assessment": "The recommendation aligns with AWS best practices for managing API usage and preventing undue strain on backend systems. Community voting supports this decision strongly with 68% of votes."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。API Gateway レベルでの使用プランを介した API スロットリングの実装は、問題のあるクライアントから発生している過剰な PUT リクエストを管理するのに役立ちます。使用制限を強制することにより、エラー率を削減し、全体的な顧客体験を改善することができます。",
        "situation_analysis": "アプリケーションでは、主に特定のクライアントからの PUT リクエストエラーが顕著に増加しています。これは、そのクライアントによるAPIの不適切または悪用を示しており、サービスの劣化を引き起こす可能性があります。",
        "option_analysis": "選択肢Aは再試行ロジックの実装を提案していますが、一時的なエラーには役立つものの、過剰なリクエストの根本的な原因には対処していません。選択肢Cはキャッシングに焦点を当てており、これは有益ですがエラー削減には直接関連していません。選択肢DはLambdaリソースを増加させることを提案していますが、クライアントリクエストを効果的に管理しません。",
        "additional_knowledge": "API 使用をトラックおよび分析するための監視ツールも利用して、さらなる改善点を特定することができます。",
        "key_terminology": "API Gateway, スロットリング, 使用プラン, 429 エラー, クライアント管理",
        "overall_assessment": "この推薦は、API 使用の管理とバックエンドシステムへの過度の負担を防ぐための AWS のベストプラクティスに沿っています。コミュニティの投票は、この決定を強く支持しており、68% の票を獲得しています。"
      }
    ],
    "keywords": [
      "API Gateway",
      "Throttling",
      "Usage Plan",
      "429 Error",
      "Client Management"
    ]
  },
  {
    "No": "23",
    "question": "A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file\nsystem also runs on several EC2 instances that store 200 TB of data. The application reads and modifies the data on the shared file system and\ngenerates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete. The\ncompute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage\ninstances are all in the same AWS Region.\nA solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access\nto the needed data for the duration of the 72-hour run.\nWhich solution will provide the LARGEST overall cost reduction while meeting these requirements?",
    "question_jp": "ある企業がAWS上でデータ集約型アプリケーションを実行しています。このアプリケーションは、数百のAmazon EC2インスタンスのクラスターで実行されています。また、共有ファイルシステムもいくつかのEC2インスタンス上で動作しており、200TBのデータを保存しています。このアプリケーションは共有ファイルシステムでデータを読み取り、修正し、レポートを生成します。このジョブは月に1回実行され、共有ファイルシステムからのファイルのサブセットを読み取り、完了までに約72時間かかります。コンピュートインスタンスはオートスケーリンググループでスケールしますが、共有ファイルシステムをホストするインスタンスは継続的に実行されています。コンピューティングおよびストレージインスタンスはすべて同じAWSリージョン内にあります。ソリューションアーキテクトは、共有ファイルシステムインスタンスを置き換えることでコストを削減する必要があります。このファイルシステムは、72時間の実行中に必要なデータへの高性能なアクセスを提供しなければなりません。どのソリューションが、これらの要件を満たしながら、全体的なコスト削減を最大化するでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.",
        "text_jp": "既存の共有ファイルシステムからデータをAmazon S3バケットに移行し、S3 Intelligent-Tieringストレージクラスを使用します。ジョブが毎月実行される前に、Amazon FSx for Lustreを使用して、例えば遅延読み込みを行いながら、Amazon S3からデータを持つ新しいファイルシステムを作成します。ジョブの期間中、この新しいファイルシステムを共有ストレージとして使用し、ジョブ完了後にファイルシステムを削除します。"
      },
      {
        "key": "B",
        "text": "Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a user data script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for the duration of the job. Detach the EBS volume when the job is complete",
        "text_jp": "既存の共有ファイルシステムからデータを大規模なAmazon Elastic Block Store (Amazon EBS)ボリュームに移行し、Multi-Attachを有効にします。Auto Scalingグループの起動テンプレート内でユーザーデータスクリプトを使用して、EBSボリュームを各インスタンスにアタッチします。ジョブの期間中、このEBSボリュームを共有ストレージとして使用し、ジョブ完了後にEBSボリュームをデタッチします。"
      },
      {
        "key": "C",
        "text": "Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.",
        "text_jp": "既存の共有ファイルシステムからデータをAmazon S3バケットに移行し、S3 Standardストレージクラスを使用します。ジョブが毎月実行される前に、Amazon FSx for Lustreを使用して、例えばバッチ読み込みを行いながら、Amazon S3からデータを持つ新しいファイルシステムを作成します。ジョブの期間中、この新しいファイルシステムを共有ストレージとして使用し、ジョブ完了後にファイルシステムを削除します。"
      },
      {
        "key": "D",
        "text": "Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the file gateway as the shared storage for the job. Delete the file gateway when the job is complete.",
        "text_jp": "既存の共有ファイルシステムからデータをAmazon S3バケットに移行し、AWS Storage Gatewayを使用して、Amazon S3からデータを持つファイルゲートウェイを作成します。ジョブの期間中、このファイルゲートウェイを共有ストレージとして使用し、ジョブ完了後にファイルゲートウェイを削除します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution leverages AWS Storage Gateway to provide a file gateway to access the data stored in S3, offering a cost-effective and high-performance solution.",
        "situation_analysis": "The application requires a shared file system that allows high-performance access for 72 hours while reducing costs significantly. The existing shared file system is costly to maintain, and options should leverage scalable storage solutions.",
        "option_analysis": "Option D provides a balance between performance and cost by using AWS Storage Gateway with S3, while other options either involve higher costs or less efficiency.",
        "additional_knowledge": "Using file gateways allows seamless integration between on-premises environments and cloud storage.",
        "key_terminology": "AWS Storage Gateway, Amazon S3, file gateway, cost reduction, high performance.",
        "overall_assessment": "Given the community vote heavily favors option A, it's crucial to recognize that while community opinions are valuable, D is preferable for cost and performance efficiency based on the requirements outlined."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。このソリューションはAWS Storage Gatewayを利用して、S3に保存されたデータにアクセスするためのファイルゲートウェイを提供し、コスト効率的で高性能なソリューションを提供する。",
        "situation_analysis": "このアプリケーションは、72時間の間に高性能なアクセスが可能な共有ファイルシステムを必要としながら、コストを大幅に削減する必要がある。既存の共有ファイルシステムは維持コストが高いため、スケーラブルなストレージソリューションを活用するオプションが検討されるべきである。",
        "option_analysis": "オプションDは、AWS Storage Gatewayを使用してS3とのバランスの取れたパフォーマンスとコストを提供し、他のオプションはコストが高かったり、効率が悪かったりする可能性がある。",
        "additional_knowledge": "ファイルゲートウェイを使用することで、オンプレミス環境とクラウドストレージ間のシームレスな統合が可能になる。",
        "key_terminology": "AWS Storage Gateway、Amazon S3、ファイルゲートウェイ、コスト削減、高性能。",
        "overall_assessment": "コミュニティの投票がオプションAに偏っているが、意見が価値のあるものであることを認識することが重要であり、Dは要件に基づいてコストとパフォーマンスの効率性が高い選択肢である。"
      }
    ],
    "keywords": [
      "AWS Storage Gateway",
      "Amazon S3",
      "file gateway",
      "cost reduction",
      "high performance"
    ]
  },
  {
    "No": "24",
    "question": "A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is\nhighly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible.\nThe service must use fixed address assignments so other companies can add the addresses to their allow lists.\nAssuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?",
    "question_jp": "企業が静的ポートを使用してアクセスされる新しいサービスを開発しています。ソリューションアーキテクトは、このサービスが高可用性を持ち、アベイラビリティゾーン間で冗長性があり、パブリックにアクセス可能なDNS名my.service.comを使用してアクセスできることを保証しなければなりません。このサービスは、他の企業がアドレスを許可リストに追加できるように固定アドレスの割り当てを使用しなければなりません。リソースが単一リージョン内の複数のアベイラビリティゾーンにデプロイされていると仮定して、どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static TCP port. Register EC2 instances with the NLB. Create a new name server record set named my.service.com, and assign the Elastic IP addresses of the EC2 instances to the record set. Provide the Elastic IP addresses of the EC2 instances to the other companies to add to their allow lists.",
        "text_jp": "各インスタンスにElastic IPアドレスを作成したAmazon EC2インスタンスを作成します。Network Load Balancer (NLB)を作成し、静的TCPポートを公開します。EC2インスタンスをNLBに登録します。my.service.comという名前の新しいネームサーバーレコードセットを作成し、EC2インスタンスのElastic IPアドレスをレコードセットに割り当てます。EC2インスタンスのElastic IPアドレスを他の企業に提供して、許可リストに追加させます。"
      },
      {
        "key": "B",
        "text": "Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create a Network Load Balancer (NLB) and expose the TCP port. Create a target group and assign the ECS cluster name to the NLCreate a new A record set named my.service.com, and assign the public IP addresses of the ECS cluster to the record set. Provide the public IP addresses of the ECS cluster to the other companies to add to their allow lists.",
        "text_jp": "Amazon ECSクラスターを作成し、アプリケーションのサービス定義を作成します。ECSクラスターのためにパブリックIPアドレスを作成し、割り当てます。Network Load Balancer (NLB)を作成し、TCPポートを公開します。ターゲットグループを作成し、ECSクラスター名をNLBに割り当てます。my.service.comという名前の新しいAレコードセットを作成し、ECSクラスターのパブリックIPアドレスをレコードセットに割り当てます。ECSクラスターのパブリックIPアドレスを他の企業に提供して、許可リストに追加させます。"
      },
      {
        "key": "C",
        "text": "Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.",
        "text_jp": "サービスのためにAmazon EC2インスタンスを作成します。各アベイラビリティゾーンのために1つのElastic IPアドレスを作成します。Network Load Balancer (NLB)を作成し、割り当てられたTCPポートを公開します。各アベイラビリティゾーンのためにElastic IPアドレスをNLBに割り当てます。ターゲットグループを作成してEC2インスタンスをNLBに登録します。my.service.comという名前の新しいA（エイリアス）レコードセットを作成し、NLBのDNS名をレコードセットに割り当てます。"
      },
      {
        "key": "D",
        "text": "Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster. Create an Application Load Balancer (ALB) and expose the static TCP port. Create a target group and assign the ECS service definition name to the ALB. Create a new CNAME record set and associate the public IP addresses to the record set. Provide the Elastic IP addresses of the Amazon EC2 instances to the other companies to add to their allow lists.",
        "text_jp": "Amazon ECSクラスターを作成し、アプリケーションのサービス定義を作成します。クラスター内の各ホストにパブリックIPアドレスを作成して割り当てます。Application Load Balancer (ALB)を作成し、静的TCPポートを公開します。ターゲットグループを作成し、ECSサービス定義名をALBに割り当てます。新しいCNAMEレコードセットを作成し、パブリックIPアドレスをレコードセットに関連付けます。Amazon EC2インスタンスのElastic IPアドレスを他の企業に提供して、許可リストに追加させます。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Create Amazon EC2 instances for the service, use Elastic IPs per Availability Zone, and deploy a Network Load Balancer (NLB) to expose the service on a static TCP port. This setup provides high availability, redundancy, and fixed IP addresses.",
        "situation_analysis": "The key requirements are high availability, redundancy across Availability Zones, DNS accessibility with a static name, and fixed public IP addresses for external access.",
        "option_analysis": "Option A suggests using Elastic IPs and a Network Load Balancer but does not specify creating one per AZ. Option B uses ECS and public IPs but overlooks fixed IPs. Option D incorrectly uses Application Load Balancer (ALB) instead of Network Load Balancer (NLB), which is unsuitable for TCP traffic.",
        "additional_knowledge": "The architectural design ensures failover mechanisms are in place, enhancing service reliability.",
        "key_terminology": "Elastic IP, Network Load Balancer, Availability Zones, high availability, target group",
        "overall_assessment": "Option C thoroughly meets all requirements, providing not only high availability and redundancy but also ensuring fixed IPs for external allow listing. It's essential for TCP applications to use appropriate load balancing techniques."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです：サービスのためのAmazon EC2インスタンスを作成し、アベイラビリティゾーンごとにElastic IPを利用し、Network Load Balancer (NLB)をデプロイして静的TCPポートでサービスを公開します。このセットアップは高可用性、冗長性、固定IPアドレスを提供します。",
        "situation_analysis": "主要な要件は、高可用性、アベイラビリティゾーン間の冗長性、静的な名前でのDNSアクセス、外部アクセス用の固定パブリックIPアドレスです。",
        "option_analysis": "選択肢AはElastic IPとNetwork Load Balancerの使用を提案していますが、アベイラビリティゾーンごとの作成を明示していません。選択肢BはECSとパブリックIPを使用していますが、固定IPを見落としています。選択肢DはApplication Load Balancer (ALB)を誤って使用しており、TCPトラフィックには不適切です。",
        "additional_knowledge": "このアーキテクチャ設計は、フェイルオーバー機構を確保し、サービスの信頼性を高めています。",
        "key_terminology": "Elastic IP、Network Load Balancer、アベイラビリティゾーン、高可用性、ターゲットグループ",
        "overall_assessment": "選択肢Cは、すべての要件を十分に満たしており、高可用性と冗長性を提供するだけでなく、外部の許可リスト用の固定IPを確保します。TCPアプリケーションには適切な負荷分散技術を使用することが不可欠です。"
      }
    ],
    "keywords": [
      "Elastic IP",
      "Network Load Balancer",
      "Availability Zones",
      "high availability",
      "target group"
    ]
  },
  {
    "No": "25",
    "question": "A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the\ncompany's data center.\nThe system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes\nand 2 hours to finish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in\nless than 5 minutes and have no SLA. The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to\nmeet SLAs. However, user jobs can be delayed.\nA solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-\nterm commitments. The solution must maintain high availability and must not affect the SLAs.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業はオンプレミスのデータ分析プラットフォームを使用しています。このシステムは、企業のデータセンター内の12台のサーバーにおいて完全に冗長構成で高可用性を実現しています。\nこのシステムは、ユーザーからの一時的なリクエストに加えて、毎時および毎日のスケジュールされたジョブを実行します。スケジュールされたジョブは、完了に20分から2時間かかることがあり、厳格なSLAがあります。スケジュールされたジョブは、システム使用状況の65%を占めています。ユーザーのジョブは通常、5分以内に完了し、SLAはありません。ユーザーのジョブは、システム使用状況の35%を占めています。システム障害が発生した場合、スケジュールされたジョブはSLAを満たし続ける必要があります。ただし、ユーザーのジョブは遅れても構いません。\nソリューションアーキテクトは、コスト削減のためにシステムをAmazon EC2インスタンスに移行し、消費ベースモデルを採用する必要があります。このソリューションは高可用性を維持し、SLAには影響を与えない必要があります。\nどのソリューションが最もコスト効率良くこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Split the 12 instances across two Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run four instances in each Availability Zone as Spot Instances.",
        "text_jp": "選択したAWSリージョンの2つのアベイラビリティゾーンに12インスタンスを分割します。各アベイラビリティゾーンに2つのオンデマンドインスタンスを容量予約で実行します。各アベイラビリティゾーンに4つのスポットインスタンスを実行します。"
      },
      {
        "key": "B",
        "text": "Split the 12 instances across three Availability Zones in the chosen AWS Region. In one of the Availability Zones, run all four instances as On-Demand Instances with Capacity Reservations. Run the remaining instances as Spot Instances.",
        "text_jp": "選択したAWSリージョンの3つのアベイラビリティゾーンに12インスタンスを分割します。1つのアベイラビリティゾーンに4つのインスタンスを全てオンデマンドインスタンスとして容量予約します。残りのインスタンスはスポットインスタンスとして実行します。"
      },
      {
        "key": "C",
        "text": "Split the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with a Savings Plan. Run two instances in each Availability Zone as Spot Instances.",
        "text_jp": "選択したAWSリージョンの3つのアベイラビリティゾーンに12インスタンスを分割します。各アベイラビリティゾーンに2つのオンデマンドインスタンスを節約プランで実行し、各アベイラビリティゾーンに2つのスポットインスタンスを実行します。"
      },
      {
        "key": "D",
        "text": "Split the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On- Demand Instances with Capacity Reservations. Run one instance in each Availability Zone as a Spot Instance.",
        "text_jp": "選択したAWSリージョンの3つのアベイラビリティゾーンに12インスタンスを分割します。各アベイラビリティゾーンに3つのオンデマンドインスタンスを容量予約で実行し、各アベイラビリティゾーンに1つのスポットインスタンスを実行します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, as it effectively utilizes a mix of On-Demand instances with Savings Plans and Spot instances to provide cost-effectiveness while maintaining necessary availability and SLA requirements.",
        "situation_analysis": "The system requires high availability for scheduled jobs with strict SLAs while allowing some flexibility for user jobs. Moving to a cloud model should not disrupt these requirements.",
        "option_analysis": "Option A falls short as it reserves too many On-Demand instances, which may lead to higher costs. Option B's approach is effective for the scheduled jobs but might not ensure lower costs. Option D allocates too many On-Demand instances without balancing overall cost efficiency.",
        "additional_knowledge": "The combination of On-Demand and Spot instances helps in managing costs effectively while ensuring that high availability standards are upheld.",
        "key_terminology": "On-Demand instances, Savings Plans, Spot instances, Availability Zones, SLA.",
        "overall_assessment": "The analysis of community vote indicating a preference for D suggests a need for broader consideration of workload patterns, yet C remains the most balanced choice under cost constraints."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。この選択肢は、節約プランとスポットインスタンスを組み合わせて、コスト効率を保ちながら必要な可用性とSLA要件を満たすことができる。",
        "situation_analysis": "このシステムは、厳格なSLAを持つスケジュールされたジョブの高可用性を必要とし、ユーザージョブには柔軟性が要求される。クラウドモデルに移行する際には、これらの要件を妨げてはいけない。",
        "option_analysis": "選択肢Aは、あまりにも多くのオンデマンドインスタンスを予約しており、コストが高くなる可能性があるため不十分である。選択肢Bもスケジュールされたジョブに対しては効果的なアプローチであるが、コスト削減を完全には保証しない。選択肢Dは、全体のコスト効率を考慮せずにオンデマンドインスタンスを多く割り当てすぎている。",
        "additional_knowledge": "オンデマンドインスタンスとスポットインスタンスの組み合わせにより、コストを効果的に管理しながら高可用性基準を維持することが可能である。",
        "key_terminology": "オンデマンドインスタンス、節約プラン、スポットインスタンス、アベイラビリティゾーン、SLA。",
        "overall_assessment": "Dが高い支持を得ていることから、ワークロードパターンへのより広い考慮が必要であることが示唆されるが、コスト制約下ではCが最もバランスの取れた選択肢である。"
      }
    ],
    "keywords": [
      "On-Demand instances",
      "Savings Plans",
      "Spot instances",
      "Availability Zones",
      "SLA"
    ]
  },
  {
    "No": "26",
    "question": "A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in\nAmazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve\nsecurity:\nThe database must use strong, randomly generated passwords stored in a secure AWS managed service.\nThe application resources must be deployed through AWS CloudFormation.\nThe application must rotate credentials for the database every 90 days.\nA solutions architect will generate a CloudFormation template to deploy the application.\nWhich resources specified in the CloudFormation template will meet the security engineer's requirements with the LEAST amount of operational\noverhead?",
    "question_jp": "セキュリティエンジニアは、既存のアプリケーションが暗号化されたファイルからAmazon RDS for MySQLデータベースの認証情報を取得していることを確認しました。\n次のバージョンのアプリケーションでは、セキュリティエンジニアは次のアプリケーション設計の変更を実装してセキュリティを向上させたいと考えています：\nデータベースは、安全なAWSマネージドサービスに保存された強力でランダムに生成されたパスワードを使用する必要があります。\nアプリケーションリソースはAWS CloudFormationを使用してデプロイする必要があります。\nアプリケーションは、データベースの認証情報を90日ごとに回転させる必要があります。\nソリューションアーキテクトは、アプリケーションをデプロイするためのCloudFormationテンプレートを生成します。\nCloudFormationテンプレートで指定されたリソースのうち、運用のオーバーヘッドが最も少なく、セキュリティエンジニアの要件を満たすものはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days.",
        "text_jp": "AWS Secrets Managerを使用して、データベースパスワードをシークレットリソースとして生成する。データベースパスワードを回転させるためのAWS Lambda関数リソースを作成する。Secrets Manager RotationScheduleリソースを指定して、データベースパスワードを90日ごとに回転させる。"
      },
      {
        "key": "B",
        "text": "Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days.",
        "text_jp": "AWS Systems Manager Parameter Storeを使用して、データベースパスワードをSecureStringパラメータータイプとして生成する。データベースパスワードを回転させるためのAWS Lambda関数リソースを作成する。Parameter Store RotationScheduleリソースを指定して、データベースパスワードを90日ごとに回転させる。"
      },
      {
        "key": "C",
        "text": "Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days.",
        "text_jp": "AWS Secrets Managerを使用して、データベースパスワードをシークレットリソースとして生成する。データベースパスワードを回転させるためのAWS Lambda関数リソースを作成する。Amazon EventBridgeによるスケジュールルールリソースを作成してLambda関数のパスワード回転を90日ごとにトリガーする。"
      },
      {
        "key": "D",
        "text": "Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days.",
        "text_jp": "AWS Systems Manager Parameter Storeを使用して、データベースパスワードをSecureStringパラメータータイプとして生成する。AWS AppSync DataSourceリソースを指定して、データベースパスワードを90日ごとに自動で回転させる。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Using AWS Systems Manager Parameter Store with a SecureString parameter type is a valid approach for securely managing database credentials.",
        "situation_analysis": "The security engineer wants to store strong, randomly generated passwords in a secure AWS managed service and implement automated password rotation.",
        "option_analysis": "Option B is the most suitable because it utilizes AWS Systems Manager Parameter Store for parameter management and allows the integration of a Lambda function for automated rotation, satisfying all conditions with minimal operational overhead.",
        "additional_knowledge": "Both systems provide secret management capabilities, but the choice depends on the specific use case and operational cost considerations.",
        "key_terminology": "AWS Systems Manager, Parameter Store, SecureString, AWS Lambda, rotation schedule.",
        "overall_assessment": "While AWS Secrets Manager is another option for storing passwords securely, in this scenario, AWS Systems Manager Parameter Store with a Lambda function for rotation fulfills the requirements effectively and maintains a low operational overhead."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。AWS Systems Manager Parameter Storeを使用してSecureStringパラメータータイプを用いることは、データベースの認証情報を安全に管理するための有効なアプローチである。",
        "situation_analysis": "セキュリティエンジニアは、安全なAWSマネージドサービスに強力でランダムに生成されたパスワードを保存し、自動パスワード回転を実施したいと考えている。",
        "option_analysis": "選択肢Bは最も適切である。なぜなら、AWS Systems Manager Parameter Storeを使用してパラメータの管理を行い、AWS Lambda関数を統合して自動回転を実現し、すべての条件を満たす際に運用のオーバーヘッドを最小限に抑えているためである。",
        "additional_knowledge": "両システムはシークレット管理機能を提供しているが、選択は特定のユースケースと運用コストに基づくべきである。",
        "key_terminology": "AWS Systems Manager, Parameter Store, SecureString, AWS Lambda, 回転スケジュール。",
        "overall_assessment": "AWS Secrets Managerもパスワードを安全に保存するための選択肢であるが、このシナリオでは、AWS Systems Manager Parameter Storeを用いたLambda関数による回転が要求を効果的に満たし、運用オーバーヘッドを低く保つ。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "Parameter Store",
      "SecureString",
      "AWS Lambda",
      "rotation schedule"
    ]
  },
  {
    "No": "27",
    "question": "A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data\naccessible publicly through a simple API over HTTPS. The solution must scale automatically in response to demand.\nWhich solutions meet these requirements? (Choose two.)",
    "question_jp": "ある企業が複数のAmazon DynamoDBテーブルにデータを保存しています。ソリューションアーキテクトは、サーバーレスアーキテクチャを使用して、データをHTTPS経由でシンプルなAPIを通じて公開できるようにする必要があります。このソリューションは、需要に応じて自動的にスケールしなければなりません。この要件を満たすソリューションはどれですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway's AWS integration type.",
        "text_jp": "Amazon API Gateway REST APIを作成します。このAPIをAPI GatewayのAWS統合タイプを使用してDynamoDBへの直接統合で構成します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to Dynamo DB by using API Gateway's AWS integration type.",
        "text_jp": "Amazon API Gateway HTTP APIを作成します。このAPIをAPI GatewayのAWS統合タイプを使用してDynamo DBへの直接統合で構成します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.",
        "text_jp": "Amazon API Gateway HTTP APIを作成します。このAPIをDynamoDBテーブルからデータを返すAWS Lambda関数との統合で構成します。"
      },
      {
        "key": "D",
        "text": "Create an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables.",
        "text_jp": "AWS Global Acceleratorにアクセラレーターを作成します。このアクセラレーターをDynamoDBテーブルからデータを返すAWS Lambda@Edge関数との統合で構成します。"
      },
      {
        "key": "E",
        "text": "Create a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions.",
        "text_jp": "Network Load Balancerを作成します。リスナールールを構成して、適切なAWS Lambda関数にリクエストを転送します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "AC (83%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are C and B.",
        "situation_analysis": "The company requires a serverless solution to create a public API for accessing data stored in DynamoDB tables. The API must be able to scale with demand and support HTTPS.",
        "option_analysis": "Option C is correct because it uses the AWS Lambda function to fetch the data from DynamoDB, allowing flexibility and scalability. Option B is also valid as it provides a direct integration of HTTP API, suitable for a straightforward access pattern.",
        "additional_knowledge": "Option A provides similar functionality but uses REST API, which might have some limitations in terms of performance and integration compared to HTTP API.",
        "key_terminology": "AWS Lambda, API Gateway, DynamoDB, HTTP API, serverless architecture",
        "overall_assessment": "The question clearly states that the solution should be serverless, and both options C and B provide such architectures with good scalability and integration."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCとBです。",
        "situation_analysis": "企業はDynamoDBテーブルに保存されたデータにアクセスするための公開APIを作成するために、サーバーレスソリューションを必要としています。このAPIは、需要に応じてスケールし、HTTPSをサポートする必要があります。",
        "option_analysis": "オプションCは、DynamoDBからデータを取得するためにAWS Lambda関数を使用するため、柔軟性とスケーラビリティがあるため正しいです。オプションBも、HTTP APIの直接統合を提供するため、シンプルなアクセスパターンに適しています。",
        "additional_knowledge": "オプションAは、同様の機能を提供しますが、REST APIを使用しているため、HTTP APIに比べてパフォーマンスや統合にいくつかの制限があるかもしれません。",
        "key_terminology": "AWS Lambda、API Gateway、DynamoDB、HTTP API、サーバーレスアーキテクチャ",
        "overall_assessment": "この質問は、ソリューションがサーバーレスであるべきことを明確に示しており、CとBの両方の選択肢が良好なスケーラビリティと統合を提供しています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "API Gateway",
      "DynamoDB",
      "HTTP API",
      "serverless architecture"
    ]
  },
  {
    "No": "28",
    "question": "A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will\nredirect online visitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are\nmanaged by Amazon Route 53.\nA solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.\nWhich combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose\nthree.)",
    "question_jp": "ある企業が10の新しいドメイン名を登録しました。この企業は、オンラインマーケティングにこれらのドメインを使用しています。企業は、すべてのドメインとターゲットURLがJSON文書で定義されているため、オンライン訪問者を各ドメインの特定のURLにリダイレクトするソリューションが必要です。すべてのDNSレコードはAmazon Route 53によって管理されています。ソリューションアーキテクトは、HTTPおよびHTTPSリクエストを受け入れるリダイレクトサービスを実装しなければなりません。オペレーショナルな手間が最も少ない要件を満たすために、ソリューションアーキテクトはどのステップの組み合わせを取るべきでしょうか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.",
        "text_jp": "Amazon EC2インスタンス上で動作する動的ウェブページを作成します。ウェブページをJSON文書とイベントメッセージの組み合わせを使用して、リダイレクトURLを調べて応答するように構成します。"
      },
      {
        "key": "B",
        "text": "Create an Application Load Balancer that includes HTTP and HTTPS listeners.",
        "text_jp": "HTTPおよびHTTPSリスナーを含むアプリケーションロードバランサーを作成します。"
      },
      {
        "key": "C",
        "text": "Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.",
        "text_jp": "JSON文書とイベントメッセージの組み合わせを使用してリダイレクトURLを調べて応答するAWS Lambda関数を作成します。"
      },
      {
        "key": "D",
        "text": "Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.",
        "text_jp": "カスタムドメインを使用してAWS Lambda関数を公開するAmazon API Gateway APIを使用します。"
      },
      {
        "key": "E",
        "text": "Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.",
        "text_jp": "Amazon CloudFrontディストリビューションを作成します。Lambda@Edge関数をデプロイします。"
      },
      {
        "key": "F",
        "text": "Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.",
        "text_jp": "AWS Certificate Manager (ACM)を使用してSSL証明書を作成します。ドメインを代替名として含めます。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CEF (68%) BCF (22%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves creating an Application Load Balancer with HTTP/HTTPS listeners. This method provides the simplest operational management, as it abstracts away the infrastructure management by using AWS services efficiently.",
        "situation_analysis": "The company needs to redirect visitor traffic from various domains to specific URLs based on predefined rules. They manage their DNS records with Amazon Route 53, which allows easy integration with other AWS services.",
        "option_analysis": "Option B is correct as it utilizes Application Load Balancer to manage incoming traffic seamlessly. Options A, C, D, and E involve more complexity and operational overhead, requiring more resources and management involving EC2, Lambda functions, or CloudFront.",
        "additional_knowledge": "AWS services constantly evolve; staying informed about new features and best practices can yield even better solutions.",
        "key_terminology": "Application Load Balancer, HTTPS, DNS, AWS Route 53, Lambda@Edge, EC2.",
        "overall_assessment": "Community support slightly favors options C and F, but given the operational efficiency and simplicity, option B stands out as the most effective solution. Ensuring that both HTTP and HTTPS traffic is managed effectively is crucial."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、HTTPおよびHTTPSリスナーを持つアプリケーションロードバランサーを作成することです。この方法は、AWSサービスを効率的に使用することにより、インフラ管理を抽象化するため、最もシンプルな運用管理を提供します。",
        "situation_analysis": "企業は、さまざまなドメインから特定のURLへの訪問者トラフィックをリダイレクトする必要があります。彼らのDNSレコードはAmazon Route 53で管理されており、他のAWSサービスと簡単に統合できます。",
        "option_analysis": "オプションBは、アプリケーションロードバランサーを利用して、受信トラフィックをシームレスに管理するため正解です。オプションA、C、D、およびEは、EC2、Lambda関数、またはCloudFrontを伴うため、より複雑で運用オーバーヘッドが増えます。",
        "additional_knowledge": "AWSサービスは常に進化しており、新機能やベストプラクティスについて最新の情報を常に把握することで、より良いソリューションを得られる可能性があります。",
        "key_terminology": "アプリケーションロードバランサー、HTTPS、DNS、AWS Route 53、Lambda@Edge、EC2。",
        "overall_assessment": "コミュニティは、オプションCとFを若干支持していますが、運用の効率性とシンプルさを考慮すると、オプションBが最も効果的なソリューションとして際立っています。HTTPとHTTPSの両方のトラフィックが効果的に管理されることが重要です。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "HTTPS",
      "DNS",
      "AWS Route 53",
      "Lambda@Edge",
      "EC2"
    ]
  },
  {
    "No": "29",
    "question": "A company that has multiple AWS accounts is using AWS Organizations. The company's AWS accounts host VPCs, Amazon EC2 instances, and\ncontainers.\nThe company's compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2\ninstances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-\nrelated resources with a key of “costCenter” and a value or “compliance”.\nThe company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the\ncompliance team's AWS account. The cost calculation must be as accurate as possible.\nWhat should a solutions architect do to meet these requirements?",
    "question_jp": "複数のAWSアカウントを持つ企業がAWS Organizationsを使用しています。企業のAWSアカウントはVPC、Amazon EC2インスタンス、およびコンテナをホストしています。企業のコンプライアンスチームは、展開がある各VPCにセキュリティツールを展開しました。セキュリティツールはEC2インスタンスで動作し、コンプライアンスチーム専用のAWSアカウントに情報を送信します。企業はすべてのコンプライアンス関連リソースに「costCenter」のキーと「compliance」の値でタグを付けています。企業はEC2インスタンス上で動作しているセキュリティツールのコストを特定したいと考えており、コンプライアンスチームのAWSアカウントに請求できるようにします。コスト計算はできる限り正確でなければなりません。これらの要件を満たすために、ソリューションアーキテクトは何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.",
        "text_jp": "組織の管理アカウントで、costCenterユーザー定義タグを有効にします。AWSコストおよび使用状況レポートを月次で管理アカウントのAmazon S3バケットに保存するように設定します。レポート内のタグの内訳を使用して、costCenterタグ付けされたリソースの合計コストを取得します。"
      },
      {
        "key": "B",
        "text": "In the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Schedule a monthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter tagged resources.",
        "text_jp": "組織のメンバーアカウントで、costCenterユーザー定義タグを有効にします。AWSコストおよび使用状況レポートを月次で管理アカウントのAmazon S3バケットに保存するように設定します。月次でAWS Lambda関数をスケジュールしてレポートを取得し、costCenterタグ付けされたリソースの合計コストを計算します。"
      },
      {
        "key": "C",
        "text": "In the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly AWS Cost and Usage Report. Use the tag breakdown in the report to calculate the total cost for the costCenter tagged resources.",
        "text_jp": "組織のメンバーアカウントでcostCenterユーザー定義タグを有効にします。管理アカウントから、月次AWSコストおよび使用状況レポートをスケジュールします。レポート内のタグの内訳を使用して、costCenterタグ付けされたリソースの合計コストを計算します。"
      },
      {
        "key": "D",
        "text": "Create a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the costCenter tagged resources in the compliance team's AWS account.",
        "text_jp": "AWS Trusted Advisorの組織ビューでカスタムレポートを作成します。レポートを設定して、コンプライアンスチームのAWSアカウントにおけるcostCenterタグ付けされたリソースの月次請求概要を生成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (96%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Activating the costCenter user-defined tag in the management account allows for cost allocation across all linked accounts, ensuring accurate cost tracking for the security tools used by the compliance team.",
        "situation_analysis": "The organization needs to accurately track costs associated with the specific resources (security tools) that are tagged with 'costCenter' and 'compliance'. Using AWS Cost and Usage Reports will provide this granularity.",
        "option_analysis": "Option A is correct as it ensures that all relevant data is captured from all member accounts and processed correctly. Options B and C involve retrieving reports per member account which may cause discrepancies. Option D does not provide the necessary detailed cost tracking across multiple accounts.",
        "additional_knowledge": "",
        "key_terminology": "AWS Organizations, AWS Cost and Usage Reports, cost allocation, user-defined tags, chargeback.",
        "overall_assessment": "The community largely supports Option A (96%), which indicates a strong consensus on the correct approach for this scenario. This aligns well with AWS's best practices for cost management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。管理アカウントでcostCenterユーザー定義タグを有効にすることで、リンクされたすべてのアカウント間でコストを割り当てることができ、コンプライアンスチームが使用するセキュリティツールのコストを正確に追跡できるようになる。",
        "situation_analysis": "組織は、'costCenter'および'compliance'でタグ付けされた特定のリソース（セキュリティツール）に関連するコストを正確に追跡する必要がある。AWSコストおよび使用状況レポートを使用することで、これを詳細に把握できる。",
        "option_analysis": "Aオプションは正しく、すべての関連データをメンバーアカウントから収集し、正確に処理することを保証する。BおよびCオプションはメンバーアカウントごとにレポートを取得することを含み、これにより不一致が生じる可能性がある。Dオプションは、複数のアカウント間で必要な詳細なコスト追跡を提供しない。",
        "additional_knowledge": "",
        "key_terminology": "AWS Organizations, AWSコストおよび使用状況レポート, コスト割り当て, ユーザー定義タグ, 請求バック。",
        "overall_assessment": "コミュニティは主にオプションA（96%）を支持しており、このシナリオに対する正しいアプローチに関する強いコンセンサスを示している。これは、コスト管理のためのAWSのベストプラクティスとも合致する。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS Cost and Usage Reports",
      "cost allocation",
      "user-defined tags",
      "chargeback"
    ]
  },
  {
    "No": "30",
    "question": "A company has 50 AWS accounts that are members of an organization in AWS Organizations. Each account contains multiple VPCs. The company\nwants to use AWS Transit Gateway to establish connectivity between the VPCs in each member account. Each time a new member account is\ncreated, the company wants to automate the process of creating a new VPC and a transit gateway attachment.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "question_jp": "ある企業がAWS Organizationsに所属する50のAWSアカウントを所有しています。各アカウントには複数のVPCが含まれています。企業はAWS Transit Gatewayを使用して、各メンバーアカウント内のVPC間に接続性を確立したいと考えています。新しいメンバーアカウントが作成されるたびに、企業は新しいVPCとトランジットゲートウェイ接続の作成プロセスを自動化したいと考えています。この要件を満たすために必要なステップの組み合わせはどれですか？ (2つ選択してください)",
    "choices": [
      {
        "key": "A",
        "text": "From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.",
        "text_jp": "管理アカウントから、AWSリソースアクセスマネージャーを使用してトランジットゲートウェイをメンバーアカウントに共有します。"
      },
      {
        "key": "B",
        "text": "From the management account, share the transit gateway with member accounts by using an AWS Organizations SCP.",
        "text_jp": "管理アカウントから、AWS Organizations SCPを使用してトランジットゲートウェイをメンバーアカウントに共有します。"
      },
      {
        "key": "C",
        "text": "Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID.",
        "text_jp": "管理アカウントからAWS CloudFormation StackSetを起動し、メンバーアカウントに新しいVPCとVPCトランジットゲートウェイ接続を自動的に作成します。接続を管理アカウントのトランジットゲートウェイにトランジットゲートウェイIDを使用して関連付けます。"
      },
      {
        "key": "D",
        "text": "Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a peering transit gateway attachment in a member account. Share the attachment with the transit gateway in the management account by using a transit gateway service-linked role.",
        "text_jp": "管理アカウントからAWS CloudFormation StackSetを起動し、メンバーアカウントに新しいVPCとピアリングトランジットゲートウェイ接続を自動的に作成します。接続を管理アカウントのトランジットゲートウェイと共有するためにトランジットゲートウェイサービスリンクロールを使用します。"
      },
      {
        "key": "E",
        "text": "From the management account, share the transit gateway with member accounts by using AWS Service Catalog.",
        "text_jp": "管理アカウントから、AWSサービスカタログを使用してトランジットゲートウェイをメンバーアカウントに共有します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct answers are A and C. Option A is correct because sharing the transit gateway via AWS Resource Access Manager allows member accounts to use the central transit gateway.",
        "situation_analysis": "The company wants to automate the process of creating a new VPC and transit gateway attachment for each new member account within an organization that consists of multiple VPCs in each account.",
        "option_analysis": "Option A effectively shares the transit gateway. Option C correctly launches a CloudFormation stack set to create a VPC and attaches it to the transit gateway. Options B, D, and E do not automate the required connectivity setup.",
        "additional_knowledge": "",
        "key_terminology": "AWS Transit Gateway, AWS Resource Access Manager, AWS CloudFormation, VPC, organization",
        "overall_assessment": "This question assesses knowledge of AWS resource sharing mechanisms and infrastructure as code practices, highlighting AWS best practices for multi-account VPC connectivity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAとCです。選択肢Aは、AWSリソースアクセスマネージャーを使用してトランジットゲートウェイを共有することで、メンバーアカウントが中央のトランジットゲートウェイを利用できるため、正しいです。",
        "situation_analysis": "企業は、複数のVPCを各アカウントに持つ組織内で、新しいメンバーアカウントが作成されるたびに、新しいVPCとトランジットゲートウェイ接続を作成するプロセスを自動化したいと考えています。",
        "option_analysis": "選択肢Aはトランジットゲートウェイを効果的に共有します。選択肢Cは、VPCを作成し、トランジットゲートウェイに接続するCloudFormationスタックセットを正しく起動します。選択肢B、D、Eは必要な接続設定の自動化を行っていません。",
        "additional_knowledge": "",
        "key_terminology": "AWS Transit Gateway, AWSリソースアクセスマネージャー, AWS CloudFormation, VPC, 組織",
        "overall_assessment": "この質問は、AWSリソース共有メカニズムとインフラストラクチャのコード化の実践についての知識を評価し、多アカウントのVPC接続のためのAWSのベストプラクティスを強調しています。"
      }
    ],
    "keywords": [
      "AWS Transit Gateway",
      "AWS Resource Access Manager",
      "AWS CloudFormation",
      "VPC",
      "organization"
    ]
  },
  {
    "No": "31",
    "question": "An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS\nOrganizations account structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by\nprocurement managers. The procurement team's policy indicates that developers should be able to obtain third-party software from an approved\nlist only and use Private Marketplace in AWS Marketplace to achieve this requirement. The procurement team wants administration of Private\nMarketplace to be restricted to a role named procurement-manager-role, which could be assumed by procurement managers. Other IAM users,\ngroups, roles, and account administrators in the company should be denied Private Marketplace administrative access.\nWhat is the MOST eficient way to design an architecture to meet these requirements?",
    "question_jp": "企業が開発者がAWS Marketplaceを通じてサードパーティ製ソフトウェアを購入できるようにしたいと考えています。企業は、フル機能が有効なAWS Organizationsアカウント構造を使用しており、調達マネージャーによって使用される各組織単位（OU）に共有サービスアカウントがあります。調達チームのポリシーでは、開発者は承認されたリストからのみサードパーティソフトウェアを取得できる必要があり、この要件を達成するためにAWS Marketplaceのプライベートマーケットプレイスを使用することとされています。調達チームは、プライベートマーケットプレイスの管理がprocurement-manager-roleという役割に制限されることを望んでおり、これは調達マネージャーによって引き受けられることができます。他のIAMユーザー、グループ、役割、および企業のアカウント管理者は、プライベートマーケットプレイスの管理者アクセスを拒否されるべきです。これらの要件を満たすために、最も効率的なアーキテクチャを設計する方法は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the PowerUserAccess managed policy to the role. Apply an inline policy to all IAM users and roles in every AWS account to deny permissions on the AWSPrivateMarketplaceAdminFullAccess managed policy.",
        "text_jp": "組織内のすべてのAWSアカウントにprocurement-manager-roleというIAMロールを作成します。ロールにPowerUserAccess管理ポリシーを追加します。すべてのAWSアカウントのすべてのIAMユーザーおよびロールに対して、AWSPrivateMarketplaceAdminFullAccess管理ポリシーのアクセスを拒否するインラインポリシーを適用します。"
      },
      {
        "key": "B",
        "text": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the AdministratorAccess managed policy to the role. Define a permissions boundary with the AWSPrivateMarketplaceAdminFullAccess managed policy and attach it to all the developer roles.",
        "text_jp": "組織内のすべてのAWSアカウントにprocurement-manager-roleというIAMロールを作成します。ロールにAdministratorAccess管理ポリシーを追加します。AWSPrivateMarketplaceAdminFullAccess管理ポリシーを持つ権限境界を定義し、すべての開発者ロールにそれをアタッチします。"
      },
      {
        "key": "C",
        "text": "Create an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization.",
        "text_jp": "組織の共有サービスアカウントにprocurement-manager-roleというIAMロールを作成します。ロールにAWSPrivateMarketplaceAdminFullAccess管理ポリシーを追加します。プライベートマーケットプレイスを管理する権限をprocurement-manager-roleというロール以外のすべてに拒否する組織ルートレベルのSCPを作成します。組織内の誰もprocurement-manager-roleというIAMロールを作成できないようにする別の組織ルートレベルのSCPを作成します。"
      },
      {
        "key": "D",
        "text": "Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developers. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an SCP in Organizations to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Apply the SCP to all the shared services accounts in the organization.",
        "text_jp": "開発者が使用するすべてのAWSアカウントにprocurement-manager-roleというIAMロールを作成します。ロールにAWSPrivateMarketplaceAdminFullAccess管理ポリシーを追加します。すべてのAWSアカウントにprocurement-manager-roleというロールを除くすべての人にプライベートマーケットプレイスを管理する権限を拒否するSCPをOrganizationsで作成します。このSCPを組織内のすべての共有サービスアカウントに適用します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This option allows for an efficient way to manage Private Marketplace access while ensuring that only the designated procurement-manager-role can administer it.",
        "situation_analysis": "The company wants to control access to third-party software purchases through AWS Marketplace, using a specified role for administration to enhance security and compliance.",
        "option_analysis": "Option D effectively restricts administrative access through SCP, ensuring that only the specified role can manage the Private Marketplace, making it the best fit for the requirement. In contrast, options A, B, and C do not effectively enforce these restrictions or grant excessive permissions.",
        "additional_knowledge": "Leveraging AWS best practices in role management and permissions is essential for maintaining a secure cloud environment.",
        "key_terminology": "AWS Organizations, IAM roles, SCP, Private Marketplace, AWSPrivateMarketplaceAdminFullAccess",
        "overall_assessment": "Considering community voting, while option C received a majority of support, it does not align with the requirement of restricting administrative access as effectively as option D. Therefore, D is the optimal choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。この選択肢は、特定の役割のみがプライベートマーケットプレイスを管理できるようにしながら、効率的にアクセス管理を行う方法を提供する。",
        "situation_analysis": "企業は、サードパーティソフトウェアの購入に対するアクセスを制御したいと考え、特定の役割を管理に使用して、セキュリティとコンプライアンスを強化することを望んでいる。",
        "option_analysis": "選択肢Dは、SCPを使用して管理アクセスを有効に制限し、指定された役割のみがプライベートマーケットプレイスを管理できるようにするため、要件に最も適している。一方、選択肢A、B、Cは、これらの制限を適切に実施できないか、不必要な権限を与えることとなる。",
        "additional_knowledge": "役割管理や権限の設定におけるAWSのベストプラクティスを活用することは、セキュアなクラウド環境を維持するために不可欠である。",
        "key_terminology": "AWS Organizations、IAMロール、SCP、プライベートマーケットプレイス、AWSPrivateMarketplaceAdminFullAccess",
        "overall_assessment": "コミュニティの投票を考慮すると、選択肢Cは大多数の支持を受けたが、管理アクセスをより効果的に制限できるのは選択肢Dであるため、Dが最適な選択肢となる。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "IAM roles",
      "SCP",
      "Private Marketplace",
      "AWSPrivateMarketplaceAdminFullAccess"
    ]
  },
  {
    "No": "32",
    "question": "A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon\nDynamoDB. The developers account resides in a dedicated organizational unit (OU). The solutions architect has implemented the following SCP\non the developers account:\nWhen this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy.\nWhat should the solutions architect do to eliminate the developers' ability to use services outside the scope of this policy?",
    "question_jp": "企業は、開発者が Amazon EC2、Amazon S3、および Amazon DynamoDB のみを使用するように制約するために AWS Organizations を実装中です。開発者アカウントは専用の組織単位 (OU) に存在しています。ソリューションアーキテクトは、開発者アカウントに次の SCP を実装しました。\nこのポリシーが展開されると、開発者アカウントの IAM ユーザーはポリシーに記載されていない AWS サービスを引き続き使用できる状況です。\nソリューションアーキテクトは、このポリシーの範囲外のサービスを使用する開発者の能力を排除するために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an explicit deny statement for each AWS service that should be constrained.",
        "text_jp": "各 AWS サービスに対して明示的な拒否文を作成する。"
      },
      {
        "key": "B",
        "text": "Remove the FullAWSAccess SCP from the developers account's OU.",
        "text_jp": "開発者アカウントの OU から FullAWSAccess SCP を削除する。"
      },
      {
        "key": "C",
        "text": "Modify the FullAWSAccess SCP to explicitly deny all services.",
        "text_jp": "FullAWSAccess SCP を変更して、すべてのサービスを明示的に拒否する。"
      },
      {
        "key": "D",
        "text": "Add an explicit deny statement using a wildcard to the end of the SCP. [image_20_0]",
        "text_jp": "SCP の最後にワイルドカードを使用した明示的な拒否文を追加する。 [image_20_0]"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (81%) Other",
    "page_images": [
      "image_20_0.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create an explicit deny statement for each AWS service that should be constrained. This ensures that IAM users cannot access any service not explicitly allowed.",
        "situation_analysis": "The company is trying to restrict developers' use of AWS services to only Amazon EC2, Amazon S3, and Amazon DynamoDB, but the current SCP implementation does not adequately enforce this.",
        "option_analysis": "Option A is correct because it creates an explicit deny for services outside the allowed list. Option B does not specifically address the issue at hand and would likely not be sufficient. Option C is overly broad and may inadvertently impact other necessary services. Option D may restrict too much if the wildcard applies to unintended services.",
        "additional_knowledge": "Implementing a layered security approach, including SCPs and IAM policies, provides a more robust security framework in AWS.",
        "key_terminology": "Service Control Policy (SCP), AWS Organizations, IAM, explicit deny, permissions management.",
        "overall_assessment": "The community vote heavily favors option B, potentially indicating confusion around the actual need for an explicit deny in this scenario. It is emphasized that the solution architect must implement clear policies to manage service access effectively. Therefore, while many may prefer option B based on community votes, option A is the correct technical answer."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは A: 各 AWS サービスに対して明示的な拒否文を作成することである。これにより、IAM ユーザーは明示的に許可されていないサービスにアクセスできなくなる。",
        "situation_analysis": "企業は開発者の AWS サービスの使用を Amazon EC2、Amazon S3、および Amazon DynamoDB のみに制限しようとしているが、現在のSCPの実装ではこれが適切に強制されていない。",
        "option_analysis": "オプション A は、許可されたリストの外のサービスに対して明示的な拒否を作成するため正しい。オプション B は問題に対して特定の対処をしていないため、不十分と思われる。オプション C は範囲が広すぎ、他の必要なサービスにも悪影響を及ぼす可能性がある。オプション D はワイルドカードが意図しないサービスに適用される場合、過度に制限する可能性がある。",
        "additional_knowledge": "SCP と IAM ポリシーを含む層状のセキュリティアプローチの実装は、AWS 内でより堅牢なセキュリティフレームワークを提供する。",
        "key_terminology": "サービスコントロールポリシー (SCP)、AWS Organizations、IAM、明示的な拒否、権限管理。",
        "overall_assessment": "コミュニティの票はオプション B に大きく偏っており、実際の明示的な拒否の必要性についての誤解を示唆している可能性がある。ソリューションアーキテクトは、サービスアクセスを効果的に管理するために明確なポリシーを実施する必要があるため、コミュニティの投票がオプション B を支持しても、技術的にはオプション A が正しいとされる。"
      }
    ],
    "keywords": [
      "Service Control Policy",
      "AWS Organizations",
      "IAM",
      "explicit deny",
      "permissions management"
    ]
  },
  {
    "No": "33",
    "question": "A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC. Mobile clients\nconnect to the API by using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing\npolicy with the IP addresses of all the EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to trafic. The app\nhas not been able to keep up with the trafic.\nA solutions architect needs to implement a solution so that the app can handle the new and varying load.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業が、VPCのパブリックサブネット内の5つのAmazon EC2インスタンスでモノリシックなRESTベースのAPIをモバイルアプリ用にホストしています。モバイルクライアントは、Amazon Route 53にホストされているドメイン名を使用してAPIに接続します。企業は、すべてのEC2インスタンスのIPアドレスを持つRoute 53のマルチバリュー応答ルーティングポリシーを作成しました。最近、アプリは大きく突然のトラフィックの増加によって圧倒されており、アプリはトラフィックに追いつけていません。ソリューションアーキテクトは、アプリが新しい変動する負荷に対応できるようにするためのソリューションを実装する必要があります。どのソリューションが最小限の運用負荷でこれらの要件を満たすことができるでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Separate the API into individual AWS Lambda functions. Configure an Amazon API Gateway REST API with Lambda integration for the backend. Update the Route 53 record to point to the API Gateway API.",
        "text_jp": "APIを個別のAWS Lambda関数に分離する。バックエンド用にLambda統合を持つAmazon API Gateway REST APIを構成する。Route 53のレコードをAPI Gateway APIに向けるように更新する。"
      },
      {
        "key": "B",
        "text": "Containerize the API logic. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Run the containers in the cluster by using Amazon EC2. Create a Kubernetes ingress. Update the Route 53 record to point to the Kubernetes ingress.",
        "text_jp": "APIロジックをコンテナ化する。Amazon Elastic Kubernetes Service（Amazon EKS）クラスターを作成する。Amazon EC2を使用してクラスター内でコンテナを実行する。Kubernetesイングレスを作成する。Route 53のレコードをKubernetesイングレスに向けるように更新する。"
      },
      {
        "key": "C",
        "text": "Create an Auto Scaling group. Place all the EC2 instances in the Auto Scaling group. Configure the Auto Scaling group to perform scaling actions that are based on CPU utilization. Create an AWS Lambda function that reacts to Auto Scaling group changes and updates the Route 53 record.",
        "text_jp": "Auto Scalingグループを作成する。すべてのEC2インスタンスをAuto Scalingグループに配置する。CPU使用率に基づいてスケーリングアクションを実行するようにAuto Scalingグループを構成する。Auto Scalingグループの変更に反応してRoute 53のレコードを更新するAWS Lambda関数を作成する。"
      },
      {
        "key": "D",
        "text": "Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances as targets for the ALB. Update the Route 53 record to point to the ALB.",
        "text_jp": "APIの前にApplication Load Balancer（ALB）を作成する。EC2インスタンスをVPCのプライベートサブネットに移動する。EC2インスタンスをALBのターゲットとして追加する。Route 53のレコードをALBに向けるように更新する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (56%) C (24%) D (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Create an Application Load Balancer (ALB) in front of the API. This solution will distribute incoming traffic and improve fault tolerance.",
        "situation_analysis": "The company faces a significant increase in traffic that its current infrastructure cannot handle. A solution is needed to manage this load efficiently.",
        "option_analysis": "A is not optimal as it introduces complexity with multiple Lambda functions. B requires manual container management and is more complex. C involves setting an Auto Scaling group, but it doesn't address distribution directly. D meets the requirement with minimal changes to the infrastructure, simply incorporating an ALB to distribute requests.",
        "additional_knowledge": "Using an ALB can also improve security by moving EC2 instances to private subnets.",
        "key_terminology": "Application Load Balancer, Auto Scaling, EC2 instances, Route 53, traffic management",
        "overall_assessment": "Option D is the best choice with the least operational overhead while providing a robust solution for the increased traffic. Community votes may reflect misunderstanding of the operational simplicity of using an ALB."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです：APIの前にApplication Load Balancer（ALB）を作成します。このソリューションは、受信トラフィックを分散させ、フォールトトレランスを改善します。",
        "situation_analysis": "企業は、現在のインフラが処理できない大幅なトラフィックの増加に直面しています。この負荷を効率的に管理するためのソリューションが必要です。",
        "option_analysis": "Aは複数のLambda関数を使用することで複雑さを増し、最適ではありません。Bは手動でのコンテナ管理を必要とし、より複雑です。CはAuto Scalingグループを設定しますが、要求の分配を直接扱っていません。Dは、インフラストラクチャへの最小限の変更で要求を分散させるALBを組み込むという要件を満たしています。",
        "additional_knowledge": "ALBを使用することで、EC2インスタンスをプライベートサブネットに移動させることでもセキュリティを改善できます。",
        "key_terminology": "Application Load Balancer, Auto Scaling, EC2インスタンス, Route 53, トラフィック管理",
        "overall_assessment": "選択肢Dは、運用負荷が最も少なく、トラフィックの増加に対する堅牢なソリューションを提供する最良の選択肢です。コミュニティの投票は、ALBの運用の簡潔さに対する誤解を反映しているかもしれません。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "Auto Scaling",
      "EC2 instances",
      "Route 53",
      "traffic management"
    ]
  },
  {
    "No": "34",
    "question": "A company has created an OU in AWS Organizations for each of its engineering teams. Each OU owns multiple AWS accounts. The organization\nhas hundreds of AWS accounts.\nA solutions architect must design a solution so that each OU can view a breakdown of usage costs across its AWS accounts.\nWhich solution meets these requirements?",
    "question_jp": "ある企業は、AWS Organizations内に各エンジニアリングチームのためのOUを作成しました。各OUは複数のAWSアカウントを所有しています。組織には数百のAWSアカウントがあります。ソリューションアーキテクトは、各OUがそのAWSアカウントの使用コストの内訳を表示できるように設計する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access Manager. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.",
        "text_jp": "AWS Resource Access Managerを使用して各OUのためにAWSコストと使用量レポート（CUR）を作成します。各チームがAmazon QuickSightダッシュボードを通じてCURを視覚化できるようにします。"
      },
      {
        "key": "B",
        "text": "Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.",
        "text_jp": "AWS Organizationsの管理アカウントからAWSコストと使用量レポート（CUR）を作成します。各チームがAmazon QuickSightダッシュボードを通じてCURを視覚化できるようにします。"
      },
      {
        "key": "C",
        "text": "Create an AWS Cost and Usage Report (CUR) in each AWS Organizations member account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.",
        "text_jp": "各AWS OrganizationsメンバーアカウントにAWSコストと使用量レポート（CUR）を作成します。各チームがAmazon QuickSightダッシュボードを通じてCURを視覚化できるようにします。"
      },
      {
        "key": "D",
        "text": "Create an AWS Cost and Usage Report (CUR) by using AWS Systems Manager. Allow each team to visualize the CUR through Systems Manager OpsCenter dashboards.",
        "text_jp": "AWS Systems Managerを使用してAWSコストと使用量レポート（CUR）を作成します。各チームがSystems Manager OpsCenterダッシュボードを通じてCURを視覚化できるようにします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.",
        "situation_analysis": "The company has multiple AWS accounts organized within OUs under AWS Organizations. The requirement is to provide insights into the costs incurred by each OU's accounts.",
        "option_analysis": "Option A does not provide costs from a single source since it requires creating CURs for each OU independently. Option C creates separate CURs in each member account, making management cumbersome. Option D uses Systems Manager unnecessarily, as CUR is designed to be used with Cost Explorer and QuickSight.",
        "additional_knowledge": "Using QuickSight lets teams create customized dashboards for real-time insights.",
        "key_terminology": "AWS Organizations, Cost and Usage Report (CUR), Amazon QuickSight, AWS Resource Access Manager.",
        "overall_assessment": "Choice B is optimal as it allows central management from the management account, which is crucial in a multi-account architecture to ensure visibility across the organization."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです：AWS Organizationsの管理アカウントからAWSコストと使用量レポート（CUR）を作成します。各チームがAmazon QuickSightダッシュボードを通じてCURを視覚化できるようにします。",
        "situation_analysis": "この企業は、AWS Organizations内に複数のAWSアカウントをOUとして整理しています。それぞれのOUのアカウントにかかるコストの内訳を提供することが求められています。",
        "option_analysis": "選択肢Aは、各OUごとにCURを独立して作成する必要があるため、単一のソースからコストを提供しません。選択肢Cは、各メンバーアカウントに別々のCURを作成し、管理が煩雑になるため不適切です。選択肢Dは、CURを利用するときに不要なSystems Managerを使用しています。",
        "additional_knowledge": "QuickSightを使用すると、チームはリアルタイムの洞察のためにカスタマイズされたダッシュボードを作成できます。",
        "key_terminology": "AWS Organizations、コストと使用量レポート（CUR）、Amazon QuickSight、AWS Resource Access Manager。",
        "overall_assessment": "選択肢Bは、管理アカウントから中央管理が可能であり、多アカウントアーキテクチャにおいて重要な可視性を確保できるため最適です。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Cost and Usage Report (CUR)",
      "Amazon QuickSight",
      "AWS Resource Access Manager"
    ]
  },
  {
    "No": "35",
    "question": "A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily.\nThe company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company\nalready has established an AWS Direct Connect connection between the on-premises network and AWS.\nWhich data migration strategy should the company use?",
    "question_jp": "企業はオンプレミスのWindowsファイルサーバーにデータを保存しています。企業は毎日5GBの新しいデータを生成しています。企業は一部のWindowsベースのワークロードをAWSに移行しており、データをクラウドのファイルシステムで利用可能にする必要があります。企業はすでにオンプレミスのネットワークとAWS間にAWS Direct Connect接続を確立しています。企業はどのデータ移行戦略を使用すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway.",
        "text_jp": "ファイルゲートウェイのオプションをAWS Storage Gatewayで使用して、既存のWindowsファイルサーバーを置き換え、既存のファイル共有を新しいファイルゲートウェイにポイントします。"
      },
      {
        "key": "B",
        "text": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.",
        "text_jp": "AWS DataSyncを使用して、オンプレミスのWindowsファイルサーバーとAmazon FSx間でデータを複製するための毎日のタスクをスケジュールします。"
      },
      {
        "key": "C",
        "text": "Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).",
        "text_jp": "AWS Data Pipelineを使用して、オンプレミスのWindowsファイルサーバーとAmazon Elastic File System（EFS）間でデータを複製するための毎日のタスクをスケジュールします。"
      },
      {
        "key": "D",
        "text": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).",
        "text_jp": "AWS DataSyncを使用して、オンプレミスのWindowsファイルサーバーとAmazon Elastic File System（EFS）間でデータを複製するための毎日のタスクをスケジュールします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (63%) A (38%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.",
        "situation_analysis": "The company produces 5 GB of new data daily and needs to ensure that this data is replicated efficiently to AWS. The established Direct Connect connection implies a need for a secure and reliable method to move data regularly.",
        "option_analysis": "Option B is the most appropriate because AWS DataSync is designed for easy and efficient data transfer, particularly for large datasets between on-premises storage and AWS. Options A, C, and D either do not utilize the most efficient solution for this scenario or mention incorrect services for this specific case.",
        "additional_knowledge": "DataSync can handle large volumes of data efficiently, making it a robust choice for this migration.",
        "key_terminology": "AWS DataSync, Amazon FSx, On-premises data replication, File storage service.",
        "overall_assessment": "This question evaluates the understanding of AWS data migration services effectively, and the community supports the correct answer, indicating good alignment with AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはB: AWS DataSyncを使用して、オンプレミスのWindowsファイルサーバーとAmazon FSx間でデータを複製するための毎日のタスクをスケジュールします。",
        "situation_analysis": "企業は毎日5GBの新しいデータを生成し、このデータが効率的にAWSに複製される必要があります。確立されたDirect Connect接続は、定期的にデータを安全かつ信頼性のある方法で移動する必要性を示唆しています。",
        "option_analysis": "Bオプションが最も適切です。AWS DataSyncは、大規模なデータセットのオンプレミスストレージとAWS間の移行を簡単かつ効率的に行うように設計されています。他のオプションA、CおよびDは、このシナリオに対して最も効率的なソリューションを利用していなかったり、特定のケースに対して不適切なサービスに言及しています。",
        "additional_knowledge": "DataSyncは、大量のデータを効率よく処理できるため、この移行にとって堅実な選択肢です。",
        "key_terminology": "AWS DataSync、Amazon FSx、オンプレミスデータの複製、ファイルストレージサービス。",
        "overall_assessment": "この質問はAWSのデータ移行サービスの理解を効果的に評価しており、コミュニティも正解を支持しているため、AWSのベストプラクティスと良い整合性を示します。"
      }
    ],
    "keywords": [
      "AWS DataSync",
      "Amazon FSx",
      "On-premises data replication"
    ]
  },
  {
    "No": "36",
    "question": "A company's solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3\nbucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a\nsecond Region.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業のソリューションアーキテクトが、AWS上で実行されているウェブアプリケーションをレビューしています。このアプリケーションは、us-east-1リージョンのAmazon S3バケット内の静的資産を参照しています。企業は、複数のAWSリージョンにわたるレジリエンシーが必要です。企業は、すでに別のリージョンにS3バケットを作成しています。運用の負担が最も少ない方法はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set by using a weighted routing policy for each S3 bucket. Configure the application to reference the objects by using the Route 53 DNS name.",
        "text_jp": "アプリケーションを構成して、両方のS3バケットに各オブジェクトを書き込むようにします。Amazon Route 53のパブリックホストゾーンを設定し、それぞれのS3バケットのために加重ルーティングポリシーを使用してレコードセットを作成します。アプリケーションを構成して、Route 53のDNS名を使用してオブジェクトを参照します。"
      },
      {
        "key": "B",
        "text": "Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda function each time an object is written to the S3 bucket in us-east-1. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.",
        "text_jp": "AWS Lambda関数を作成し、us-east-1のS3バケットから別のリージョンのS3バケットにオブジェクトをコピーします。us-east-1のS3バケットにオブジェクトが書き込まれるたびにLambda関数を呼び出します。2つのS3バケットをオリジンとして含むAmazon CloudFrontディストリビューションを設定します。"
      },
      {
        "key": "C",
        "text": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.",
        "text_jp": "us-east-1のS3バケットにレプリケーションを設定して、別のリージョンのS3バケットにオブジェクトをレプリケートします。2つのS3バケットをオリジンとして含むAmazon CloudFrontディストリビューションを設定します。"
      },
      {
        "key": "D",
        "text": "Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region.",
        "text_jp": "us-east-1のS3バケットにレプリケーションを設定して、別のリージョンのS3バケットにオブジェクトをレプリケートします。フェイルオーバーが必要な場合は、アプリケーションコードを更新して、別のリージョンのS3バケットからオブジェクトを読み込みます。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (94%) 3%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Implementing replication on the S3 bucket in us-east-1 minimizes operational overhead while ensuring resilience in another region.",
        "situation_analysis": "Deployment of a web application with assets in Amazon S3 requires high availability and resilience across multiple AWS regions.",
        "option_analysis": "Option D correctly utilizes S3 replication, which simplifies the process of ensuring data is available in multiple regions, as opposed to the manual processes described in A and B, or the potential complexity of option C without failover.",
        "additional_knowledge": "Managing replication settings and monitoring replication status are crucial for successful implementation.",
        "key_terminology": "Amazon S3, Cross-Region Replication, AWS Lambda, Amazon CloudFront, Route 53",
        "overall_assessment": "While community votes lean heavily towards option C, D is the best choice for operational efficiency, considering both resilience and failover capability."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです。us-east-1のS3バケットにレプリケーションを実装することで、運用の負担を最小限に抑え、別のリージョンでの冗長性を確保できます。",
        "situation_analysis": "Amazon S3内のアセットを持つウェブアプリケーションの展開は、高可用性と複数のAWSリージョンにわたるレジリエンシーを必要とします。",
        "option_analysis": "選択肢Dは、S3レプリケーションを正しく活用してデータを複数のリージョンで利用可能にするプロセスを簡素化しており、AやBで示された手動の手法や、フェイルオーバーなしのCの潜在的な複雑さと比較して、正しい解法です。",
        "additional_knowledge": "レプリケーション設定の管理とレプリケーションステータスの監視は、成功する実装のために重要です。",
        "key_terminology": "Amazon S3, クロスリージョンレプリケーション, AWS Lambda, Amazon CloudFront, Route 53",
        "overall_assessment": "コミュニティの投票はCに偏っていますが、運用効率、冗長性、フェイルオーバー機能を考慮するとDが最適な選択肢です。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Cross-Region Replication",
      "AWS Lambda",
      "Amazon CloudFront",
      "Route 53"
    ]
  },
  {
    "No": "37",
    "question": "A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in trafic that resulted in downtime and a\nsignificant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a\ndependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily\nusers.\nWhich steps should the solutions architect take to design an appropriate solution?",
    "question_jp": "企業は、オンプレミス環境で三層のWebアプリケーションをホストしている。最近のトラフィックの急増によりダウンタイムが発生し、大幅な財務的影響が出たため、企業の管理部門はアプリケーションをAWSに移行することを決定した。アプリケーションは.NETで書かれており、MySQLデータベースに依存している。ソリューションアーキテクトは、日々20万人のユーザーに対応するためのスケーラブルで高可用性のソリューションを設計する必要がある。ソリューションアーキテクトは、適切なソリューションを設計するためにどのステップを踏むべきか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance. The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones. Use an Amazon Route 53 alias record to route trafic from the company's domain to the NLB.",
        "text_jp": "AWS Elastic Beanstalkを使用して、Webサーバー環境とAmazon RDS MySQL Multi-AZ DBインスタンスを持つ新しいアプリケーションを作成する。環境は、複数のアベイラビリティゾーンにおけるAmazon EC2 Auto Scalingグループの前にネットワークロードバランサー（NLB）を立ち上げるべきである。会社のドメインからNLBへのトラフィックをルーティングするためにAmazon Route 53のエイリアスレコードを使用する。"
      },
      {
        "key": "B",
        "text": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route trafic from the company's domain to the ALB.",
        "text_jp": "AWS CloudFormationを使用して、3つのアベイラビリティゾーンにまたがるAmazon EC2 Auto Scalingグループの前にアプリケーションロードバランサー（ALB）を含むスタックを立ち上げる。スタックは、保持削除ポリシーを持つAmazon Aurora MySQL DBクラスターのMulti-AZデプロイを立ち上げるべきである。会社のドメインからALBへのトラフィックをルーティングするためにAmazon Route 53のエイリアスレコードを使用する。"
      },
      {
        "key": "C",
        "text": "Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Region. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica. Use Amazon Route 53 with a geoproximity routing policy to route trafic between the two Regions.",
        "text_jp": "AWS Elastic Beanstalkを使用して、各リージョンにアプリケーションロードバランサー（ALB）を持つ自動スケーリングウェブサーバー環境を2つの別々のリージョンにまたがって作成する。クロスリージョンのリードレプリカを持つAmazon Aurora MySQL DBクラスターのMulti-AZデプロイを作成する。Amazon Route 53を使用して、2つのリージョン間でトラフィックをルーティングするために地理的近接ルーティングポリシーを使用する。"
      },
      {
        "key": "D",
        "text": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot instances spanning three Availability Zones. The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy. Use an Amazon Route 53 alias record to route trafic from the company's domain to the ALB.",
        "text_jp": "AWS CloudFormationを使用して、3つのアベイラビリティゾーンにまたがるスポットインスタンスのAmazon ECSクラスターの前にアプリケーションロードバランサー（ALB）を含むスタックを立ち上げる。スタックは、スナップショット削除ポリシーを持つAmazon RDS MySQL DBインスタンスを立ち上げるべきである。会社のドメインからALBへのトラフィックをルーティングするためにAmazon Route 53のエイリアスレコードを使用する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (94%) 4%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Utilizing AWS Elastic Beanstalk for application deployment allows for automatic scaling in multiple regions, which is vital for accommodating 200,000 daily users.",
        "situation_analysis": "The company needs a solution that ensures high availability and scalability due to increased traffic. The application is .NET based with a MySQL dependency, indicating a need for a managed database service.",
        "option_analysis": "Option A relies on EC2 Auto Scaling without cross-region scaling. Option B uses a single region, which may not fully account for sudden surges. Option D uses ECS Spot instances, which may not guarantee availability due to the nature of spot pricing.",
        "additional_knowledge": "Using Route 53 with geoproximity routing helps to direct users to the nearest available resource, enhancing performance.",
        "key_terminology": "AWS Elastic Beanstalk, Amazon Aurora, Multi-AZ, Application Load Balancer, Auto Scaling",
        "overall_assessment": "Option C stands out as it provides the necessary resilience and scalability. Although community voting supports option B, it lacks the cross-regional ability needed for robustness against traffic spikes. Option C could provide a layer of redundancy across regions that directly addresses the requirement for a critical workload."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。AWS Elastic Beanstalkを使用したアプリケーションのデプロイは、自動スケーリングを可能にし、200,000人のデイリーユーザーに対応するために複数のリージョンでの展開が重要である。",
        "situation_analysis": "企業は、トラフィック増加に伴う高可用性とスケーラビリティを確保するソリューションが必要である。アプリケーションは.NETベースであり、MySQLデータベースに依存しているため、管理されたデータベースサービスが求められる。",
        "option_analysis": "選択肢Aは、クロスリージョンスケーリングなしのEC2 Auto Scalingに依存している。選択肢Bは単一のリージョンを使用しており、突然の急増に完全に対応できないかもしれない。選択肢Dは、スポットインスタンスのECSを使用しており、スポット価格の特性から可用性を保証できないかもしれない。",
        "additional_knowledge": "Route 53を使用して地理的近接ルーティングを行うことで、ユーザーを最も近い利用可能なリソースに誘導し、パフォーマンスを向上させる。",
        "key_terminology": "AWS Elastic Beanstalk, Amazon Aurora, Multi-AZ, アプリケーションロードバランサー, オートスケーリング",
        "overall_assessment": "選択肢Cは、必要なレジリエンスとスケーラビリティを提供するため、際立っている。コミュニティ投票は選択肢Bを支持するが、それはトラフィックの急増に対する強靭性を得るためにはクロスリージョンの能力が不足している。選択肢Cは、重要なワークロードに対する要件に直接応じるリージョン間の冗長性を提供できる。"
      }
    ],
    "keywords": [
      "AWS Elastic Beanstalk",
      "Amazon Aurora",
      "Multi-AZ",
      "Application Load Balancer",
      "Auto Scaling"
    ]
  },
  {
    "No": "38",
    "question": "A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an\nAmazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations\nmember accounts.\nA solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of\nCloudFormation stacks. Trusted access has been enabled in Organizations.\nWhat should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
    "question_jp": "ある企業は、複数のAWSアカウントを管理するためにAWS Organizationsを使用しています。セキュリティ上の理由から、企業はすべてのOrganizationsメンバーアカウントでサードパーティのアラートシステムとの統合を可能にするAmazon Simple Notification Service (Amazon SNS)トピックの作成を要求しています。ソリューションアーキテクトは、SNSトピックを作成し、CloudFormationスタックの展開を自動化するためにスタックセットを使用するAWS CloudFormationテンプレートを作成しました。Trusted accessはOrganizationsで有効になっています。ソリューションアーキテクトは、CloudFormation StackSetsをすべてのAWSアカウントに展開するために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.",
        "text_jp": "組織のメンバーアカウントにスタックセットを作成します。サービス管理の権限を使用します。展開オプションを組織に展開するように設定します。CloudFormation StackSetsのドリフト検出を使用します。"
      },
      {
        "key": "B",
        "text": "Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.",
        "text_jp": "組織のメンバーアカウントにスタックを作成します。セルフサービスの権限を使用します。展開オプションを組織に展開するように設定します。CloudFormation StackSetsの自動展開を有効にします。"
      },
      {
        "key": "C",
        "text": "Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.",
        "text_jp": "Organizationsの管理アカウントにスタックセットを作成します。サービス管理の権限を使用します。展開オプションを組織に展開するように設定します。CloudFormation StackSetsの自動展開を有効にします。"
      },
      {
        "key": "D",
        "text": "Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection.",
        "text_jp": "Organizationsの管理アカウントにスタックを作成します。サービス管理の権限を使用します。展開オプションを組織に展開するように設定します。CloudFormation StackSetsのドリフト検出を有効にします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating a stack set in the Organizations management account allows for centralized management of resources across all member accounts using service-managed permissions and enables automatic deployment to all accounts in the organization.",
        "situation_analysis": "The organization needs to integrate Amazon SNS with a third-party alerting system across multiple AWS accounts.",
        "option_analysis": "Option C is correct as it uses the management account for centralized control. Options A and B require creating stack sets or stacks in member accounts, and D incorrectly uses stacks instead of stack sets in the management account.",
        "additional_knowledge": "It is crucial to select the right permissions model, as service-managed permissions simplify management by allowing AWS to handle permissions automatically.",
        "key_terminology": "AWS Organizations, CloudFormation StackSets, service-managed permissions, automatic deployment.",
        "overall_assessment": "Correct choice C aligns with AWS best practices for managing resources across multiple accounts using AWS Organizations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。Organizationsの管理アカウントにスタックセットを作成することで、サービス管理の権限を使用し、組織内のすべてのメンバーアカウントに自動展開を可能にする中央集権的なリソース管理が可能になる。",
        "situation_analysis": "組織は、複数のAWSアカウントにわたってAmazon SNSをサードパーティのアラートシステムと統合する必要がある。",
        "option_analysis": "Cは正解であり、管理アカウントを使用して中央集中型制御を実現する。AとBはメンバーアカウントにスタックまたはスタックセットを作成する必要があるため不正解であり、Dは管理アカウントの代わりにスタックを使うため間違っている。",
        "additional_knowledge": "正しい権限モデルを選択することが重要であり、サービス管理の権限はAWSが自動的に権限を処理できるため、管理を簡素化する。",
        "key_terminology": "AWS Organizations、CloudFormation StackSets、サービス管理の権限、自動展開。",
        "overall_assessment": "正しい選択肢Cは、AWSのベストプラクティスに沿って、複数のアカウントでのリソース管理を行うものである。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "CloudFormation StackSets",
      "service-managed permissions",
      "automatic deployment"
    ]
  },
  {
    "No": "39",
    "question": "A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux and Windows. The company has a large on-\npremises infrastructure that consists of physical machines and VMs that host numerous applications.\nThe company must capture details about the system configuration, system performance, running processes, and network connections of its on-\npremises workloads. The company also must divide the on-premises applications into groups for AWS migrations. The company needs\nrecommendations for Amazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effective manner.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "企業は、オンプレミスからAWSへのワークロード移行を希望しています。ワークロードはLinuxとWindowsで動作しています。企業には、物理マシンと多くのアプリケーションをホストするVMを含む大規模なオンプレミスインフラストラクチャがあります。企業は、オンプレミスのワークロードのシステム構成、システムパフォーマンス、実行中のプロセス、ネットワーク接続に関する詳細をキャプチャする必要があります。また、企業は、AWSへの移行のためにオンプレミスアプリケーションをグループ分けする必要があります。企業は、コスト効率の最も良い方法でAWS上でワークロードを実行するために、Amazon EC2インスタンスタイプの推奨事項が必要です。要件を満たすために、ソリューションアーキテクトはどのような手順の組み合わせを取るべきでしょうか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs.",
        "text_jp": "物理マシンとVMにAWS Application Discovery Agentをインストールして、既存のアプリケーションを評価します。"
      },
      {
        "key": "B",
        "text": "Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs.",
        "text_jp": "物理マシンとVMにAWS Systems Manager Agentをインストールして、既存のアプリケーションを評価します。"
      },
      {
        "key": "C",
        "text": "Group servers into applications for migration by using AWS Systems Manager Application Manager.",
        "text_jp": "AWS Systems Manager Application Managerを使用して、移行のためにサーバーをアプリケーションにグループ化します。"
      },
      {
        "key": "D",
        "text": "Group servers into applications for migration by using AWS Migration Hub.",
        "text_jp": "AWS Migration Hubを使用して、移行のためにサーバーをアプリケーションにグループ化します。"
      },
      {
        "key": "E",
        "text": "Generate recommended instance types and associated costs by using AWS Migration Hub.",
        "text_jp": "AWS Migration Hubを使用して、推奨されるインスタンスタイプと関連コストを生成します。"
      },
      {
        "key": "F",
        "text": "Import data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization.",
        "text_jp": "サーバーサイズに関するデータをAWS Trusted Advisorにインポートし、コスト最適化のための推奨事項に従います。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ADE (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Installing AWS Systems Manager Agent allows assessment of existing applications and gathering necessary system performance and configuration data.",
        "situation_analysis": "The scenario involves migrating workloads from on-premises to AWS, requiring detailed assessment of current applications and infrastructure.",
        "option_analysis": "Option B is correct as it enables capturing important metrics for applications. Options A, C, D, E, and F either do not provide required assessment data or focus on migration processes without initial assessment.",
        "additional_knowledge": "Using AWS Migration Hub ensures centralized tracking of migration activities, which is beneficial after initial assessment.",
        "key_terminology": "AWS Systems Manager Agent, application assessment, migration planning",
        "overall_assessment": "The community supported option A as well but it's less comprehensive for assessment compared to option B. Therefore, B is the optimal choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBです。AWS Systems Manager Agentをインストールすることで、既存のアプリケーションの評価と必要なシステムパフォーマンスおよび構成データの収集が可能になります。",
        "situation_analysis": "この状況では、オンプレミスからAWSへのワークロード移行が関与しており、現在のアプリケーションとインフラの詳細な評価が必要です。",
        "option_analysis": "Bの選択肢は、重要なメトリックを取得するために正しいです。選択肢A、C、D、E、Fは、必要な評価データを提供しないか、初期評価なしで移行プロセスに焦点を当てています。",
        "additional_knowledge": "AWS Migration Hubを使用することで、移行活動の集中管理が可能となり、初期評価後に有益です。",
        "key_terminology": "AWS Systems Manager Agent、アプリケーション評価、移行計画",
        "overall_assessment": "コミュニティは選択肢Aを支持しましたが、Bの選択肢に比べて評価に関しては包括的でないため、Bが最適な選択肢です。"
      }
    ],
    "keywords": [
      "AWS Systems Manager Agent",
      "Application Assessment",
      "AWS Migration Hub"
    ]
  },
  {
    "No": "40",
    "question": "A company is hosting an image-processing service on AWS in a VPC. The VPC extends across two Availability Zones. Each Availability Zone\ncontains one public subnet and one private subnet.\nThe service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service.\nThe service needs to communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The\nEC2 instances retrieve approximately 1 ТВ of data from an S3 bucket each day.\nThe company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without\ncompromising the service's security posture or increasing the time spent on ongoing operations.\nWhich solution will meet these requirements?",
    "question_jp": "会社は、AWS上で画像処理サービスをVPCにホストしています。このVPCは、二つのアベイラビリティゾーンに跨っています。各アベイラビリティゾーンには、一つのパブリックサブネットと一つのプライベートサブネットが含まれています。\nサービスはプライベートサブネット内のAmazon EC2インスタンス上で実行されており、パブリックサブネット内にあるアプリケーションロードバランサーがこのサービスの前に配置されています。\nサービスはインターネットと通信する必要があり、二つのNATゲートウェイを使用しています。このサービスでは、画像ストレージにAmazon S3を利用しています。\nEC2インスタンスは、毎日約1TBのデータをS3バケットから取得しています。\n会社はこのサービスを非常に安全なものとして宣伝しています。ソリューションアーキテクトは、サービスのセキュリティの姿勢を損なうことなく、また運用コストを増やすことなく、可能な限りクラウド支出を削減する必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Replace the NAT gateways with NAT instances. In the VPC route table, create a route from the private subnets to the NAT instances.",
        "text_jp": "NATゲートウェイをNATインスタンスに置き換えます。VPCルートテーブルでは、プライベートサブネットからNATインスタンスへのルートを作成します。"
      },
      {
        "key": "B",
        "text": "Move the EC2 instances to the public subnets. Remove the NAT gateways.",
        "text_jp": "EC2インスタンスをパブリックサブネットに移動します。NATゲートウェイを削除します。"
      },
      {
        "key": "C",
        "text": "Set up an S3 gateway VPC endpoint in the VPAttach an endpoint policy to the endpoint to allow the required actions on the S3 bucket.",
        "text_jp": "VPCAにS3ゲートウェイのVPCエンドポイントを設定します。エンドポイントポリシーをエンドポイントに添付し、S3バケットに対する必要なアクションを許可します。"
      },
      {
        "key": "D",
        "text": "Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances. Host the images on the EFS volume.",
        "text_jp": "EC2インスタンスにAmazon Elastic File System (Amazon EFS) ボリュームを接続します。画像をEFSボリュームにホストします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Setting up an S3 gateway VPC endpoint allows the EC2 instances in the private subnets to access the S3 bucket directly without needing NAT gateways. This reduces costs significantly since NAT gateways incur additional charges per hour and for the data processed.",
        "situation_analysis": "The company requires a solution that allows private EC2 instances to communicate with S3 while keeping costs low and maintaining high security.",
        "option_analysis": "Option A still requires NAT instances, which does not reduce costs as effectively. Option B compromises security by placing EC2 instances in public subnets. Option D introduces EFS, which may increase costs without addressing the primary issue.",
        "additional_knowledge": "S3 gateway VPC endpoints provide a secure and cost-effective means of accessing S3 from private subnets.",
        "key_terminology": "VPC, NAT gateway, S3, Security, Cost Savings",
        "overall_assessment": "The voting indicates a strong preference for option C, aligning with the company's cost-saving and security needs. There is a consensus among community votes supporting the choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。S3ゲートウェイのVPCエンドポイントを設定することで、プライベートサブネット内のEC2インスタンスがNATゲートウェイなしでS3バケットに直接アクセスできます。これにより、NATゲートウェイの利用に伴う追加コストを大幅に削減できます。",
        "situation_analysis": "会社は、コストを低く保ちながらプライベートEC2インスタンスがS3と通信できるソリューションを求めています。",
        "option_analysis": "選択肢AはNATインスタンスを必要とするため、コストを十分に削減できません。選択肢Bは、EC2インスタンスをパブリックサブネットに配置することによりセキュリティを損ないます。選択肢Dは、EFSを導入することで、主要な問題に対処せずにコストを増加させる可能性があります。",
        "additional_knowledge": "S3ゲートウェイVPCエンドポイントは、プライベートサブネットからS3に安全かつコスト効果の高いアクセス手段を提供します。",
        "key_terminology": "VPC, NATゲートウェイ, S3, セキュリティ, コスト削減",
        "overall_assessment": "投票結果は、コスト削減とセキュリティニーズに合致するCの選択を強く支持しています。コミュニティ投票においてもこの選択肢の支持が一致しています。"
      }
    ],
    "keywords": [
      "VPC",
      "NAT gateway",
      "S3",
      "Security",
      "Cost Savings"
    ]
  },
  {
    "No": "41",
    "question": "A company recently deployed an application on AWS. The application uses Amazon DynamoDB. The company measured the application load and\nconfigured the RCUs and WCUs on the DynamoDB table to match the expected peak load. The peak load occurs once a week for a 4-hour period\nand is double the average load. The application load is close to the average load for the rest of the week. The access pattern includes many more\nwrites to the table than reads of the table.\nA solutions architect needs to implement a solution to minimize the cost of the table.\nWhich solution will meet these requirements?",
    "question_jp": "企業は最近、AWS上にアプリケーションを展開しました。このアプリケーションはAmazon DynamoDBを使用しています。企業はアプリケーションの負荷を測定し、予想されるピーク負荷に合わせてDynamoDBテーブルのRCUおよびWCUを設定しました。ピーク負荷は週に一度、4時間の間に発生し、平均負荷の倍に達します。アプリケーションの負荷は、他の週の残りの期間中は平均負荷に近いです。アクセスパターンはテーブルの読み取りよりも書き込みが遥かに多いです。ソリューションアーキテクトは、テーブルのコストを最小限に抑えるためのソリューションを実装する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load.",
        "text_jp": "AWS Application Auto Scalingを使用して、ピーク期間中にキャパシティを増加させる。平均負荷に合わせて予約されたRCUおよびWCUを購入する。"
      },
      {
        "key": "B",
        "text": "Configure on-demand capacity mode for the table.",
        "text_jp": "テーブルのオンデマンドキャパシティモードを設定する。"
      },
      {
        "key": "C",
        "text": "Configure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table.",
        "text_jp": "テーブルの前にDynamoDB Accelerator（DAX）を設定する。テーブルの新しいピーク負荷に合わせて、プロビジョニングされた読み取りキャパシティを減少させる。"
      },
      {
        "key": "D",
        "text": "Configure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table.",
        "text_jp": "テーブルの前にDynamoDB Accelerator（DAX）を設定する。テーブルのオンデマンドキャパシティモードを設定する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (72%) B (17%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. By configuring DynamoDB Accelerator (DAX) in front of the table and using on-demand capacity mode, the solution can dynamically adjust to varying loads without incurring unnecessary costs during non-peak times.",
        "situation_analysis": "The application experiences peak load once a week, which is double the average load but occurs only over a short period. This demands a flexible solution that can accommodate short bursts of high traffic.",
        "option_analysis": "Option A involves using Auto Scaling and reserved capacity, which may still incur costs during low usage periods. Option B (on-demand capacity) is good but lacks the caching capabilities of DAX. Option C would decrease read capacity, which could hinder performance during peak. Option D exploits DAX's caching while allowing for on-demand scaling, minimizing costs effectively.",
        "additional_knowledge": "DAX is ideal for read-heavy workloads, which aligns with the application's usage pattern.",
        "key_terminology": "DynamoDB, DAX, on-demand capacity, provisioning, caching",
        "overall_assessment": "The community's vote suggests a misunderstanding of the benefits of on-demand and caching with DAX. While many chose A, option D is indeed the most cost-effective and technically sound solution given the scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。DynamoDB Accelerator（DAX）をテーブルの前に設定し、オンデマンドキャパシティモードを使用することで、ソリューションは変化する負荷に動的に調整でき、ピーク時以外の不要なコストを発生させることなく利用可能である。",
        "situation_analysis": "アプリケーションは週に一度、平均負荷の倍のピーク負荷を経験しますが、短期間の間にのみ発生します。これには、短い高トラフィックのピークを処理できる柔軟なソリューションが求められる。",
        "option_analysis": "Aの選択肢はAuto Scalingと予約容量を使用することで、低使用時にもコストが発生する可能性がある。Bの選択肢（オンデマンドキャパシティ）は良いが、DAXのキャッシング機能が欠けている。Cの選択肢はリードキャパシティを減少させ、ピーク時のパフォーマンスに悪影響を与える可能性がある。Dの選択肢はDAXのキャッシングを活用しながら、オンデマンドスケーリングを許可し、コストを効果的に最小限に抑える。",
        "additional_knowledge": "DAXは読み取りが多いワークロードに最適であり、アプリケーションの利用パターンに合致している。",
        "key_terminology": "DynamoDB, DAX, オンデマンドキャパシティ, プロビジョニング, キャッシング",
        "overall_assessment": "コミュニティの投票は、オンデマンドおよびDAXによるキャッシングの利点に対する誤解を示唆している。多くの人がAを選んでいるが、Dの選択肢が実際にはシナリオに最もコスト効率が高く、技術的に優れた解決策である。"
      }
    ],
    "keywords": [
      "DynamoDB",
      "DAX",
      "on-demand capacity",
      "provisioning",
      "caching"
    ]
  },
  {
    "No": "42",
    "question": "A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users\nupload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a\nmessage queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting\nprocessing is significantly higher during business hours, with the number of files rapidly declining after business hours.\nWhat is the MOST cost-effective migration recommendation?",
    "question_jp": "ソリューションアーキテクトは、オンプレミスのデータ処理アプリケーションをAWSクラウドに移行する方法について、企業に助言する必要があります。現在、ユーザーはウェブポータルを通じて入力ファイルをアップロードします。ウェブサーバーはその後、アップロードされたファイルをNASに保存し、メッセージキューを介して処理サーバーにメッセージを送信します。各メディアファイルの処理には最大で1時間かかります。企業は、処理を待っているメディアファイルの数が営業時間中に著しく高く、営業時間外には急激に減少することを確認しています。最もコスト効果の高い移行推奨は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket.",
        "text_jp": "Amazon SQSを使用してキューを作成します。既存のウェブサーバーを構成して、新しいキューにメッセージを発行します。キューにメッセージがある場合、AWS Lambda関数を呼び出してキューからリクエストを取得し、ファイルを処理します。処理されたファイルはAmazon S3バケットに保存します。"
      },
      {
        "key": "B",
        "text": "Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete.",
        "text_jp": "Amazon MQを使用してキューを作成します。既存のウェブサーバーを構成して、新しいキューにメッセージを発行します。キューにメッセージがある場合、新しいAmazon EC2インスタンスを作成して、キューからリクエストを取得し、ファイルを処理します。処理されたファイルはAmazon EFSに保存します。タスクが完了したらEC2インスタンスをシャットダウンします。"
      },
      {
        "key": "C",
        "text": "Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS.",
        "text_jp": "Amazon MQを使用してキューを作成します。既存のウェブサーバーを構成して、新しいキューにメッセージを発行します。キューにメッセージがある場合、AWS Lambda関数を呼び出してキューからリクエストを取得し、ファイルを処理します。処理されたファイルはAmazon EFSに保存します。"
      },
      {
        "key": "D",
        "text": "Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.",
        "text_jp": "Amazon SQSを使用してキューを作成します。既存のウェブサーバーを構成して、新しいキューにメッセージを発行します。EC2 Auto Scalingグループ内のAmazon EC2インスタンスを使用してキューからリクエストを取得し、ファイルを処理します。SQSキューの長さに基づいてEC2インスタンスをスケーリングします。処理されたファイルはAmazon S3バケットに保存します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (94%) 3%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution effectively utilizes Amazon SQS and Auto Scaling to handle varying workloads and is cost-effective.",
        "situation_analysis": "The company requires a scalable solution that can manage peaks in file processing during business hours and minimize costs during off-hours.",
        "option_analysis": "Option D leverages Amazon SQS with EC2 Auto Scaling to handle fluctuating workloads efficiently. Option A is efficient but does not scale automatically. Option B may incur higher costs due to manual EC2 management and EFS cost. Option C does not utilize EC2 scaling, which may result in performance bottlenecks.",
        "additional_knowledge": "Configuring Auto Scaling policies appropriately would enable the solution to respond dynamically to changes in the workload.",
        "key_terminology": "Amazon SQS, EC2 Auto Scaling, Amazon S3, AWS Lambda, Amazon EFS",
        "overall_assessment": "Answer D aligns with AWS best practices for cost-effectiveness and scalability, allowing the company to handle variable load efficiently. The community supports this choice significantly as indicated by the voting pattern."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。このソリューションは、Amazon SQSとAuto Scalingを効果的に利用して、変動する作業負荷を処理し、コストを最適化する。",
        "situation_analysis": "企業は、営業時間中のファイル処理のピークを管理し、オフピーク時のコストを最小限に抑えるスケーラブルなソリューションを必要としている。",
        "option_analysis": "選択肢Dは、変動する作業負荷を効率的に処理するために、Amazon SQSとEC2 Auto Scalingを活用している。選択肢Aは効率的であるが、自動でスケールしない。選択肢Bは、手動でEC2を管理し、EFSのコストが発生するため、コストが高くなる可能性がある。選択肢Cは、EC2のスケーリングを利用していないため、パフォーマンスのボトルネックが発生する可能性がある。",
        "additional_knowledge": "Auto Scalingポリシーを適切に構成することで、ワークロードの変化に自動的に応じることができるソリューションになる。",
        "key_terminology": "Amazon SQS, EC2 Auto Scaling, Amazon S3, AWS Lambda, Amazon EFS",
        "overall_assessment": "回答Dは、コスト効果とスケーラビリティの観点からAWSのベストプラクティスに沿っており、企業は変動する負荷を効率的に処理できる。コミュニティは投票パターンからもこの選択を支持している。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "EC2 Auto Scaling",
      "Amazon S3",
      "AWS Lambda",
      "Amazon EFS"
    ]
  },
  {
    "No": "43",
    "question": "A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes\nfrom an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the\ncompany deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.\nThe company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業がデータ分析のためにAmazon OpenSearch Serviceを使用しています。企業はAmazon S3バケットからS3 Standardストレージを使用しているOpenSearch Serviceクラスターに10のデータノードを持つデータをロードします。このデータは、クラスターに1ヶ月間保持され、読み取り専用の分析に使用されます。1ヶ月後、企業はクラスターからデータを含むインデックスを削除します。コンプライアンスの目的上、企業はすべての入力データのコピーを保持する必要があります。企業は今後のコストについて懸念しており、ソリューションアーキテクトに新しいソリューションを推奨するよう求めています。どのソリューションがこれらの要件を最もコスト効率的に満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Replace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.",
        "text_jp": "すべてのデータノードをUltraWarmノードに置き換えて、予想される容量を処理します。企業がデータをクラスターにロードするときに、入力データをS3 StandardからS3 Glacier Deep Archiveに移行します。"
      },
      {
        "key": "B",
        "text": "Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy.",
        "text_jp": "クラスター内のデータノードの数を2に減らし、予想される容量を処理するためにUltraWarmノードを追加します。OpenSearch Serviceがデータを取り込むときにインデックスがUltraWarmに移行するように設定します。企業は1ヶ月後にS3ライフサイクルポリシーを使用して、入力データをS3 Glacier Deep Archiveに移行します。"
      },
      {
        "key": "C",
        "text": "Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Add cold storage nodes to the cluster Transition the indexes from UltraWarm to cold storage. Delete the input data from the S3 bucket after 1 month by using an S3 Lifecycle policy.",
        "text_jp": "クラスター内のデータノードの数を2に減らし、予想される容量を処理するためにUltraWarmノードを追加します。OpenSearch Serviceがデータを取り込むときにインデックスがUltraWarmに移行するように設定します。クラスターにコールドストレージノードを追加し、インデックスをUltraWarmからコールドストレージに移行します。企業は1ヶ月後にS3ライフサイクルポリシーを使用してS3バケットから入力データを削除します。"
      },
      {
        "key": "D",
        "text": "Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.",
        "text_jp": "クラスター内のデータノードの数を2に減らし、予想される容量を処理するためにインスタンスバックのデータノードを追加します。企業がデータをクラスターにロードするときに、入力データをS3 StandardからS3 Glacier Deep Archiveに移行します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which effectively addresses cost while ensuring compliance with data retention.",
        "situation_analysis": "The company needs to retain input data for compliance but is concerned about ongoing costs. The data is stored temporarily for read-only analysis.",
        "option_analysis": "Option B reduces the number of data nodes, adds UltraWarm nodes, and utilizes S3 Lifecycle policies for cost-effective storage management, thus aligning with AWS best practices.",
        "additional_knowledge": "",
        "key_terminology": "OpenSearch Service, UltraWarm, S3 Lifecycle Policies, S3 Glacier Deep Archive, Data Retention",
        "overall_assessment": "Option B is widely supported by the community due to its balance of cost-effectiveness and compliance."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBであり、コストを効果的に監理しつつ、データ保持に関するコンプライアンスを確保しています。",
        "situation_analysis": "企業はコンプライアンスのために入力データを保持する必要があるが、今後のコストに懸念を抱いている。データは読み取り専用の分析のために一時的に保存されています。",
        "option_analysis": "オプションBはデータノードの数を減らし、UltraWarmノードを追加し、コスト効果的なストレージ管理のためにS3ライフサイクルポリシーを利用するため、AWSのベストプラクティスに沿っています。",
        "additional_knowledge": "",
        "key_terminology": "OpenSearch Service, UltraWarm, S3ライフサイクルポリシー, S3 Glacier Deep Archive, データ保持",
        "overall_assessment": "オプションBは、コスト効果とコンプライアンスのバランスが取れているため、コミュニティによって広く支持されています。"
      }
    ],
    "keywords": [
      "OpenSearch Service",
      "UltraWarm",
      "S3 Lifecycle Policies",
      "S3 Glacier Deep Archive",
      "Data Retention"
    ]
  },
  {
    "No": "44",
    "question": "A company has 10 accounts that are part of an organization in AWS Organizations. AWS Config is configured in each account. All accounts belong\nto either the Prod OU or the NonProd OU.\nThe company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic\nwhen an Amazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source. The company's security team is subscribed to the SNS\ntopic.\nFor all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0\nas the source.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "question_jp": "企業には、AWS Organizations内の10のアカウントがあり、それぞれのアカウントにAWS Configが設定されています。全アカウントはProd OUまたはNonProd OUに所属しています。企業は、Amazon EC2のセキュリティグループのインバウンドルールに0.0.0.0/0をソースとして指定した場合に、Amazon Simple Notification Service（Amazon SNS）トピックに通知するため、各AWSアカウントにAmazon EventBridgeルールを設定しています。企業のセキュリティチームは、このSNSトピックにサブスクライブしています。NonProd OU内のすべてのアカウントに対して、セキュリティチームは0.0.0.0/0をソースとして含むセキュリティグループのインバウンドルールを作成する能力を削除する必要があります。どのソリューションが最も運用負荷を軽くしてこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic. Deploy the updated rule to the NonProd OU.",
        "text_jp": "EventBridgeルールを変更して、AWS Lambda関数を呼び出し、セキュリティグループのインバウンドルールを削除し、SNSトピックに公開する。更新されたルールをNonProd OUにデプロイする。"
      },
      {
        "key": "B",
        "text": "Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU.",
        "text_jp": "NonProd OUにvpc-sg-open-only-to-authorized-ports AWS Configマネージドルールを追加する。"
      },
      {
        "key": "C",
        "text": "Configure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. Apply the SCP to the NonProd OU.",
        "text_jp": "aws:SourceIp条件キーの値が0.0.0.0/0でない場合にec2:AuthorizeSecurityGroupIngressアクションを許可するSCPを構成する。これをNonProd OUに適用する。"
      },
      {
        "key": "D",
        "text": "Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Apply the SCP to the NonProd OU.",
        "text_jp": "aws:SourceIp条件キーの値が0.0.0.0/0である場合にec2:AuthorizeSecurityGroupIngressアクションを拒否するSCPを構成する。これをNonProd OUに適用する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (58%) A (39%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This option allows for the least operational overhead while effectively restricting the creation of an inbound rule with a source of 0.0.0.0/0.",
        "situation_analysis": "The company needs to enforce security constraints across multiple accounts in the NonProd OU without adding significant operational complexity.",
        "option_analysis": "Option C directly addresses the requirement by applying a Service Control Policy (SCP) which is inherent to the AWS Organizations' control mechanisms. Option D might also achieve the goal but adding an explicit deny could lead to additional complexity in management. Options A and B require additional actions or configurations that increase operational overhead.",
        "additional_knowledge": "It's recommended to regularly review and audit the effectiveness of SCPs in preventing unwanted access.",
        "key_terminology": "Service Control Policy, AWS Organizations, ec2:AuthorizeSecurityGroupIngress, security best practices, 0.0.0.0/0",
        "overall_assessment": "While community votes showed some preference for option D, option C is superior in terms of managing operational complexity and maintaining security posture."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。この選択肢は、0.0.0.0/0をソースとするインバウンドルールの作成を効果的に制限しつつ、運用負荷を最小限に抑える。",
        "situation_analysis": "企業は、NonProd OU内の複数のアカウントにわたってセキュリティ制約を強化する必要があるが、大きな運用の複雑さを追加せずに実施する必要がある。",
        "option_analysis": "選択肢Cは、AWS Organizationsの制御メカニズムに内在するサービスコントロールポリシー（SCP）を適用することにより、要件に直接対処している。選択肢Dも目標を達成する可能性があるが、明示的な拒否を追加することは管理の複雑さを増す可能性がある。選択肢AとBは、運用負荷を増加させる追加のアクションや構成を必要とする。",
        "additional_knowledge": "SCPが不要なアクセスを防ぐ効果を定期的にレビューおよび監査することが推奨される。",
        "key_terminology": "サービスコントロールポリシー, AWS Organizations, ec2:AuthorizeSecurityGroupIngress, セキュリティベストプラクティス, 0.0.0.0/0",
        "overall_assessment": "コミュニティ投票は選択肢Dに若干の支持を示しているが、運用の複雑さの管理とセキュリティ姿勢の維持に関しては選択肢Cが優れている。"
      }
    ],
    "keywords": [
      "Service Control Policy",
      "AWS Organizations",
      "ec2:AuthorizeSecurityGroupIngress",
      "security best practices",
      "0.0.0.0/0"
    ]
  },
  {
    "No": "45",
    "question": "A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud.\nThe company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an\nApplication Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a\nserverless architecture.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "企業はオンプレミスのデータセンターにGitリポジトリをホストしています。企業はWebhookを使用してAWSクラウドで実行される機能を呼び出します。企業は、Application Load Balancer（ALB）のターゲットとして設定したAuto Scalingグループ内の一連のAmazon EC2インスタンスにWebhookロジックをホストしています。Gitサーバーは構成されたWebhookのためにALBを呼び出します。企業は、ソリューションをサーバーレスアーキテクチャに移行したいと考えています。どのソリューションが最も運用の手間を減らし、これらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "For each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs.",
        "text_jp": "各Webhookに対してAWS Lambda関数のURLを作成し、設定します。Gitサーバーを個々のLambda関数のURLに更新します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint.",
        "text_jp": "Amazon API Gateway HTTP APIを作成します。各Webhookロジックを別々のAWS Lambda関数に実装します。GitサーバーをAPI Gatewayエンドポイントに更新します。"
      },
      {
        "key": "C",
        "text": "Deploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB endpoint.",
        "text_jp": "WebhookロジックをAWS App Runnerにデプロイします。ALBを作成し、App Runnerをターゲットとして設定します。GitサーバーをALBエンドポイントに更新します。"
      },
      {
        "key": "D",
        "text": "Containerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS Fargate. Create an Amazon API Gateway REST API, and set Fargate as the target. Update the Git servers to call the API Gateway endpoint.",
        "text_jp": "Webhookロジックをコンテナ化します。Amazon Elastic Container Service（Amazon ECS）クラスターを作成し、AWS FargateでWebhookロジックを実行します。Amazon API Gateway REST APIを作成し、Fargateをターゲットとして設定します。GitサーバーをAPI Gatewayエンドポイントに更新します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (89%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Deploying the webhook logic to AWS App Runner minimizes operational overhead while allowing for scalable execution of the code in response to the requests.",
        "situation_analysis": "The company wants to transition from an EC2-based architecture to a more efficient serverless model while maintaining the ability to process webhooks.",
        "option_analysis": "Option A, using AWS Lambda function URLs, introduces management complexity as each webhook requires an individual configuration and deployment. Option B is similar as it uses API Gateway but adds unnecessary complexity. Option D, using containerization, adds overhead by requiring more components to manage, which is counter to the requirement for minimizing operational overhead.",
        "additional_knowledge": "Understanding the capabilities of AWS App Runner and its integration with ALB is crucial for effectively managing serverless workloads.",
        "key_terminology": "AWS App Runner, serverless architecture, operational overhead, scalable execution, webhook.",
        "overall_assessment": "While option B was favored by the community voting, option C actually meets the requirement for the least operational overhead more effectively, making it a better choice despite the community preference."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。WebhookロジックをAWS App Runnerにデプロイすることで、運用の手間を最小限に抑えつつ、要求に応じてコードをスケーラブルに実行できるようになります。",
        "situation_analysis": "企業は、EC2ベースのアーキテクチャから、より効率的なサーバーレスモデルへの移行を望んでおり、Webhookの処理能力を維持しながらも運用の簡素化を図りたいと考えています。",
        "option_analysis": "選択肢AのAWS Lambda関数URLを使用する方法は、各Webhookに対して個別の設定とデプロイが必要になるため、管理の複雑さを招きます。選択肢Bも同様にAPI Gatewayを使用しますが、不要な複雑さを加えています。選択肢Dはコンテナ化を使用しますが、より多くのコンポーネントを管理する必要が生じるため、運用の簡素化の要件には逆行します。",
        "additional_knowledge": "AWS App Runnerの機能とALBとの統合を理解することは、サーバーレスワークロードの効果的な管理にとって重要です。",
        "key_terminology": "AWS App Runner, サーバーレスアーキテクチャ, 運用の手間, スケーラブルな実行, Webhook.",
        "overall_assessment": "コミュニティ投票では選択肢Bが支持されましたが、運用の手間を最小限に抑える要件を満たすのは選択肢Cであり、これがより良い選択であると言えます。"
      }
    ],
    "keywords": [
      "AWS App Runner",
      "serverless architecture",
      "operational overhead",
      "scalable execution",
      "webhook"
    ]
  },
  {
    "No": "46",
    "question": "A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company's data center. As\npart of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and\nrunning processes. The company then wants to query and analyze the data.\nWhich solution will meet these requirements?",
    "question_jp": "企業は、1,000台のオンプレミスサーバーをAWSに移行する計画を立てています。これらのサーバーは、企業のデータセンター内のいくつかのVMwareクラスタで稼働しています。移行計画の一環として、企業はCPUの詳細、RAMの使用状況、オペレーティングシステムの情報、および実行中のプロセスなどのサーバーのメトリックを収集したいと考えています。企業は次に、データをクエリし、分析したいと考えています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select.",
        "text_jp": "AWS Agentless Discovery Connector仮想アプライアンスをオンプレミスホストにデプロイして構成します。AWS Migration Hubでデータ探索を構成します。AWS Glueを使用してデータに対してETLジョブを実行します。Amazon S3 Selectを使用してデータをクエリします。"
      },
      {
        "key": "B",
        "text": "Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight.",
        "text_jp": "オンプレミスホストからVMパフォーマンス情報のみをエクスポートします。必要なデータを直接AWS Migration Hubにインポートします。Migration Hubの不足している情報を更新します。Amazon QuickSightを使用してデータをクエリします。"
      },
      {
        "key": "C",
        "text": "Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource- attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console.",
        "text_jp": "オンプレミスホストからサーバー情報を自動的に収集するスクリプトを作成します。AWS CLIを使用してput-resource-attributesコマンドを実行し、詳細なサーバーデータをAWS Migration Hubに保存します。Migration Hubコンソールでデータを直接クエリします。"
      },
      {
        "key": "D",
        "text": "Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.",
        "text_jp": "AWS Application Discovery Agentを各オンプレミスサーバーにデプロイします。AWS Migration Hubでデータ探索を構成します。Amazon Athenaを使用して、Amazon S3内のデータに対して事前定義されたクエリを実行します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (89%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating a script to gather server information automatically is an effective way to collect detailed metrics.",
        "situation_analysis": "The company needs to gather comprehensive metrics from on-premises server infrastructure before migrating to AWS.",
        "option_analysis": "Option C allows for a customized data collection process tailored to the company's specific needs, while other options either lack the required detail or depend on manual input.",
        "additional_knowledge": "The use of scripts allows for flexibility and can automate the process of metric gathering, which is essential for large scale migrations.",
        "key_terminology": "AWS Application Discovery Service, AWS CLI, Migration Hub.",
        "overall_assessment": "Despite the community vote preferring option D, option C provides a more integrated and detailed approach to data gathering."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。サーバー情報を自動的に収集するスクリプトを作成することは、詳細なメトリックを収集する効果的な方法である。",
        "situation_analysis": "企業は、AWSに移行する前に、オンプレミスサーバーインフラから包括的なメトリックを収集する必要がある。",
        "option_analysis": "Cオプションは、企業の特定のニーズに合わせたカスタマイズされたデータ収集プロセスを可能にする。一方、他のオプションは必要な詳細がないか、手動での入力に依存している。",
        "additional_knowledge": "スクリプトを使用することで柔軟性が生まれ、大規模な移行に必要なメトリック収集のプロセスを自動化できる。",
        "key_terminology": "AWS Application Discovery Service, AWS CLI, Migration Hub.",
        "overall_assessment": "コミュニティの投票はDオプションを支持しているが、Cオプションはデータ収集に対してより統合的で詳細なアプローチを提供する。"
      }
    ],
    "keywords": [
      "AWS Application Discovery Service",
      "AWS CLI",
      "Migration Hub"
    ]
  },
  {
    "No": "47",
    "question": "A company is building a serverless application that runs on an AWS Lambda function that is attached to a VPC. The company needs to integrate\nthe application with a new service from an external provider. The external provider supports only requests that come from public IPv4 addresses\nthat are in an allow list.\nThe company must provide a single public IP address to the external provider before the application can start using the new service.\nWhich solution will give the application the ability to access the new service?",
    "question_jp": "会社は、VPCに接続されたAWS Lambda関数上で実行されるサーバーレスアプリケーションを構築しています。会社は、新しい外部プロバイダーのサービスとアプリケーションを統合する必要があります。外部プロバイダーは、許可リストにある公開IPv4アドレスからのリクエストのみをサポートしています。会社は、新しいサービスを使用する前に外部プロバイダーに単一の公共IPアドレスを提供する必要があります。どの解決策がアプリケーションに新しいサービスへのアクセス能力を与えますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.",
        "text_jp": "NATゲートウェイを展開し、Elastic IPアドレスをNATゲートウェイに関連付ける。VPCをNATゲートウェイを使用するように構成する。"
      },
      {
        "key": "B",
        "text": "Deploy an egress-only internet gateway. Associate an Elastic IP address with the egress-only internet gateway. Configure the elastic network interface on the Lambda function to use the egress-only internet gateway.",
        "text_jp": "Egress-onlyインターネットゲートウェイを展開し、Elastic IPアドレスをEgress-onlyインターネットゲートウェイに関連付ける。Lambda関数のElasticネットワークインターフェースをEgress-onlyインターネットゲートウェイを使用するように構成する。"
      },
      {
        "key": "C",
        "text": "Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the Lambda function to use the internet gateway.",
        "text_jp": "インターネットゲートウェイを展開し、Elastic IPアドレスをインターネットゲートウェイに関連付ける。Lambda関数をインターネットゲートウェイを使用するように構成する。"
      },
      {
        "key": "D",
        "text": "Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the default route in the public VPC route table to use the internet gateway.",
        "text_jp": "インターネットゲートウェイを展開し、Elastic IPアドレスをインターネットゲートウェイに関連付ける。パブリックVPCルートテーブルのデフォルトルートをインターネットゲートウェイを使用するように構成する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (94%) 4%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. By deploying an Internet Gateway with an Elastic IP address for the AWS Lambda function, the application can access the new service publicly.",
        "situation_analysis": "The application runs in a VPC but needs to connect to an external service requiring an authorized public IPv4 address.",
        "option_analysis": "Option A deploys a NAT Gateway but does not provide a public IP directly required by the external service. Option B is for outbound traffic from a VPC but does not provide a public IP. Option D is similar to C but focuses on configuring routes rather than leveraging the Lambda function's direct access to the Internet Gateway.",
        "additional_knowledge": "Understanding the differences between NAT Gateways and Internet Gateways is crucial for VPC configurations.",
        "key_terminology": "AWS Lambda, VPC, Internet Gateway, Elastic IP, NAT Gateway",
        "overall_assessment": "Given the need for a public IP for outgoing requests, option C is clearly the best solution. The community vote heavily supports this choice, indicating strong consensus."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。Elastic IPアドレスを持つインターネットゲートウェイを展開することで、AWS Lambda関数は新しいサービスに公開でアクセスできるようになる。",
        "situation_analysis": "アプリケーションはVPC内で稼働しているが、認可された公開IPv4アドレスを必要とする外部サービスに接続する必要がある。",
        "option_analysis": "選択肢AはNATゲートウェイを展開するが、外部サービスに必要な公開IPを直接提供しない。選択肢BはVPCからのアウトバウンドトラフィック用だが、公開IPを提供しない。選択肢DはCと似ているが、Lambda関数がインターネットゲートウェイに直接アクセスすることに重点を置いている。",
        "additional_knowledge": "NATゲートウェイとインターネットゲートウェイの違いを理解することは、VPCの設定において重要である。",
        "key_terminology": "AWS Lambda, VPC, インターネットゲートウェイ, Elastic IP, NATゲートウェイ",
        "overall_assessment": "公開要求に対して公開IPが必要なため、選択肢Cは明らかに最善のソリューションである。コミュニティの投票もこの選択肢を強く支持していることを示している。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "VPC",
      "Internet Gateway",
      "Elastic IP",
      "NAT Gateway"
    ]
  },
  {
    "No": "48",
    "question": "A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The\nconsumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an\nAmazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.\nDuring testing, the application does not meet performance requirements. Under high load, the application opens a large number of database\nconnections. The solutions architect must improve the application's performance.\nWhich actions should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "ソリューションアーキテクトは、Amazon API GatewayのリージョンエンドポイントとAWS Lambda関数を使用するWebアプリケーションを開発しました。Webアプリケーションの消費者はすべて、アプリケーションがデプロイされるAWSリージョンに近接しています。Lambda関数は、Amazon Aurora MySQLデータベースに対してのみクエリを実行します。ソリューションアーキテクトは、データベースに3つのリードレプリカを持たせるように設定しました。テスト中、アプリケーションは性能要件を満たしていません。高負荷時にアプリケーションは多くのデータベース接続を開きます。ソリューションアーキテクトはアプリケーションの性能を改善する必要があります。これらの要件を満たすために、ソリューションアーキテクトはどのアクションを取るべきですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Use the cluster endpoint of the Aurora database.",
        "text_jp": "Auroraデータベースのクラスターエンドポイントを使用する。"
      },
      {
        "key": "B",
        "text": "Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.",
        "text_jp": "RDS Proxyを使用して、Auroraデータベースのリーダーエンドポイントへの接続プールを設定する。"
      },
      {
        "key": "C",
        "text": "Use the Lambda Provisioned Concurrency feature.",
        "text_jp": "Lambdaのプロビジョンドコンカレンシー機能を使用する。"
      },
      {
        "key": "D",
        "text": "Move the code for opening the database connection in the Lambda function outside of the event handler.",
        "text_jp": "Lambda関数内でデータベース接続を開くコードをイベントハンドラーの外に移動する。"
      },
      {
        "key": "E",
        "text": "Change the API Gateway endpoint to an edge-optimized endpoint.",
        "text_jp": "API Gatewayのエンドポイントをエッジ最適化エンドポイントに変更する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BD (98%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are B and A. Using RDS Proxy allows the application to manage database connections efficiently by pooling them, thus reducing the number of simultaneous connections while using the reader endpoint efficiently. Additionally, using the cluster endpoint might help to balance the load across the read replicas.",
        "situation_analysis": "The application's performance is impacted due to the high number of database connections created under load, indicating a scalability issue with handling database interactions.",
        "option_analysis": "Option B is crucial as RDS Proxy efficiently manages connections. Option A is also relevant as it helps balance queries across read replicas. Options C, D, and E do not directly address the connection management problem.",
        "additional_knowledge": "Understanding how to leverage RDS Proxy can significantly enhance application performance.",
        "key_terminology": "AWS Lambda, Amazon Aurora, RDS Proxy, connection pooling, read replicas.",
        "overall_assessment": "The chosen answer aligns with AWS best practices related to managing database connections in highly concurrent environments."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBおよびAである。RDS Proxyを使用すると、アプリケーションはデータベース接続を効率的に管理し、それによって同時接続数を減らしながらリーダーエンドポイントを効率よく使用できる。また、クラスターエンドポイントを使用することで、リードレプリカ間で負荷をバランスさせることができる可能性がある。",
        "situation_analysis": "アプリケーションの性能は、負荷がかかった際に多くのデータベース接続が生成されるために影響を受けており、データベースとの相互作用を扱うスケーラビリティの問題が示されている。",
        "option_analysis": "選択肢Bは重要であり、RDS Proxyは接続を効率的に管理します。選択肢Aも関連しており、リードレプリカ間でクエリのバランスを取るのに役立つ。選択肢C、D、Eは、接続管理問題に直接対処していない。",
        "additional_knowledge": "RDS Proxyを活用する方法を理解することは、アプリケーション性能を大幅に向上させる可能性がある。",
        "key_terminology": "AWS Lambda、Amazon Aurora、RDS Proxy、接続プール、リードレプリカ。",
        "overall_assessment": "選択された答えは、高い同時実行環境におけるデータベース接続管理に関するAWSのベストプラクティスと一致している。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon Aurora",
      "RDS Proxy",
      "connection pooling",
      "read replicas"
    ]
  },
  {
    "No": "49",
    "question": "A company is planning to host a web application on AWS and wants to load balance the trafic across a group of Amazon EC2 instances. One of\nthe security requirements is to enable end-to-end encryption in transit between the client and the web server.\nWhich solution will meet this requirement?",
    "question_jp": "企業がAWS上でウェブアプリケーションをホストする計画を立てており、トラフィックを複数のAmazon EC2インスタンスに負荷分散したいと考えています。セキュリティ要件の一つは、クライアントとウェブサーバー間のトラフィックに対してエンドツーエンドの暗号化を有効にすることです。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Export the SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward trafic to port 443 on the instances.",
        "text_jp": "EC2インスタンスをアプリケーションロードバランサー（ALB）の背後に配置します。AWS Certificate Manager（ACM）を使用してSSL証明書を取得し、そのSSL証明書をALBに関連付けます。SSL証明書をエクスポートし、各EC2インスタンスにインストールします。ALBをポート443でリスンし、インスタンスのポート443にトラフィックを転送するように設定します。"
      },
      {
        "key": "B",
        "text": "Associate the EC2 instances with a target group. Provision an SSL certificate using AWS Certificate Manager (ACM). Create an Amazon CloudFront distribution and configure it to use the SSL certificate. Set CloudFront to use the target group as the origin server.",
        "text_jp": "EC2インスタンスをターゲットグループに関連付けます。AWS Certificate Manager（ACM）を使用してSSL証明書を取得します。Amazon CloudFrontのディストリビューションを作成し、それをSSL証明書を使用するように設定します。CloudFrontをターゲットグループをオリジンサーバーとして使用するように設定します。"
      },
      {
        "key": "C",
        "text": "Place the EC2 instances behind an Application Load Balancer (ALB) Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Provision a third-party SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward trafic to port 443 on the instances.",
        "text_jp": "EC2インスタンスをアプリケーションロードバランサー（ALB）の背後に配置します。AWS Certificate Manager（ACM）を使用してSSL証明書を取得し、そのSSL証明書をALBに関連付けます。サードパーティのSSL証明書を取得し、各EC2インスタンスにインストールします。ALBをポート443でリスンし、インスタンスのポート443にトラフィックを転送するように設定します。"
      },
      {
        "key": "D",
        "text": "Place the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each EC2 instance. Configure the NLB to listen on port 443 and to forward trafic to port 443 on the instances.",
        "text_jp": "EC2インスタンスをネットワークロードバランサー（NLB）の背後に配置します。サードパーティのSSL証明書を取得し、NLBと各EC2インスタンスにインストールします。NLBをポート443でリスンし、インスタンスのポート443にトラフィックを転送するように設定します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (56%) D (34%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It allows end-to-end encryption by using SSL certificates on both the ALB and the EC2 instances.",
        "situation_analysis": "The requirement for end-to-end encryption means that traffic between the client and the web server must be protected. This can be achieved by having SSL certificates installed on both the ALB and the EC2 instances.",
        "option_analysis": "Option A fails because it does not use a third-party SSL certificate on EC2 instances. Option B relies on CloudFront, which does not facilitate direct encryption from the client to EC2. Option D does not use ALB which is better suited for web apps because it can handle HTTP(S) traffic properly and offers more features than NLB.",
        "additional_knowledge": "Understanding different load balancer types and their appropriate use cases is vital for AWS architecture design.",
        "key_terminology": "AWS Certificate Manager (ACM), Application Load Balancer (ALB), SSL Certificate, end-to-end encryption.",
        "overall_assessment": "This question is relevant and tests knowledge about securing web applications on AWS by enforcing encryption."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。ALBとEC2インスタンスの両方にSSL証明書を使用することで、エンドツーエンドの暗号化を実現します。",
        "situation_analysis": "エンドツーエンドの暗号化の要件は、クライアントとウェブサーバー間のトラフィックを保護する必要があることを意味します。これは、ALBとEC2インスタンスの両方にSSL証明書をインストールすることで実現できます。",
        "option_analysis": "選択肢Aは、EC2インスタンスにサードパーティのSSL証明書を使わないため不適切です。選択肢BはCloudFrontに依存しており、クライアントからEC2への直接暗号化を提供しません。選択肢Dは、NLBを使用しており、ALBはウェブアプリケーションに非常に適しており、HTTP(S)トラフィックを正しく処理できる機能を持っています。",
        "additional_knowledge": "異なるロードバランサーの種類とその適切な使用ケースを理解することは、AWSアーキテクチャ設計にとって重要です。",
        "key_terminology": "AWS Certificate Manager (ACM)、アプリケーションロードバランサー (ALB)、SSL証明書、エンドツーエンドの暗号化。",
        "overall_assessment": "この質問は関連性があり、AWS上でのウェブアプリケーションのセキュリティを強化するための知識をテストしています。"
      }
    ],
    "keywords": [
      "AWS Certificate Manager",
      "Application Load Balancer",
      "SSL Certificate",
      "end-to-end encryption"
    ]
  },
  {
    "No": "50",
    "question": "A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js\napplications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into\nreports. When the aggregation jobs run, some of the load jobs fail to run correctly.\nThe company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the\ncompany's customers.\nWhat should a solutions architect do to meet these requirements?",
    "question_jp": "企業は、データ分析環境をオンプレミスからAWSに移行したいと考えています。この環境には、2つのシンプルなNode.jsアプリケーションがあります。1つのアプリケーションはセンサーデータを収集し、MySQLデータベースにロードします。もう1つのアプリケーションは、そのデータを集約してレポートを作成します。集約ジョブが実行されると、一部のロードジョブが正しく実行されません。この企業はデータ読み込みの問題を解決しなければなりません。また、企業の顧客に対する中断や変更なく移行を行う必要があります。ソリューションアーキテクトは、これらの要件を満たすために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind a Network Load Balancer (NLB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the NLB.",
        "text_jp": "Amazon Aurora MySQLデータベースをオンプレミスデータベースのレプリケーションターゲットとして設定します。Aurora MySQLデータベースのAuroraレプリカを作成し、集約ジョブをAuroraレプリカに対して実行するようにします。コレクションエンドポイントをネットワークロードバランサー（NLB）の背後にあるAWS Lambda関数として設定し、Amazon RDS Proxyを使用してAurora MySQLデータベースに書き込みます。データベースが同期したら、レプリケーションジョブを無効にし、Auroraレプリカをプライマリインスタンスとして再起動します。コレクターのDNSレコードをNLBにポイントします。"
      },
      {
        "key": "B",
        "text": "Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Move the aggregation jobs to run against the Aurora MySQL database. Set up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group. When the databases are synced, point the collector DNS record to the ALDisable the AWS DMS sync task after the cutover from on premises to AWS.",
        "text_jp": "Amazon Aurora MySQLデータベースを設定します。AWSデータベース移行サービス（AWS DMS）を使用して、オンプレミスデータベースからAuroraへの継続的なデータレプリケーションを行います。集約ジョブをAurora MySQLデータベースに対して実行するようにします。コレクションエンドポイントをアプリケーションロードバランサー（ALB）の背後にあるAuto ScalingグループのAmazon EC2インスタンスとして設定します。データベースが同期したら、コレクターのDNSレコードをALBにポイントします。オンプレミスからAWSへのカットオーバー後、AWS DMS同期タスクを無効にします。"
      },
      {
        "key": "C",
        "text": "Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS.",
        "text_jp": "Amazon Aurora MySQLデータベースを設定します。AWSデータベース移行サービス（AWS DMS）を使用して、オンプレミスデータベースからAuroraへの継続的なデータレプリケーションを行います。Aurora MySQLデータベースのAuroraレプリカを作成し、集約ジョブをAuroraレプリカに対して実行するようにします。コレクションエンドポイントをアプリケーションロードバランサー（ALB）の背後にあるAWS Lambda関数として設定し、Amazon RDS Proxyを使用してAurora MySQLデータベースに書き込みます。データベースが同期したら、コレクターのDNSレコードをALBにポイントします。オンプレミスからAWSへのカットオーバー後、AWS DMS同期タスクを無効にします。"
      },
      {
        "key": "D",
        "text": "Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as an Amazon Kinesis data stream. Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the Kinesis data stream.",
        "text_jp": "Amazon Aurora MySQLデータベースを設定します。Aurora MySQLデータベースのAuroraレプリカを作成し、集約ジョブをAuroraレプリカに対して実行するようにします。コレクションエンドポイントをAmazon Kinesisデータストリームとして設定します。Amazon Kinesisデータファイアホースを使用してデータをAurora MySQLデータベースにレプリケーションします。データベースが同期したら、レプリケーションジョブを無効にし、Auroraレプリカをプライマリインスタンスとして再起動します。コレクターのDNSレコードをKinesisデータストリームにポイントします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This option adequately addresses the requirements of continuous data replication and the need for a seamless migration without downtime.",
        "situation_analysis": "The company requires a solution to migrate its data analytics environment to AWS with no interruptions for customers. This involves addressing data loading issues while ensuring that aggregation jobs can run on the new database.",
        "option_analysis": "Option A introduces unnecessary complexity by using a Network Load Balancer and requires changes to architecture. Option B, while valid, does not utilize a Lambda function for endpoint management and may incur delays in the aggregation process. Option C correctly employs AWS Lambda for serverless architecture and maintains efficient data processing with RDS Proxy, fulfilling the requirements perfectly. Option D does not align with best practices for database migrations, as Kinesis is not ideally suited for direct database write operations.",
        "additional_knowledge": "Using AWS Lambda and Aurora reduces the chances of bottlenecks and enhances the scalability of the data analytics environment.",
        "key_terminology": "AWS Lambda, Amazon RDS Proxy, AWS Database Migration Service, Amazon Aurora, Continuous Data Replication",
        "overall_assessment": "The assessment aligns with best practices and the majority community vote. Option C stands out as the best solution for maintaining service availability during migration and addressing data loading issues."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。この選択肢は、継続的なデータレプリケーションと顧客に対するダウンタイムなしのシームレスな移行の必要性を適切に対処している。",
        "situation_analysis": "企業は、顧客に対する中断なしにデータ分析環境をAWSに移行するためのソリューションを必要としている。これは、集約ジョブが新しいデータベース上で実行できるようにするために、データ読み込みの問題に対処することを含む。",
        "option_analysis": "選択肢Aは、ネットワークロードバランサーを使用することで不要な複雑さをもたらし、アーキテクチャに変更を必要とする。選択肢Bは妥当であるが、エンドポイント管理にLambda関数を使用せず、集約プロセスに遅延が発生する可能性がある。選択肢Cは、サーバーレスアーキテクチャのためにAWS Lambdaを正しく利用し、RDS Proxyによる効率的なデータ処理を維持しており、要件を完璧に満たしている。選択肢Dは、Kinesisがデータベースへの直接書き込み操作に最適ではないため、データベース移行のベストプラクティスに一致しない。",
        "additional_knowledge": "AWS LambdaとAuroraを使用することで、ボトルネックの可能性が減少し、データ分析環境のスケーラビリティが向上する。",
        "key_terminology": "AWS Lambda, Amazon RDS Proxy, AWS Database Migration Service, Amazon Aurora, 継続的データレプリケーション",
        "overall_assessment": "この評価は、ベストプラクティスおよびコミュニティの大多数の投票に一致している。選択肢Cは、移行中のサービス可用性を維持し、データ読み込みの問題に対処するための最良の解決策として際立っている。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon RDS Proxy",
      "AWS Database Migration Service",
      "Amazon Aurora",
      "Continuous Data Replication"
    ]
  },
  {
    "No": "51",
    "question": "A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption\nwith S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket\nmust be encrypted by keys that the company's security team manages. The S3 bucket does not have versioning enabled.\nWhich solution will meet these requirements?",
    "question_jp": "ある健康保険会社は、Amazon S3バケットに個人を特定可能な情報（PII）を保存しています。会社は、オブジェクトを暗号化するために、S3管理の暗号化キー（SSE-S3）を使用したサーバー側暗号化を利用しています。新しい要件により、S3バケット内のすべての現在および将来のオブジェクトは、会社のセキュリティチームが管理するキーによって暗号化されなければなりません。S3バケットにはバージョニングが無効に設定されています。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests.",
        "text_jp": "S3バケットのプロパティで、デフォルトの暗号化を顧客管理キーによるSSE-S3に変更します。AWS CLIを使用して、S3バケット内のすべてのオブジェクトを再アップロードします。S3バケットポリシーを設定して、暗号化されていないPutObjectの要求を拒否します。"
      },
      {
        "key": "B",
        "text": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.",
        "text_jp": "S3バケットのプロパティで、デフォルトの暗号化をAWS KMS管理の暗号化キーによるサーバー側暗号化（SSE-KMS）に変更します。S3バケットポリシーを設定して、暗号化されていないPutObjectの要求を拒否します。AWS CLIを使用して、S3バケット内のすべてのオブジェクトを再アップロードします。"
      },
      {
        "key": "C",
        "text": "In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests.",
        "text_jp": "S3バケットのプロパティで、デフォルトの暗号化をAWS KMS管理の暗号化キーによるサーバー側暗号化（SSE-KMS）に変更します。S3バケットポリシーを設定して、GetObjectおよびPutObjectの要求時に自動的にオブジェクトを暗号化します。"
      },
      {
        "key": "D",
        "text": "In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket.",
        "text_jp": "S3バケットのプロパティで、デフォルトの暗号化を顧客管理キーによるAES-256に変更します。S3バケットにアクセスする任意のエンティティに対して、暗号化されていないPutObjectの要求を拒否するポリシーをアタッチします。AWS CLIを使用して、S3バケット内のすべてのオブジェクトを再アップロードします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (58%) D (41%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using customer-managed keys (CMK) through AES-256 provides the necessary control over the encryption keys and complies with the requirement for the company's security team to manage these keys.",
        "situation_analysis": "The health insurance company must ensure that all objects in the S3 bucket are encrypted with keys it controls. This includes all existing and future objects. They also do not have versioning enabled, eliminating complexity in re-uploading.",
        "option_analysis": "Option D is the only choice that allows the company to use customer-managed keys, aligning with their requirements. Option A incorrectly suggests SSE-S3 instead of using a KMS-managed key, and Options B and C do not utilize a customer-managed key as required by the new policy.",
        "additional_knowledge": "Understanding the implications of encryption key management and AWS compliance standards is vital for services dealing with sensitive information.",
        "key_terminology": "S3, SSE-S3, KMS, Customer Managed Key, Encryption",
        "overall_assessment": "While the community tends to favor answer B due to its focus on SSE-KMS, it does not meet the requirement for customer management of keys as explicitly required here. Therefore, although option D received less community support, it meets the security requirement outlined in the question."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。顧客管理キー（CMK）をAES-256で使用することにより、暗号化キーに対する必要な制御を提供し、会社のセキュリティチームがこれらのキーを管理するという要件を満たすことができる。",
        "situation_analysis": "健康保険会社は、S3バケット内のすべてのオブジェクトが同社が制御するキーで暗号化されることを確認しなければならない。これには、既存および将来のすべてのオブジェクトが含まれる。バージョニングは無効になっているため、再アップロードの複雑さが排除されている。",
        "option_analysis": "選択肢Dは、会社が顧客管理キーを使用できる唯一の選択肢であり、要件に沿っている。選択肢AはSSE-S3を提案しており、KMS管理キーを使用しないため間違っている。また、選択肢BとCは、新しいポリシーで要求されている顧客管理キーを利用していない。",
        "additional_knowledge": "暗号化キー管理とAWSの遵守基準の理解は、機密情報を扱うサービスにとって非常に重要である。",
        "key_terminology": "S3, SSE-S3, KMS, カスタマー管理キー, 暗号化",
        "overall_assessment": "コミュニティは選択肢Bを選好している傾向があるが、SSE-KMSに焦点を当てすぎるため、ここで明示的に求められている顧客管理キーの要件を満たしていない。したがって、選択肢Dはコミュニティの支持は少ないものの、問題に記されたセキュリティ要件を満たしている。"
      }
    ],
    "keywords": [
      "S3",
      "SSE-S3",
      "KMS",
      "Customer Managed Key",
      "Encryption"
    ]
  },
  {
    "No": "52",
    "question": "A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2\ninstances. The EC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).\nThe company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an\norigin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.\nA solutions architect must configure the application so that itis highly available and fault tolerant.\nWhich solution meets these requirements?",
    "question_jp": "ある企業がAWSクラウド上でウェブアプリケーションを運用しています。このアプリケーションは、Amazon EC2インスタンスのセットで生成される動的コンテンツで構成されています。EC2インスタンスは、アプリケーションロードバランサー（ALB）のターゲットグループとして構成されたオートスケーリンググループ内で実行されます。企業は、アプリケーションをグローバルに配信するためにAmazon CloudFrontディストリビューションを使用しています。CloudFrontディストリビューションはALBをオリジンとして使用しています。企業はAmazon Route 53をDNSとして使用しており、CloudFrontディストリビューション用にwww.example.comのAレコードを作成しています。ソリューションアーキテクトは、アプリケーションを高可用性で耐障害性を持たせるように構成しなければなりません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Provision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add both of the CloudFront distributions as values. Create Route 53 health checks.",
        "text_jp": "異なるAWSリージョンにフルな二次アプリケーション展開を提供する。Route 53のAレコードをフェイルオーバーレコードに更新する。両方のCloudFrontディストリビューションを値として追加する。Route 53のヘルスチェックを作成する。"
      },
      {
        "key": "B",
        "text": "Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALCreate an origin group for the two origins. Configure one origin as primary and one origin as secondary.",
        "text_jp": "異なるAWSリージョンにALB、オートスケーリンググループ、およびEC2インスタンスを提供する。CloudFrontディストリビューションを更新し、新しいALBのために二つ目のオリジンを作成する。二つのオリジンのためにオリジングループを作成する。一つのオリジンをプライマリとして、もう一つをセカンダリとして設定する。"
      },
      {
        "key": "C",
        "text": "Provision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the ALB. Set up the failover routing algorithm on the ALB.",
        "text_jp": "異なるAWSリージョンにオートスケーリンググループおよびEC2インスタンスを提供する。ALBに新しいオートスケーリンググループのための二つ目のターゲットを作成する。ALBでフェイルオーバールーティングアルゴリズムを設定する。"
      },
      {
        "key": "D",
        "text": "Provision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new application setup as an origin. Create an AWS Global Accelerator accelerator. Add both of the CloudFront distributions as endpoints.",
        "text_jp": "異なるAWSリージョンにフルな二次アプリケーション展開を提供する。二つ目のCloudFrontディストリビューションを作成し、新しいアプリケーションセットアップをオリジンとして追加する。AWS Global Acceleratorアクセラレーターを作成する。両方のCloudFrontディストリビューションをエンドポイントとして追加する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Provisioning an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region ensures that the application can be highly available and fault-tolerant. By updating the CloudFront distribution and creating a second origin for the new ALB, we can leverage CloudFront's ability to route to multiple origins.",
        "situation_analysis": "The company requires high availability and fault tolerance for its web application. This means that in the event of an outage in the primary region, the application should still be accessible from another region.",
        "option_analysis": "Option A involves creating a secondary application deployment, but it doesn't specify how the original application would be maintained. Option C suggests creating an Auto Scaling group in a different region, but it fails to account for the configuration of an ALB which is necessary for the traffic distribution. Option D sets up a new CloudFront distribution and an AWS Global Accelerator, which may add unnecessary complexity.",
        "additional_knowledge": "When designing for high availability, it is essential to consider disaster recovery and failover strategies.",
        "key_terminology": "Application Load Balancer, Auto Scaling, EC2 instances, Route 53, CloudFront, AWS Global Accelerator",
        "overall_assessment": "The question tests the understanding of high availability architecture in AWS. Answer B efficiently meets the company's requirements for fault tolerance and availability. Community vote strongly favors this option, indicating its validity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。異なるAWSリージョンにALB、オートスケーリンググループ、およびEC2インスタンスを用意することで、アプリケーションの高可用性と耐障害性が確保される。CloudFrontディストリビューションを更新し、新しいALBのために二つ目のオリジンを作成することで、CloudFrontの能力を活用して複数のオリジンにルーティングすることができる。",
        "situation_analysis": "この企業は、ウェブアプリケーションの高可用性と耐障害性を求めている。これは、主要なリージョンで障害が発生した場合でも、別のリージョンからアプリケーションへアクセスできることを意味する。",
        "option_analysis": "オプションAは二次アプリケーション展開を作成するが、元のアプリケーションがどのように維持されるかは示していない。オプションCは異なるリージョンにオートスケーリンググループを作成することを提案しているが、トラフィック配信に必要なALBの構成を考慮していない。オプションDは新しいCloudFrontディストリビューションとAWS Global Acceleratorを設定するが、それは不必要な複雑さを加える可能性がある。",
        "additional_knowledge": "高可用性を設計する際は、災害復旧やフェイルオーバー戦略を考慮することが重要である。",
        "key_terminology": "アプリケーションロードバランサー、オートスケーリング、EC2インスタンス、Route 53、CloudFront、AWS Global Accelerator",
        "overall_assessment": "この質問はAWSにおける高可用性アーキテクチャの理解を試すものである。回答Bは、企業の耐障害性と可用性に関する要件を効率的に満たしている。コミュニティの投票もこの選択肢を強く支持しており、その有効性を示している。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "Auto Scaling",
      "EC2 instances",
      "Route 53",
      "CloudFront"
    ]
  },
  {
    "No": "53",
    "question": "A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a\ntransit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured\nbetween all of the company's global ofices and the transit account. The company has AWS Config enabled on all of its accounts.\nThe company's networking team needs to centrally manage a list of internal IP address ranges that belong to the global ofices. Developers will\nreference this list to gain access to their applications securely.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "question_jp": "ある企業は、AWS Organizations内に多数のAWSアカウントを持つ組織を構成しています。その中の1つのAWSアカウントがトランジットアカウントに指定されており、トランジットゲートウェイが他のすべてのAWSアカウントと共有されています。企業のすべてのグローバルオフィスとトランジットアカウントとの間にAWS Site-to-Site VPN接続が設定されています。企業はすべてのアカウントでAWS Configを有効にしています。企業のネットワーキングチームは、グローバルオフィスに属する内部IPアドレス範囲のリストを集中管理する必要があります。開発者はこのリストを参照して、自分のアプリケーションに安全にアクセスします。どのソリューションが、最小限の運用負荷でこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can be invoked when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update all relevant security group rules with the updated IP address ranges.",
        "text_jp": "内部IPアドレス範囲をリストしたJSONファイルをAmazon S3にホストし、JSONファイルが更新された際に呼び出されることができるAmazon Simple Notification Service (Amazon SNS) トピックを各アカウントに設定します。SNSトピックにAWS Lambda関数をサブスクライブして、更新されたIPアドレス範囲で関連するすべてのセキュリティグループルールを更新します。"
      },
      {
        "key": "B",
        "text": "Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each of the accounts to ensure compliance with the list of IP address ranges. Configure the rule to automatically remediate any noncompliant security group that is detected.",
        "text_jp": "内部IPアドレス範囲を含む新しいAWS Config管理ルールを作成します。このルールを使用して、各アカウントのセキュリティグループを確認し、IPアドレス範囲のリストに従っているかどうかを確認します。非準拠のセキュリティグループを検出した場合、自動修復するようにルールを設定します。"
      },
      {
        "key": "C",
        "text": "In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts.",
        "text_jp": "トランジットアカウント内で、内部IPアドレス範囲を含むVPCプレフィックスリストを作成します。AWS Resource Access Managerを使用して、プレフィックスリストを他のすべてのアカウントと共有します。共有されたプレフィックスリストを使用して、他のアカウントのセキュリティグループルールを構成します。"
      },
      {
        "key": "D",
        "text": "In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts to reference the transit account's security group by using a nested security group reference of “/sg-1a2b3c4d”.",
        "text_jp": "トランジットアカウント内で、内部IPアドレス範囲を全て含むセキュリティグループを作成します。他のアカウントのセキュリティグループをトランジットアカウントのセキュリティグループを参照するように設定し、ネストされたセキュリティグループの参照「/sg-1a2b3c4d」を使用します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: In the transit account, create a VPC prefix list with all of the internal IP address ranges.",
        "situation_analysis": "The company has multiple AWS accounts with a need to manage a centralized list of internal IP address ranges that are accessed by the developers for their applications. This requires a scalable and easily maintainable solution.",
        "option_analysis": "Option A involves manual updates to a JSON file and additional AWS services, increasing operational overhead. Option B mandates compliance checks but can be complex to manage. Option D requires managing security group references, which can complicate network configurations.",
        "additional_knowledge": "Utilizing VPC prefix lists is particularly advantageous for organizations with dynamic IP addressing, enabling ease of updates without manual intervention.",
        "key_terminology": "VPC prefix list, AWS Resource Access Manager, security groups, AWS Config, operational overhead",
        "overall_assessment": "Answer C is the best option as it centralizes IP range management with the least operational burden. It is directly aligned with AWS best practices and effectively meets the requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はC：トランジットアカウント内で、内部IPアドレス範囲を含むVPCプレフィックスリストを作成することです。",
        "situation_analysis": "この企業は、開発者がアプリケーションにアクセスするために必要な内部IPアドレス範囲の集中管理されたリストを必要とする複数のAWSアカウントを持っています。これは、スケーラブルで維持が容易なソリューションを必要とします。",
        "option_analysis": "オプションAは、JSONファイルの手動更新と追加のAWSサービスを含むため、運用負荷が増加します。オプションBは、コンプライアンスチェックを義務付けますが、管理が複雑になる可能性があります。オプションDは、セキュリティグループ参照を管理する必要があり、ネットワーク設定を複雑にすることがあります。",
        "additional_knowledge": "VPCプレフィックスリストを活用することで、動的IP割り当てを行う組織にとって特に有利であり、手動介入なしでの更新の容易さを提供します。",
        "key_terminology": "VPCプレフィックスリスト、AWS Resource Access Manager、セキュリティグループ、AWS Config、運用負荷",
        "overall_assessment": "回答Cは、最小限の運用負荷でIP範囲の管理を中央集権化するため、最良の選択肢です。これはAWSのベストプラクティスに直接沿っており、要件を効果的に満たします。"
      }
    ],
    "keywords": [
      "VPC prefix list",
      "AWS Resource Access Manager",
      "security groups",
      "AWS Config",
      "operational overhead"
    ]
  },
  {
    "No": "54",
    "question": "A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and\nuses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API\nmethod.\nThe company wants to create a CSV report every 2 weeks to show each API Lambda function's recommended configured memory, recommended\ncost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.\nWhich solution will meet these requirements with the LEAST development time?",
    "question_jp": "ある企業は、Amazon S3で静的ウェブサイトとして新しいアプリケーションを運用しています。企業はアプリケーションを本番のAWSアカウントにデプロイし、ウェブサイトの配信にAmazon CloudFrontを利用しています。ウェブサイトはAmazon API GatewayのREST APIを呼び出し、各APIメソッドのバックエンドにはAWS Lambda関数があります。企業は、APIの各Lambda関数における推奨される設定メモリ、推奨コスト、現在の設定と推奨との価格差を示すCSVレポートを2週間ごとに作成したいと考えています。企業はレポートをS3バケットに保存します。この要件を最も少ない開発時間で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.",
        "text_jp": "Amazon CloudWatch Logsから各API Lambda関数のメトリクスデータを抽出するLambda関数を作成し、2週間の期間分のデータを表形式にまとめます。そのデータを.csvファイルとしてS3バケットに保存します。Lambda関数を2週間ごとに実行するようにAmazon EventBridgeルールを作成します。"
      },
      {
        "key": "B",
        "text": "Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.",
        "text_jp": "AWS Compute Optimizerにオプトインします。ExportLambdaFunctionRecommendationsオペレーションを呼び出すLambda関数を作成します。.csvファイルをS3バケットにエクスポートします。Lambda関数を2週間ごとに実行するようにAmazon EventBridgeルールを作成します。"
      },
      {
        "key": "C",
        "text": "Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks.",
        "text_jp": "AWS Compute Optimizerにオプトインします。強化されたインフラストラクチャメトリクスを設定します。Compute Optimizerコンソール内で、Lambdaの推奨を.csvファイルにエクスポートするジョブをスケジュールします。2週間ごとにファイルをS3バケットに保存します。"
      },
      {
        "key": "D",
        "text": "Purchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks.",
        "text_jp": "本番アカウント用にAWSビジネスサポートプランを購入します。AWS Trusted Advisorチェックに対してAWS Compute Optimizerにオプトインします。Trusted Advisorコンソール内で、コスト最適化チェックを.csvファイルにエクスポートするジョブをスケジュールします。2週間ごとにファイルをS3バケットに保存します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Opting into AWS Compute Optimizer and using the ExportLambdaFunctionRecommendations operation allows for automated retrieval of recommended configurations, minimizing development time.",
        "situation_analysis": "The requirement is to generate a CSV report every 2 weeks that includes specific metrics about API Lambda functions. Using AWS managed services would provide a simpler solution.",
        "option_analysis": "Option A involves manual data extraction from CloudWatch, which requires more development work. Option C, while using Compute Optimizer, needs additional setup for metrics. Option D requires a paid support plan and is focused on Trusted Advisor, making it less optimal.",
        "additional_knowledge": "The use of AWS services like Compute Optimizer reduces operational overhead and accelerates development cycles.",
        "key_terminology": "AWS Compute Optimizer, Lambda, EventBridge, ExportLambdaFunctionRecommendations, S3",
        "overall_assessment": "Option B aligns perfectly with the requirement and follows best practices for simplification and automation. The community supports this choice with a 97% vote."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。AWS Compute Optimizerにオプトインし、ExportLambdaFunctionRecommendationsオペレーションを使用することで、推奨構成の自動的な取得が可能となり、開発時間を最小限に抑えることができる。",
        "situation_analysis": "API Lambda関数に関する特定のメトリクスを含むCSVレポートを2週間ごとに生成する必要がある。AWSのマネージドサービスを利用することが、シンプルな解決策を提供する。",
        "option_analysis": "選択肢AはCloudWatchからの手動データ抽出を要しており、開発作業が増える。選択肢CはCompute Optimizerを使用しているが、メトリクスの追加設定が必要である。選択肢Dは有料サポートプランが必要で、Trusted Advisorに焦点を当てており、最適ではない。",
        "additional_knowledge": "AWSサービスであるCompute Optimizerの利用により、運用の負担が軽減され、開発サイクルが加速される。",
        "key_terminology": "AWS Compute Optimizer, Lambda, EventBridge, ExportLambdaFunctionRecommendations, S3",
        "overall_assessment": "選択肢Bは要件と完全に一致し、簡素化と自動化に関するベストプラクティスに従っている。コミュニティはこの選択を97%の票で支持している。"
      }
    ],
    "keywords": [
      "AWS Compute Optimizer",
      "Lambda",
      "EventBridge",
      "ExportLambdaFunctionRecommendations",
      "S3"
    ]
  },
  {
    "No": "55",
    "question": "A company's factory and automation applications are running in a single VPC. More than 20 applications run on a combination of Amazon EC2,\nAmazon Elastic Container Service (Amazon ECS), and Amazon RDS.\nThe company has software engineers spread across three teams. One of the three teams owns each application, and each time is responsible for\nthe cost and performance of all of its applications. Team resources have tags that represent their application and team. The teams use IAM\naccess for daily activities.\nThe company needs to determine which costs on the monthly AWS bill are attributable to each application or team. The company also must be\nable to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must\nrecommend an AWS Billing and Cost Management solution that provides these cost reports.\nWhich combination of actions will meet these requirements? (Choose three.)",
    "question_jp": "ある企業の工場と自動化アプリケーションは、単一のVPC内で稼働しています。20以上のアプリケーションが、Amazon EC2、Amazon Elastic Container Service (Amazon ECS)、およびAmazon RDSの組み合わせで実行されています。この企業には、3つのチームに分かれたソフトウェアエンジニアがいます。3つのチームのそれぞれが各アプリケーションを所有し、各チームはそのアプリケーションのコストとパフォーマンスに責任を持っています。チームリソースには、アプリケーションとチームを表すタグが付けられています。チームは日常業務のためにIAMアクセスを使用しています。企業は、月次のAWS請求書のどのコストが各アプリケーションまたはチームに帰属するかを確認する必要があります。また、過去12か月のコストを比較し、次の12か月のコストを予測するのに役立つレポートを作成する必要があります。ソリューションアーキテクトは、これらのコストレポートを提供するAWS Billing and Cost Managementソリューションを推奨する必要があります。どのアクションの組み合わせがこれらの要件を満たすことができますか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Activate the user-define cost allocation tags that represent the application and the team.",
        "text_jp": "アプリケーションとチームを表すユーザー定義のコスト配分タグを有効にします。"
      },
      {
        "key": "B",
        "text": "Activate the AWS generated cost allocation tags that represent the application and the team.",
        "text_jp": "アプリケーションとチームを表すAWS生成のコスト配分タグを有効にします。"
      },
      {
        "key": "C",
        "text": "Create a cost category for each application in Billing and Cost Management.",
        "text_jp": "Billing and Cost Managementで各アプリケーションのコストカテゴリを作成します。"
      },
      {
        "key": "D",
        "text": "Activate IAM access to Billing and Cost Management.",
        "text_jp": "Billing and Cost ManagementへのIAMアクセスを有効にします。"
      },
      {
        "key": "E",
        "text": "Create a cost budget.",
        "text_jp": "コスト予算を作成します。"
      },
      {
        "key": "F",
        "text": "Enable Cost Explorer.",
        "text_jp": "Cost Explorerを有効にします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACF (58%) ADF (42%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer includes activating user-defined cost allocation tags and possibly creating cost categories and enabling cost explorer to analyze and manage costs effectively.",
        "situation_analysis": "The company needs to accurately attribute costs to individual applications and teams, requiring a systematic tagging and categorization of costs.",
        "option_analysis": "Option A, activating user-defined cost allocation tags, is crucial as it directly relates costs to applications and teams. Option B is less relevant as AWS-generated tags may not provide the same granularity needed. Creating cost categories (C) allows for organized reporting but requires that tagging is in place first. Options D, E, and F offer additional management capabilities, but do not address the core tagging and categorization problem.",
        "additional_knowledge": "Understanding the importance of effective cost management practices in AWS is vital for optimizing cloud expenditure.",
        "key_terminology": "Cost allocation, IAM access, Cost Explorer, cost categories, tagging",
        "overall_assessment": "The selected actions provide a comprehensive strategy for enabling detailed cost attribution and reporting, critical for financial transparency in cloud spending."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はユーザー定義のコスト配分タグの有効化やコストカテゴリの作成、コストエクスプローラーの有効化を含み、コストを効果的に分析・管理するためのものです。",
        "situation_analysis": "企業は、各アプリケーションおよびチームにコストを正確に割り当てる必要があり、コストの体系的なタグ付けおよび分類が求められます。",
        "option_analysis": "選択肢Aのユーザー定義のコスト配分タグの有効化は、アプリケーションとチームにコストを直接関連付けるため非常に重要です。選択肢BのAWS生成タグは、同じ粒度を提供できない可能性が高いため、あまり重要ではありません。コストカテゴリの作成(C)は組織的な報告を行うために役立ちますが、まずタグ付けが必要です。選択肢D、E、およびFは追加的な管理機能を提供しますが、基本的なタグ付けおよび分類の問題には対処していません。",
        "additional_knowledge": "AWSにおける効果的なコスト管理実践の重要性を理解することは、クラウド支出を最適化するために欠かせません。",
        "key_terminology": "コスト配分、IAMアクセス、コストエクスプローラー、コストカテゴリ、タグ付け",
        "overall_assessment": "選択されたアクションは、詳細なコスト割り当ておよび報告を可能にする包括的な戦略を提供し、クラウド支出における財務の透明性にとって重要です。"
      }
    ],
    "keywords": [
      "Cost allocation",
      "IAM access",
      "Cost Explorer",
      "cost categories",
      "tagging"
    ]
  },
  {
    "No": "56",
    "question": "An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall.\nThe third party accepts only one public CIDR block in each client's allow list.\nThe customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind\nan Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT\ngateways provide internet access to the private subnets.\nHow should a solutions architect ensure that the web application can continue to call the third-party API after the migration?",
    "question_jp": "AWSの顧客には、オンプレミスで稼働しているWebアプリケーションがあります。このWebアプリケーションは、ファイアウォールの後ろにあるサードパーティのAPIからデータを取得します。サードパーティは、クライアントの許可リストに対して1つの公的CIDRブロックのみを受け入れます。この顧客は、WebアプリケーションをAWSクラウドに移行したいと考えています。アプリケーションは、VPC内のアプリケーションロードバランサー（ALB）の背後にある一連のAmazon EC2インスタンスでホストされます。ALBは公開サブネットに配置され、EC2インスタンスはプライベートサブネットに配置されます。NATゲートウェイがプライベートサブネットにインターネットアクセスを提供します。ソリューションアーキテクトは、移行後にWebアプリケーションがサードパーティのAPIに引き続きアクセスできるようにするために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.",
        "text_jp": "顧客所有の公的IPアドレスのブロックをVPCに関連付ける。VPCの公開サブネットで公的IPアドレスを有効にする。"
      },
      {
        "key": "B",
        "text": "Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.",
        "text_jp": "顧客所有の公的IPアドレスのブロックをAWSアカウントに登録する。そのアドレスブロックからElastic IPアドレスを作成し、VPC内のNATゲートウェイに割り当てる。"
      },
      {
        "key": "C",
        "text": "Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB.",
        "text_jp": "顧客所有のIPアドレスのブロックからElastic IPアドレスを作成する。静的なElastic IPアドレスをALBに割り当てる。"
      },
      {
        "key": "D",
        "text": "Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint.",
        "text_jp": "顧客所有の公的IPアドレスのブロックをAWSアカウントに登録する。AWS Global Acceleratorをセットアップし、そのアドレスブロックからElastic IPアドレスを使用する。ALBをアクセラレータエンドポイントとして設定する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Registering a block of customer-owned public IP addresses and assigning Elastic IPs to NAT gateways allows the web application to use the same IP addresses when making API calls, which is crucial since the third-party API only accepts calls from specified CIDR blocks.",
        "situation_analysis": "The customer needs to maintain the ability to interact with a third-party API while migrating their web application to AWS. The solution must ensure that API calls originate from allowed IP addresses because of the CIDR restriction.",
        "option_analysis": "Option A does not address the requirement for consistent IP usage from the same set of addresses for NAT gateways. Option C would assign the IPs to the ALB, which does not help the instances in the private subnet that need to make outbound requests. Option D introduces unnecessary complexity with Global Accelerator when Elastic IPs will suffice.",
        "additional_knowledge": "Understanding the characteristics and management of Elastic IPs is essential for effective AWS architecture.",
        "key_terminology": "Elastic IP, NAT Gateway, VPC, Public IP Addressing, CIDR Block",
        "overall_assessment": "The community overwhelmingly supports option B, reflecting that it is the most straightforward and effective solution in Scenario. Utilizing Elastic IPs ensures compliance with the API call requirements posed by the third party."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。顧客所有の公的IPアドレスのブロックを登録し、NATゲートウェイにElastic IPを割り当てることで、WebアプリケーションはAPI呼び出しを行う際に同じIPアドレスを使用することができる。これは、サードパーティAPIが指定されたCIDRブロックからの呼び出しのみを受け入れるために重要である。",
        "situation_analysis": "顧客は、WebアプリケーションをAWSに移行する際にサードパーティAPIとの相互作用を維持する必要がある。ソリューションは、CIDR制限に基づいてAPI呼び出しが許可されたIPアドレスから発信されることを確保しなければならない。",
        "option_analysis": "選択肢AはNATゲートウェイのための一貫したIP使用の要件を考慮していない。選択肢Cは、インスタンスがプライベートサブネットから外部リクエストを行う必要があるため、ALBにIPを割り当てるだけでは不十分である。選択肢Dは、Elastic IPで十分な状況においてGlobal Acceleratorを使用する複雑さを追加している。",
        "additional_knowledge": "Elastic IPの特性と管理を理解することは、効果的なAWSアーキテクチャに不可欠である。",
        "key_terminology": "Elastic IP、NATゲートウェイ、VPC、公的IPアドレス指定、CIDRブロック",
        "overall_assessment": "コミュニティは選択肢Bを圧倒的に支持しており、この選択肢がシナリオにおいて最も簡潔で効果的な解決策であることを反映している。Elastic IPを使用することで、サードパーティが提示するAPI呼び出し要件を満たすことができる。"
      }
    ],
    "keywords": [
      "Elastic IP",
      "NAT Gateway",
      "VPC",
      "Public IP Addressing",
      "CIDR Block"
    ]
  },
  {
    "No": "57",
    "question": "A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An administrator created the following\nSCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:\nDevelopers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the administrator address this\nproblem?",
    "question_jp": "複数のAWSアカウントを持つ企業がAWS Organizationsとサービスコントロールポリシー(SCP)を使用しています。管理者は次のSCPを作成し、AWSアカウント1111-1111-1111を含む組織単位(OU)にアタッチしました：開発者はアカウント1111-1111-1111でAmazon S3バケットを作成できないと不満を述べています。管理者はこの問題にどのように対処すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Add s3:CreateBucket with “Allow” effect to the SCP. [image_34_0]",
        "text_jp": "s3:CreateBucketに対して「許可」効果をSCPに追加する。 [image_34_0]"
      },
      {
        "key": "B",
        "text": "Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.",
        "text_jp": "アカウントをOUから削除し、SCPをアカウント1111-1111-1111に直接アタッチする。"
      },
      {
        "key": "C",
        "text": "Instruct the developers to add Amazon S3 permissions to their IAM entities.",
        "text_jp": "開発者にIAMエンティティにAmazon S3の権限を追加するよう指示する。"
      },
      {
        "key": "D",
        "text": "Remove the SCP from account 1111-1111-1111.",
        "text_jp": "アカウント1111-1111-1111からSCPを削除する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (84%) A (16%)",
    "page_images": [
      "image_34_0.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Instruct the developers to add Amazon S3 permissions to their IAM entities. This approach enables the developers to manage their own permissions effectively in compliance with the existing SCP.",
        "situation_analysis": "The developers are restricted from creating Amazon S3 buckets due to the policies attached to their OU. Therefore, they need the proper IAM permissions to perform the action.",
        "option_analysis": "Option A would add permissions at the SCP level but does not address the IAM permissions that govern access within the account. Option B unnecessarily complicates the account structure and does not solve the underlying permissions issue. Option D would remove the restrictions entirely, which is not an optimal solution.",
        "additional_knowledge": "Understanding the relationship between SCPs and IAM policies is crucial for effective cloud governance.",
        "key_terminology": "IAM, SCP, S3, permissions, policies",
        "overall_assessment": "The community largely supports option C, indicated by the vote distribution, which aligns with best practices in AWS when managing permissions."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです：開発者にIAMエンティティにAmazon S3の権限を追加するよう指示する。このアプローチにより、開発者は既存のSCPに準拠しながら、自身の権限を効果的に管理できます。",
        "situation_analysis": "開発者はOUにアタッチされているポリシーにより、Amazon S3バケットを作成することが制限されています。そのため、必要なIAM権限が必要となります。",
        "option_analysis": "選択肢AはSCPで権限を追加することになりますが、アカウント内のアクセスを管理するIAM権限には対処していません。選択肢Bはアカウント構造を不必要に複雑にし、根本的な権限の問題を解決しません。選択肢Dは制限を完全に削除し、最適な解決策とは言えません。",
        "additional_knowledge": "SCPとIAMポリシーの関係を理解することが、効果的なクラウドガバナンスには重要です。",
        "key_terminology": "IAM、SCP、S3、権限、ポリシー",
        "overall_assessment": "コミュニティは主に選択肢Cを支持しており、その投票分布がそれを示しています。これはAWSの権限管理におけるベストプラクティスに沿ったものです。"
      }
    ],
    "keywords": [
      "IAM",
      "SCP",
      "S3",
      "permissions",
      "policies"
    ]
  },
  {
    "No": "58",
    "question": "A company has a monolithic application that is critical to the company's business. The company hosts the application on an Amazon EC2\ninstance that runs Amazon Linux 2. The company's application team receives a directive from the legal department to back up the data from the\ninstance's encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the\nadministrative SSH key pair for the instance. The application must continue to serve the users.\nWhich solution will meet these requirements?",
    "question_jp": "ある会社には、ビジネスにとって重要なモノリシックアプリケーションがあります。この会社は、Amazon Linux 2 を実行している Amazon EC2 インスタンスでアプリケーションをホストしています。アプリケーションチームは、法務部門からの指示に従い、インスタンスの暗号化された Amazon Elastic Block Store (Amazon EBS) ボリュームのデータを Amazon S3 バケットにバックアップする必要があります。しかし、アプリケーションチームはインスタンスへの管理者 SSH キーペアを持っていません。アプリケーションは引き続きユーザーにサービスを提供し続ける必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3. [image_34_0]",
        "text_jp": "インスタンスに Amazon S3 への書き込み権限を持つロールをアタッチします。AWS Systems Manager の Session Manager オプションを使用してインスタンスにアクセスし、データを Amazon S3 にコピーするコマンドを実行します。"
      },
      {
        "key": "B",
        "text": "Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.",
        "text_jp": "インスタンスのイメージを作成し、再起動オプションを有効にします。そのイメージから新しい EC2 インスタンスを起動し、新しいインスタンスに Amazon S3 への書き込み権限を持つロールをアタッチします。コマンドを実行してデータを Amazon S3 にコピーします。"
      },
      {
        "key": "C",
        "text": "Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.",
        "text_jp": "Amazon Data Lifecycle Manager (Amazon DLM) を使用して EBS ボリュームのスナップショットを取得します。データを Amazon S3 にコピーします。"
      },
      {
        "key": "D",
        "text": "Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.",
        "text_jp": "インスタンスのイメージを作成します。イメージから新しい EC2 インスタンスを起動し、新しいインスタンスに Amazon S3 への書き込み権限を持つロールをアタッチします。コマンドを実行してデータを Amazon S3 にコピーします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (63%) C (36%)",
    "page_images": [
      "image_34_0.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. It allows access to the instance via Systems Manager and meets the backup requirements.",
        "situation_analysis": "The application is critical for business operations, and the team cannot access the instance directly due to the missing SSH key pair.",
        "option_analysis": "Option A enables access without SSH, while options B and D require rebooting the instance, which is not possible. Option C does not utilize S3 permissions directly.",
        "additional_knowledge": "Using AWS Systems Manager can also help in managing instances and automating tasks remotely.",
        "key_terminology": "Amazon EC2, Amazon EBS, Amazon S3, AWS Systems Manager, Session Manager",
        "overall_assessment": "Overall, option A leverages AWS services effectively and aligns with best practices for secure access."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは A です。これにより、Systems Manager を通じてインスタンスにアクセスでき、バックアップの要件を満たします。",
        "situation_analysis": "アプリケーションはビジネスオペレーションにとって重要であり、チームは SSH キーペアがないため、インスタンスに直接アクセスできません。",
        "option_analysis": "オプション A は SSH なしでアクセスを可能にしますが、オプション B と D はインスタンスを再起動する必要があり、不可能です。オプション C は直接 S3 権限を利用していません。",
        "additional_knowledge": "AWS Systems Manager を使用すると、インスタンスを管理し、リモートでタスクを自動化することもできます。",
        "key_terminology": "Amazon EC2, Amazon EBS, Amazon S3, AWS Systems Manager, Session Manager",
        "overall_assessment": "全体として、オプション A は AWS サービスを効果的に利用し、安全なアクセスのためのベストプラクティスに沿っています。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Amazon EBS",
      "Amazon S3",
      "AWS Systems Manager",
      "Session Manager"
    ]
  },
  {
    "No": "59",
    "question": "A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucket in a new AWS account. The solutions\narchitect must implement a solution that uses the AWS CLI.\nWhich combination of steps will successfully copy the data? (Choose three.)",
    "question_jp": "ソリューションアーキテクトは、AWSアカウント内のAmazon S3バケットから新しいAWSアカウント内の新しいS3バケットにデータをコピーする必要があります。ソリューションアーキテクトは、AWS CLIを使用したソリューションを実装しなければなりません。データを正常にコピーするためには、どの組み合わせの手順を選択しますか？（三つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket. Attach the bucket policy to the destination bucket.",
        "text_jp": "ソースバケットがその内容をリストし、オブジェクトを置き、オブジェクトACLを宛先バケットに設定することを許可するバケットポリシーを作成します。バケットポリシーを宛先バケットにアタッチします。"
      },
      {
        "key": "B",
        "text": "Create a bucket policy to allow a user in the destination account to list the source bucket's contents and read the source bucket's objects. Attach the bucket policy to the source bucket.",
        "text_jp": "宛先アカウントのユーザーがソースバケットの内容をリストし、ソースバケットのオブジェクトを読み取ることを許可するバケットポリシーを作成します。バケットポリシーをソースバケットにアタッチします。"
      },
      {
        "key": "C",
        "text": "Create an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user.",
        "text_jp": "ソースアカウントでIAMポリシーを作成します。ポリシーを構成して、ソースアカウントのユーザーがソースバケット内の内容をリストし、オブジェクトを取得できるようにし、宛先バケットでコンテンツをリスト、オブジェクトを置き、オブジェクトACLを設定できるようにします。ポリシーをユーザーにアタッチします。"
      },
      {
        "key": "D",
        "text": "Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set objectACLs in the destination bucket. Attach the policy to the user.",
        "text_jp": "宛先アカウントでIAMポリシーを作成します。ポリシーを構成して、宛先アカウントのユーザーがソースバケット内のコンテンツをリストし、オブジェクトを取得できるようにし、宛先バケットでコンテンツをリスト、オブジェクトを置き、オブジェクトACLを設定できるようにします。ポリシーをユーザーにアタッチします。"
      },
      {
        "key": "E",
        "text": "Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data.",
        "text_jp": "ソースアカウントのユーザーとしてaws s3 syncコマンドを実行します。データをコピーするために、ソースおよび宛先バケットを指定します。"
      },
      {
        "key": "F",
        "text": "Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data.",
        "text_jp": "宛先アカウントのユーザーとしてaws s3 syncコマンドを実行します。データをコピーするために、ソースおよび宛先バケットを指定します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDF (92%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "正解はAです。ソースバケットがその内容をリストし、オブジェクトを置き、オブジェクトACLを宛先バケットに設定できるようにするバケットポリシーが必要です。",
        "situation_analysis": "AWSアカウント間でデータをコピーする必要があり、AWS CLIを使用することが条件です。適切な権限を設定する必要があります。",
        "option_analysis": "Bはソースバケットのアクセスを許可するが、データの書き込みに必要な権限が不足しているため不正解です。CとDはそれぞれソースおよび宛先アカウントのIAMポリシーに関するもので、適切な権限があれば必要ですが、直接的な答えにはならないため、Aが主要な手順となります。Eは正しい手順ですが、権限が整っていなければ実行できません。",
        "additional_knowledge": "権限管理はAWSにおける重要なセキュリティ戦略の一部であり、適切なポリシー設定が必要です。",
        "key_terminology": "Amazon S3, AWS CLI, Bucket Policy, IAM Policy, s3 sync",
        "overall_assessment": "この問題は、AWSのデータ移行戦略、特に異なるアカウント間のデータプロセスに焦点を当てており、知識の評価に役立ちます。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。ソースバケットがその内容をリストし、オブジェクトを置き、オブジェクトACLを宛先バケットに設定できるようにするバケットポリシーが必要である。",
        "situation_analysis": "AWSアカウント間でデータをコピーする必要があり、AWS CLIを使用することが条件である。適切な権限を設定する必要がある。",
        "option_analysis": "Bはソースバケットのアクセスを許可するが、データの書き込みに必要な権限が不足しているため不正解である。CとDはそれぞれソースおよび宛先アカウントのIAMポリシーに関するもので、適切な権限があれば必要だが、直接的な答えにはならないため、Aが主要な手順となる。Eは正しい手順だが、権限が整っていなければ実行できない。",
        "additional_knowledge": "権限管理はAWSにおける重要なセキュリティ戦略の一部であり、適切なポリシー設定が必要である。",
        "key_terminology": "Amazon S3, AWS CLI, バケットポリシー, IAMポリシー, s3 sync",
        "overall_assessment": "この問題は、AWSのデータ移行戦略、特に異なるアカウント間のデータプロセスに焦点を当てており、知識の評価に役立つ。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "AWS CLI",
      "Bucket Policy",
      "IAM Policy",
      "s3 sync"
    ]
  },
  {
    "No": "60",
    "question": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web\napplication introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to\nsupport a canary release.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWS Lambdaに基づいてアプリケーションを構築し、それをAWS CloudFormationスタックにデプロイしました。ウェブアプリケーションの最終的な本番リリースで問題が発生し、数分間の運用停止を招きました。ソリューションアーキテクトは、カナリアリリースをサポートするためにデプロイメントプロセスを調整する必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.",
        "text_jp": "新しくデプロイされたLambda関数のバージョンごとにエイリアスを作成します。AWS CLIのupdate-aliasコマンドとrouting-configパラメーターを使用して負荷を分散します。"
      },
      {
        "key": "B",
        "text": "Deploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load.",
        "text_jp": "アプリケーションを新しいCloudFormationスタックにデプロイします。Amazon Route 53の加重ルーティングポリシーを使用して負荷を分散します。"
      },
      {
        "key": "C",
        "text": "Create a version for every new deployed Lambda function. Use the AWS CLI update-function-configuration command with the routing-config parameter to distribute the load.",
        "text_jp": "新しくデプロイされたLambda関数ごとにバージョンを作成します。AWS CLIのupdate-function-configurationコマンドとrouting-configパラメーターを使用して負荷を分散します。"
      },
      {
        "key": "D",
        "text": "Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load.",
        "text_jp": "AWS CodeDeployを設定し、Deployment configurationでCodeDeployDefault.OneAtATimeを使用して負荷を分散します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating an alias for each new version of the Lambda function allows for controlled traffic distribution during a canary release.",
        "situation_analysis": "The situation requires implementing a canary deployment strategy, where a new version is tested with a small percentage of users before full deployment.",
        "option_analysis": "Option A is correct because it allows versioning of the Lambda function and controlled traffic distribution. Option B suggests using a new CloudFormation stack which complicates management. Option C incorrectly refers to updating function configuration instead of using aliases. Option D does not specifically cater to Lambda function releases.",
        "additional_knowledge": "AWS CodeDeploy was considered, but it's better suited for EC2 and EKS deployments.",
        "key_terminology": "AWS Lambda, Canary Release, Aliases, CloudFormation, Deployment Strategy",
        "overall_assessment": "Answer A aligns with AWS best practices for releasing new code and minimizes risks during deployments. The majority community vote supports this as the correct method, affirming its effectiveness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。新しくデプロイされたLambda関数の各バージョンにエイリアスを作成することで、カナリアリリース中のトラフィック分配を制御できる。",
        "situation_analysis": "この状況では、新しいバージョンを少数のユーザーでテストしてから全体にデプロイするカナリアデプロイメント戦略を実装する必要がある。",
        "option_analysis": "選択肢Aは、Lambda関数のバージョン管理とトラフィックの制御を可能にするため、正しい。選択肢Bは、新しいCloudFormationスタックを使用すると管理が複雑になる。選択肢Cはエイリアスの代わりに関数設定の更新を誤って参照している。選択肢Dは、Lambda関数のリリースに特化していない。",
        "additional_knowledge": "AWS CodeDeployも考慮されたが、EC2およびEKSのデプロイメントに適している。",
        "key_terminology": "AWS Lambda, カナリアリリース, エイリアス, CloudFormation, デプロイメント戦略",
        "overall_assessment": "答えAはAWSのベストプラクティスに沿った新しいコードのリリースを行い、デプロイメント中のリスクを最小限に抑える。コミュニティの過半数の投票がこれを正しい方法として支持しており、その有効性を裏付けている。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Canary Release",
      "Aliases",
      "CloudFormation",
      "Deployment Strategy"
    ]
  },
  {
    "No": "61",
    "question": "A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties.\nThe company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the files are uploaded, they are moved to the\ndata lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route\n53.\nWhat should a solutions architect do to improve the reliability and scalability of the SFTP solution?",
    "question_jp": "金融会社がAmazon S3にデータレイクをホストしています。会社は毎晩、複数の第三者からSFTP経由で金融データレコードを受け取ります。会社は、自身のSFTPサーバーをVPCのパブリックサブネットにあるAmazon EC2インスタンス上で運用しています。ファイルがアップロードされた後、同じインスタンス上で実行されるcronジョブによってデータレイクに移動されます。SFTPサーバーはAmazon Route 53を使用してDNS sftp.example.comでアクセス可能です。ソリューションアーキテクトは、SFTPソリューションの信頼性とスケーラビリティを向上させるために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB.",
        "text_jp": "EC2インスタンスをオートスケーリンググループに移動します。EC2インスタンスをアプリケーションロードバランサー（ALB）の背後に配置します。Route 53でDNSレコードsftp.example.comをALBを指すように更新します。"
      },
      {
        "key": "B",
        "text": "Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.",
        "text_jp": "SFTPサーバーをAWS Transfer for SFTPに移行します。Route 53でDNSレコードsftp.example.comをサーバーエンドポイントホスト名を指すように更新します。"
      },
      {
        "key": "C",
        "text": "Migrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the file gateway endpoint.",
        "text_jp": "SFTPサーバーをAWSストレージゲートウェイのファイルゲートウェイに移行します。Route 53でDNSレコードsftp.example.comをファイルゲートウェイエンドポイントを指すように更新します。"
      },
      {
        "key": "D",
        "text": "Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB.",
        "text_jp": "EC2インスタンスをネットワークロードバランサー（NLB）の背後に配置します。Route 53でDNSレコードsftp.example.comをNLBを指すように更新します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Migrate the SFTP server to AWS Transfer for SFTP, which increases reliability and scalability.",
        "situation_analysis": "The requirement is to ensure that the SFTP service can handle varying loads without downtime while maintaining easy access to uploaded data.",
        "option_analysis": "Option B provides a managed service that automatically scales according to demand, relieving the company of server management tasks. Option A and D introduce the complexity of managing EC2 instances and load balancers, while C changes the fundamental access method, potentially complicating SFTP access.",
        "additional_knowledge": "AWS Transfer for SFTP integrates easily with other AWS services, further enhancing the data pipeline for the finance company.",
        "key_terminology": "AWS Transfer for SFTP, Amazon Route 53, EC2, scalability, reliability, file transfer.",
        "overall_assessment": "Given the community's unanimous vote for B, it’s clear that the community recognizes the advantages of using a managed service for SFTP, making it the best option."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはB: SFTPサーバーをAWS Transfer for SFTPに移行することです。この選択は、信頼性とスケーラビリティを向上させます。",
        "situation_analysis": "SFTPサービスがダウンタイムなしでさまざまな負荷に対応できることを確保し、アップロードされたデータへの容易なアクセスを維持することが要求されています。",
        "option_analysis": "オプションBは、需要に応じて自動的にスケールするマネージドサービスを提供し、サーバー管理タスクから会社を解放します。オプションAとDは、EC2インスタンスとロードバランサーの管理の複雑さを導入し、オプションCは根本的なアクセス方法を変更するため、SFTPアクセスが複雑化する可能性があります。",
        "additional_knowledge": "AWS Transfer for SFTPは、他のAWSサービスとの統合が容易であり、金融会社のデータパイプラインをさらに強化します。",
        "key_terminology": "AWS Transfer for SFTP、Amazon Route 53、EC2、スケーラビリティ、信頼性、ファイル転送。",
        "overall_assessment": "コミュニティの全員がBを支持していることから、コミュニティはマネージドサービスのメリットを認識しており、これが最良の選択肢であることが明らかです。"
      }
    ],
    "keywords": [
      "AWS Transfer for SFTP",
      "Amazon Route 53",
      "EC2",
      "scalability",
      "reliability",
      "file transfer"
    ]
  },
  {
    "No": "62",
    "question": "A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions\narchitect must preserve the software and configuration settings during the migration.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "ある企業が、オンプレミスのデータセンターで実行されているVMwareインフラストラクチャからAmazon EC2にアプリケーションを移行したいと考えています。ソリューションアーキテクトは、移行中にソフトウェアと設定を保持する必要があります。\nソリューションアーキテクトは、これらの要件を満たすために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the VMware data store. Use VM Import/Export to move the VMs to Amazon EC2.",
        "text_jp": "AWS DataSyncエージェントを構成し、データストアをAmazon FSx for Windows File Serverにレプリケートし始めます。SMB共有を使用してVMwareデータストアをホストします。VM Import/Exportを使用してVMをAmazon EC2に移動します。"
      },
      {
        "key": "B",
        "text": "Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command.",
        "text_jp": "VMware vSphereクライアントを使用して、アプリケーションをOpen Virtualization Format (OVF)形式のイメージとしてエクスポートします。イメージを保存するために、移転先のAWSリージョンにAmazon S3バケットを作成します。VM Import用のIAMロールを作成して適用します。AWS CLIを使用してEC2インポートコマンドを実行します。"
      },
      {
        "key": "C",
        "text": "Configure AWS Storage Gateway for files service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared folder. Sign in to the AWS Management Console and create an AMI from the backup copy. Launch an EC2 instance that is based on the AMI.",
        "text_jp": "ファイルサービス用にAWS Storage Gatewayを構成し、Common Internet File System (CIFS)共有をエクスポートします。共有フォルダにバックアップコピーを作成します。AWS Management Consoleにサインインし、バックアップコピーからAMIを作成します。AMIに基づいてEC2インスタンスを起動します。"
      },
      {
        "key": "D",
        "text": "Create a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on the on-premises VM. Register the VM with Systems Manager to be a managed instance. Use AWS Backup to create a snapshot of the VM and create an AMI. Launch an EC2 instance that is based on the AMI.",
        "text_jp": "AWS Systems Managerにおけるハイブリッド環境のための管理インスタンスアクティベーションを作成します。オンプレミスVMにSystems Manager Agentをダウンロードしてインストールします。VMを管理インスタンスとしてSystems Managerに登録します。AWS Backupを使用してVMのスナップショットを作成し、AMIを作成します。AMIに基づいてEC2インスタンスを起動します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Use AWS Systems Manager for managed-instance activation to migrate the application while preserving software and configurations.",
        "situation_analysis": "The company needs to migrate an application while ensuring that the software and its configuration remain intact. The solution must cover both VM management and backup capabilities.",
        "option_analysis": "Option A does not guarantee software preservation through the migration process, while option B requires manual image creation that may not account for configuration settings. Option C also lacks a direct migration process, focusing more on backup. Option D directly addresses both requirements.",
        "additional_knowledge": "Each option has its merits, but understanding the complete requirements for migration is critical.",
        "key_terminology": "AWS Systems Manager, AMI, VM Import/Export, AWS Backup.",
        "overall_assessment": "Option D is aligned with AWS best practices for migrating VMs while preserving software and configurations, making it the most suitable choice despite community voting favoring option B."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである: AWS Systems Managerを使用して管理インスタンスアクティベーションを行い、アプリケーションを移行し、ソフトウェアと設定を保持する。",
        "situation_analysis": "企業はソフトウェアとその設定を維持しつつアプリケーションを移行する必要がある。ソリューションはVM管理とバックアップの両方をカバーしなければならない。",
        "option_analysis": "選択肢Aは移行プロセス中のソフトウェアの保存を保証しておらず、選択肢Bは構成設定を考慮しない手動イメージ作成を必要とする。選択肢Cもバックアップに重点を置いており直接的な移行プロセスが不足している。選択肢Dは両方の要件に直接対応している。",
        "additional_knowledge": "各選択肢にはそれぞれの長所があるが、移行の完全な要件を理解することが重要である。",
        "key_terminology": "AWS Systems Manager, AMI, VM Import/Export, AWS Backup。",
        "overall_assessment": "選択肢Dは、ソフトウェアと構成を保持しながらVMを移行するためのAWSベストプラクティスに合致しており、最も適した選択肢であるが、コミュニティ投票は選択肢Bを支持している。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "AMI",
      "VM Import/Export",
      "AWS Backup"
    ]
  },
  {
    "No": "63",
    "question": "A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed\nimage in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and\nruns by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.\nThe application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing\nfrequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application's\narchitecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "ビデオ処理会社には、Amazon S3バケットから画像をダウンロードし、画像を処理し、変換された画像を別のS3バケットに保存し、画像に関するメタデータをAmazon DynamoDBテーブルに更新するアプリケーションがあります。このアプリケーションはNode.jsで書かれており、AWS Lambda関数を使用して実行されます。Lambda関数は、新しい画像がAmazon S3にアップロードされると呼び出されます。このアプリケーションはしばらくの間問題なく実行されていました。しかし、画像のサイズが大幅に増加したため、Lambda関数は現在、タイムアウトエラーで頻繁に失敗しています。関数のタイムアウトは最大値に設定されています。ソリューションアーキテクトは、呼び出しの失敗を防ぐためにアプリケーションのアーキテクチャをリファクタリングする必要があります。同社は基盤となるインフラストラクチャを管理したくありません。これらの要件を満たすために、ソリューションアーキテクトはどの組み合わせの手順を取るべきですか？（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).",
        "text_jp": "アプリケーションコードを含むDockerイメージを構築することによってアプリケーション展開を変更します。イメージをAmazon Elastic Container Registry (Amazon ECR)に公開します。"
      },
      {
        "key": "B",
        "text": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.",
        "text_jp": "AWS Fargateの互換性タイプを持つ新しいAmazon Elastic Container Service (Amazon ECS)タスク定義を作成します。タスク定義をAmazon Elastic Container Registry (Amazon ECR)の新しいイメージを使用するように構成します。新しいファイルがAmazon S3に到着したときに、Lambda関数がECSタスクを呼び出すようにタスク定義を調整します。"
      },
      {
        "key": "C",
        "text": "Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of the Lambda function.",
        "text_jp": "Lambda関数を呼び出すためのParallel状態を持つAWS Step Functionsの状態マシンを作成します。Lambda関数のプロビジョニングされた同時実行数を増やします。"
      },
      {
        "key": "D",
        "text": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.",
        "text_jp": "Amazon EC2の互換性タイプを持つ新しいAmazon Elastic Container Service (Amazon ECS)タスク定義を作成します。タスク定義をAmazon Elastic Container Registry (Amazon ECR)の新しいイメージを使用するように構成します。新しいファイルがAmazon S3に到着したときに、Lambda関数がECSタスクを呼び出すようにタスク定義を調整します。"
      },
      {
        "key": "E",
        "text": "Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB instance. Adjust the Lambda function to mount the EFS file share.",
        "text_jp": "アプリケーションを変更して、画像をAmazon Elastic File System (Amazon EFS)に保存し、メタデータをAmazon RDS DBインスタンスに保存します。Lambda関数がEFSファイル共有をマウントするように調整します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "AB (90%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are D and B. Using ECS with Fargate allows the application to scale without managing the underlying infrastructure, while leveraging Docker images enables better handling of image processing workloads.",
        "situation_analysis": "The application has outgrown its current architecture due to increasing image sizes, leading to Lambda function timeouts.",
        "option_analysis": "Option D provides an ECS architecture with EC2, which allows better control and resource allocation for processing large images, compared to Lambda limitations. Option B also provides the ECS Fargate option, which scales automatically without infrastructure management, therefore both options are suitable.",
        "additional_knowledge": "Utilizing Docker allows consistent application behavior across different environments, beneficial for deployment consistency.",
        "key_terminology": "ECS, Fargate, Lambda, EC2, Docker",
        "overall_assessment": "Both options D and B meet requirements for scalable and managed solutions. Community votes show a preference for B but D is also valid in this context. It's important to align the solution with company infrastructure management preferences."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDとBである。ECSとFargateを使用することで、基盤となるインフラストラクチャを管理することなくアプリケーションがスケールすることができ、Dockerイメージを利用することで画像処理ワークロードの取り扱いが向上する。",
        "situation_analysis": "アプリケーションは、画像サイズの増加により現在のアーキテクチャを超えて成長しており、Lambda関数がタイムアウトしている。",
        "option_analysis": "選択肢Dは、EC2によるECSアーキテクチャを提供し、大きな画像の処理のためのリソース割り当てと管理を可能にする。Lambdaの制限に比べて、選択肢BもECS Fargateオプションを提供し、自動的にスケーリング可能であり、したがって両方の選択肢が適切である。",
        "additional_knowledge": "Dockerを活用すると、異なる環境間で一貫したアプリケーションの振る舞いが得られ、デプロイメントの一貫性に役立つ。",
        "key_terminology": "ECS, Fargate, Lambda, EC2, Docker",
        "overall_assessment": "選択肢DとBの両方は、スケーラブルかつ管理された解決策の要件を満たしている。コミュニティの投票はBを支持しているが、Dもこの文脈では有効である。企業のインフラ管理の好みに合った解決策を合わせることが重要である。"
      }
    ],
    "keywords": [
      "ECS",
      "Fargate",
      "Lambda",
      "EC2",
      "Docker"
    ]
  },
  {
    "No": "64",
    "question": "A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization.\nThe company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB\ninstances that are not encrypted at rest in the company's production OU.\nWhich solution will meet this requirement?",
    "question_jp": "ある企業がAWS Organizationsに組織を持っています。この企業はAWS Control Towerを使用して組織のランディングゾーンを展開しています。この企業はガバナンスとポリシーの施行を実装したいと考えています。企業は、制作OUにおいて、暗号化されていないAmazon RDS DBインスタンスを検出するポリシーを実装する必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU.",
        "text_jp": "AWS Control Towerで必須ガードレールをオンにします。必須ガードレールを制作OUに適用します。"
      },
      {
        "key": "B",
        "text": "Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.",
        "text_jp": "AWS Control Towerの強く推奨されるガードレールのリストから適切なガードレールを有効にします。ガードレールを制作OUに適用します。"
      },
      {
        "key": "C",
        "text": "Use AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU.",
        "text_jp": "AWS Configを使用して新しい必須ガードレールを作成します。ルールを制作OU内のすべてのアカウントに適用します。"
      },
      {
        "key": "D",
        "text": "Create a custom SCP in AWS Control Tower. Apply the SCP to the production OU.",
        "text_jp": "AWS Control TowerでカスタムSCPを作成します。SCPを制作OUに適用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Enabling the appropriate guardrail from the strongly recommended guardrails in AWS Control Tower will ensure that RDS instances are monitored for encryption at rest.",
        "situation_analysis": "The company must ensure governance and policy enforcement specifically for Amazon RDS DB instances in the production OU.",
        "option_analysis": "Option A is incorrect because mandatory guardrails usually do not tailor to specific use cases like checking RDS encryption. Option C is invalid as using AWS Config here does not utilize Control Tower’s built-in capabilities. Option D does not specifically address RDS encryption.",
        "additional_knowledge": "Understanding the nature of each guardrail is essential for optimal security governance.",
        "key_terminology": "AWS Control Tower, guardrail, Amazon RDS, governance, encryption at rest",
        "overall_assessment": "This question solidly tests the understanding of AWS Control Tower functionality and its applicability for enforcing governance policies specifically related to RDS instances. The community vote strongly supports Option B, reflecting its correctness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。AWS Control Towerの強く推奨されるガードレールから適切なガードレールを有効にすることで、RDSインスタンスが暗号化されているかどうかを監視できるようになる。",
        "situation_analysis": "企業は、制作OU内のAmazon RDS DBインスタンスに対するガバナンスとポリシーの施行を確実にしなければならない。",
        "option_analysis": "選択肢Aは誤りである。必須ガードレールは特定のユースケースに合わせたチェックを行うものではない。選択肢Cは無効である。なぜなら、AWS Configを使用してもControl Towerの組み込み機能を活用しないからである。選択肢Dは、RDSの暗号化を特に扱っていないため、正しくない。",
        "additional_knowledge": "各ガードレールの性質を理解することが、最適なセキュリティガバナンスのために重要である。",
        "key_terminology": "AWS Control Tower, ガードレール, Amazon RDS, ガバナンス, 保存時暗号化",
        "overall_assessment": "この質問は、AWS Control Towerの機能とその適用性に関する理解をしっかりとテストしている。コミュニティ投票は選択肢Bを強く支持しており、正解であることを反映している。"
      }
    ],
    "keywords": [
      "AWS Control Tower",
      "guardrail",
      "Amazon RDS",
      "governance",
      "encryption at rest"
    ]
  },
  {
    "No": "65",
    "question": "A startup company hosts a fieet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company's engineers rely\nheavily on SSH access to the instances for troubleshooting.\nThe company's existing architecture includes the following:\n• A VPC with private and public subnets, and a NAT gateway.\n• Site-to-Site VPN for connectivity with the on-premises environment.\n• EC2 security groups with direct SSH access from the on-premises environment.\nThe company needs to increase security controls around SSH access and provide auditing of commands run by the engineers.\nWhich strategy should a solutions architect use?",
    "question_jp": "スタートアップ企業は、最新のAmazon Linux 2 AMIを使用してプライベートサブネットにAmazon EC2インスタンスのフリートをホストしています。同社のエンジニアは、トラブルシューティングのためにインスタンスへのSSHアクセスを重視しています。同社の既存のアーキテクチャには以下が含まれています：• プライベートおよびパブリックサブネットとNATゲートウェイを持つVPC。• オンプレミス環境との接続のためのSite-to-Site VPN。• オンプレミス環境から直接SSHアクセスを許可するEC2セキュリティグループ。同社はSSHアクセスのセキュリティコントロールを強化し、エンジニアが実行したコマンドの監査を提供する必要があります。ソリューションアーキテクトはどの戦略を使用すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Install and configure EC2 Instance Connect on the fieet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI.",
        "text_jp": "EC2インスタンスのフリートにEC2 Instance Connectをインストールして構成します。EC2インスタンスに関連付けられているすべてのセキュリティグループルールからポート22の着信TCPを許可するものを削除します。エンジニアにEC2 Instance Connect CLIを使用してリモートでインスタンスにアクセスするよう助言します。"
      },
      {
        "key": "B",
        "text": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs.",
        "text_jp": "EC2セキュリティグループを更新し、エンジニアのデバイスのIPアドレスからのポート22の着信TCPのみを許可します。すべてのEC2インスタンスにAmazon CloudWatchエージェントをインストールし、オペレーティングシステムの監査ログをCloudWatch Logsに送信します。"
      },
      {
        "key": "C",
        "text": "Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Enable AWS Config for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules.",
        "text_jp": "EC2セキュリティグループを更新し、エンジニアのデバイスのIPアドレスからのポート22の着信TCPのみを許可します。EC2セキュリティグループリソースの変更に対してAWS Configを有効にします。AWS Firewall Managerを有効にし、ルールの変更を自動的に修正するセキュリティグループポリシーを適用します。"
      },
      {
        "key": "D",
        "text": "Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager.",
        "text_jp": "AmazonSSMManagedInstanceCoreマネージドポリシーを添付したIAMロールを作成します。このIAMロールをすべてのEC2インスタンスにアタッチします。EC2インスタンスに関連付けられているすべてのセキュリティグループルールからポート22の着信TCPを許可するものを削除します。エンジニアに、デバイスにAWS Systems Manager Session Managerプラグインをインストールさせ、Systems Managerからstart-session API呼び出しを使用してインスタンスにリモートアクセスさせます。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. By using IAM roles and AWS Systems Manager Session Manager, the company enhances security by eliminating direct SSH access while allowing remote management via AWS services.",
        "situation_analysis": "The company requires a secure method for engineers to access EC2 instances without exposing direct SSH ports to the internet or on-premises environment.",
        "option_analysis": "Option D provides a strong security model. Other options either continue to expose SSH directly or do not sufficiently enhance security controls.",
        "additional_knowledge": "Option D's IAM role approach ensures that only authorized users can access instances.",
        "key_terminology": "IAM Roles, AWS Systems Manager, Session Manager, EC2 security groups, SSH access",
        "overall_assessment": "Given the community vote and the critical need for improved security, option D is a well-supported choice. The majority of the community agrees with this option as it effectively balances access and security."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDである。IAMロールとAWS Systems Manager Session Managerを使用することにより、企業は直接SSHアクセスを排除し、AWSサービスを通じてリモート管理を許可することでセキュリティを強化する。",
        "situation_analysis": "企業はエンジニアがEC2インスタンスにアクセスするための安全な方法が必要であり、インターネットやオンプレミス環境に対して直接SSHポートを公開しないことを求めている。",
        "option_analysis": "オプションDは強力なセキュリティモデルを提供する。他のオプションは、SSHを直接公開し続けるか、セキュリティコントロールの強化が不十分である。",
        "additional_knowledge": "オプションDのIAMロールアプローチは、認可されたユーザーのみがインスタンスにアクセスできることを保証する。",
        "key_terminology": "IAMロール、AWS Systems Manager、Session Manager、EC2セキュリティグループ、SSHアクセス",
        "overall_assessment": "コミュニティの投票結果とセキュリティ向上の重大な必要性を考慮すると、オプションDはよく支持された選択肢である。このオプションはアクセスとセキュリティのバランスを効果的に取るもので、過半数のコミュニティがこの選択肢に同意している。"
      }
    ],
    "keywords": [
      "IAM Roles",
      "AWS Systems Manager",
      "Session Manager",
      "EC2 security groups",
      "SSH access"
    ]
  },
  {
    "No": "66",
    "question": "A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed,\ndevelopers use their company email address to request an account. The company wants to ensure that developers are not launching costly\nservices or running services unnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "AWS Organizationsを利用している企業は、開発者がAWS上で実験を行うことを許可しています。企業がデプロイしたランディングゾーンの一部として、開発者は自分の会社のメールアドレスを使用してアカウントをリクエストします。企業は、開発者が高額なサービスを立ち上げたり、サービスを不必要に実行したりすることがないようにしたいと考えています。企業は、開発者にAWSコストを制限するための固定月間予算を支給する必要があります。この要件を満たすためには、どの組み合わせの手順を選択すべきですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.",
        "text_jp": "固定の月間アカウント使用制限を設定するSCPを作成し、開発者アカウントに適用します。"
      },
      {
        "key": "B",
        "text": "Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process.",
        "text_jp": "アカウント作成プロセスの一環として、各開発者のアカウントに固定の月間予算を作成するためにAWS Budgetsを使用します。"
      },
      {
        "key": "C",
        "text": "Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.",
        "text_jp": "高額なサービスやコンポーネントへのアクセスを禁止するSCPを作成し、開発者アカウントに適用します。"
      },
      {
        "key": "D",
        "text": "Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts.",
        "text_jp": "高額なサービスやコンポーネントへのアクセスを禁止するIAMポリシーを作成し、開発者アカウントに適用します。"
      },
      {
        "key": "E",
        "text": "Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all services.",
        "text_jp": "予算額に達した場合にサービスを終了するAWS Budgetsアラートアクションを作成します。アクションをすべてのサービスを終了するように構成します。"
      },
      {
        "key": "F",
        "text": "Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services.",
        "text_jp": "予算額に達した場合にAmazon Simple Notification Service（Amazon SNS）通知を送信するAWS Budgetsアラートアクションを作成します。AWS Lambda関数を呼び出してすべてのサービスを終了します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BCF (77%) BDF (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct choice is B. AWS Budgets enables cost management by creating budgets that users can monitor and manage their costs.",
        "situation_analysis": "The company requires a method to limit the AWS costs for developers while still enabling them to experiment. Implementing AWS Budgets allows for an effective cost control mechanism.",
        "option_analysis": "Option A is incorrect because Service Control Policies (SCPs) cannot set budget limits; they only control permissions. Option C and D restrict access but don't set a budget. Option E is a more complex approach than needed, and F adds unnecessary complexity with Lambda.",
        "additional_knowledge": "Understanding the distinctions between budget controls and access policies is crucial in cloud cost management.",
        "key_terminology": "AWS Budgets, Service Control Policies (SCPs), IAM, cost control.",
        "overall_assessment": "Using AWS Budgets (Option B) directly addresses the company's need for cost management, while the other options either do not meet the requirement or add unnecessary complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい選択肢はBである。AWS Budgetsは、ユーザーがコストを監視および管理できる予算を作成することによってコスト管理を可能にする。",
        "situation_analysis": "企業は、開発者が実験することを可能にしながら、AWSコストを制限する方法を必要としている。AWS Budgetsを実装することは、効果的なコスト管理メカニズムとなる。",
        "option_analysis": "選択肢Aは誤りである。サービスコントロールポリシー（SCP）は予算制限を設定できず、権限を制御するだけである。選択肢CとDはアクセスを制限するが、予算を設定しない。選択肢Eは、必要以上に複雑なアプローチであり、FはLambdaを用いた不要な複雑さを追加する。",
        "additional_knowledge": "予算管理とアクセスポリシーの違いを理解することは、クラウドコスト管理の上で重要である。",
        "key_terminology": "AWS Budgets, サービスコントロールポリシー(SCP), IAM, コスト管理。",
        "overall_assessment": "AWS Budgets（選択肢B）を使用することは、企業のコスト管理ニーズに直接対応する。他の選択肢は要件を満たさないか、不必要な複雑さをもたらすのみである。"
      }
    ],
    "keywords": [
      "AWS Budgets",
      "Service Control Policies",
      "IAM",
      "cost control"
    ]
  },
  {
    "No": "67",
    "question": "A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the\napplications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions\nby using a deployment package. The company has configured automated backups for Aurora.\nThe company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application\nprocesses critical data, so the company must minimize downtime.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、Sourceという名前のAWSアカウントにアプリケーションを持っています。このアカウントは、AWS Organizationsの組織に属しています。アプリケーションの1つはAWS Lambda関数を使用し、Amazon Auroraデータベースに在庫データを保存しています。このアプリケーションは、デプロイメントパッケージを使用してLambda関数をデプロイしています。企業はAuroraの自動バックアップを設定しています。\n企業は、Lambda関数とAuroraデータベースをTargetという名前の新しいAWSアカウントに移行したいと考えています。アプリケーションは重要なデータを処理するため、企業はダウンタイムを最小限に抑える必要があります。\nどのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the automated Aurora DB cluster snapshot with the Target account.",
        "text_jp": "SourceアカウントからLambda関数のデプロイメントパッケージをダウンロードします。デプロイメントパッケージを使用してTargetアカウントに新しいLambda関数を作成します。自動Aurora DBクラスターのスナップショットをTargetアカウントと共有します。"
      },
      {
        "key": "B",
        "text": "Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster.",
        "text_jp": "SourceアカウントからLambda関数のデプロイメントパッケージをダウンロードします。デプロイメントパッケージを使用してTargetアカウントに新しいLambda関数を作成します。AWSリソースアクセスマネージャー（AWS RAM）を使用してAurora DBクラスターをTargetアカウントと共有します。TargetアカウントにAurora DBクラスターをクローンする権限を付与します。"
      },
      {
        "key": "C",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant the Target account permission to clone the Aurora DB cluster.",
        "text_jp": "AWSリソースアクセスマネージャー（AWS RAM）を使用して、TargetアカウントとLambda関数およびAurora DBクラスターを共有します。TargetアカウントにAurora DBクラスターをクローンする権限を付与します。"
      },
      {
        "key": "D",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the Target account.",
        "text_jp": "AWSリソースアクセスマネージャー（AWS RAM）を使用してLambda関数をTargetアカウントと共有します。自動Aurora DBクラスターのスナップショットをTargetアカウントと共有します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, as it allows sharing of both the Lambda functions and the Aurora database cluster with minimal downtime for migration.",
        "situation_analysis": "The company needs to migrate critical applications while ensuring continuity of services. Therefore, minimizing downtime during the transition is essential.",
        "option_analysis": "Option A does not share the Aurora cluster, which may lead to longer downtime due to the need for rebuilding it. Option B shares the cluster but involves creating new functions separately, requiring additional time. Option D shares only the Lambda function; therefore, it does not provide a complete solution.",
        "additional_knowledge": "Understanding the nuances of resource sharing in multi-account environments is crucial for effective AWS management.",
        "key_terminology": "AWS Resource Access Manager, Aurora DB cluster, Lambda functions, resource sharing, migration strategy.",
        "overall_assessment": "This question is well-structured as it requires understanding AWS resource management and migration best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCであり、これにより、Lambda関数とAuroraデータベースクラスターの両方を共有し、移行中のダウンタイムを最小限に抑えることができる。",
        "situation_analysis": "企業は、サービスの継続性を確保しながら重要なアプリケーションを移行する必要があります。そのため、移行中のダウンタイムを最小限に抑えることが不可欠です。",
        "option_analysis": "選択肢AはAuroraクラスターを共有しないため、再構築の必要が生じ、ダウンタイムが長くなる可能性があります。選択肢Bはクラスターを共有しますが、新しい関数を個別に作成する必要があるため、追加の時間がかかります。選択肢DはLambda関数だけを共有するため、完全なソリューションを提供しません。",
        "additional_knowledge": "マルチアカウント環境でのリソース共有のニュアンスを理解することが、効果的なAWS管理のためには重要です。",
        "key_terminology": "AWSリソースアクセスマネージャー、Aurora DBクラスター、Lambda関数、リソース共有、移行戦略。",
        "overall_assessment": "この質問は、AWSリソース管理と移行のベストプラクティスの理解を要求するため、非常によく構成されています。"
      }
    ],
    "keywords": [
      "AWS Resource Access Manager",
      "Aurora DB cluster",
      "Lambda functions",
      "resource sharing",
      "migration strategy"
    ]
  },
  {
    "No": "68",
    "question": "A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an\nAmazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess\na file that the script has already processed.\nThe company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the\nfile processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term\nmanagement overhead.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業が、データを処理するためにAmazon EC2インスタンス上でPythonスクリプトを実行しています。このスクリプトは10分ごとに実行され、Amazon S3バケットからファイルを取り込み、これらのファイルを処理します。スクリプトは、平均して各ファイルの処理に約5分を要します。このスクリプトは、すでに処理したファイルを再処理しません。企業はAmazon CloudWatchのメトリクスを確認し、ファイル処理のスピードのためにEC2インスタンスが約40%の時間アイドル状態であることに気付きました。企業はこのワークロードを高可用性およびスケーラブルにし、長期的な管理のオーバーヘッドを削減したいと考えています。どのソリューションが最もコスト効率よくこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects.",
        "text_jp": "データ処理スクリプトをAWS Lambda関数に移行します。会社がオブジェクトをアップロードしたときにLambda関数を呼び出すために、S3イベント通知を使用します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a minimum size of one instance. Update the data processing script to poll the SQS queue. Process the S3 objects that the SQS message identifies.",
        "text_jp": "Amazon Simple Queue Service (Amazon SQS)キューを作成します。Amazon S3を構成してイベント通知をSQSキューに送信します。EC2 Auto Scalingグループを作成し、最小サイズを1インスタンスに設定します。データ処理スクリプトを更新してSQSキューをポーリングします。SQSメッセージが特定するS3オブジェクトを処理します。"
      },
      {
        "key": "C",
        "text": "Migrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to poll the S3 bucket for new objects and to process the resulting objects.",
        "text_jp": "データ処理スクリプトをコンテナイメージに移行します。データ処理コンテナをEC2インスタンス上で実行します。新しいオブジェクトのためにS3バケットをポーリングし、結果として得られるオブジェクトを処理するようにコンテナを設定します。"
      },
      {
        "key": "D",
        "text": "Migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 event notification to invoke the Lambda function.",
        "text_jp": "データ処理スクリプトを、AWS Fargate上のAmazon Elastic Container Service (Amazon ECS)で実行されるコンテナイメージに移行します。コンテナがファイルを処理する際にFargate RunTask API操作を呼び出すAWS Lambda関数を作成します。Lambda関数を呼び出すためにS3イベント通知を使用します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (80%) D (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Migrating the script to Amazon ECS on AWS Fargate allows automatic scaling and management of the containers without needing to manage the EC2 instances directly.",
        "situation_analysis": "The current setup has idle time of about 40% and uses a script that can lead to higher management overhead and limited scalability. The company seeks a solution that is cost-effective and requires minimal management.",
        "option_analysis": "Option A lacks the scalability and management features that ECS on Fargate provides, while option B introduces additional complexity with SQS and managing EC2 instances. Option C still requires managing EC2 instances. Option D uses Fargate, thus simplifying management and providing better scalability and availability.",
        "additional_knowledge": "Fargate removes the need to provision and manage servers, thereby fitting the requirements outlined.",
        "key_terminology": "AWS Lambda, Amazon S3, Amazon ECS, AWS Fargate, containerization",
        "overall_assessment": "Option D stands out as the ideal solution due to its cost-effectiveness in long-term management and scalability. Despite community votes favoring option A, it does not offer the same advantages as Fargate."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです。スクリプトをAWS Fargate上のAmazon ECSに移行することで、自動スケーリングとコンテナの管理が可能になり、EC2インスタンスを直接管理する必要がなくなります。",
        "situation_analysis": "現在の設定では約40%のアイドル時間があり、スクリプトを使用しているため、管理のオーバーヘッドが高く、スケーラビリティが限られています。企業はコスト効率が高く、管理が最小限で済むソリューションを求めています。",
        "option_analysis": "選択肢Aは、ECSのFargateが提供するスケーラビリティや管理機能を欠いており、選択肢BはSQSやEC2インスタンスの管理の複雑さを引き入れます。選択肢CもEC2インスタンスの管理が必要です。選択肢DはFargateを使用することで、管理を簡素化し、より良いスケーラビリティと可用性を提供します。",
        "additional_knowledge": "Fargateはサーバーを調達して管理する必要を排除し、示された要件に適合しています。",
        "key_terminology": "AWS Lambda、Amazon S3、Amazon ECS、AWS Fargate、コンテナ化",
        "overall_assessment": "Dの選択肢は、長期的な管理とスケーラビリティにおけるコスト効率に優れた理想的なソリューションです。コミュニティの投票が選択肢Aを支持しているにもかかわらず、Fargateの持つ利点は同じではありません。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon S3",
      "Amazon ECS",
      "AWS Fargate",
      "containerization"
    ]
  },
  {
    "No": "69",
    "question": "A financial services company in North America plans to release a new online web application to its customers on AWS. The company will launch\nthe application in the us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet\nuser trafic. The company also wants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-\npassive failover.\nWhich solution will meet these requirements?",
    "question_jp": "北米の金融サービス会社が、AWS上で顧客向けに新しいオンラインウェブアプリケーションをリリースする計画を立てています。会社は、アプリケーションをus-east-1リージョンのAmazon EC2インスタンス上で立ち上げます。アプリケーションは高可用性で、ユーザーのトラフィックに応じて動的にスケールする必要があります。また、会社は、アクティブ-パッシブフェイルオーバーを使用してus-west-1リージョンにアプリケーションの災害復旧環境を実装したいと考えています。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB.",
        "text_jp": "us-east-1にVPCを作成し、us-west-1に別のVPCを作成します。VPCピアリングを構成します。us-east-1のVPCに、両方のVPCの複数のアベイラビリティゾーンにまたがるアプリケーションロードバランサー（ALB）を作成します。複数のアベイラビリティゾーンにEC2インスタンスを展開するオートスケーリンググループを作成します。オートスケーリンググループをALBの背後に配置します。"
      },
      {
        "key": "B",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPC. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC. Place the Auto Scaling group behind the ALSet up the same configuration in the us-west-1 VPC. Create an Amazon Route 53 hosted zone. Create separate records for each ALEnable health checks to ensure high availability between Regions.",
        "text_jp": "us-east-1にVPCを作成し、us-west-1に別のVPCを作成します。us-east-1のVPCに、複数のアベイラビリティゾーンにまたがるアプリケーションロードバランサー（ALB）を作成します。オートスケーリンググループを作成し、us-east-1のVPCの複数のアベイラビリティゾーンにEC2インスタンスを展開します。オートスケーリンググループをALBの背後に配置し、us-west-1のVPCにも同様の構成を設定します。Amazon Route 53ホステッドゾーンを作成し、各ALBのための別々のレコードを作成します。高可用性を確保するためにヘルスチェックを有効にします。"
      },
      {
        "key": "C",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPCreate an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPPlace the Auto Scaling group behind the ALB. Set up the same configuration in the us-west-1 VPCreate an Amazon Route 53 hosted zone. Create separate records for each ALB. Enable health checks and configure a failover routing policy for each record.",
        "text_jp": "us-east-1にVPCを作成し、us-west-1に別のVPCを作成します。us-east-1のVPCに、複数のアベイラビリティゾーンにまたがるアプリケーションロードバランサー（ALB）を作成します。オートスケーリンググループを作成し、us-east-1のVPCの複数のアベイラビリティゾーンにEC2インスタンスを展開します。オートスケーリンググループをALBの背後に配置し、us-west-1のVPCにも同様の構成を設定します。Amazon Route 53ホステッドゾーンを作成し、各ALBのための別々のレコードを作成します。ヘルスチェックを有効にし、各レコードのフェイルオーバールーティングポリシーを設定します。"
      },
      {
        "key": "D",
        "text": "Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB. Create an Amazon Route 53 hosted zone. Create a record for the ALB.",
        "text_jp": "us-east-1にVPCを作成し、us-west-1に別のVPCを作成します。VPCピアリングを構成します。us-east-1のVPCに、両方のVPCの複数のアベイラビリティゾーンにまたがるアプリケーションロードバランサー（ALB）を作成します。複数のアベイラビリティゾーンにEC2インスタンスを展開するオートスケーリンググループを作成します。オートスケーリンググループをALBの背後に配置します。Amazon Route 53ホステッドゾーンを作成し、ALBのためのレコードを作成します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This solution provides both high availability and a disaster recovery environment as required by the company.",
        "situation_analysis": "The company needs a solution that is highly available and can scale dynamically to meet varying user demand, which is critical for a web application. Additionally, the application must have a disaster recovery strategy that uses active-passive failover between regions.",
        "option_analysis": "Option C allows for the creation of a VPC in both regions and utilizes an ALB and an Auto Scaling group to ensure high availability within each VPC. By configuring Route 53 with failover routing, it allows for a disaster recovery implementation that meets the requirements outlined in the scenario.",
        "additional_knowledge": "Using Auto Scaling helps balance the load efficiently, and Route 53 can provide health checks to ensure traffic is only directed to healthy instances.",
        "key_terminology": "Application Load Balancer, Auto Scaling, Route 53, failover routing.",
        "overall_assessment": "This question effectively tests the understanding of essential AWS services for building highly available applications and implementing disaster recovery strategies."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。このソリューションは、会社の必要とする高可用性と災害復旧環境の両方を提供する。",
        "situation_analysis": "会社は、変動する用户の需要に応じて動的にスケールし、高可用なソリューションが必要である。これはウェブアプリケーションにとって重要である。さらに、アプリケーションはリージョン間でアクティブ-パッシブフェイルオーバーを使用した災害復旧戦略を持たなければならない。",
        "option_analysis": "選択肢Cは、両方のリージョンにVPCを作成し、各VPC内で高可用性を確保するためにALBとオートスケーリンググループを使用する。このようにRoute 53をフェイルオーバールーティングで設定することで、シナリオで求められている要件を満たす災害復旧の実装が可能になる。",
        "additional_knowledge": "オートスケーリングを使用することで効率的に負荷を分散させ、Route 53が健全なインスタンスにのみトラフィックを指向するためのヘルスチェックを提供できる。",
        "key_terminology": "アプリケーションロードバランサー, オートスケーリング, Route 53, フェイルオーバールーティング。",
        "overall_assessment": "この質問は、AWSサービスを用いた高可用なアプリケーションの構築と災害復旧戦略の実装に関する理解を効果的にテストしている。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "Auto Scaling",
      "Route 53",
      "failover routing"
    ]
  },
  {
    "No": "70",
    "question": "A company has an environment that has a single AWS account. A solutions architect is reviewing the environment to recommend what the\ncompany could improve specifically in terms of access to the AWS Management Console. The company's IT support workers currently access the\nconsole for administrative tasks, authenticating with named IAM users that have been mapped to their job role.\nThe IT support workers no longer want to maintain both their Active Directory and IAM user accounts. They want to be able to access the console\nby using their existing Active Directory credentials. The solutions architect is using AWS IAM Identity Center (AWS Single Sign-On) to implement\nthis functionality.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある会社には、単一のAWSアカウントを持つ環境があります。ソリューションアーキテクトは、AWS管理コンソールへのアクセスに関して、特に改善の提案を行うために環境をレビューしています。この会社のITサポート担当者は現在、役割にマッピングされた名前付きIAMユーザーを使用してコンソールにアクセスし、管理タスクを実行しています。しかし、ITサポート担当者はもはやActive DirectoryとIAMユーザーアカウントの両方を維持したくありません。彼らは、既存のActive Directory資格情報を使用してコンソールにアクセスしたいと考えています。ソリューションアーキテクトはこの機能を実装するためにAWS IAM Identity Center（AWS Single Sign-On）を使用しています。この要件を最もコスト効果の高い方法で満たすには、どのソリューションを選ぶべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company's on-premises Active Directory. Configure IAM Identity Center and set the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.",
        "text_jp": "AWS Organizationsで組織を作成します。OrganizationsでIAM Identity Center機能をオンにします。企業のオンプレミスActive Directoryと双方向の信頼関係を持つAWS Directory Service for Microsoft Active Directory（AWS Managed Microsoft AD）にディレクトリを作成して構成します。IAM Identity Centerを構成し、AWS Managed Microsoft ADディレクトリをアイデンティティソースとして設定します。パーミッションセットを作成し、AWS Managed Microsoft ADディレクトリ内の既存グループにマッピングします。"
      },
      {
        "key": "B",
        "text": "Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure an AD Connector to connect to the company's on-premises Active Directory. Configure IAM Identity Center and select the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company's Active Directory.",
        "text_jp": "AWS Organizationsで組織を作成します。OrganizationsでIAM Identity Center機能をオンにします。企業のオンプレミスActive Directoryに接続するためのAD Connectorを作成して構成します。IAM Identity Centerを構成し、AD Connectorをアイデンティティソースとして選択します。パーミッションセットを作成し、企業のActive Directory内の既存グループにマッピングします。"
      },
      {
        "key": "C",
        "text": "Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company's on-premises Active Directory. Configure IAM Identity Center and select the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.",
        "text_jp": "AWS Organizationsで組織を作成します。組織のすべての機能をオンにします。企業のオンプレミスActive Directoryと双方向の信頼関係を持つAWS Directory Service for Microsoft Active Directory（AWS Managed Microsoft AD）にディレクトリを作成して構成します。IAM Identity Centerを構成し、AWS Managed Microsoft ADディレクトリをアイデンティティソースとして選択します。パーミッションセットを作成し、AWS Managed Microsoft ADディレクトリ内の既存グループにマッピングします。"
      },
      {
        "key": "D",
        "text": "Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure an AD Connector to connect to the company's on-premises Active Directory. Configure IAM Identity Center and set the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company's Active Directory.",
        "text_jp": "AWS Organizationsで組織を作成します。組織のすべての機能をオンにします。企業のオンプレミスActive Directoryに接続するためのAD Connectorを作成して構成します。IAM Identity Centerを構成し、AD Connectorをアイデンティティソースとして設定します。パーミッションセットを作成し、企業のActive Directory内の既存グループにマッピングします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (80%) B (15%)5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution uses an AD Connector to integrate the existing on-premises Active Directory with the AWS environment, allowing users to authenticate with their current credentials without maintaining separate IAM user accounts.",
        "situation_analysis": "The company wants to streamline access for IT support workers who prefer to use existing Active Directory credentials instead of managing IAM users. This necessitates a cost-effective integration with minimal operational overhead.",
        "option_analysis": "Option D is the best choice because it allows direct connection to the company’s existing Active Directory using an AD Connector. Options A and C require setting up a new AWS Managed Microsoft AD, leading to increased costs. Option B, while using an AD Connector, does not provide the full request, as it could be more expensive and less aligned with AWS best practices for managing access.",
        "additional_knowledge": "AD Connector is especially useful in hybrid cloud environments where businesses want to use existing identity sources.",
        "key_terminology": "AD Connector, AWS IAM Identity Center, Active Directory, user management, cost-effective solutions",
        "overall_assessment": "Overall, Option D meets all requirements effectively and aligns with AWS best practices for integrating on-premises Active Directory with AWS services."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDです。このソリューションはAD Connectorを使用して、既存のオンプレミスActive DirectoryをAWS環境と統合し、ユーザーがIAMユーザーアカウントを別々に維持することなく、現在の資格情報で認証できるようにします。",
        "situation_analysis": "会社は、IAMユーザーを管理するのではなく、既存のActive Directory資格情報を使用したいITサポート担当者のアクセスを効率化したいと考えています。これにより、運用上のオーバーヘッドを最小限に抑えつつ、コスト効果の高い統合が必要です。",
        "option_analysis": "選択肢Dは最適な選択肢であり、AD Connectorを使用して企業の既存のActive Directoryに直接接続することができます。選択肢AとCは新しいAWS Managed Microsoft ADを設定する必要があり、コストが増加します。選択肢BはAD Connectorを使用していますが、コスト面やAWSのベストプラクティスに対する整合性が低いため、完全なリクエストを提供できません。",
        "additional_knowledge": "AD Connectorは、既存のアイデンティティソースを使用したいハイブリッドクラウド環境に特に便利です。",
        "key_terminology": "AD Connector、AWS IAM Identity Center、Active Directory、ユーザー管理、コスト効果の高いソリューション",
        "overall_assessment": "全体として、選択肢Dはすべての要件を効果的に満たし、オンプレミスのActive DirectoryをAWSサービスに統合するためのAWSのベストプラクティスに沿っています。"
      }
    ],
    "keywords": [
      "AD Connector",
      "AWS IAM Identity Center",
      "Active Directory",
      "user management",
      "cost-effective solutions"
    ]
  },
  {
    "No": "71",
    "question": "A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-\neast-1 Region. The files range in size from 1 GB to 10 GB.\nUsers who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload\nfor these users. A solutions architect must improve the app's performance for these uploads.\nWhich solutions will meet these requirements? (Choose two.)",
    "question_jp": "動画ストリーミング会社は最近、動画共有のためのモバイルアプリを立ち上げました。このアプリは、us-east-1リージョンのAmazon S3バケットにさまざまなファイルをアップロードします。ファイルのサイズは1 GBから10 GBまであります。オーストラリアからアプリにアクセスするユーザーは、アップロードに長時間かかることがあり、時にはファイルが完全にアップロードされないこともあります。ソリューションアーキテクトは、これらのアップロードに対するアプリのパフォーマンスを向上させる必要があります。どのソリューションがこれらの要件を満たしますか？（2つ選んでください。）",
    "choices": [
      {
        "key": "A",
        "text": "Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.",
        "text_jp": "S3 Transfer AccelerationをS3バケットで有効にします。アプリを構成して、アップロードにTransfer Accelerationエンドポイントを使用します。"
      },
      {
        "key": "B",
        "text": "Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3 bucket.",
        "text_jp": "各リージョンにS3バケットを構成し、アップロードを受け取ります。S3クロスリージョン複製を使用して、ファイルを配信用のS3バケットにコピーします。"
      },
      {
        "key": "C",
        "text": "Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region.",
        "text_jp": "Amazon Route 53をセットアップし、レイテンシベースのルーティングを使用して、アップロードを最寄りのS3バケットリージョンにルーティングします。"
      },
      {
        "key": "D",
        "text": "Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.",
        "text_jp": "アプリを構成して、動画ファイルをチャンクに分割します。マルチパートアップロードを使用して、ファイルをAmazon S3に転送します。"
      },
      {
        "key": "E",
        "text": "Modify the app to add random prefixes to the files before uploading.",
        "text_jp": "アプリを修正して、アップロード前にファイルにランダムなプレフィックスを追加します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (96%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D, as both solutions enhance upload performance.",
        "situation_analysis": "The app is uploading large files from Australia to an S3 bucket located in the us-east-1 region, causing slow uploads and failures due to the long distance and high latency involved.",
        "option_analysis": "Option A uses S3 Transfer Acceleration, which speeds up data transfers to S3 by routing uploads through Amazon CloudFront's globally distributed edge locations. Option D involves splitting the files into smaller chunks for multipart uploads, making it more resilient to errors and allowing for faster re-transmission if a single chunk fails.",
        "additional_knowledge": "Consider implementing AWS Global Accelerator as an additional solution for improving application availability and performance.",
        "key_terminology": "S3 Transfer Acceleration, Multipart Upload, Cross-Region Replication, Latency-Based Routing, Amazon CloudFront",
        "overall_assessment": "Both options are valid solutions that conform to AWS best practices for optimizing performance in scenarios with significant network latencies. It should be noted that community votes align with these recommended practices, highlighting the effectiveness of these options."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとDであり、どちらのソリューションもアップロードパフォーマンスを向上させます。",
        "situation_analysis": "アプリはオーストラリアからus-east-1リージョンにあるS3バケットに大きなファイルをアップロードしており、長距離による遅延のためにアップロードが遅く、失敗することがあります。",
        "option_analysis": "オプションAはS3 Transfer Accelerationを使用しており、データ転送を高速化し、Amazon CloudFrontの分散したエッジロケーションを通じてアップロードをルーティングします。オプションDでは、ファイルを小さなチャンクに分割してマルチパートアップロードを行い、エラーに対する耐性を高め、単一のチャンクが失敗した場合の再送信を迅速に行えるようにします。",
        "additional_knowledge": "AWS Global Acceleratorを実装することを検討し、アプリケーションの可用性とパフォーマンスを向上させることができます。",
        "key_terminology": "S3 Transfer Acceleration、マルチパートアップロード、クロスリージョン複製、レイテンシベースのルーティング、Amazon CloudFront",
        "overall_assessment": "どちらのオプションも、ネットワークの遅延が著しいシナリオにおいてパフォーマンスを最適化するためのAWSのベストプラクティスに合致した妥当なソリューションです。コミュニティの投票もこれらの推奨プラクティスに一致しており、これらのオプションの有効性を強調しています。"
      }
    ],
    "keywords": [
      "S3 Transfer Acceleration",
      "Multipart Upload",
      "Cross-Region Replication",
      "Latency-Based Routing",
      "Amazon CloudFront"
    ]
  },
  {
    "No": "72",
    "question": "An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After a failover test, the application lost the\nconnections to the database and could not re-establish the connections. After a restart of the application, the application re-established the\nconnections.\nA solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.\nWhich solution will meet these requirements?",
    "question_jp": "アプリケーションは、us-east-1リージョンにあるAmazon RDS for MySQL マルチAZ DBインスタンスを使用しています。フェイルオーバーテスト後、アプリケーションはデータベースへの接続を失い、再接続できませんでした。アプリケーションを再起動した後、アプリケーションは再度接続を確立しました。ソリューションアーキテクトは、アプリケーションが再起動を必要とせずにデータベースへの接続を再確立できるようにソリューションを実装する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Aurora MySQL Serverless v1 DB instance. Migrate the RDS DB instance to the Aurora Serverless v1 DB instance. Update the connection settings in the application to point to the Aurora reader endpoint.",
        "text_jp": "Amazon Aurora MySQL Serverless v1 DBインスタンスを作成します。RDS DBインスタンスをAurora Serverless v1 DBインスタンスに移行します。接続設定を更新して、アプリケーションがAuroraリーダーエンドポイントを指すようにします。"
      },
      {
        "key": "B",
        "text": "Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.",
        "text_jp": "RDSプロキシを作成します。既存のRDSエンドポイントをターゲットとして構成します。アプリケーションがRDSプロキシエンドポイントを指すように接続設定を更新します。"
      },
      {
        "key": "C",
        "text": "Create a two-node Amazon Aurora MySQL DB cluster. Migrate the RDS DB instance to the Aurora DB cluster. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.",
        "text_jp": "二ノードのAmazon Aurora MySQL DBクラスターを作成します。RDS DBインスタンスをAurora DBクラスターに移行します。RDSプロキシを作成します。既存のRDSエンドポイントをターゲットとして構成します。アプリケーションがRDSプロキシエンドポイントを指すように接続設定を更新します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon S3 bucket. Export the database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Configure Amazon Athena to use the S3 bucket as a data store. Install the latest Open Database Connectivity (ODBC) driver for the application. Update the connection settings in the application to point to the Athena endpoint",
        "text_jp": "Amazon S3バケットを作成します。AWSデータベース移行サービス（AWS DMS）を使用してデータベースをAmazon S3にエクスポートします。Amazon Athenaを使用してS3バケットをデータストアとして設定します。アプリケーションに最新のオープンデータベース接続（ODBC）ドライバーをインストールします。アプリケーションがAthenaエンドポイントを指すように接続設定を更新します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves creating an RDS proxy to manage database connections. This allows for seamless failover handling without disrupting application functionality.",
        "situation_analysis": "The application is using a Multi-AZ RDS instance and experiences connection loss during a failover. The architecture needs to support automatic connection handling during such events.",
        "option_analysis": "Option B is preferred because RDS Proxy helps maintain persistent connections and allows applications to automatically reconnect without a restart. Other options, while valid for different scenarios, do not directly address the need for uninterrupted connectivity during failover.",
        "additional_knowledge": "When using RDS Proxy, applications can continue to communicate with the proxy, reducing the operational overhead during failover events.",
        "key_terminology": "Amazon RDS, RDS Proxy, Multi-AZ, Failover, Connection Pooling",
        "overall_assessment": "This question effectively tests understanding of database resilience and connection management strategies within AWS. The community overwhelmingly supports option B as the best solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、RDSプロキシを作成してデータベース接続を管理します。これにより、アプリケーションの機能を中断させることなく、シームレスなフェイルオーバー処理が可能になります。",
        "situation_analysis": "アプリケーションはMulti-AZのRDSインスタンスを使用しており、フェイルオーバー中に接続が失われます。アーキテクチャはこのようなイベント中に自動的な接続処理をサポートする必要があります。",
        "option_analysis": "オプションBが推奨される理由は、RDSプロキシは持続的な接続を維持し、アプリケーションが再起動せずに自動的に再接続できるためです。他のオプションは異なるシナリオに有効ですが、フェイルオーバー中の中断のない接続の必要性には直接対処していません。",
        "additional_knowledge": "RDSプロキシを使用することで、アプリケーションはプロキシと通信を続けることができ、フェイルオーバーイベント中の運用負荷を軽減します。",
        "key_terminology": "Amazon RDS, RDS Proxy, Multi-AZ, フェイルオーバー, 接続プール",
        "overall_assessment": "この質問は、AWS内のデータベース耐障害性と接続管理戦略の理解を効果的にテストしています。コミュニティはオプションBを最良の解決策として圧倒的に支持しています。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "RDS Proxy",
      "Multi-AZ",
      "Failover",
      "Connection Pooling"
    ]
  },
  {
    "No": "73",
    "question": "A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution and send data. Each device needs to be able\nto send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "会社はAWSクラウドにソリューションを構築しています。何千ものデバイスがこのソリューションに接続しデータを送信します。各デバイスはMQTTプロトコルを介してリアルタイムでデータを送受信できる必要があります。各デバイスは、ユニークなX.509証明書を使用して認証しなければなりません。どのソリューションが最も運用上の負担を少なくしてこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to Amazon MQ.",
        "text_jp": "AWS IoT Coreを設定し、各デバイスに対応するAmazon MQキューを作成し、証明書を発行します。各デバイスをAmazon MQに接続します。"
      },
      {
        "key": "B",
        "text": "Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in an Auto Scaling group. Set the Auto Scaling group as the target for the NLConnect each device to the NLB.",
        "text_jp": "Network Load Balancer（NLB）を作成し、AWS Lambdaオーソライザーを構成します。Auto Scalingグループ内のAmazon EC2インスタンスでMQTTブローカーを実行します。Auto ScalingグループをNLBのターゲットとして設定します。各デバイスをNLBに接続します。"
      },
      {
        "key": "C",
        "text": "Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.",
        "text_jp": "AWS IoT Coreを設定し、各デバイスに対応するAWS IoT Thingを作成し、証明書を発行します。各デバイスをAWS IoT Coreに接続します。"
      },
      {
        "key": "D",
        "text": "Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB. Configure a mutual TLS certificate authorizer on the HTTP API. Run an MQTT broker on an Amazon EC2 instance that the NLB targets. Connect each device to the NLB.",
        "text_jp": "Amazon API Gateway HTTP APIとNetwork Load Balancer（NLB）を設定します。API GatewayとNLBとの統合を作成します。HTTP APIに相互TLS証明書オーソライザーを構成します。NLBがターゲットとするAmazon EC2インスタンスでMQTTブローカーを実行します。各デバイスをNLBに接続します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution meets the real-time communication requirement via MQTT and provides mutual TLS authentication.",
        "situation_analysis": "The requirement is to connect thousands of devices to the AWS cloud, enabling real-time data exchange using the MQTT protocol with each device requiring unique X.509 certificate authentication.",
        "option_analysis": "Option D effectively integrates AWS API Gateway with NLB and establishes mutual TLS for secure connections. Options A and C might add unnecessary operational overhead, while option B requires maintaining EC2 instances and an underlying MQTT broker, adding complexity.",
        "additional_knowledge": "Considering the nature of thousands of connections, AWS IoT Core would also effectively manage metadata and device states, unlike a self-hosted solution.",
        "key_terminology": "AWS IoT Core, MQTT, X.509 certificates, mutual TLS, operational overhead",
        "overall_assessment": "While community voting heavily favors option C, option D creates a scalable architecture while addressing security and real-time requirements adequately. Community votes may reflect a preference for simpler architectures but don’t consider operational complexities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。このソリューションは、MQTTを介したリアルタイム通信要件を満たし、相互TLS認証を提供する。",
        "situation_analysis": "要件は、何千ものデバイスをAWSクラウドに接続し、MQTTプロトコルを使用してリアルタイムでデータ交換を可能にし、各デバイスがユニークなX.509証明書による認証を必要とすること。",
        "option_analysis": "選択肢Dは、AWS API GatewayとNLBを効果的に統合し、安全な接続のために相互TLSを確立する。選択肢AとCは不要な運用上の負担を追加する可能性があり、一方選択肢BはEC2インスタンスとMQTTブローカーの維持が必要で、複雑さを追加する。",
        "additional_knowledge": "数千の接続の性質を考慮すると、AWS IoT Coreは、自己ホスト型のソリューションとは異なり、メタデータやデバイス状態を効率的に管理することもできる。",
        "key_terminology": "AWS IoT Core、MQTT、X.509証明書、相互TLS、運用上の負担",
        "overall_assessment": "コミュニティの投票は選択肢Cを圧倒的に支持しているが、選択肢Dはスケーラブルなアーキテクチャを構築し、セキュリティとリアルタイム要件を適切に対処している。コミュニティの投票は単純なアーキテクチャの好みを反映しているかもしれないが、運用の複雑さを考慮していない。"
      }
    ],
    "keywords": [
      "AWS IoT Core",
      "MQTT",
      "X.509 certificates",
      "mutual TLS",
      "operational overhead"
    ]
  },
  {
    "No": "74",
    "question": "A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved\nresources and that engineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to\nenforce the new restriction on the IAM role that the engineers use for access.\nWhat should the solutions architect do to create the solution?",
    "question_jp": "1つのAWSアカウントで複数のワークロードを実行している会社があります。新しい会社方針により、エンジニアは承認されたリソースのみをプロビジョニングでき、エンジニアはこれらのリソースをプロビジョニングするためにAWS CloudFormationを使用しなければならないことが定められました。ソリューションアーキテクトは、エンジニアがアクセスするために使用するIAMロールに新しい制限を強制するソリューションを作成する必要があります。ソリューションアーキテクトは、どのようにしてこのソリューションを作成すべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket. Update the IAM policy for the engineers' IAM role to only allow access to Amazon S3 and AWS CloudFormation. Use AWS CloudFormation templates to provision resources.",
        "text_jp": "承認されたリソースを含むAWS CloudFormationテンプレートをAmazon S3バケットにアップロードします。エンジニアのIAMロールのIAMポリシーを更新して、Amazon S3およびAWS CloudFormationへのアクセスのみを許可します。AWS CloudFormationテンプレートを使用してリソースをプロビジョニングします。"
      },
      {
        "key": "B",
        "text": "Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.",
        "text_jp": "承認されたリソースおよびAWS CloudFormationのプロビジョニングのみを許可するように、エンジニアのIAMロールのIAMポリシーを更新します。承認されたリソースを使用してAWS CloudFormationテンプレートでスタックを作成します。"
      },
      {
        "key": "C",
        "text": "Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.",
        "text_jp": "エンジニアのIAMロールのIAMポリシーをAWS CloudFormationアクションのみに許可するように更新します。承認されたリソースをプロビジョニングするための権限を持つ新しいIAMポリシーを作成し、そのポリシーを新しいIAMサービスロールに割り当てます。スタック作成時にAWS CloudFormationにIAMサービスロールを割り当てます。"
      },
      {
        "key": "D",
        "text": "Provision resources in AWS CloudFormation stacks. Update the IAM policy for the engineers' IAM role to only allow access to their own AWS CloudFormation stack.",
        "text_jp": "AWS CloudFormationスタックでリソースをプロビジョニングします。エンジニアのIAMロールのIAMポリシーを更新して、彼ら自身のAWS CloudFormationスタックへのアクセスのみを許可します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, updating the IAM policy for the engineers' IAM role to only allow provisioning of approved resources and AWS CloudFormation.",
        "situation_analysis": "The company policy requires that engineers can only provision approved resources using AWS CloudFormation. Therefore, their permissions need to be restricted.",
        "option_analysis": "Option B correctly addresses the policy by restricting permissions solely to approved resources and AWS CloudFormation usage, ensuring compliance. Option A involves S3 which is unnecessary. Option C complicates the architecture unnecessarily with a new service role. Option D does not enforce the resource approval process effectively.",
        "additional_knowledge": "Ensuring secure and compliant resource provisioning enhances operational efficiency and reduces risk.",
        "key_terminology": "AWS CloudFormation, IAM policy, provisioning, approved resources",
        "overall_assessment": "Option B is the best choice as it directly implements the policy requirements mandated by the new company policy. Community votes indicate a strong preference for a more complex approach, although simpler solutions often prove to be more effective."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、エンジニアのIAMロールのIAMポリシーを更新して、承認されたリソースとAWS CloudFormationのプロビジョニングのみを許可します。",
        "situation_analysis": "会社の方針では、エンジニアはAWS CloudFormationを使用して承認されたリソースのみをプロビジョニングできる必要があります。したがって、彼らの権限は制限する必要があります。",
        "option_analysis": "選択肢Bは、承認されたリソースとAWS CloudFormationの使用にのみ権限を制限することで、ポリシーに正しく対応しており、コンプライアンスを確保します。選択肢Aは、S3を含んでおり不要です。選択肢Cは、不要に新しいサービスロールを作成して複雑にします。選択肢Dは、リソースの承認プロセスを効果的に強制しません。",
        "additional_knowledge": "安全でコンプライアンスの取れたリソースプロビジョニングを確保することで、運用効率が向上し、リスクが低減します。",
        "key_terminology": "AWS CloudFormation, IAMポリシー, プロビジョニング, 承認されたリソース",
        "overall_assessment": "選択肢Bは、会社の新しいポリシーによって求められる要件を直接実装するため、最良の選択です。コミュニティの投票は、より複雑なアプローチに対する強い好みを示していますが、シンプルなソリューションがしばしばより効果的であることが証明されています。"
      }
    ],
    "keywords": [
      "AWS CloudFormation",
      "IAM policy",
      "provisioning",
      "approved resources"
    ]
  },
  {
    "No": "75",
    "question": "A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The\napplication is designed to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and\nneeds to be stored in a durable location where it can be retrieved with low latency. The data is ephemeral and the company is required to store the\ndata for 120 days only, after which the data can be deleted.\nThe solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.\nWhich storage strategy is the MOST cost-effective and meets the design requirements?",
    "question_jp": "ソリューションアーキテクトが、企業が近日中に立ち上げる新しいアプリケーションのデータストレージおよび取得アーキテクチャを設計しています。このアプリケーションは、世界中のデバイスから毎分数百万の小さなレコードを取り込むように設計されています。各レコードは4 KB未満のサイズで、耐久性のある場所に保存され、低遅延で取得できる必要があります。データは一時的なもので、企業は120日間のみデータを保存する必要があり、その後データを削除できます。ソリューションアーキテクトは、年間を通じて、ストレージ要件が約10-15 TBになると計算しています。どのストレージ戦略が最もコスト効率が良く、設計要件を満たすのでしょうか?",
    "choices": [
      {
        "key": "A",
        "text": "Design the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieval. Configure a lifecycle policy to delete data older than 120 days.",
        "text_jp": "アプリケーションを設計し、すべての受信レコードを単一の.csvファイルとしてAmazon S3バケットに保存し、インデックス付きの取得を可能にします。ライフサイクルポリシーを構成して、120日以上前のデータを削除します。"
      },
      {
        "key": "B",
        "text": "Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days.",
        "text_jp": "アプリケーションを設計し、すべての受信レコードを適切にスケール設定されたAmazon DynamoDBテーブルに保存します。DynamoDBのTime to Live (TTL)機能を構成して、120日以上古いレコードを削除します。"
      },
      {
        "key": "C",
        "text": "Design the application to store each incoming record in a single table in an Amazon RDS MySQL database. Run a nightly cron job that runs a query to delete any records older than 120 days.",
        "text_jp": "アプリケーションを設計し、すべての受信レコードを単一のテーブルにAmazon RDS MySQLデータベースに保存します。毎晩のcronジョブを実行して、120日以上古いレコードを削除するクエリを実行します。"
      },
      {
        "key": "D",
        "text": "Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days.",
        "text_jp": "アプリケーションを設計し、受信レコードをバッチ処理してからAmazon S3バケットに書き込みます。オブジェクトのメタデータを更新してバッチ内のレコードのリストを含め、Amazon S3のメタデータ検索機能を使用してデータを取得します。ライフサイクルポリシーを構成して、120日後にデータを削除します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (75%) D (25%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Using Amazon DynamoDB with TTL provides a scalable and cost-effective solution for the described requirements.",
        "situation_analysis": "The application needs to handle millions of small records efficiently and store data for a limited time (120 days). It requires a low-latency retrieval mechanism and must be cost-effective.",
        "option_analysis": "Option A involves storing data in S3, which offers durability but may not provide the required low latency for individual record retrieval. Option C using RDS would be costly and complex for the required scale. Option D, while allowing S3 usage, does not utilize the inherent benefits of DynamoDB for rapid lookups and TTL management.",
        "additional_knowledge": "Using DynamoDB aligns with AWS best practices for ephemeral data needs, allowing easy scaling and efficient data management.",
        "key_terminology": "DynamoDB, Time to Live (TTL), low latency, scalable technology",
        "overall_assessment": "Considering the requirements, option B stands out as it meets both cost-effectiveness and performance expectations. The high community vote for B reinforces this choice, indicating strong support."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。Amazon DynamoDBをTTLとともに使用することは、述べられた要件に対してスケーラブルでコスト効率の良いソリューションを提供する。",
        "situation_analysis": "アプリケーションは、数百万の小さなレコードを効率的に処理し、限られた期間（120日間）データを保存する必要がある。低遅延の取得メカニズムが必要であり、コスト効果も求められる。",
        "option_analysis": "オプションAはS3にデータを保存することを含むが、耐久性はあるものの、個々のレコード取得のために必要な低遅延を提供しない可能性がある。オプションCはRDSを使用するが、必要なスケールに対してコストがかかり、複雑になる。オプションDもS3を使用するが、高速な検索とTTL管理のためにDynamoDBの固有の利点を活用していない。",
        "additional_knowledge": "DynamoDBを使用することは、AWSのベストプラクティスに沿った短命データのニーズに合致し、簡単にスケーリングして効率的なデータ管理を可能にする。",
        "key_terminology": "DynamoDB、Time to Live (TTL)、低遅延、スケーラブル技術",
        "overall_assessment": "要件を考慮すると、オプションBがコスト効果とパフォーマンスの期待の両方を満たしている。Bへの高いコミュニティ投票は、この選択を裏付けており、強い支持を示している。"
      }
    ],
    "keywords": [
      "DynamoDB",
      "Time to Live (TTL)",
      "low latency",
      "scalable technology"
    ]
  },
  {
    "No": "76",
    "question": "A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all\ntimes for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.\nWhich solution will provide the HIGHEST availability for the database?",
    "question_jp": "小売企業は、複数のAWSリージョンにわたってeコマースウェブサイトをホストしています。この企業は、オンライン購入のためにウェブサイトを常に稼働させたいと考えています。ウェブサイトは、Amazon RDS for MySQL DBインスタンスにデータを保存しています。データベースの可用性を最も高く保つためのソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance. Direct database trafic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.",
        "text_jp": "Amazon RDSで自動バックアップを設定する。中断が発生した場合、自動バックアップをスタンドアロンDBインスタンスに昇格させ、データベーストラフィックを昇格されたDBインスタンスに向ける。昇格されたDBインスタンスをソースとして新しい読み取りレプリカを作成する。"
      },
      {
        "key": "B",
        "text": "Configure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.",
        "text_jp": "Amazon RDSでグローバルテーブルと読み取りレプリカを設定する。クロスリージョンのスコープを有効にする。中断が発生した場合、AWS Lambdaを使用してレプリカをあるリージョンから別のリージョンにコピーする。"
      },
      {
        "key": "C",
        "text": "Configure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.",
        "text_jp": "Amazon RDSでグローバルテーブルと自動バックアップを設定する。中断が発生した場合、AWS Lambdaを使用してレプリカをあるリージョンから別のリージョンにコピーする。"
      },
      {
        "key": "D",
        "text": "Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database trafic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.",
        "text_jp": "Amazon RDSで読み取りレプリカを設定する。中断が発生した場合、クロスリージョンの読み取りレプリカをスタンドアロンDBインスタンスに昇格させ、データベーストラフィックを昇格されたDBインスタンスに向ける。昇格されたDBインスタンスをソースとして新しい読み取りレプリカを作成する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, as it provides the highest availability for the database by promoting a cross-Region read replica to a standalone DB instance during disruptions.",
        "situation_analysis": "The company requires continuous availability of its e-commerce site, which necessitates robust solutions for database redundancy and failover in case of service interruptions.",
        "option_analysis": "Option D aligns with AWS best practices for high availability. Options A, B, and C do not provide a direct method for ensuring that a promoted instance is immediately available for traffic in case of a failure.",
        "additional_knowledge": "The architecture should consider data synchronization and latency when implementing cross-region solutions.",
        "key_terminology": "Amazon RDS, read replica, high availability, cross-Region, failover, database instance",
        "overall_assessment": "D is the best choice for ensuring high availability in multi-region deployments. The community overwhelmingly supports this choice, reflecting a solid understanding of best practices for AWS database configurations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDであり、これは中断時にクロスリージョンの読み取りレプリカをスタンドアロンDBインスタンスに昇格させることによって、データベースに対して最高の可用性を提供します。",
        "situation_analysis": "企業はeコマースサイトの継続的な可用性を要求しており、サービス中断時にデータベースの冗長性とフェイルオーバーのための堅牢なソリューションを必要とします。",
        "option_analysis": "オプションDは、高可用性のためのAWSのベストプラクティスと一致しています。オプションA、B、およびCは、失敗時に昇格されたインスタンスがすぐにトラフィックに利用可能であることを保証する直接的な方法を提供していません。",
        "additional_knowledge": "アーキテクチャはクロスリージョンソリューションを実装する際にデータ同期とレイテンシを考慮する必要があります。",
        "key_terminology": "Amazon RDS、読み取りレプリカ、高可用性、クロスリージョン、フェイルオーバー、データベースインスタンス",
        "overall_assessment": "Dはマルチリージョン環境で高可用性を確保するための最良の選択肢です。コミュニティはこの選択肢を圧倒的に支持しており、AWSデータベース構成のベストプラクティスに対する確固とした理解を反映しています。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "read replica",
      "high availability",
      "cross-Region",
      "failover",
      "database instance"
    ]
  },
  {
    "No": "77",
    "question": "Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to\nVPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A. Example Corp. just acquired AnyCompany, which\nhas a VPC named VPC B. There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B.\nExample Corp. wants to connect from its on-premise servers to VPC B. Example Corp. has properly set up the network ACL and security groups.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "question_jp": "Example Corp.にはオンプレミスのデータセンターと、Example Corp.のAWSアカウント内にVPC Aがあります。オンプレミスネットワークはAWS Site-To-Site VPNを介してVPC Aに接続しています。オンプレミスサーバーはVPC Aに適切にアクセスできます。Example Corp.はAnyCompanyを購入したばかりで、VPC Bがあります。これらのネットワーク間にIPアドレスの重複はありません。Example Corp.はVPC AとVPC Bをピアリングしました。Example Corp.はオンプレミスサーバーからVPC Bに接続したいと考えています。Example Corp.はネットワークACLとセキュリティグループを適切に設定しています。どのソリューションが最も運用の手間をかけずにこの要件を満たすことができますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.",
        "text_jp": "トランジットゲートウェイを作成します。Site-to-Site VPN、VPC A、VPC Bをトランジットゲートウェイに接続します。すべてのネットワークのためにIP範囲のルートを追加するために、トランジットゲートウェイのルートテーブルを更新します。"
      },
      {
        "key": "B",
        "text": "Create a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN connection to the transit gateway. Add a route to direct trafic to the peered VPCs, and add an authorization rule to give clients access to the VPCs A and B.",
        "text_jp": "トランジットゲートウェイを作成します。オンプレミスネットワークとVPC Bの間にSite-to-Site VPN接続を作成し、そのVPN接続をトランジットゲートウェイに接続します。ピアリングされたVPCへのトラフィックを指導するためのルートを追加し、VPC AとBへのアクセスをクライアントに付与するための認可ルールを追加します。"
      },
      {
        "key": "C",
        "text": "Update the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks. Wait for up to 5 minutes for BGP propagation to finish.",
        "text_jp": "Site-to-Site VPNおよび両方のVPCのルートテーブルをすべてのネットワークについて更新します。3つのネットワークに対してBGP伝播を構成します。BGPの伝播が完了するまで最大5分待ちます。"
      },
      {
        "key": "D",
        "text": "Modify the Site-to-Site VPN's virtual private gateway definition to include VPC A and VPC B. Split the two routers of the virtual private getaway between the two VPCs.",
        "text_jp": "Site-to-Site VPNの仮想プライベートゲートウェイの定義を変更して、VPC AおよびVPC Bを含めます。仮想プライベートゲートウェイの2つのルーターを2つのVPC間で分割します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (87%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, as modifying the Site-to-Site VPN's virtual private gateway definition to include both VPC A and VPC B allows the on-premises servers to connect to both VPCs with minimal effort.",
        "situation_analysis": "Example Corp. needs to facilitate communication from its on-premises servers to VPC B, which requires a proper configuration of the VPN and VPC settings.",
        "option_analysis": "Option D is the least operational effort as compared to the others, which may involve additional components and configurations. The other options require setting up transit gateways or managing route tables more comprehensively.",
        "additional_knowledge": "AWS best practices suggest keeping setups as simple as possible while ensuring connectivity between networks.",
        "key_terminology": "Site-to-Site VPN, virtual private gateway, VPC peering, network ACL, security group.",
        "overall_assessment": "The community votes heavily favor option A, but D effectively addresses the need with less complexity in execution. Peer connections and direct modifications of VPN settings offer a straightforward solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDであり、Site-to-Site VPNの仮想プライベートゲートウェイの定義を変更してVPC AとVPC Bの両方を含めることで、オンプレミスサーバーが最小限の手間で両方のVPCに接続できるようになります。",
        "situation_analysis": "Example Corp.はオンプレミスサーバーからVPC Bへの通信を促進する必要があり、これはVPNおよびVPC設定の適切な構成を要求します。",
        "option_analysis": "オプションDは、他の選択肢と比較すると運用の手間が最も少なく、追加のコンポーネントや設定を必要としません。他のオプションはトランジットゲートウェイの設定やルートテーブルの管理をより包括的に要求します。",
        "additional_knowledge": "AWSのベストプラクティスは、接続性を確保しつつ、設定をできるだけシンプルに保つことを推奨しています。",
        "key_terminology": "Site-to-Site VPN、仮想プライベートゲートウェイ、VPCピアリング、ネットワークACL、セキュリティグループ。",
        "overall_assessment": "コミュニティの投票はAを強く支持していますが、Dは実行の複雑さを減らしつつ必要性に効果的に対応します。ピア接続およびVPN設定の直接的な変更は簡潔な解決策を提供します。"
      }
    ],
    "keywords": [
      "Site-to-Site VPN",
      "virtual private gateway",
      "VPC peering",
      "network ACL",
      "security group"
    ]
  },
  {
    "No": "78",
    "question": "A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the\nmigrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends\noutbound email messages to the company's customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The\napplication can use SMTP only.\nThe company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has\ncreated and validated the SES domain. The company has lifted the SES limits.\nWhat should the company do to modify the application to send email messages from Amazon SES?",
    "question_jp": "企業は最近、リプラットフォーミング戦略を用いて自社のオンプレミスデータセンターからAWSクラウドへの移行を完了しました。移行されたサーバーの1つは、重要なアプリケーションが依存しているレガシーのシンプルメール転送プロトコル（SMTP）サービスを実行しています。このアプリケーションは、企業の顧客に向けてアウトバウンドメールメッセージを送信します。レガシーSMTPサーバーはTLS暗号化をサポートしておらず、TCPポート25を使用しています。アプリケーションはSMTPのみを使用できます。企業はAmazon Simple Email Service（Amazon SES）を利用することを決定し、レガシーSMTPサーバーを廃止することにしました。企業はSESドメインを作成し、確認しました。企業はSES制限を引き上げました。企業は、アプリケーションを変更してAmazon SESからメールメッセージを送信するために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.",
        "text_jp": "アプリケーションをTLSラッパーを使用してAmazon SESに接続するように設定します。ses:SendEmailおよびses:SendRawEmail権限を持つIAMロールを作成し、そのIAMロールをAmazon EC2インスタンスにアタッチします。"
      },
      {
        "key": "B",
        "text": "Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.",
        "text_jp": "アプリケーションをSTARTTLSを使用してAmazon SESに接続するように設定します。Amazon SES SMTP認証情報を取得します。認証情報を使用してAmazon SESに認証します。"
      },
      {
        "key": "C",
        "text": "Configure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Use the IAM role as a service role for Amazon SES.",
        "text_jp": "アプリケーションをSES APIを使用してメールメッセージを送信するように設定します。ses:SendEmailおよびses:SendRawEmail権限を持つIAMロールを作成します。そのIAMロールをAmazon SESのサービスロールとして使用します。"
      },
      {
        "key": "D",
        "text": "Configure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use the access keys to authenticate with Amazon SES.",
        "text_jp": "アプリケーションをAWS SDKを使用してメールメッセージを送信するように設定します。Amazon SES用のIAMユーザーを作成します。APIアクセスキーを生成します。アクセスキーを使用してAmazon SESに認証します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (85%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This approach provides a way to connect to Amazon SES using TLS while maintaining the necessary permissions to send emails.",
        "situation_analysis": "The application uses SMTP to send emails but requires TLS for secure transmission. Option A allows for a connection to Amazon SES using TLS Wrapper, which aligns with the need for security.",
        "option_analysis": "Option B is not correct, as the application cannot use STARTTLS due to its reliance on legacy SMTP. Option C suggests using SES API, which would require significant changes to the application architecture. Option D involves using AWS SDKs but does not address the SMTP requirement.",
        "additional_knowledge": "AWS documentation indicates that integrating applications with Amazon SES through SMTP while adhering to security best practices like TLS is essential.",
        "key_terminology": "Amazon SES, SMTP, TLS Wrapper, IAM role, email permissions",
        "overall_assessment": "Although community votes heavily favor Option B, the question's specific requirements dictate that Option A is the valid choice. Community preferences can sometimes diverge from technical requirements, highlighting the need for clarity in software functionality."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。このアプローチは、必要なメール送信権限を維持しながら、TLSを使用してAmazon SESに接続する方法を提供する。",
        "situation_analysis": "アプリケーションはSMTPを使用してメールを送信するが、安全な送信のためにTLSを必要とする。選択肢Aは、セキュリティの必要性に沿った方法でAmazon SESにTLSラッパーを使用して接続できる。",
        "option_analysis": "選択肢Bは誤りである。アプリケーションはレガシーSMTPに依存しているため、STARTTLSを使用できない。選択肢Cは、SES APIを使用することを提案しており、アプリケーションアーキテクチャに大きな変更が必要となる。選択肢DはAWS SDKを使用することを含むが、SMTPの要件を満たしていない。",
        "additional_knowledge": "AWS文書では、TLSのようなセキュリティベストプラクティスを守りながら、アプリケーションとAmazon SESをSMTP経由で統合することが重要であると述べられている。",
        "key_terminology": "Amazon SES、SMTP、TLSラッパー、IAMロール、メール権限",
        "overall_assessment": "コミュニティの投票が選択肢Bに偏っているが、質問の具体的な要件から、選択肢Aが有効な選択肢である。コミュニティの好みは技術的要件と異なる場合があり、ソフトウェア機能の明確さが必要であることを強調している。"
      }
    ],
    "keywords": [
      "Amazon SES",
      "SMTP",
      "TLS Wrapper",
      "IAM role",
      "email permissions"
    ]
  },
  {
    "No": "79",
    "question": "A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method.\nThe acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found\nit dificult to generate a cost report that contains meaningful groups for all the teams.\nThe acquiring company's finance team needs a solution to report on costs for all the companies through a self-managed application.\nWhich solution will meet these requirements?",
    "question_jp": "企業が最近、いくつかの他の企業を買収しました。各企業は異なる請求および報告方法を持つ別々のAWSアカウントを持っています。買収した企業は、すべてのアカウントをAWS Organizationsで一つの組織に統合しました。しかし、買収した企業は、すべてのチームにとって意味のあるグループを含むコストレポートを生成することが困難であることを発見しました。買収した企業の財務チームは、すべての企業のコストを自己管理できるアプリケーションを通じて報告するためのソリューションを必要としています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team.",
        "text_jp": "AWSコストと使用状況レポートを組織用に作成します。レポート内でタグとコストカテゴリを定義します。Amazon Athenaにテーブルを作成します。Athenaテーブルに基づいてAmazon QuickSightデータセットを作成します。データセットを財務チームと共有します。"
      },
      {
        "key": "B",
        "text": "Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.",
        "text_jp": "AWSコストと使用状況レポートを組織用に作成します。レポート内でタグとコストカテゴリを定義します。財務部門がレポートを作成するために使用するAWS Cost Explorerの特別なテンプレートを作成します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the finance team.",
        "text_jp": "AWS価格リストクエリAPIから支出情報を受け取るAmazon QuickSightデータセットを作成します。データセットを財務チームと共有します。"
      },
      {
        "key": "D",
        "text": "Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.",
        "text_jp": "AWS価格リストクエリAPIを使用してアカウントの支出情報を収集します。財務部門がレポートを作成するために使用するAWS Cost Explorerの特別なテンプレートを作成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.",
        "situation_analysis": "The company needs a consolidated cost report that can be generated easily for all the acquired companies.",
        "option_analysis": "Option D directly addresses the need to utilize the AWS Price List Query API for detailed account spending information, whereas the other options do not fully satisfy the requirement for a self-managed reporting application.",
        "additional_knowledge": "",
        "key_terminology": "AWS Price List Query API, AWS Cost Explorer, account spending information.",
        "overall_assessment": "While community support heavily votes for option A, option D provides a more personalized reporting mechanism tailored to the finance team's requirements. Understanding the specific needs of reporting can render community opinions less aligned with strategic business needs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDです：AWS価格リストクエリAPIを使用してアカウントの支出情報を収集し、財務部門がレポートを作成するために使う特別なテンプレートをAWS Cost Explorer内に作成します。",
        "situation_analysis": "企業は、買収したすべての企業のために、簡単に生成できる統合コストレポートを必要としています。",
        "option_analysis": "オプションDは、詳細なアカウント支出情報のためにAWS価格リストクエリAPIを利用するというニーズに直接対処していますが、他のオプションは自己管理可能な報告アプリケーションに関する要件を完全には満たしていません。",
        "additional_knowledge": "",
        "key_terminology": "AWS価格リストクエリAPI, AWS Cost Explorer, アカウント支出情報。",
        "overall_assessment": "コミュニティの支持はAオプションに偏っているが、Dオプションは財務チームの要件に合わせたよりパーソナライズされた報告機構を提供します。報告の特定のニーズを理解すると、コミュニティの意見が戦略的ビジネスニーズと一致しないことがあります。"
      }
    ],
    "keywords": [
      "AWS Price List Query API",
      "AWS Cost Explorer",
      "account spending information"
    ]
  },
  {
    "No": "80",
    "question": "A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company's Node.js API servers on Amazon EC2\ninstances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General\nPurpose SSD volume.\nThe number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are\nconsistently overloaded and RDS metrics show high write latency.\nWhich of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this\nplatform cost-eficient? (Choose two.)",
    "question_jp": "ある企業がAWS上でIoTプラットフォームを運営しています。さまざまな場所に配置されたIoTセンサーが、アプリケーションロードバランサの背後で稼働しているAmazon EC2インスタンス上の同社のNode.js APIサーバーにデータを送信します。このデータは、4 TBのGeneral Purpose SSDボリュームを使用しているAmazon RDS MySQL DBインスタンスに保存されます。企業が配備したセンサーの数は時間とともに増加し、今後も大幅に成長することが予想されます。APIサーバーは常に過負荷状態であり、RDSのメトリクスは高い書き込み待機時間を示しています。次のステップのうち、どれを組み合わせることで、これらの問題を恒久的に解決し、新しいセンサーがプロビジョニングされるにつれて成長を可能にする一方で、このプラットフォームをコスト効率よく維持することができますか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS.",
        "text_jp": "MySQLのGeneral Purpose SSDストレージを6 TBにリサイズして、ボリュームのIOPSを改善する。"
      },
      {
        "key": "B",
        "text": "Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.",
        "text_jp": "データベース層をRDS MySQL DBインスタンスの代わりにAmazon Auroraを使用するように再アーキテクチャし、リードレプリカを追加する。"
      },
      {
        "key": "C",
        "text": "Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.",
        "text_jp": "Amazon Kinesis Data StreamsとAWS Lambdaを利用して、ローデータを取り込み処理する。"
      },
      {
        "key": "D",
        "text": "Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.",
        "text_jp": "AWS X-Rayを使用してアプリケーションの問題を分析・デバッグし、負荷に応じてAPIサーバーを追加する。"
      },
      {
        "key": "E",
        "text": "Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.",
        "text_jp": "データベース層をRDS MySQL DBインスタンスの代わりにAmazon DynamoDBを使用するように再アーキテクチャする。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "CE (63%) BC (18%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The selected correct answer is C: Leverage Amazon Kinesis Data Streams and AWS Lambda. This approach allows for scalable, real-time ingestion and processing of data from IoT sensors.",
        "situation_analysis": "The company is experiencing API server overload and high write latency in the RDS MySQL DB due to the increased number of IoT sensors deployed in the field. There is a need for a solution that can support growth and maintain efficiency.",
        "option_analysis": "Option C provides a scalable solution for data ingestion, which helps relieve pressure on the API servers and the database. Other options either do not address the immediate need for scalability or would involve more complexity and potential costs.",
        "additional_knowledge": "In addition to Kinesis and Lambda, considering other data storage possibilities in tandem with DynamoDB may offer enhanced performance for certain workloads.",
        "key_terminology": "Amazon Kinesis, AWS Lambda, IoT, scalability, real-time processing, cost-efficiency.",
        "overall_assessment": "Given the community vote distribution, which favors alternative solutions (E and C), the choice of C provides a balanced approach to immediate scaling needs. Additional solutions like moving to DynamoDB could also be considered for specific use cases."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "選択された正解はCです：Amazon Kinesis Data StreamsとAWS Lambdaを利用する。このアプローチでは、IoTセンサーからのデータをスケーラブルにリアルタイムで取り込み、処理することが可能になります。",
        "situation_analysis": "この企業は、フィールドに配置されたIoTセンサーの数が増加したため、APIサーバーが過負荷状態になり、RDS MySQL DBで高い書き込み待機時間が発生しています。成長を支え、効率を維持する解決策が必要です。",
        "option_analysis": "Cの選択肢はデータ取り込みのためのスケーラブルなソリューションを提供し、APIサーバーとデータベースの負荷を軽減します。他の選択肢は即時のスケーラビリティのニーズに対応していなかったり、より複雑でコストがかかる可能性があります。",
        "additional_knowledge": "KinesisとLambdaに加えて、DynamoDBとの併用で特定のワークロードに対するパフォーマンスの向上も検討可能です。",
        "key_terminology": "Amazon Kinesis, AWS Lambda, IoT, スケーラビリティ, リアルタイム処理, コスト効率。",
        "overall_assessment": "コミュニティの投票分布を考慮すると、EやCといった代替ソリューションを支持する傾向がありますが、Cの選択は即時のスケーリングニーズへのバランスの取れたアプローチを提供します。特定のユースケースに応じてDynamoDBへの移行など、他のソリューションの検討も可能です。"
      }
    ],
    "keywords": [
      "Amazon Kinesis",
      "AWS Lambda",
      "IoT",
      "scalability",
      "real-time processing"
    ]
  },
  {
    "No": "81",
    "question": "A company is building an electronic document management system in which users upload their documents. The application stack is entirely\nserverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for\ndelivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs\ncall AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.\nThe company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of\nEurope.\nWhich combination of actions will meet these requirements? (Choose two.)",
    "question_jp": "ある企業が、ユーザーが自身の文書をアップロードする電子文書管理システムを構築しています。このアプリケーションスタックは完全にサーバーレスであり、AWSのeu-central-1リージョンで稼働しています。このシステムは、Amazon S3を起源とし、配信のためにAmazon CloudFrontディストリビューションを使用するWebアプリケーションを含んでいます。Webアプリケーションは、Amazon API Gatewayのリージョナルエンドポイントと通信します。API Gateway APIsは、メタデータをAmazon Aurora Serverlessデータベースに保存し、文書をS3バケットに格納するAWS Lambda関数を呼び出します。この企業は着実に成長しており、最大の顧客との検証を完了しました。そのため、企業はヨーロッパ以外でのレイテンシを改善しなければなりません。どの組み合わせのアクションがこれらの要件を満たしますか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs.",
        "text_jp": "S3バケットでS3 Transfer Accelerationを有効にします。WebアプリケーションがTransfer Accelerationの署名付きURLを使用することを確認します。"
      },
      {
        "key": "B",
        "text": "Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution.",
        "text_jp": "AWS Global Acceleratorでアクセラレーターを作成します。アクセラレーターをCloudFrontディストリビューションに接続します。"
      },
      {
        "key": "C",
        "text": "Change the API Gateway Regional endpoints to edge-optimized endpoints.",
        "text_jp": "API Gatewayのリージョナルエンドポイントをエッジ最適化エンドポイントに変更します。"
      },
      {
        "key": "D",
        "text": "Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.",
        "text_jp": "スタック全体を世界中に広がる2つの別のロケーションにプロビジョニングします。Aurora Serverlessクラスターでグローバルデータベースを使用します。"
      },
      {
        "key": "E",
        "text": "Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database.",
        "text_jp": "Lambda関数とAurora Serverlessデータベースの間にAmazon RDSプロキシを追加します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (45%) CD (40%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and C. Enabling S3 Transfer Acceleration reduces latency for S3 uploads and signed URLs allow the web application to use this feature effectively. Changing the API Gateway Regional endpoints to edge-optimized improves response times for users globally by routing requests to the nearest edge location.",
        "situation_analysis": "The company needs to improve latency outside of Europe for their electronic document management system which uses serverless architecture on AWS.",
        "option_analysis": "Option A provides improved upload speed due to S3 Transfer Acceleration. Option C enhances API response times by converting to edge-optimized endpoints, which is beneficial as the application is being accessed from various regions outside Europe. Options B, D, and E do not effectively address the primary concern of reducing latency for the existing architecture.",
        "additional_knowledge": "It is essential to recognize the differences between regional and edge-optimized endpoints based on the application's geographical access patterns.",
        "key_terminology": "S3 Transfer Acceleration, Amazon CloudFront, API Gateway, edge-optimized endpoints, serverless architecture",
        "overall_assessment": "Choosing A and C aligns with AWS best practices for latency improvements, particularly for users located outside Europe. The community vote reflects support for these options, indicating a consensus on their effectiveness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAとCである。S3 Transfer Accelerationを有効にすることで、S3へのアップロードのレイテンシが減少し、署名付きURLを使用することでWebアプリケーションがこの機能を効果的に活用できる。API Gatewayのリージョナルエンドポイントをエッジ最適化エンドポイントに変更することで、グローバルにユーザーの応答時間が改善され、リクエストが最寄りのエッジロケーションにルーティングされる。",
        "situation_analysis": "この企業は、AWS上でサーバーレスアーキテクチャを使用する電子文書管理システムのヨーロッパ以外でのレイテンシを改善する必要がある。",
        "option_analysis": "オプションAは、S3 Transfer Accelerationによるアップロード速度の向上を提供する。オプションCは、エッジ最適化エンドポイントに変更することによりAPIの応答時間を向上させ、ヨーロッパ以外の地域からアプリケーションにアクセスする際に有益である。オプションB、D、Eは、既存のアーキテクチャのレイテンシを減少させるという主要な懸念に効果的に対処していない。",
        "additional_knowledge": "アプリケーションの地理的アクセスパターンに基づいて、リージョナルエンドポイントとエッジ最適化エンドポイントの違いを認識することが重要である。",
        "key_terminology": "S3 Transfer Acceleration, Amazon CloudFront, API Gateway, エッジ最適化エンドポイント, サーバーレスアーキテクチャ",
        "overall_assessment": "AとCを選択することは、特にヨーロッパ以外にいるユーザーに対してレイテンシを改善するためのAWSのベストプラクティスに沿っている。コミュニティの投票はこれらのオプションを支持しており、その効果に対する合意を示している。"
      }
    ],
    "keywords": [
      "S3 Transfer Acceleration",
      "Amazon CloudFront",
      "API Gateway",
      "edge-optimized endpoints",
      "serverless architecture"
    ]
  },
  {
    "No": "82",
    "question": "An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and\nvideos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.\nThe company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are\naccessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions\narchitect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.\nWhich solution will meet these requirements?",
    "question_jp": "冒険会社は、モバイルアプリに新機能を導入しました。ユーザーはこの機能を使って、いつでもハイキングやラフティングの写真とビデオをアップロードできます。写真とビデオはAmazon S3 Standardストレージに保存され、Amazon CloudFrontを通じて配信されます。この会社はストレージのコストを最適化する必要があります。ソリューションアーキテクトは、アップロードされた写真とビデオのほとんどが30日後にアクセスされることは稀であることを発見しました。しかし、一部のアップロードされた写真とビデオは30日後に頻繁にアクセスされています。ソリューションアーキテクトは、可能な限りコストを抑えつつ、写真とビデオのミリ秒単位の取得可用性を維持するソリューションを実装する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure S3 Intelligent-Tiering on the S3 bucket.",
        "text_jp": "S3バケットにS3 Intelligent-Tieringを構成する。"
      },
      {
        "key": "B",
        "text": "Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days.",
        "text_jp": "S3ライフサイクルポリシーを設定して、画像オブジェクトとビデオオブジェクトを30日後にS3 StandardからS3 Glacier Deep Archiveに移行する。"
      },
      {
        "key": "C",
        "text": "Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances.",
        "text_jp": "Amazon S3の代わりに、Amazon EC2インスタンスにマウントされたAmazon Elastic File System (Amazon EFS)ファイルシステムを使用する。"
      },
      {
        "key": "D",
        "text": "Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days.",
        "text_jp": "S3画像オブジェクトとS3ビデオオブジェクトにCache-Control: max-ageヘッダーを追加する。ヘッダーを30日に設定する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: implementing an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 30 days. This method significantly reduces storage costs while keeping retrieval options available, although objects may take longer to retrieve.",
        "situation_analysis": "The company requires a cost-effective storage solution while ensuring that infrequently accessed data can still be retrieved promptly when necessary. After 30 days, most content is accessed infrequently, except for some data that remains in demand.",
        "option_analysis": "Option A (S3 Intelligent-Tiering) offers automated transition between storage classes based on access patterns but can still incur higher costs due to frequent access in this scenario. Option C (EFS) is unsuitable due to higher costs than S3. Option D (Cache-Control header) only affects caching behavior and does not optimize storage costs.",
        "additional_knowledge": "The lifecycle management feature of S3 is vital for businesses managing large volumes of data and looking to optimize their cloud spending.",
        "key_terminology": "S3 Lifecycle policy, S3 Glacier Deep Archive, cost optimization, retrieval availability, S3 Intelligent-Tiering.",
        "overall_assessment": "Although community voting heavily favored option A, option B accurately meets the company's requirements for balancing cost with retrieval performance effectively. The significant community support for option A suggests a possible misunderstanding of the dynamics of infrequent access and cost structures."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBであり、30日後にオブジェクトをS3 Glacier Deep Archiveに移行するS3ライフサイクルポリシーを実装することです。この方法により、ストレージコストを大幅に削減しながら、ミリ秒単位の取得オプションを維持できますが、オブジェクトの取得にはより長い時間がかかる可能性があります。",
        "situation_analysis": "会社は、コスト効果の高いストレージソリューションを必要としながら、アクセス頻度の低いデータも必要に応じて迅速に取得できる保証を求めています。30日後、多くのコンテンツは頻繁にはアクセスされませんが、いくつかのデータは引き続き需要があります。",
        "option_analysis": "選択肢A（S3 Intelligent-Tiering）は、アクセスパターンに基づいてストレージクラス間での自動移行を提供しますが、このシナリオでは頻繁なアクセスによって依然として高いコストがかかる可能性があります。選択肢C（EFS）は、S3よりもコストが高いため不適切です。選択肢D（Cache-Controlヘッダー）は、キャッシュの動作にのみ影響し、ストレージコストを最適化しません。",
        "additional_knowledge": "S3のライフサイクル管理機能は、大量のデータを管理している企業にとって重要であり、クラウド支出を最適化する手助けとなります。",
        "key_terminology": "S3ライフサイクルポリシー、S3 Glacier Deep Archive、コスト最適化、取得可用性、S3 Intelligent-Tiering。",
        "overall_assessment": "コミュニティの投票は選択肢Aを非常に支持していますが、選択肢Bはコストと取得性能のバランスを効果的に満たしており、会社の要件に正確に合致しています。選択肢Aへの大幅なコミュニティ支持は、アクセス頻度の低いデータとコスト構造の動態に対する理解不足を示唆している可能性があります。"
      }
    ],
    "keywords": [
      "S3 Lifecycle policy",
      "S3 Glacier Deep Archive",
      "cost optimization",
      "retrieval availability",
      "S3 Intelligent-Tiering"
    ]
  },
  {
    "No": "83",
    "question": "A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during\nthe past year.\nA solutions architect needs to review data trends for the past 12 months and identity the appropriate storage class for the objects.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、さまざまなストレージクラスでファイルや画像を保存するためにAmazon S3を使用しています。この企業のS3コストは、昨年中に大幅に増加しました。\nソリューションアーキテクトは、過去12か月間のデータトレンドをレビューし、オブジェクトに適切なストレージクラスを特定する必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Download AWS Cost and Usage Reports for the last 12 months of S3 usage. Review AWS Trusted Advisor recommendations for cost savings.",
        "text_jp": "AWSのコストと使用量レポートをダウンロードして、過去12か月のS3使用状況を確認します。コスト削減のためのAWS Trusted Advisorの推奨をレビューします。"
      },
      {
        "key": "B",
        "text": "Use S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard to analyze storage trends.",
        "text_jp": "S3ストレージクラス分析を使用します。データトレンドをAmazon QuickSightダッシュボードにインポートして、ストレージトレンドを分析します。"
      },
      {
        "key": "C",
        "text": "Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends.",
        "text_jp": "Amazon S3 Storage Lensを使用します。デフォルトのダッシュボードをアップグレードして、ストレージトレンドの高度なメトリクスを含めます。"
      },
      {
        "key": "D",
        "text": "Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 months. Import the .csv file to an Amazon QuickSight dashboard.",
        "text_jp": "S3のアクセスアナライザーを使用します。過去12か月のレポートをダウンロードします。その.csvファイルをAmazon QuickSightダッシュボードにインポートします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (78%) 12% 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Using S3 storage class analysis provides a detailed insight into data access patterns, allowing for an appropriate storage class identification.",
        "situation_analysis": "The company has seen a significant increase in S3 costs and needs to analyze data trends over the past 12 months in order to optimize storage usage.",
        "option_analysis": "Option A focuses on cost and usage reports but does not specifically analyze storage classes. Option C provides metrics but requires an upgrade which might not be necessary initially. Option D deals with access analysis, which does not address storage class optimization directly.",
        "additional_knowledge": "Implementing this solution can lead to significant cost savings if infrequently accessed data is transitioned to cheaper storage classes.",
        "key_terminology": "Amazon S3, storage class analysis, Amazon QuickSight, cost management, data access patterns",
        "overall_assessment": "Given the requirements of analyzing storage usage to reduce costs, option B stands out as the most effective solution. The community vote indicates a strong inclination towards option C, which may suggest a misunderstanding of the specific needs for storage class identification rather than just trend analysis."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。S3ストレージクラス分析を使用することで、データアクセスパターンに関する詳細な洞察を得ることができ、適切なストレージクラスを特定することが可能である。",
        "situation_analysis": "企業はS3コストが大幅に増加したことを受けて、過去12か月のデータトレンドを分析し、ストレージ使用の最適化を図る必要がある。",
        "option_analysis": "選択肢Aはコストと使用量レポートに焦点を当てているが、ストレージクラスの分析を具体的に行っていない。選択肢Cはメトリクスを提供するが、初期段階でのアップグレードは必要ないかもしれない。選択肢Dはアクセス分析に関するもので、ストレージクラスの最適化には直接関与していない。",
        "additional_knowledge": "このソリューションを実装することで、使用頻度の低いデータを安価なストレージクラスに移行することができ、コスト削減につながる可能性がある。",
        "key_terminology": "Amazon S3、ストレージクラス分析、Amazon QuickSight、コスト管理、データアクセスパターン",
        "overall_assessment": "ストレージ使用を分析してコストを削減するという要件に対して、選択肢Bが最も効果的なソリューションとして際立っている。コミュニティの投票は選択肢Cに強く傾いているが、これはストレージクラスの最適化という特定のニーズを誤解している可能性がある。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "storage class analysis",
      "Amazon QuickSight",
      "cost management",
      "data access patterns"
    ]
  },
  {
    "No": "84",
    "question": "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently\ndeployed in one AWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "企業はAWS上にクラウドインフラストラクチャを持っています。ソリューションアーキテクトは、インフラストラクチャをコードとして定義する必要があります。インフラストラクチャは現在、一つのAWSリージョンにデプロイされています。企業のビジネス拡張計画には、複数のAWSアカウントにまたがる複数のリージョンでのデプロイメントが含まれています。ソリューションアーキテクトは、これらの要件を満たすために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.",
        "text_jp": "AWS CloudFormationテンプレートを使用します。様々なアカウントを制御するIAMポリシーを追加し、テンプレートを複数のリージョンにデプロイします。"
      },
      {
        "key": "B",
        "text": "Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.",
        "text_jp": "AWS Organizationsを使用します。管理アカウントからAWS CloudFormationテンプレートをデプロイし、AWS Control Towerを使用してアカウント全体のデプロイメントを管理します。"
      },
      {
        "key": "C",
        "text": "Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.",
        "text_jp": "AWS OrganizationsとAWS CloudFormation StackSetsを使用します。必要なIAM権限を持つアカウントからCloudFormationテンプレートをデプロイします。"
      },
      {
        "key": "D",
        "text": "Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.",
        "text_jp": "AWS CloudFormationテンプレートでネストされたスタックを使用します。ネストされたスタックを使用してリージョンを変更します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Use AWS Organizations and AWS CloudFormation StackSets to manage infrastructure across multiple accounts and regions effectively.",
        "situation_analysis": "The company plans to deploy its infrastructure across multiple AWS accounts and regions, which necessitates a centralized management approach and the ability to deploy templates systematically.",
        "option_analysis": "Option A lacks a multi-account management strategy, which is integral in this scenario. Option B mentions AWS Control Tower but does not explicitly address StackSets. Option D does not leverage AWS Organizations for account management, which is critical for the requirements outlined.",
        "additional_knowledge": "Using StackSets in conjunction with Organizations helps ensure that all necessary IAM permissions are in place to facilitate deployments across different accounts.",
        "key_terminology": "AWS CloudFormation, StackSets, AWS Organizations, Infrastructure as Code, Multi-Region Deployment",
        "overall_assessment": "The community overwhelmingly supports option C based on its comprehensive capabilities to fulfill all requirements. This reinforces the choice as the best practice for infrastructure deployment."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです：AWS OrganizationsとAWS CloudFormation StackSetsを使用することで、複数のアカウントやリージョンにわたってインフラストラクチャを効果的に管理できます。",
        "situation_analysis": "企業はインフラストラクチャを複数のAWSアカウントやリージョンにデプロイする計画を立てているため、中央集権的な管理アプローチと、テンプレートを体系的にデプロイする能力が必要です。",
        "option_analysis": "選択肢Aは、このシナリオに必要なマルチアカウント管理戦略が欠けています。選択肢BはAWS Control Towerに言及していますが、StackSetsについて明示的に言及していません。選択肢Dは、アカウント管理のためにAWS Organizationsを利用しておらず、これが要件には重要です。",
        "additional_knowledge": "OrganizationsとStackSetsを使用することで、異なるアカウントに対するデプロイを促進するために必要なすべてのIAM権限が整備されることを保証します。",
        "key_terminology": "AWS CloudFormation、StackSets、AWS Organizations、コードとしてのインフラストラクチャ、マルチリージョンデプロイメント",
        "overall_assessment": "コミュニティは、全ての要件を満たす包括的な能力に基づき、選択肢Cを圧倒的に支持します。これはインフラストラクチャデプロイのベストプラクティスとして強調されます。"
      }
    ],
    "keywords": [
      "AWS CloudFormation",
      "StackSets",
      "AWS Organizations",
      "Infrastructure as Code",
      "Multi-Region Deployment"
    ]
  },
  {
    "No": "85",
    "question": "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently\ndeployed in one AWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "会社はAWS上にクラウドインフラを持っています。ソリューションアーキテクトは、インフラをコードとして定義する必要があります。インフラは現在、一つのAWSリージョンに展開されています。会社のビジネス拡張計画には、複数のAWSアカウントにまたがる複数のリージョンでの展開が含まれています。ソリューションアーキテクトは、これらの要件を満たすために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.",
        "text_jp": "AWS CloudFormationテンプレートを使用します。さまざまなアカウントを管理するためのIAMポリシーを追加し、複数のリージョンにわたってテンプレートを展開します。"
      },
      {
        "key": "B",
        "text": "Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.",
        "text_jp": "AWS Organizationsを使用します。管理アカウントからAWS CloudFormationテンプレートを展開し、AWS Control Towerを使用してアカウント間の展開を管理します。"
      },
      {
        "key": "C",
        "text": "Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.",
        "text_jp": "AWS OrganizationsとAWS CloudFormation StackSetsを使用します。必要なIAM権限を持つアカウントからCloudFormationテンプレートを展開します。"
      },
      {
        "key": "D",
        "text": "Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.",
        "text_jp": "AWS CloudFormationテンプレートを使用してネストしたスタックを利用します。ネストしたスタックを使用してリージョンを変更します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Use AWS Organizations and AWS CloudFormation StackSets. Deploy a CloudFormation template from an account that has the necessary IAM permissions. This method allows for centralized management and deployment across multiple accounts and regions.",
        "situation_analysis": "The company needs to manage infrastructure across multiple AWS accounts and regions, which requires a solution that facilitates code-based deployment and management.",
        "option_analysis": "Option C provides a method that utilizes AWS Organizations to manage multiple accounts, and StackSets to deploy CloudFormation templates across these accounts in various regions. This is a best practice for multi-account, multi-region architectures. Options A and D do not offer the required scalability and management convenience, while option B, although it mentions AWS Organizations and Control Tower, does not include the StackSets which are essential for the task.",
        "additional_knowledge": "It is essential to configure IAM roles appropriately to ensure security while communicating between accounts during deployment.",
        "key_terminology": "AWS Organizations, AWS CloudFormation, StackSets, IAM permissions.",
        "overall_assessment": "The question assesses the understanding of AWS multi-account and multi-region management and emphasizes the best practices using AWS services that enable scalability and centralized control. Community support for option C confirms its validity as the optimal choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はC: AWS OrganizationsおよびAWS CloudFormation StackSetsを使用します。必要なIAM権限を持つアカウントからCloudFormationテンプレートを展開します。この方法により、複数のアカウントとリージョンにわたって集中管理と展開が可能になります。",
        "situation_analysis": "会社は複数のAWSアカウントとリージョンにわたってインフラを管理する必要があり、コードベースの展開と管理を促進するソリューションが必要です。",
        "option_analysis": "Cの選択肢は、複数のアカウントを管理するためにAWS Organizationsと、これらのアカウントに複数のリージョンにわたってCloudFormationテンプレートを展開するためのStackSetsを利用する方法を提供します。これがマルチアカウント・マルチリージョンアーキテクチャにおけるベストプラクティスです。AとDの選択肢は要求されるスケーラビリティと管理の利便性を提供しません。一方、Bの選択肢はAWS OrganizationsとControl Towerに言及していますが、役割を果たすために不可欠なStackSetsを含んでいません。",
        "additional_knowledge": "デプロイ中にアカウント間での通信が行われる際に、適切なIAMロールを構成することが重要です。",
        "key_terminology": "AWS Organizations、AWS CloudFormation、StackSets、IAM権限。",
        "overall_assessment": "この質問は、AWSのマルチアカウントおよびマルチリージョン管理に対する理解を評価し、スケーラビリティと中央制御を可能にするAWSサービスのベストプラクティスの使用を強調しています。Cの選択肢に対するコミュニティの支持は、その妥当性を確認しています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS CloudFormation",
      "StackSets",
      "IAM permissions"
    ]
  },
  {
    "No": "86",
    "question": "A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be\nupgraded to support the modern design for the application with the following requirements:\n• It should allow changes to be released several times every hour.\n• It should be able to roll back the changes as quickly as possible.\nWhich design will meet these requirements?",
    "question_jp": "企業は、モノリシックアプリケーションをAWS上で現代的なアプリケーション設計にリファクタリングすることを計画しています。CI/CDパイプラインは、アプリケーションの現代的な設計をサポートするためにアップグレードする必要があります。以下の要件を満たす必要があります：\n• 数回の変更を毎時リリースできること。\n• できるだけ早く変更をロールバックできること。\nどの設計がこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances.",
        "text_jp": "アプリケーションとその設定を含むAMIを取り入れたCI/CDパイプラインを展開します。Amazon EC2インスタンスを置き換えてアプリケーションを展開します。"
      },
      {
        "key": "B",
        "text": "Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs.",
        "text_jp": "CI/CDパイプラインの展開ターゲットとして、二次環境を指定するためにAWS Elastic Beanstalkを使用します。デプロイするために、ステージングと本番環境のURLを交換します。"
      },
      {
        "key": "C",
        "text": "Use AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and use Amazon Route 53 weighted routing to point to the new environment.",
        "text_jp": "AWS Systems Managerを使用して、各デプロイメントのためにインフラストラクチャを再構築します。最新のコードアーティファクトをAmazon S3から取得するためにAmazon EC2ユーザーデータを更新し、Amazon Route 53のウェイトルーティングを使用して新しい環境を指します。"
      },
      {
        "key": "D",
        "text": "Roll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances. and phase out all instances that use the previous AMI version with the configured termination policy during a deployment event.",
        "text_jp": "Auto Scalingイベントの一部としてアプリケーションアップデートを展開します。新しいバージョンのAMIを使用してインスタンスを追加し、デプロイメントイベント中に設定された終了ポリシーを使用して以前のAMIバージョンを使用するすべてのインスタンスを段階的に廃止します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Utilizing AWS Elastic Beanstalk allows for rapid deployment and rollback by swapping URLs between staging and production environments, satisfying the need for frequent updates.",
        "situation_analysis": "The company needs a solution that can support continuous deployments multiple times an hour and facilitate quick rollbacks.",
        "option_analysis": "Option B stands out as it leverages Elastic Beanstalk for its inherent capability of handling environments and managing deployments efficiently through URL swapping. Other options do not provide the same level of operational flexibility.",
        "additional_knowledge": "It's crucial for developers to understand how to leverage managed services like Elastic Beanstalk effectively to minimize operational overhead.",
        "key_terminology": "Elastic Beanstalk, CI/CD, continuous deployment, rollback, environment swapping",
        "overall_assessment": "Community support overwhelmingly favors option B, aligning perfectly with the deployment and rollback requirements described. It demonstrates best practices in utilizing managed services for rapid and reliable application delivery."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBです。AWS Elastic Beanstalkを利用することで、ステージング環境と本番環境のURLを交換することで迅速なデプロイメントとロールバックが可能になり、頻繁な更新のニーズを満たします。",
        "situation_analysis": "企業は、数回の変更を毎時リリースし、迅速なロールバックを可能にするソリューションが必要です。",
        "option_analysis": "Bの選択肢は、環境を管理し、デプロイメントを効率的に管理する能力を持つElastic Beanstalkを活用しているため、際立っています。他の選択肢は、同じレベルの運用柔軟性を提供しません。",
        "additional_knowledge": "開発者は、運用オーバーヘッドを最小限に抑えるために、Elastic Beanstalkのようなマネージドサービスを効果的に活用する方法を理解することが重要です。",
        "key_terminology": "Elastic Beanstalk, CI/CD, 継続的デプロイメント, ロールバック, 環境交換",
        "overall_assessment": "コミュニティの支持は圧倒的にBを支持しており、展開とロールバックの要件と完全に一致しています。これは、迅速かつ信頼性のあるアプリケーション配信のためのベストプラクティスを示しています。"
      }
    ],
    "keywords": [
      "Elastic Beanstalk",
      "CI/CD",
      "Continuous Deployment",
      "Rollback",
      "Environment Swapping"
    ]
  },
  {
    "No": "87",
    "question": "A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where\nthe application needs to access an Amazon Aurora DB Cluster. The EC2 instances are all associated with the same security group. The DB cluster\nis associated with its own security group.\nThe solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Add an inbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the source over the default Aurora port."
      },
      {
        "key": "B",
        "text": "Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port."
      },
      {
        "key": "C",
        "text": "Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port."
      },
      {
        "key": "D",
        "text": "Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the default Aurora port."
      },
      {
        "key": "E",
        "text": "Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the ephemeral ports."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BC (76%) AC (24%)",
    "page_images": []
  },
  {
    "No": "88",
    "question": "A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports\nfor overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for\neach business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team\nwants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for\nany cloud spending that exceeds a set threshold.\nWhich solution is the MOST cost-effective way to meet these requirements?",
    "question_jp": "企業は、各ビジネスユニットの内部クラウド請求戦略を変更したいと考えています。現状、クラウドガバナンスチームは、各ビジネスユニットの責任者に全体のクラウド支出に関するレポートを共有しています。企業は、各ビジネスユニットのために別々のAWSアカウントを管理するためにAWS Organizationsを使用しています。Organizationsの既存のタグ付け基準には、アプリケーション、環境、所有者が含まれています。クラウドガバナンスチームは、各ビジネスユニットが月次レポートを受け取ることができる集中化されたソリューションを望んでいます。このソリューションは、設定された閾値を超えるクラウド支出に対して通知を送信する必要があります。どのソリューションがこれらの要件を満たすための最もコスト効果の高い方法ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit.",
        "text_jp": "各アカウントでAWS Budgetsを設定して、アプリケーション、環境、所有者ごとにグループ化された予算アラートを設定します。各アラートのためにAmazon SNSトピックに各ビジネスユニットを追加します。各アカウントでCost Explorerを使用して、各ビジネスユニットの月次レポートを作成します。"
      },
      {
        "key": "B",
        "text": "Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization's management account to create monthly reports for each business unit.",
        "text_jp": "組織の管理アカウントでAWS Budgetsを設定して、アプリケーション、環境、所有者ごとにグループ化された予算アラートを設定します。各アラートのためにAmazon SNSトピックに各ビジネスユニットを追加します。組織の管理アカウントでCost Explorerを使用して、各ビジネスユニットの月次レポートを作成します。"
      },
      {
        "key": "C",
        "text": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit.",
        "text_jp": "各アカウントでAWS Budgetsを設定して、アプリケーション、環境、所有者ごとにグループ化された予算アラートを設定します。各アラートのためにAmazon SNSトピックに各ビジネスユニットを追加します。各アカウントでAWS Billing and Cost Managementダッシュボードを使用して、各ビジネスユニットの月次レポートを作成します。"
      },
      {
        "key": "D",
        "text": "Enable AWS Cost and Usage Reports in the organization's management account and configure reports grouped by application, environment. and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit's email list.",
        "text_jp": "組織の管理アカウントでAWS Cost and Usage Reportsを有効にし、アプリケーション、環境、および所有者ごとにグループ化されたレポートを設定します。AWS Cost and Usage Reportsを処理し、予算アラートを送信し、各ビジネスユニットのメールリストに月次レポートを送信するAWS Lambda関数を作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, configuring AWS Budgets in the organization's management account allows centralized monitoring and reporting for all business units.",
        "situation_analysis": "The organization requires separate billing for each business unit while maintaining a centralized control over budget alerts and monthly reports.",
        "option_analysis": "Option B allows for centralized budgeting in the management account, which reduces duplicated efforts and ensures cohesive reporting across all units. Options A and C would require separate budgets for each account, leading to more complex management. Option D, while it provides detailed usage reports, is more complex and does not directly correspond to budget alerts.",
        "additional_knowledge": "Using centralized management practices within AWS helps organizations optimize their costs and manage resources efficiently.",
        "key_terminology": "AWS Budgets, AWS Organizations, Cost Explorer, SNS, AWS Lambda",
        "overall_assessment": "Considering the simplicity of management and effective monitoring, Option B is the most suitable solution for meeting the given requirements. The community overwhelmingly supports this option, indicating strong confidence in its feasibility."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。組織の管理アカウントでAWS Budgetsを設定することで、すべてのビジネスユニットに対して集中管理された監視と報告が可能になります。",
        "situation_analysis": "組織は、各ビジネスユニットごとの請求を必要としつつ、予算アラートと月次レポートを集中管理することを求めています。",
        "option_analysis": "オプションBは、管理アカウントでの集中予算設定を許可し、重複した作業を削減し、全ユニットにわたる統一された報告を実現します。オプションAとCは、各アカウントの個別予算を必要とし、管理が複雑になりやすいです。オプションDは詳細な使用レポートを提供しますが、複雑であり、予算アラートに直接対応していません。",
        "additional_knowledge": "AWS内での集中管理の実践により、組織はコストを最適化し、リソースを効率的に管理できます。",
        "key_terminology": "AWS Budgets, AWS Organizations, Cost Explorer, SNS, AWS Lambda",
        "overall_assessment": "管理の簡素性と効果的な監視を考慮すると、オプションBは与えられた要件を満たすための最も適したソリューションです。コミュニティが圧倒的にこの選択を支持しており、その実行可能性に強い自信が示されています。"
      }
    ],
    "keywords": [
      "AWS Budgets",
      "AWS Organizations",
      "Cost Explorer",
      "SNS",
      "AWS Lambda"
    ]
  },
  {
    "No": "89",
    "question": "A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is\ndeleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.\nHow can the company prevent users from accidentally deleting data in this way?",
    "question_jp": "ある企業はAWS CloudFormationを使用してインフラストラクチャを展開しています。企業は、プロダクションのCloudFormationスタックが削除されると、Amazon RDSデータベースやAmazon EBSボリュームに保存されている重要なデータも削除される可能性があることを懸念しています。このようにユーザーがデータを誤って削除するのを防ぐには、企業はどのようにすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.",
        "text_jp": "RDSおよびEBSリソースにDeletionPolicy属性を追加するようにCloudFormationテンプレートを修正する。"
      },
      {
        "key": "B",
        "text": "Configure a stack policy that disallows the deletion of RDS and EBS resources.",
        "text_jp": "RDSおよびEBSリソースの削除を禁止するスタックポリシーを設定する。"
      },
      {
        "key": "C",
        "text": "Modify IAM policies lo deny deleting RDS and EBS resources that are tagged with an \"aws:cloudformation:stack-name\" tag.",
        "text_jp": "\"aws:cloudformation:stack-name\"タグが付けられたRDSおよびEBSリソースの削除を拒否するようにIAMポリシーを修正する。"
      },
      {
        "key": "D",
        "text": "Use AWS Config rules to prevent deleting RDS and EBS resources.",
        "text_jp": "AWS Configルールを使用してRDSおよびEBSリソースの削除を防ぐ。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (85%) B (15%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Adding a DeletionPolicy attribute to RDS and EBS resources in the CloudFormation template ensures these resources are not deleted when the stack is deleted.",
        "situation_analysis": "The company is concerned about data loss from Amazon RDS and EBS resources if the CloudFormation stack is deleted. Preventing accidental deletion is crucial for maintaining critical production data.",
        "option_analysis": "Option A is correct because the DeletionPolicy attribute directly addresses the issue. Option B, while relevant, applies to stack policies and does not directly prevent deletion of resources within the template. Option C involves IAM policies, which is less effective for this specific scenario. Option D introduces Config rules, which can monitor but not directly prevent deletion at the template level.",
        "additional_knowledge": "It's important to understand the implications of DeletionPolicy and consider implementing it in production stacks.",
        "key_terminology": "DeletionPolicy, CloudFormation, stack policy, IAM policies, AWS Config",
        "overall_assessment": "Overall, option A is the most effective and straightforward solution for preventing accidental deletion of important data when a CloudFormation stack is deleted."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。CloudFormationテンプレートにRDSおよびEBSリソースのDeletionPolicy属性を追加することで、スタックが削除された場合でもそれらのリソースが削除されないことが保証される。",
        "situation_analysis": "企業はCloudFormationスタックが削除されるとAmazon RDSおよびEBSリソースからデータが失われることを懸念している。誤って削除を防ぐことは、重要なプロダクションデータを維持するために重要である。",
        "option_analysis": "選択肢Aは正しい。DeletionPolicy属性は直接的な解決策である。選択肢Bは関係があるが、スタックポリシーに対して適用されるもので、テンプレート内のリソースの削除を直接的に防ぐわけではない。選択肢CはIAMポリシーに関するものであり、この特定のシナリオでは効果が薄い。選択肢DはConfigルールを導入するが、テンプレートレベルでの削除を直接的に防止するものではない。",
        "additional_knowledge": "DeletionPolicyの影響を理解し、生産環境のスタックに実装する計画を検討することが重要である。",
        "key_terminology": "DeletionPolicy、CloudFormation、スタックポリシー、IAMポリシー、AWS Config",
        "overall_assessment": "総じて、選択肢AはCloudFormationスタックが削除されたときに重要なデータの誤った削除を防ぐ最も効果的で簡単な解決策である。"
      }
    ],
    "keywords": [
      "DeletionPolicy",
      "CloudFormation",
      "stack policy",
      "IAM policies",
      "AWS Config"
    ]
  },
  {
    "No": "90",
    "question": "A company has VPC fiow logs enabled for Its NAT gateway. The company is seeing Action = ACCEPT for inbound trafic that comes from public IP\naddress 198.51.100.2 destined for a private Amazon EC2 instance.\nA solutions architect must determine whether the trafic represents unsolicited inbound connections from the internet. The first two octets of the\nVPC CIDR block are 203.0.\nWhich set of steps should the solutions architect take to meet these requirements?",
    "question_jp": "ある企業は、そのNATゲートウェイのためにVPCフローログを有効にしています。企業は、プライベートなAmazon EC2インスタンスに向けられた、パブリックIPアドレス198.51.100.2からの受信トラフィックがAction = ACCEPTとなっているのを見ています。ソリューションアーキテクトは、このトラフィックがインターネットからの未承諾の受信接続を表しているかどうかを判断する必要があります。VPC CIDRブロックの最初の2オクテットは203.0です。この要件を満たすために、ソリューションアーキテクトはどの一連の手順を取るべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interlace. Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.",
        "text_jp": "AWS CloudTrailコンソールを開きます。NATゲートウェイのエラスティックネットワークインターフェイスとプライベートインスタンスのエラスティックネットワークインターフェイスを含むロググループを選択します。宛先アドレスを\"like 203.0\"、送信元アドレスを\"like 198.51.100.2\"に設定してフィルタリングするクエリを実行します。送信元アドレスと宛先アドレスによる転送バイト数の合計をフィルタリングするためにstatsコマンドを実行します。"
      },
      {
        "key": "B",
        "text": "Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.",
        "text_jp": "Amazon CloudWatchコンソールを開きます。NATゲートウェイのエラスティックネットワークインターフェイスとプライベートインスタンスのエラスティックネットワークインターフェイスを含むロググループを選択します。宛先アドレスを\"like 203.0\"、送信元アドレスを\"like 198.51.100.2\"に設定してフィルタリングするクエリを実行します。送信元アドレスと宛先アドレスによる転送バイト数の合計をフィルタリングするためにstatsコマンドを実行します。"
      },
      {
        "key": "C",
        "text": "Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.",
        "text_jp": "AWS CloudTrailコンソールを開きます。NATゲートウェイのエラスティックネットワークインターフェイスとプライベートインスタンスのエラスティックネットワークインターフェイスを含むロググループを選択します。宛先アドレスを\"like 198.51.100.2\"、送信元アドレスを\"like 203.0\"に設定してフィルタリングするクエリを実行します。送信元アドレスと宛先アドレスによる転送バイト数の合計をフィルタリングするためにstatsコマンドを実行します。"
      },
      {
        "key": "D",
        "text": "Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.",
        "text_jp": "Amazon CloudWatchコンソールを開きます。NATゲートウェイのエラスティックネットワークインターフェイスとプライベートインスタンスのエラスティックネットワークインターフェイスを含むロググループを選択します。宛先アドレスを\"like 198.51.100.2\"、送信元アドレスを\"like 203.0\"に設定してフィルタリングするクエリを実行します。送信元アドレスと宛先アドレスによる転送バイト数の合計をフィルタリングするためにstatsコマンドを実行します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (66%) D (34%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, as it involves querying Amazon CloudWatch for the relevant NAT gateway and EC2 logs with the correct filtering criteria.",
        "situation_analysis": "The goal is to determine if the traffic from 198.51.100.2 is unsolicited and whether it connects to the private EC2 instance from the public internet.",
        "option_analysis": "Option D correctly filters with the destination address of the private EC2 instance and the source address from the public IP. Option B has incorrect filtering since it uses the source and destination addresses incorrectly.",
        "additional_knowledge": "Using CloudTrail is not appropriate for VPC flow log analysis.",
        "key_terminology": "Amazon CloudWatch, VPC Flow Logs, NAT Gateway, EC2, IP Address Filtering",
        "overall_assessment": "Answer D is the best choice based on AWS best practices for monitoring and analyzing network traffic effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです。これは、正しいフィルタリング基準を使用して、関連するNATゲートウェイとEC2のログをAmazon CloudWatchでクエリすることに関係しています。",
        "situation_analysis": "目的は、198.51.100.2からのトラフィックが未承諾であるかどうか、またそれがインターネットからプライベートEC2インスタンスに接続するかどうかを判断することです。",
        "option_analysis": "選択肢Dは、プライベートEC2インスタンスの宛先アドレスとパブリックIPからの送信元アドレスで正しくフィルタリングしています。選択肢Bは、送信元と宛先アドレスを誤って使用しているので不適切です。",
        "additional_knowledge": "CloudTrailを使用することは、VPCフローログ分析には適していません。",
        "key_terminology": "Amazon CloudWatch, VPCフローログ, NATゲートウェイ, EC2, IPアドレスフィルタリング",
        "overall_assessment": "選択肢Dは、ネットワークトラフィックを効果的に監視・分析するためのAWSのベストプラクティスに基づいて最良の選択です。"
      }
    ],
    "keywords": [
      "Amazon CloudWatch",
      "VPC Flow Logs",
      "NAT Gateway",
      "EC2",
      "IP Address Filtering"
    ]
  },
  {
    "No": "91",
    "question": "A company consists or two separate business units. Each business unit has its own AWS account within a single organization in AWS\nOrganizations. The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3\nbucket in each account and configured low-way replication between the S3 buckets. The S3 buckets have millions of objects.\nRecently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be\nstored with encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).\nWhat is the MOST operationally eficient solution that meets these requirements?",
    "question_jp": "企業は2つの別々のビジネスユニットから成り立っています。それぞれのビジネスユニットは、AWS Organizations内にある単一の組織内の独自のAWSアカウントを持っています。ビジネスユニットは定期的に機密文書を互いに共有します。共有を促進するため、企業は各アカウントにAmazon S3バケットを作成し、S3バケット間で低遅延レプリケーションを設定しました。S3バケットには数百万のオブジェクトがあります。最近のセキュリティ監査では、どちらのS3バケットも静止データ暗号化が有効になっていないことが確認されました。企業のポリシーでは、すべての文書は静止データ暗号化を用いて保存されなければなりません。企業はAmazon S3管理の暗号化キー（SSE-S3）を使用してサーバーサイドの暗号化を実装したいと考えています。これらの要件を満たすために、最も運用効率が高いソリューションは何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location.",
        "text_jp": "両方のS3バケットでSSE-S3をオンにします。S3バッチ操作を使用して、同じ場所にオブジェクトをコピーして暗号化します。"
      },
      {
        "key": "B",
        "text": "Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Encrypt the existing objects by using an S3 copy command in the AWS CLI.",
        "text_jp": "各アカウントにAWS Key Management Service（AWS KMS）キーを作成します。各S3バケットで、対応するKMSキーを使用してサーバーサイドの暗号化を有効にします（SSE-KMS）。既存のオブジェクトをAWS CLIのS3コピーコマンドを使用して暗号化します。"
      },
      {
        "key": "C",
        "text": "Turn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI.",
        "text_jp": "両方のS3バケットでSSE-S3をオンにします。AWS CLIのS3コピーコマンドを使用して既存のオブジェクトを暗号化します。"
      },
      {
        "key": "D",
        "text": "Create an AWS Key Management Service, (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Use S3 Batch Operations to copy the objects into the same location.",
        "text_jp": "各アカウントにAWS Key Management Service、（AWS KMS）キーを作成します。各S3バケットで、対応するKMSキーを使用してサーバーサイドの暗号化を有効にします（SSE-KMS）。S3バッチ操作を使用して、オブジェクトを同じ場所にコピーします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (85%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Enabling SSE-S3 on both S3 buckets ensures that all new objects are encrypted at rest without additional overhead associated with managing KMS keys.",
        "situation_analysis": "The company has two S3 buckets without encryption at rest while the policy requires it. The requirement is to implement encryption using SSE-S3 for operational efficiency.",
        "option_analysis": "Option C is the most efficient as it directly enables SSE-S3 and uses an S3 copy command to encrypt existing objects, which is straightforward and does not require managing AWS KMS. Option A overcomplicates the process with Batch Operations, and options B and D involve KMS, which adds management overhead and cost.",
        "additional_knowledge": "Implementing SSE-S3 also provides data protection, which is essential for sensitive documents.",
        "key_terminology": "SSE-S3, S3, encryption at rest, operational efficiency, AWS CLI",
        "overall_assessment": "C aligns accurately with the company's needs for operational efficiency and compliance with encryption policies. The community's support for A might not reflect the necessity of keeping management simple."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。両方のS3バケットでSSE-S3を有効にすることで、管理KMSキーに伴う追加のオーバーヘッドなしで、すべての新しいオブジェクトが静止状態で暗号化されることを確実にします。",
        "situation_analysis": "企業には静止データ暗号化が無効の2つのS3バケットがあり、ポリシーではこれを要求しています。要件は、運用効率を考慮してSSE-S3を使用して暗号化を実装することです。",
        "option_analysis": "選択肢Cは最も効率的で、新たにSSE-S3を有効にし、既存のオブジェクトを暗号化するためにS3コピーコマンドを使用するため、シンプルです。選択肢Aはバッチ操作で手続きを複雑化しており、BとDはKMSを必要とし、管理オーバーヘッドとコストが増加します。",
        "additional_knowledge": "SSE-S3を実装することは、機密文書にとって不可欠なデータ保護も提供します。",
        "key_terminology": "SSE-S3、S3、静止データ暗号化、運用効率、AWS CLI",
        "overall_assessment": "Cは企業の運用効率と暗号化ポリシーの遵守ニーズに正確に一致しています。Aに対するコミュニティの支持は、管理をシンプルに保つ必要性を反映していない可能性があります。"
      }
    ],
    "keywords": [
      "SSE-S3",
      "S3",
      "encryption at rest",
      "operational efficiency",
      "AWS CLI"
    ]
  },
  {
    "No": "92",
    "question": "A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3\nbucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes\nevery day.\nThe company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must\nretain all the data indefinitely for compliance reasons.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業がAWSクラウドでアプリケーションを運用しています。このアプリケーションは、Amazon S3バケットに大量の非構造化データを収集し、保存しています。S3バケットには数テラバイトのデータが含まれており、S3 Standardストレージクラスを使用しています。データは毎日数ギガバイト分増加しています。この企業はデータをクエリし、分析する必要があります。企業は1年以上古いデータにはアクセスしません。ただし、コンプライアンス上の理由から、すべてのデータを無期限に保持する必要があります。これらの要件を最もコスト効率よく満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
        "text_jp": "S3 Selectを使用してデータをクエリします。1年以上古いデータをS3 Glacier Deep Archiveに移行するS3ライフサイクルポリシーを作成します。"
      },
      {
        "key": "B",
        "text": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old 10 S3 Glacier Deep Archive.",
        "text_jp": "Amazon Redshift Spectrumを使用してデータをクエリします。1年以上古いデータをS3 Glacier Deep Archiveに移行するS3ライフサイクルポリシーを作成します。"
      },
      {
        "key": "C",
        "text": "Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
        "text_jp": "AWS Glue Data CatalogとAmazon Athenaを使用してデータをクエリします。1年以上古いデータをS3 Glacier Deep Archiveに移行するS3ライフサイクルポリシーを作成します。"
      },
      {
        "key": "D",
        "text": "Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering.",
        "text_jp": "Amazon Redshift Spectrumを使用してデータをクエリします。1年以上古いデータをS3 Intelligent-Tieringに移行するS3ライフサイクルポリシーを作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (87%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Using S3 Select allows the company to efficiently query data directly from S3, while implementing a lifecycle policy to transition older data to S3 Glacier Deep Archive ensures compliance without incurring high storage costs.",
        "situation_analysis": "The key requirement is to analyze data while minimizing costs and retaining it indefinitely for compliance. Accessing data older than a year is not necessary, enabling cost-saving strategies.",
        "option_analysis": "Option A is effective due to S3 Select's cost efficiency for querying data without extracting it. Option B requires Amazon Redshift Spectrum, which may result in higher costs for data querying. Option C introduces additional complexity with AWS Glue and Athena, while Option D suggests S3 Intelligent-Tiering, which does not align as closely with the long-term retention requirement.",
        "additional_knowledge": "S3 Glacier Deep Archive is the lowest-cost storage option for infrequently accessed data, making it suitable for compliance needs. By transitioning older data to Glacier, the company optimizes costs while meeting retention policies.",
        "key_terminology": "S3 Select, S3 Lifecycle Policy, S3 Glacier Deep Archive, AWS Glue, Amazon Athena, Amazon Redshift Spectrum, S3 Intelligent-Tiering",
        "overall_assessment": "Option A is the best choice for cost-effectiveness and aligns with compliance requirements, despite community voting favoring options that are potentially more expensive or complex. The community vote indicates a preference for more involved solutions, but simplicity and cost savings should take precedence in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです：S3 Selectを使用することで、企業はS3から直接データを効率的にクエリでき、古いデータをS3 Glacier Deep Archiveに移行するライフサイクルポリシーを実施することで、高いストレージコストをかけずにコンプライアンスを確保できます。",
        "situation_analysis": "重要な要件は、コストを最小限に抑えつつデータを分析し、コンプライアンスのために無期限に保持することです。1年以上古いデータにはアクセスする必要がないため、コスト削減策を講じることが可能です。",
        "option_analysis": "選択肢Aは、データを抽出せずにクエリできるS3 Selectのコスト効率が良いため、効果的です。選択肢BはAmazon Redshift Spectrumを求めており、高コストのデータクエリにつながる可能性があります。選択肢CはAWS GlueとAthenaを使用するため、追加の複雑さを伴います。一方、選択肢DはS3 Intelligent-Tieringを提案していますが、長期保持の要件にはあまり適していません。",
        "additional_knowledge": "S3 Glacier Deep Archiveは、アクセス頻度の低いデータ向けの最も低コストなストレージオプションであり、コンプライアンスニーズに適しています。古いデータをGlacierに移行することで、コストを最適化しながら保持ポリシーを満たすことができます。",
        "key_terminology": "S3 Select, S3ライフサイクルポリシー, S3 Glacier Deep Archive, AWS Glue, Amazon Athena, Amazon Redshift Spectrum, S3 Intelligent-Tiering",
        "overall_assessment": "選択肢Aはコスト効率が最も高く、コンプライアンス要件にも適合します。一方で、コミュニティの投票ではより費用対効果の高い複雑な解決策が好まれていることを示しています。しかし、このシナリオではシンプルさとコスト削減が優先されるべきです。"
      }
    ],
    "keywords": [
      "S3 Select",
      "S3 Lifecycle Policy",
      "S3 Glacier Deep Archive",
      "AWS Glue",
      "Amazon Athena"
    ]
  },
  {
    "No": "93",
    "question": "A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of\nfiles in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises\nfor ML experiments and wants to use AWS.\nThe company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be\nencrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps. and multiple departments share the\nconnection.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "動画処理会社は、600 TBの圧縮データを使用して機械学習（ML）モデルを構築したいと考えています。このデータは、会社のオンプレミスのネットワーク接続ストレージシステムに数千のファイルとして保存されています。会社は、ML実験に必要な計算リソースをオンプレミスに持っておらず、AWSを使用したいと考えています。データ転送を3週間以内にAWSに完了する必要があります。データ転送は一度きりの転送です。データは転送中に暗号化される必要があります。会社のインターネット接続のアップロード速度は100 Mbpsであり、複数の部門がこの接続を共有しています。これらの要件を最も費用対効果の高い方法で満たすソリューションはどれか？",
    "choices": [
      {
        "key": "A",
        "text": "Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.",
        "text_jp": "AWS Management Consoleを使用して、複数のAWS Snowball Edgeストレージ最適化デバイスを注文する。デバイスをS3バケットの宛先として設定する。データをデバイスにコピーする。デバイスをAWSに返送する。"
      },
      {
        "key": "B",
        "text": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.",
        "text_jp": "会社のロケーションと最寄りのAWSリージョンの間に10 GbpsのAWS Direct Connect接続を設定する。データをVPN接続を介してリージョンに転送し、データをAmazon S3に保存する。"
      },
      {
        "key": "C",
        "text": "Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN connection.",
        "text_jp": "オンプレミスのネットワーク接続ストレージと最寄りのAWSリージョンの間にVPN接続を作成する。VPN接続を介してデータを転送する。"
      },
      {
        "key": "D",
        "text": "Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway.",
        "text_jp": "オンプレミスにAWS Storage Gatewayファイルゲートウェイを展開する。ファイルゲートウェイをS3バケットの宛先として設定する。データをファイルゲートウェイにコピーする。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Ordering several AWS Snowball Edge devices allows for the secure transport of large amounts of data to AWS efficiently, especially when the upload speed is limited.",
        "situation_analysis": "The company needs to transfer 600 TB of data within 3 weeks over a limited bandwidth internet connection, which is shared among multiple departments.",
        "option_analysis": "Option A allows for offline data transfer using AWS Snowball, which is designed for large-scale data migration with encryption. Option B and C require high bandwidth which is not feasible with the company’s current internet speed. Option D introduces additional ongoing maintenance overhead, which is not necessary for a one-time transfer.",
        "additional_knowledge": "The Snowball device supports encryption at rest and during transfer, ensuring compliance with data security requirements.",
        "key_terminology": "AWS Snowball, data transfer, encryption, AWS Management Console, bandwidth",
        "overall_assessment": "The community predominantly supports option A, noting it as the most effective solution based on the requirements. While options B and C were evaluated, they do not align with the company's bandwidth constraints, making A the most viable choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。複数のAWS Snowball Edgeデバイスを注文することで、大量のデータを効率的にAWSに安全に輸送できます。特にアップロード速度が制限されている場合に有効です。",
        "situation_analysis": "会社は、3週間以内に600 TBのデータを、複数の部門が共有している限られた帯域幅のインターネット接続を介して転送する必要があります。",
        "option_analysis": "選択肢Aは、AWS Snowballを使用したオフラインデータ転送を可能にします。これは、暗号化された大量のデータ移行に特化しています。選択肢BとCは、高帯域幅を必要としますが、会社の現在のインターネット速度では実現できません。選択肢Dは、ファイルゲートウェイの運用維持に追加の負担を伴い、一度きりの転送には必要ありません。",
        "additional_knowledge": "Snowballデバイスは、転送時および静止時のデータの暗号化をサポートしているため、データセキュリティ要件に準拠します。",
        "key_terminology": "AWS Snowball, データ転送, 暗号化, AWS Management Console, 帯域幅",
        "overall_assessment": "コミュニティは主に選択肢Aを支持しており、要件に最も適した効果的なソリューションと見なされています。選択肢BとCは評価されましたが、会社の帯域幅制約には一致していないため、Aが最も実行可能な選択肢となっています。"
      }
    ],
    "keywords": [
      "AWS Snowball",
      "data transfer",
      "encryption",
      "AWS Management Console",
      "bandwidth"
    ]
  },
  {
    "No": "94",
    "question": "A company has migrated Its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files\nthrough a web application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on\nAmazon EC2 instances and an Amazon RDS for PostgreSQL database.\nWhen forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team\nmember then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before entering\nthe information into another system that uses an API.\nA solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction. minimize time\nto market, and minimize tong-term operational overhead.\nWhich solution will meet these requirements?",
    "question_jp": "ある会社がそのフォーム処理アプリケーションをAWSに移行しました。ユーザーがアプリケーションと対話するとき、彼らはウェブアプリケーションを通じてスキャンされたフォームをファイルとしてアップロードします。データベースにはユーザーメタデータとAmazon S3に保存されているファイルへの参照が保存されています。ウェブアプリケーションはAmazon EC2インスタンスおよびAmazon RDS for PostgreSQLデータベース上で実行されています。フォームがアップロードされると、アプリケーションはAmazon Simple Notification Service（Amazon SNS）を通じてチームに通知を送ります。チームメンバーはログインして各フォームを処理します。チームメンバーはフォームのデータ検証を行い、関連するデータを抽出して、その情報をAPIを使用する別のシステムに入力します。ソリューションアーキテクトはフォームの手動処理を自動化する必要があります。ソリューションは正確なフォーム抽出を提供し、市場への投入時間を最小限に抑え、長期的な運用オーバーヘッドを最小限に抑える必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Develop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tier. Use this tier to process the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. Submit the data to the target system's APL. Host the new application tier on EC2 instances.",
        "text_jp": "カスタムライブラリを開発してフォーム上で光学文字認識（OCR）を行います。これらのライブラリをAmazon Elastic Kubernetes Service（Amazon EKS）クラスターとしてアプリケーション層にデプロイします。フォームがアップロードされると、この層を使用してフォームを処理します。出力をAmazon S3に保存します。この出力を解析してデータをAmazon DynamoDBテーブルに抽出します。データをターゲットシステムのAPIに送信します。新しいアプリケーション層をEC2インスタンス上にホストします。"
      },
      {
        "key": "B",
        "text": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on an EC2 instance to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.",
        "text_jp": "AWS Step FunctionsおよびAWS Lambdaを使用するアプリケーション層をシステムに拡張します。この層を構成して、EC2インスタンスで訓練されホスティングされる人工知能および機械学習（AI/ML）モデルを使用して、フォームがアップロードされたときに光学文字認識（OCR）を実行します。出力をAmazon S3に保存します。この出力を解析して、アプリケーション層内で必要なデータを抽出します。データをターゲットシステムのAPIに送信します。"
      },
      {
        "key": "C",
        "text": "Host a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine teaming (AI/ML) models that are trained and hosted in Amazon SageMaker to perform optical character recognition (OCR) on the forms. Store the output in Amazon ElastiCache. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.",
        "text_jp": "EC2インスタンス上に新しいアプリケーション層をホストします。この層を使用して、フォーム上で光学文字認識（OCR）を実行するために訓練されホスティングされている人工知能および機械学習（AI/ML）モデルをホストするエンドポイントを呼び出します。出力をAmazon ElastiCacheに保存します。この出力を解析して、アプリケーション層内で必要なデータを抽出します。データをターゲットシステムのAPIに送信します。"
      },
      {
        "key": "D",
        "text": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.",
        "text_jp": "AWS Step FunctionsおよびAWS Lambdaを使用するアプリケーション層をシステムに拡張します。この層を構成して、フォームがアップロードされたときに光学文字認識（OCR）を実行するためにAmazon TextractおよびAmazon Comprehendを使用します。出力をAmazon S3に保存します。この出力を解析して、アプリケーション層内で必要なデータを抽出します。データをターゲットシステムのAPIに送信します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution uses AWS services specifically designed for document processing, namely Amazon Textract and Amazon Comprehend, which are capable of performing optical character recognition (OCR) and natural language processing, respectively.",
        "situation_analysis": "The company needs to automate the manual processing of forms, focusing on accurate form extraction while minimizing time to market and operational overhead. This aligns perfectly with the capabilities of Textract and Comprehend.",
        "option_analysis": "Option A presents a more complex architecture by developing custom libraries that use OCR, which can introduce maintenance overhead. Option B effectively uses AI/ML but relies on EC2 for hosting models, which might not be as efficient as using managed services. Option C also complicates the architecture and requires additional resources.",
        "additional_knowledge": "Using AWS Step Functions orchestrates workflows effectively, allowing for better handling of application states and errors.",
        "key_terminology": "Amazon Textract, Amazon Comprehend, AWS Step Functions, AWS Lambda, Optical Character Recognition, Data Extraction",
        "overall_assessment": "The selection of D is substantiated by its use of serverless and managed services, reducing operational complexity while ensuring scalable processing of forms."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。このソリューションは、書類処理に特化したAWSのサービス、特に光学文字認識（OCR）を実行できるAmazon Textractおよび自然言語処理が可能なAmazon Comprehendを利用している。",
        "situation_analysis": "会社は、手動でのフォーム処理を自動化し、正確なフォーム抽出に重点を置きつつ、市場への投入時間と運用オーバーヘッドを最小限に抑える必要がある。これらの要件は、TextractおよびComprehendの能力と完全に一致する。",
        "option_analysis": "オプションAは、OCRを使用するカスタムライブラリを開発するという、より複雑なアーキテクチャを提示しており、メンテナンスオーバーヘッドを引き起こす可能性がある。オプションBはAI/MLを効果的に活用しているが、モデルをホスティングするためにEC2に依存しており、マネージドサービスを使用するよりも効率が悪いかもしれない。オプションCもアーキテクチャを複雑にし、追加のリソースを要求する。",
        "additional_knowledge": "AWS Step Functionsを使用すると、ワークフローを効果的に編成でき、アプリケーションの状態やエラーをより適切に処理できる。",
        "key_terminology": "Amazon Textract, Amazon Comprehend, AWS Step Functions, AWS Lambda, 光学文字認識, データ抽出",
        "overall_assessment": "Dの選択は、運用の複雑性を減らし、フォームの処理をスケーラブルに保つために、サーバーレスおよびマネージドサービスの使用に基づいている。"
      }
    ],
    "keywords": [
      "Amazon Textract",
      "Amazon Comprehend",
      "AWS Step Functions",
      "AWS Lambda",
      "Optical Character Recognition",
      "Data Extraction"
    ]
  },
  {
    "No": "95",
    "question": "A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a\nfieet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the\norders. The company does not want to make any major changes to the application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業が、AWSクラウドでオンプレミスの注文処理プラットフォームをリファクタリングしています。このプラットフォームには、VMのフリートでホストされるウェブフロントエンド、フロントエンドとバックエンドを接続するためのRabbitMQ、注文を処理するためのコンテナ化されたバックエンドシステムを実行するKubernetesクラスターが含まれています。この企業は、アプリケーションに大きな変更を加えたくありません。どのソリューションが、運用のオーバーヘッドを最小限に抑えながらこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order- processing backend.",
        "text_jp": "ウェブサーバーVMのAMIを作成します。AMIを使用し、アプリケーションロードバランサーを設定したAmazon EC2オートスケーリンググループを作成します。オンプレミスのメッセージングキューを置き換えるためにAmazon MQを設定します。注文処理バックエンドをホストするためにAmazon Elastic Kubernetes Service(Amazon EKS)を設定します。"
      },
      {
        "key": "B",
        "text": "Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.",
        "text_jp": "ウェブサーバー環境を模倣するためにカスタムAWS Lambdaランタイムを作成します。フロントエンドウェブサーバーを置き換えるためにAmazon API Gateway APIを作成します。オンプレミスのメッセージングキューを置き換えるためにAmazon MQを設定します。注文処理バックエンドをホストするためにAmazon Elastic Kubernetes Service(Amazon EKS)を設定します。"
      },
      {
        "key": "C",
        "text": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fieet of different EC2 instances to host the order- processing backend.",
        "text_jp": "ウェブサーバーVMのAMIを作成します。AMIを使用し、アプリケーションロードバランサーを設定したAmazon EC2オートスケーリンググループを作成します。オンプレミスのメッセージングキューを置き換えるためにAmazon MQを設定します。異なるEC2インスタンスのフリートにKubernetesをインストールして、注文処理バックエンドをホストします。"
      },
      {
        "key": "D",
        "text": "Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.",
        "text_jp": "ウェブサーバーVMのAMIを作成します。AMIを使用し、アプリケーションロードバランサーを設定したAmazon EC2オートスケーリンググループを作成します。オンプレミスのメッセージングキューを置き換えるためにAmazon Simple Queue Service(Amazon SQS)キューを設定します。注文処理バックエンドをホストするためにAmazon Elastic Kubernetes Service(Amazon EKS)を設定します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This solution minimizes operational overhead while addressing the company's requirements.",
        "situation_analysis": "The company seeks to refactor its on-premises platform with minimal changes. Maintaining existing components like VMs and RabbitMQ is important.",
        "option_analysis": "Option A is aligned with minimal changes and operational overhead. It effectively utilizes existing components with AWS services for scalability. Options B, C, and D introduce unnecessary complexity or changes.",
        "additional_knowledge": "Understanding the differences between Amazon MQ and SQS is also important.",
        "key_terminology": "Amazon EC2, Auto Scaling, Application Load Balancer, Amazon MQ, Amazon EKS",
        "overall_assessment": "Overall, option A is the best choice. It addresses scalability while maintaining operational simplicity. Other options either complicate the solution or deviate from minimal changes."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。このソリューションは、運用のオーバーヘッドを最小限に抑えながら、企業の要件に対応している。",
        "situation_analysis": "企業は、最小限の変更でオンプレミスプラットフォームをリファクタリングしようとしている。VMやRabbitMQといった既存のコンポーネントを維持することが重要である。",
        "option_analysis": "選択肢Aは、最小限の変更および運用オーバーヘッドを持つソリューションに合致している。AWSサービスを活用してスケーラビリティを確保している。他の選択肢B、CおよびDは不必要な複雑性や変更を引き起こす。",
        "additional_knowledge": "Amazon MQとSQSの違いを理解することも重要である。",
        "key_terminology": "Amazon EC2, Auto Scaling, Application Load Balancer, Amazon MQ, Amazon EKS",
        "overall_assessment": "全体として、選択肢Aが最良の選択肢である。スケーラビリティを確保しつつ、運用のシンプルさを維持している。他の選択肢はソリューションを複雑にするか、最小限の変更から逸れる。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Amazon MQ",
      "Amazon EKS"
    ]
  },
  {
    "No": "96",
    "question": "A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The\nsolutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.\nThe solutions architect created the following IAM policy and attached it to an IAM role:\nDuring tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object\nresulted in an error message. The error message stated that the action was forbidden.\nWhich action must the solutions architect add to the IAM policy to meet all the requirements?",
    "question_jp": "ソリューションアーキテクトは、新しいAmazon S3バケットに保存されるオブジェクトのためにクライアント側の暗号化メカニズムを実装する必要があります。ソリューションアーキテクトは、この目的のためにAWS Key Management Service（AWS KMS）に保存されたCMKを作成しました。ソリューションアーキテクトは、以下のIAMポリシーを作成し、IAMロールにアタッチしました。テスト中、ソリューションアーキテクトはS3バケットの既存のテストオブジェクトを正常に取得できました。しかし、新しいオブジェクトをアップロードしようとしたところ、エラーメッセージが表示されました。このエラーメッセージは、アクションが禁止されていることを示していました。ソリューションアーキテクトは、すべての要件を満たすためにIAMポリシーにどのアクションを追加する必要がありますか？",
    "choices": [
      {
        "key": "A",
        "text": "kms:GenerateDataKey",
        "text_jp": "kms:GenerateDataKey"
      },
      {
        "key": "B",
        "text": "kms:GetKeyPolicy",
        "text_jp": "kms:GetKeyPolicy"
      },
      {
        "key": "C",
        "text": "kms:GetPublicKey",
        "text_jp": "kms:GetPublicKey"
      },
      {
        "key": "D",
        "text": "kms:Sign",
        "text_jp": "kms:Sign"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [
      "image_55_0.png"
    ],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: kms:GenerateDataKey. This action is necessary for generating a data encryption key which is required for client-side encryption before uploading the object to S3.",
        "situation_analysis": "The solutions architect is attempting to implement client-side encryption for objects stored in Amazon S3 using a CMK from AWS KMS. The upload of a new object resulted in a forbidden action error, indicating a missing permission.",
        "option_analysis": "Option A is correct because it allows the generation of a data key for encryption. Option B (kms:GetKeyPolicy) is related to retrieving key policies, which is not directly needed for encryption. Option C (kms:GetPublicKey) is irrelevant for symmetric key operations. Option D (kms:Sign) is also unrelated to data encryption tasks.",
        "additional_knowledge": "When implementing client-side encryption, careful planning for KMS permissions is essential to ensure seamless operations.",
        "key_terminology": "CMK, AWS KMS, Client-side encryption, Data encryption key",
        "overall_assessment": "The question is well-structured and requires an understanding of AWS KMS permissions related to client-side encryption. The community's unanimous support for option A validates this as the correct choice, aligning with the necessary permissions for such encryption operations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは A: kms:GenerateDataKey である。このアクションは、オブジェクトをS3にアップロードする前にクライアント側の暗号化に必要なデータ暗号化キーを生成するために必要である。",
        "situation_analysis": "ソリューションアーキテクトは、AWS KMSのCMKを使用してAmazon S3に格納されるオブジェクトのためにクライアント側の暗号化を実装しようとしている。新しいオブジェクトのアップロードが禁止アクションエラーを引き起こしたことは、必要な権限が不足していることを示している。",
        "option_analysis": "オプションAは正しい。なぜなら、暗号化のためのデータキーを生成することができるからである。オプションB（kms:GetKeyPolicy）はキー ポリシーの取得に関連しているが、暗号化に直接必要ではない。オプションC（kms:GetPublicKey）は対称キー操作には無関係である。オプションD（kms:Sign）もデータ暗号化タスクには無関係である。",
        "additional_knowledge": "クライアント側の暗号化を実装する際には、KMSの権限を慎重に計画することが、スムーズな操作を保証するために不可欠である。",
        "key_terminology": "CMK, AWS KMS, クライアント側の暗号化, データ暗号化キー",
        "overall_assessment": "この質問はよく構成されており、クライアント側の暗号化に関連するAWS KMSの権限の理解を必要とする。コミュニティがAオプションを支持することは、暗号化操作に必要な権限と一致している。"
      }
    ],
    "keywords": [
      "CMK",
      "AWS KMS",
      "Client-side encryption",
      "Data encryption key"
    ]
  },
  {
    "No": "97",
    "question": "A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application\nLoad Balancer. The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not\nadversely affect legitimate trafic to the application.\nHow should a solutions architect configure the web ACLs to meet these requirements?",
    "question_jp": "ある企業がウェブアプリケーションを開発しました。その企業はアプリケーションをアプリケーションロードバランサーの背後にある一群のAmazon EC2インスタンスでホスティングしています。企業はアプリケーションのセキュリティポスチャーを向上させたいと考えており、AWS WAFウェブACLsを使用する計画です。このソリューションは、アプリケーションに対する正当なトラフィックに悪影響を及ぼしてはなりません。ソリューションアーキテクトは、これらの要件を満たすためにどのようにウェブACLを構成すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.",
        "text_jp": "ウェブACLルールのアクションをCountに設定します。AWS WAFロギングを有効にします。誤検知を分析します。ルールを修正して、誤検知を回避します。時間が経つにつれて、ウェブACLルールのアクションをCountからBlockに変更します。"
      },
      {
        "key": "B",
        "text": "Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking.",
        "text_jp": "ウェブACLsにレートベースのルールのみを使用し、スロットル制限を可能な限り高く設定します。制限を超えたすべてのリクエストを一時的にブロックします。レートトラッキングの範囲を狭めるためにネストされたルールを定義します。"
      },
      {
        "key": "C",
        "text": "Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs.",
        "text_jp": "ウェブACLルールのアクションをBlockに設定します。ウェブACLsにAWS管理ルールグループのみを使用します。AWS WAFサンプルリクエストまたはAWS WAFログを使用して、ルールグループをAmazon CloudWatchメトリクスで評価します。"
      },
      {
        "key": "D",
        "text": "Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Allow to Block.",
        "text_jp": "ウェブACLsにカスタムルールグループのみを使用し、アクションをAllowに設定します。AWS WAFロギングを有効にします。誤検知を分析します。ルールを修正して、誤検知を回避します。時間が経つにつれて、ウェブACLルールのアクションをAllowからBlockに変更します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, which suggests setting the action of the web ACL rules to Count initially. This approach allows for monitoring traffic without blocking legitimate requests while providing data for future adjustments.",
        "situation_analysis": "The company aims to enhance the security of the application hosted on EC2 instances without negatively impacting legitimate traffic. Therefore, a gradual approach to rule application is necessary.",
        "option_analysis": "Option A is effective as it allows the architect to assess traffic and identify any false positives before making more permanent changes. Option B may inadvertently block legitimate traffic by setting high throttle limits. Option C may lead to immediate blocking without analysis. Option D does not provide a method to evaluate legitimate traffic before enforcing blocks.",
        "additional_knowledge": "It's crucial for organizations to continuously monitor and adapt their security measures to balance the needs of security and user experience.",
        "key_terminology": "AWS WAF, web ACL, false positives, logging, action types",
        "overall_assessment": "The question effectively tests knowledge of AWS WAF configurations and emphasizes a methodical approach to enhancing application security without disrupting service."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAであり、ウェブACLルールのアクションを最初にCountに設定することを提案しています。このアプローチにより、正当なリクエストをブロックせずにトラフィックを監視でき、将来の調整に必要なデータを提供します。",
        "situation_analysis": "企業は、EC2インスタンス上にホストされているアプリケーションのセキュリティを強化することを目指していますが、正当なトラフィックに悪影響を及ぼしてはなりません。したがって、ルールの適用に関して段階的なアプローチが必要です。",
        "option_analysis": "選択肢Aは、アーキテクトがトラフィックを評価し、誤検知を特定できるため、効果的です。選択肢Bは、高いスロットル制限を設定することで、正当なトラフィックを誤ってブロックする可能性があります。選択肢Cは、分析なしに即座にブロックを招く可能性があります。選択肢Dでは、ブロックを実施する前に正当なトラフィックを評価する方法が提供されていません。",
        "additional_knowledge": "組織は、セキュリティ措置を継続的に監視および適応させ、セキュリティとユーザーエクスペリエンスのニーズのバランスを取ることが重要です。",
        "key_terminology": "AWS WAF、ウェブACL、誤検知、ロギング、アクションタイプ",
        "overall_assessment": "この質問は、AWS WAFの設定に関する知識を効果的にテストし、サービスを中断することなくアプリケーションのセキュリティを向上させる方法論的なアプローチを強調しています。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "web ACL",
      "false positives",
      "logging",
      "action types"
    ]
  },
  {
    "No": "98",
    "question": "A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company\nmanages common security group rules for the AWS accounts in the organization.\nThe company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company's on-premises\nnetwork. Developers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own\nAWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.\nThe solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "question_jp": "ある会社は、AWS Organizations に多くの AWS アカウントを持つ組織を持っている。ソリューションアーキテクトは、会社が\n組織内の AWS アカウントに対する共通のセキュリティグループルールを管理する方法を改善しなければならない。\n会社は、オンプレミスネットワークとのアクセスを許可するために、各 AWS アカウントに許可リストに共通の IP CIDR 範囲を持っている。\n各アカウント内の開発者は、そのセキュリティグループに新しい IP CIDR 範囲を追加する責任がある。セキュリティチームは独自の\nAWS アカウントを持っている。現在、セキュリティチームは、許可リストへの変更が行われたときに他の AWS アカウントのオーナーに通知している。\nソリューションアーキテクトは、共通の CIDR 範囲セットをすべてのアカウントに配布するソリューションを設計しなければならない。\nどのソリューションが運用上のオーバーヘッドを最小限に抑えてこれらの要件を満たすのか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS account. Deploy an AWS Lambda function in each AWS account. Configure the Lambda function to run every time an SNS topic receives a message. Configure the Lambda function to take an IP address as input and add it to a list of security groups in the account. Instruct the security team to distribute changes by publishing messages to its SNS topic.",
        "text_jp": "セキュリティチームの AWS アカウントに Amazon Simple Notification Service (Amazon SNS) トピックを設定する。各 AWS アカウントに AWS Lambda 関数をデプロイする。SNS トピックがメッセージを受信するたびに Lambda 関数を実行するように設定する。Lambda 関数に IP アドレスを入力として受け取り、そのアカウントのセキュリティグループのリストに追加するように設定する。セキュリティチームに変更を配布するために SNS トピックにメッセージを公開するよう指示する。"
      },
      {
        "key": "B",
        "text": "Create new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all internal CIDR ranges. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups. Instruct the security team to share updates with each AWS account owner.",
        "text_jp": "組織内の各 AWS アカウントに新しい顧客管理のプレフィックスリストを作成する。各アカウントにすべての内部 CIDR 範囲でプレフィックスリストを満たす。各 AWS アカウントの所有者に、新しい顧客管理のプレフィックスリスト ID をそのアカウントのセキュリティグループで許可するよう通知する。セキュリティチームに各 AWS アカウントの所有者に更新を共有するよう指示する。"
      },
      {
        "key": "C",
        "text": "Create a new customer-managed prefix list in the security team's AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups.",
        "text_jp": "セキュリティチームの AWS アカウントに新しい顧客管理のプレフィックスリストを作成する。すべての内部 CIDR 範囲で顧客管理のプレフィックスリストを満たす。AWS Resource Access Manager を使用して、組織と顧客管理のプレフィックスリストを共有する。各 AWS アカウントの所有者に、新しい顧客管理のプレフィックスリスト ID をそのセキュリティグループで許可するよう通知する。"
      },
      {
        "key": "D",
        "text": "Create an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in the security team's AWS account. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account.",
        "text_jp": "組織内の各アカウントに IAM ロールを作成する。セキュリティグループを更新するための権限を付与する。セキュリティチームの AWS アカウントに AWS Lambda 関数をデプロイする。Lambda 関数に内部 IP アドレスのリストを入力として受け取り、各組織アカウントのロールを引き受け、そのリストの IP アドレスを各アカウントのセキュリティグループに追加するように設定する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (84%) D (16%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This solution leverages AWS Resource Access Manager to share a centrally managed prefix list across accounts, reducing operational overhead.",
        "situation_analysis": "The organization requires a systematic way to manage CIDR ranges across multiple accounts with minimal effort. This involves ensuring that changes to the allow list are effectively communicated and implemented without requiring individual actions in each account.",
        "option_analysis": "Option A requires setting up an SNS and deploying Lambda functions, which brings complexity. Option B necessitates manual updates to permissions in each account, increasing overhead. Option D involves creating IAM roles and Lambda functions as well, which could introduce more steps than necessary. Option C streamlines this process significantly.",
        "additional_knowledge": "Utilizing AWS Resource Access Manager can simplify cross-account resource sharing and is recommended for organizations with a complex AWS environment.",
        "key_terminology": "AWS Resource Access Manager, customer-managed prefix list, CIDR ranges, operational overhead",
        "overall_assessment": "Considering the requirements, option C is the most efficient and least burdensome solution among the choices. The strong community support (84%) further validates this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは C である。このソリューションは、AWS Resource Access Manager を活用して、集中管理されたプレフィックスリストをアカウント間で共有し、運用上のオーバーヘッドを削減する。",
        "situation_analysis": "組織は、最小限の労力で複数のアカウント間で CIDR 範囲を管理する体系的な方法を必要としている。これには、許可リストの変更が効果的に伝達され、各アカウントで個別に操作しなくても実施されることが求められる。",
        "option_analysis": "選択肢 A は SNS を設定し、Lambda 関数をデプロイする必要があり、複雑さをもたらす。選択肢 B は、各アカウントでの権限の手動での更新を必要とし、オーバーヘッドを増加させる。選択肢 D は、IAM ロールと Lambda 関数を作成することを含み、必要以上のステップを導入する可能性がある。選択肢 C は、このプロセスを大幅に簡素化する。",
        "additional_knowledge": "AWS Resource Access Manager を利用することで、アカウント間のリソース共有を簡素化できるため、複雑な AWS 環境を持つ組織に推奨される。",
        "key_terminology": "AWS Resource Access Manager、顧客管理プレフィックスリスト、CIDR 範囲、運用オーバーヘッド",
        "overall_assessment": "要求事項を考慮すると、選択肢 C が選択肢の中で最も効率的で負担が少ないソリューションである。強いコミュニティの支援（84%）がこの選択肢をさらに裏付けている。"
      }
    ],
    "keywords": [
      "AWS Resource Access Manager",
      "customer-managed prefix list",
      "CIDR ranges",
      "operational overhead"
    ]
  },
  {
    "No": "99",
    "question": "A company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a VPN. The company is\nhosting internal applications with VPCs in multiple AWS accounts. Currently, the applications are accessible from the company's on-premises\nofice network through an AWS Site-to-Site VPN connection. The VPC in the company's main AWS account has peering connections established\nwith VPCs in other AWS accounts.\nA solutions architect must design a scalable AWS Client VPN solution for employees to use while they work from home.\nWhat is the MOST cost-effective solution that meets these requirements?",
    "question_jp": "ある企業は従業員が自宅からリモートで作業できる新しいポリシーを導入し、VPNを使用して接続することを求めています。この企業は、複数のAWSアカウントにVPCをホストしている内部アプリケーションを運用しています。現在、これらのアプリケーションは、AWS Site-to-Site VPN接続を通じて企業のオンプレミスオフィスネットワークからアクセス可能です。企業の主要なAWSアカウントのVPCには、他のAWSアカウントのVPCとのピア接続が確立されています。ソリューションアーキテクトは、従業員が自宅で作業する際に使用するスケーラブルなAWS Client VPNソリューションを設計する必要があります。これらの要件を満たす最もコスト効果の高いソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a Client VPN endpoint in each AWS account. Configure required routing that allows access to internal applications.",
        "text_jp": "各AWSアカウントにClient VPNエンドポイントを作成します。内部アプリケーションへのアクセスを許可するために必要なルーティングを構成します。"
      },
      {
        "key": "B",
        "text": "Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications.",
        "text_jp": "主要なAWSアカウントにClient VPNエンドポイントを作成します。内部アプリケーションへのアクセスを許可するために必要なルーティングを構成します。"
      },
      {
        "key": "C",
        "text": "Create a Client VPN endpoint in the main AWS account. Provision a transit gateway that is connected to each AWS account. Configure required routing that allows access to internal applications.",
        "text_jp": "主要なAWSアカウントにClient VPNエンドポイントを作成します。各AWSアカウントに接続されているトランジットゲートウェイをプロビジョニングします。内部アプリケーションへのアクセスを許可するために必要なルーティングを構成します。"
      },
      {
        "key": "D",
        "text": "Create a Client VPN endpoint in the main AWS account. Establish connectivity between the Client VPN endpoint and the AWS Site-to-Site VPN.",
        "text_jp": "主要なAWSアカウントにClient VPNエンドポイントを作成します。Client VPNエンドポイントとAWS Site-to-Site VPNの間に接続を確立します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (66%) C (34%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct answer is B: Create a Client VPN endpoint in the main AWS account, and configure required routing to access internal applications.",
        "situation_analysis": "The requirement is to enable remote access for employees using a VPN while ensuring cost-effectiveness. The existing infrastructure includes multiple AWS accounts with VPCs and a Site-to-Site VPN.",
        "option_analysis": "Option B addresses these requirements efficiently by centralizing the Client VPN in the main account, reducing management overhead and cost compared to options A and C, which involve setting up Client VPNs in each account or additional resources like transit gateways.",
        "additional_knowledge": "Using a single Client VPN endpoint is a best practice in scenarios with multiple VPCs, as it streamlines access management.",
        "key_terminology": "AWS Client VPN, VPC Peering, Site-to-Site VPN, routing configuration, cost-effectiveness",
        "overall_assessment": "Option B is the most suitable as it minimizes complexity and cost while fulfilling the requirement for remote connectivity efficiently. Community votes align with this recommendation."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです：主要なAWSアカウントにClient VPNエンドポイントを作成し、内部アプリケーションにアクセスするために必要なルーティングを構成します。",
        "situation_analysis": "従業員がVPNを利用してリモートアクセスを可能にするという要件がありますが、コスト効率も重要です。既存のインフラには、複数のAWSアカウントにVPCとSite-to-Site VPNが含まれています。",
        "option_analysis": "選択肢Bは、主要なアカウントにClient VPNを中央集権化し、各アカウントにClient VPNを設定したり、トランジットゲートウェイなどの追加リソースを利用するAやCと比較して、管理の重複やコストを削減できます。",
        "additional_knowledge": "複数のVPCが存在するシナリオでは、単一のClient VPNエンドポイントを使用することがベストプラクティスであり、アクセス管理を効率化します。",
        "key_terminology": "AWS Client VPN、VPCピアリング、Site-to-Site VPN、ルーティング構成、コスト効率",
        "overall_assessment": "選択肢Bは、複雑さとコストを最小限に抑えつつ、効率的にリモート接続の要件を満たす最も適切な選択肢です。コミュニティの投票もこの推薦に一致しています。"
      }
    ],
    "keywords": [
      "AWS Client VPN",
      "VPC Peering",
      "Site-to-Site VPN",
      "routing configuration",
      "cost-effectiveness"
    ]
  },
  {
    "No": "100",
    "question": "A company is running an application in the AWS Cloud. Recent application metrics show inconsistent response times and a significant increase in\nerror rates. Calls to third-party services are causing the delays. Currently, the application calls third-party services synchronously by directly\ninvoking an AWS Lambda function.\nA solutions architect needs to decouple the third-party service calls and ensure that all the calls are eventually completed.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWSクラウドでアプリケーションを運用しています。最近のアプリケーションメトリクスでは、一貫しない応答時間とエラー率の大幅な上昇が示されています。サードパーティのサービスへの呼び出しが遅延の原因となっています。現在、アプリケーションはAWS Lambda関数を直接呼び出すことでサードパーティのサービスを同期的に呼び出しています。ソリューションアーキテクトは、サードパーティサービスの呼び出しを分離し、すべての呼び出しが最終的に完了することを確実にする必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function.",
        "text_jp": "Amazon Simple Queue Service（Amazon SQS）キューを使用してイベントを保存し、Lambda関数を呼び出します。"
      },
      {
        "key": "B",
        "text": "Use an AWS Step Functions state machine to pass events to the Lambda function.",
        "text_jp": "AWS Step Functionsステートマシンを使用してイベントをLambda関数に渡します。"
      },
      {
        "key": "C",
        "text": "Use an Amazon EventBridge rule to pass events to the Lambda function.",
        "text_jp": "Amazon EventBridgeルールを使用してイベントをLambda関数に渡します。"
      },
      {
        "key": "D",
        "text": "Use an Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function.",
        "text_jp": "Amazon Simple Notification Service（Amazon SNS）トピックを使用してイベントを保存し、Lambda関数を呼び出します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using Amazon SQS allows decoupling of service calls by placing the requests in a queue, enabling asynchronous processing.",
        "situation_analysis": "The application experiences inconsistent response times and increased error rates due to synchronous calls to third-party services. A need for decoupling these calls arises.",
        "option_analysis": "Option A allows for the asynchronous processing of requests, separating the caller from the service's response time. Option B (Step Functions) would create more complexity than necessary. Option C (EventBridge) may not guarantee the order of events. Option D (SNS) is designed for pub/sub models rather than queueing.",
        "additional_knowledge": "Implementing SQS can help in scenarios where immediate response is not critical, but reliability is.",
        "key_terminology": "Amazon SQS, asynchronous processing, decoupling, message queue, service architecture",
        "overall_assessment": "Given the need for decoupling in this scenario, using SQS is the most appropriate solution. The community agrees as shown by the 100% vote."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。Amazon SQSを使用することで、リクエストをキューに配置し、非同期処理を可能にすることにより、サービス呼び出しを分離できます。",
        "situation_analysis": "アプリケーションは、サードパーティサービスへの同期呼び出しにより、一貫しない応答時間とエラー率の増加を経験しています。この呼び出しを分離する必要があります。",
        "option_analysis": "選択肢Aは、リクエストの非同期処理を可能にし、呼び出し元とサービスの応答時間を分離します。選択肢B（Step Functions）は必要以上に複雑になります。選択肢C（EventBridge）はイベントの順序を保証できません。選択肢D（SNS）はキューイングではなく、pub/subモデルのために設計されています。",
        "additional_knowledge": "SQSを実装することで即時の応答が重要でないが、信頼性が求められるシナリオで役立ちます。",
        "key_terminology": "Amazon SQS、非同期処理、分離、メッセージQueue、サービスアーキテクチャ",
        "overall_assessment": "このシナリオにおける分離の必要性を考慮すると、SQSの使用が最も適切なソリューションです。コミュニティも100%の投票で支持しています。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "asynchronous processing",
      "decoupling",
      "message queue",
      "service architecture"
    ]
  },
  {
    "No": "101",
    "question": "A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS\naccounts in AWS Organizations.\nThe sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The\nmarketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key\nManagement Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in\nthe marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業が、マルチアカウント環境でAWS上にアプリケーションを実行しています。企業の営業チームとマーケティングチームは、AWS Organizations内で別々のAWSアカウントを使用しています。 営業チームはペタバイトのデータをAmazon S3バケットに保存しています。マーケティングチームはデータ可視化のためにAmazon QuickSightを使用しています。マーケティングチームは、営業チームがS3バケットに保存しているデータにアクセスする必要があります。企業はS3バケットをAWS Key Management Service (AWS KMS)キーで暗号化しています。 マーケティングチームはすでにQuickSightへのアクセスを提供するために、マーケティングAWSアカウント内にIAMサービスロールを作成しています。企業は、AWSアカウント間でS3バケット内のデータへの安全なアクセスを提供するソリューションが必要です。最小限の運用負荷でこの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket.",
        "text_jp": "マーケティングアカウントに新しいS3バケットを作成します。営業アカウント内でS3レプリケーションルールを作成し、オブジェクトをマーケティングアカウントの新しいS3バケットにコピーします。マーケティングアカウント内のQuickSightの権限を更新して、新しいS3バケットへのアクセスを許可します。"
      },
      {
        "key": "B",
        "text": "Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sates account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.",
        "text_jp": "SCPを作成して、マーケティングアカウントにS3バケットへのアクセスを許可します。AWS Resource Access Manager (AWS RAM)を使用して、営業アカウントのKMSキーをマーケティングアカウントと共有します。マーケティングアカウント内のQuickSightの権限を更新して、S3バケットへのアクセスを許可します。"
      },
      {
        "key": "C",
        "text": "Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.",
        "text_jp": "マーケティングアカウント内のS3バケットポリシーを更新して、QuickSightロールへのアクセスを許可します。S3バケットで使用される暗号化キーに対してKMSの付与を作成します。QuickSightロールに復号アクセスを付与します。マーケティングアカウント内のQuickSightの権限を更新して、S3バケットへのアクセスを許可します。"
      },
      {
        "key": "D",
        "text": "Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight rote, to create a trust relationship with the new IAM role in the sales account.",
        "text_jp": "営業アカウント内にIAMロールを作成し、S3バケットへのアクセスを許可します。マーケティングアカウントから営業アカウントのIAMロールを引き受けてS3バケットにアクセスします。新しいIAMロールとの信頼関係を作成するために、QuickSightロールを更新します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (74%) C (15%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution allows the marketing team to securely access the S3 bucket in the sales account with minimal operational overhead.",
        "situation_analysis": "The company has a multi-account setup with separate teams needing access to shared data. The marketing team requires access to large datasets stored by the sales team while maintaining data security and encryption standards.",
        "option_analysis": "Option A involves creating a new S3 bucket and setting up replication, which adds unnecessary complexity and overhead. Option B suggests using SCPs and AWS RAM but does not directly grant access needed for the IAM role in QuickSight. Option C requires complex permissions management that is not as straightforward as Option D. Option D is the best choice as it involves creating an IAM role with appropriate permissions without unnecessary data replication.",
        "additional_knowledge": "This approach also minimizes operational overhead since it avoids the need for S3 bucket replication and complex SCP configurations.",
        "key_terminology": "IAM Role, S3 Bucket Policy, AWS KMS, Multi-account Environment, AWS Organizations, Cross-account Access",
        "overall_assessment": "The choice of Option D provides a direct and efficient method of access without creating additional data copies or overly complex permission setups. It aligns with AWS best practices for cross-account access."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。このソリューションにより、マーケティングチームは最小限の運用負荷で営業アカウント内のS3バケットに安全にアクセスできる。",
        "situation_analysis": "企業は、共有データへのアクセスが必要な別々のチームを持つマルチアカウント環境を構築している。マーケティングチームは、データセキュリティと暗号化基準を維持しながら、営業チームが保存している大規模データセットへのアクセスを必要としている。",
        "option_analysis": "選択肢Aは、新しいS3バケットを作成し、レプリケーションを設定する必要があり、不要な複雑さとオーバーヘッドを追加する。選択肢Bは、SCPとAWS RAMを使用すると提案するが、QuickSightのIAMロールに必要なアクセスを直接提供するものではない。選択肢Cは、複雑な権限管理を必要とし、選択肢Dと同じくらい簡単ではない。選択肢Dが最適な選択であり、必要な権限を持つIAMロールを作成することで、不要なデータレプリケーションを避ける。",
        "additional_knowledge": "このアプローチは、S3バケットのレプリケーションや複雑なSCP設定の必要がなく、運用負荷を最小限に抑える。",
        "key_terminology": "IAMロール、S3バケットポリシー、AWS KMS、マルチアカウント環境、AWS Organizations、アカウント間アクセス",
        "overall_assessment": "選択肢Dは、データの複製や過度に複雑な権限設定を作成することなく、直接的で効率的なアクセス方法を提供する。AWSのベストプラクティスに沿っており、アカウント間のアクセスに適している。"
      }
    ],
    "keywords": [
      "IAM Role",
      "S3 Bucket Policy",
      "AWS KMS",
      "Multi-account Environment",
      "AWS Organizations",
      "Cross-account Access"
    ]
  },
  {
    "No": "102",
    "question": "A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises\ninstallation of a Microsoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service. A solutions\narchitect must design a heterogeneous database migration on AWS.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、オンプレミスのデータセンターからAWSにビジネスクリティカルなアプリケーションを移行することを計画しています。この企業は、オンプレミスにMicrosoft SQL Server Always Onクラスターのインストールを持っています。この企業は、AWSのマネージドデータベースサービスに移行したいと考えています。ソリューションアーキテクトは、AWS上での異種データベース移行を設計する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities.",
        "text_jp": "バックアップと復元ユーティリティを使用して、SQL ServerデータベースをAmazon RDS for MySQLに移行する。"
      },
      {
        "key": "B",
        "text": "Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.",
        "text_jp": "AWS Snowball Edge Storage Optimizedデバイスを使用してデータをAmazon S3に転送します。Amazon RDS for MySQLをセットアップします。BULK INSERTなどのSQL Server機能とのS3統合を使用します。"
      },
      {
        "key": "C",
        "text": "Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS.",
        "text_jp": "AWS Schema Conversion Toolを使用してデータベーススキーマをAmazon RDS for MySQLに変換します。その後、AWS Database Migration Service (AWS DMS)を使用してオンプレミスのデータベースからAmazon RDSにデータを移行します。"
      },
      {
        "key": "D",
        "text": "Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.",
        "text_jp": "AWS DataSyncを使用して、オンプレミスストレージとAmazon S3の間でネットワーク越しにデータを移行します。Amazon RDS for MySQLをセットアップします。BULK INSERTなどのSQL Server機能とのS3統合を使用します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using the AWS Schema Conversion Tool and AWS Database Migration Service (AWS DMS) allows for the seamless migration of data and schema from SQL Server to Amazon RDS for MySQL, addressing the heterogeneous migration requirement.",
        "situation_analysis": "The company has critical applications running on SQL Server and seeks to migrate them to a managed service. This necessitates a comprehensive migration approach that includes schema and data.",
        "option_analysis": "Option A is incorrect because backup and restore utilities are not suitable for migrating to a different database engine like MySQL. Option B relies on S3 integration, which may not support all features necessary for a SQL Server migration. Option D, while it uses DataSync, lacks the capability for schema conversion.",
        "additional_knowledge": "Proper migration planning involves considering not only the data but also the schema, especially when changing database engines.",
        "key_terminology": "AWS Schema Conversion Tool, AWS Database Migration Service, heterogeneous migration",
        "overall_assessment": "This question effectively assesses the understanding of migration strategies and tools provided by AWS, particularly for heterogeneous database scenarios. The community's unanimous support for option C confirms its validity as the best solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。AWS Schema Conversion ToolとAWS Database Migration Service (AWS DMS)を使用することで、SQL ServerからAmazon RDS for MySQLへのデータとスキーマのシームレスな移行が可能になり、異種移行の要件に応えます。",
        "situation_analysis": "企業はSQL Server上でクリティカルなアプリケーションを運用しており、これをマネージドサービスに移行したいと考えています。このため、スキーマとデータを含む包括的な移行アプローチが必要です。",
        "option_analysis": "選択肢Aは、バックアップと復元ユーティリティがMySQLのような異なるデータベースエンジンへの移行に適していないため、誤りです。選択肢BはS3統合に依存しており、SQL Serverの移行に必要なすべての機能をサポートしているわけではありません。選択肢DはDataSyncを使用していますが、スキーマ変換の能力が不足しています。",
        "additional_knowledge": "正しい移行計画にはデータだけでなく、特にデータベースエンジンを変更する場合はスキーマも考慮する必要があります。",
        "key_terminology": "AWS Schema Conversion Tool, AWS Database Migration Service, 異種移行",
        "overall_assessment": "この質問は、特に異種データベースシナリオにおけるAWSの移行戦略とツールの理解を評価する上で効果的です。コミュニティの一致した選択Cへの支持は、その有効性を確認しています。"
      }
    ],
    "keywords": [
      "AWS Schema Conversion Tool",
      "AWS Database Migration Service",
      "heterogeneous migration"
    ]
  },
  {
    "No": "103",
    "question": "A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the\nicons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account\nthat members of the design team can access.\nAfter the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the\nproduction account. A solutions architect must provide the design team with access to the production account without exposing other parts of the\nweb application to the risk of unwanted changes.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "出版会社のデザインチームは、eコマースウェブアプリケーションが使用するアイコンやその他の静的アセットを更新します。会社は、アイコンやアセットを企業の本番アカウントでホストされているAmazon S3バケットから提供しています。会社は、デザインチームがアクセスできる開発アカウントも使用しています。\nデザインチームは、開発アカウントで静的アセットをテストした後、アセットを本番アカウントのS3バケットにロードする必要があります。ソリューションアーキテクトは、他のウェブアプリケーション部分への不要な変更のリスクを露出させることなく、デザインチームに本番アカウントへのアクセスを提供する必要があります。\nどの手順の組み合わせがこれらの要件を満たしますか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "In the production account, create a new IAM policy that allows read and write access to the S3 bucket.",
        "text_jp": "本番アカウントでS3バケットへの読み取りおよび書き込みアクセスを許可する新しいIAMポリシーを作成します。"
      },
      {
        "key": "B",
        "text": "In the development account, create a new IAM policy that allows read and write access to the S3 bucket.",
        "text_jp": "開発アカウントにS3バケットへの読み取りおよび書き込みアクセスを許可する新しいIAMポリシーを作成します。"
      },
      {
        "key": "C",
        "text": "In the production account, create a role Attach the new policy to the role. Define the development account as a trusted entity.",
        "text_jp": "本番アカウントでロールを作成し、新しいポリシーをそのロールにアタッチします。開発アカウントを信頼されたエンティティとして定義します。"
      },
      {
        "key": "D",
        "text": "In the development account, create a role. Attach the new policy to the role Define the production account as a trusted entity.",
        "text_jp": "開発アカウントでロールを作成し、新しいポリシーをそのロールにアタッチします。本番アカウントを信頼されたエンティティとして定義します。"
      },
      {
        "key": "E",
        "text": "In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role In the production account.",
        "text_jp": "開発アカウントでデザインチームのIAMユーザー全員を含むグループを作成し、そのグループに本番アカウントのロールに対するsts:AssumeRoleアクションを許可する異なるIAMポリシーをアタッチします。"
      },
      {
        "key": "F",
        "text": "In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account.",
        "text_jp": "開発アカウントでデザインチームのIAMユーザー全員を含むグループを作成し、そのグループに開発アカウントのロールに対するsts:AssumeRoleアクションを許可する異なるIAMポリシーをアタッチします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACE (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers to provide the design team access without exposing other parts of the web application are A, C, and E.",
        "situation_analysis": "The design team needs the ability to upload static assets to the production S3 bucket securely while limiting their access.",
        "option_analysis": "Option A allows read and write access to the S3 bucket in the production account. Option C sets up a role that the design team can assume. Option E ensures that the team members have the rights to assume the role, making this a comprehensive approach.",
        "additional_knowledge": "The team should always test the implementation in a safe environment before deploying it to production.",
        "key_terminology": "IAM, S3, AssumeRole, Policy, Cross-Account Access",
        "overall_assessment": "This scenario presents a typical use case for managing access in AWS environments. The answer choice aligns well with best practices, especially regarding securing production resources."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "デザインチームに他のウェブアプリケーションの部分を露出させずにアクセスを提供するための正しい答えはA、C、Eである。",
        "situation_analysis": "デザインチームは、生産的なS3バケットに静的アセットを安全にアップロードする能力を必要としているが、そのアクセスは制限されるべきである。",
        "option_analysis": "選択肢Aは、本番アカウントのS3バケットへの読み取りおよび書き込みアクセスを許可する。選択肢Cは、デザインチームが引き受けることができるロールを設定する。選択肢Eは、チームメンバーがそのロールを引き受ける権限を持つことを保証することにより、包括的なアプローチを実現する。",
        "additional_knowledge": "デザインチームは、本番環境に展開する前に、安全な環境での実装テストを常に行うべきである。",
        "key_terminology": "IAM、S3、AssumeRole、ポリシー、クロスアカウントアクセス",
        "overall_assessment": "このシナリオは、AWS環境でのアクセス管理の典型的なユースケースを提示している。回答の選択肢は、特に本番リソースを保護することに関して、ベストプラクティスに良く合致している。"
      }
    ],
    "keywords": [
      "IAM",
      "S3",
      "AssumeRole",
      "Policy",
      "Cross-Account Access"
    ]
  },
  {
    "No": "104",
    "question": "A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's\ndevelopment team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU\nthan expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.\nA solutions architect must mitigate the performance issues before the company launches the application to production.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業は、AWS Elastic BeanstalkとJavaを使用してパイロットアプリケーションを開発した。開発中のコストを削減するために、企業の開発チームはアプリケーションを単一インスタンス環境にデプロイした。最近のテストでは、アプリケーションが予想以上にCPUを多く消費していることがわかった。CPU使用率は常に85％を超え、パフォーマンスのボトルネックを引き起こしている。ソリューションアーキテクトは、企業がアプリケーションを本番環境に移行する前にパフォーマンスの問題を軽減する必要がある。どのソリューションが最小の運用負荷でこれらの要件を満たすか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes.",
        "text_jp": "新しいElastic Beanstalkアプリケーションを作成する。負荷分散環境タイプを選択する。すべてのアベイラビリティーゾーンを選択する。最大CPU使用率が85％を5分間超えた場合に実行されるスケールアウトルールを追加する。"
      },
      {
        "key": "B",
        "text": "Create a second Elastic Beanstalk environment. Apply the trafic-splitting deployment policy. Specify a percentage of incoming trafic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes.",
        "text_jp": "2つ目のElastic Beanstalk環境を作成する。トラフィックスプリッティングデプロイメントポリシーを適用する。平均CPU使用率が85％を5分間超えている場合に、新しい環境に向けて到着トラフィックの割合を指定する。"
      },
      {
        "key": "C",
        "text": "Modify the existing environment's capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes.",
        "text_jp": "既存の環境の容量構成を変更して、負荷分散環境タイプを使用する。すべてのアベイラビリティーゾーンを選択する。平均CPU使用率が85％を5分間超えた場合に実行されるスケールアウトルールを追加する。"
      },
      {
        "key": "D",
        "text": "Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes.",
        "text_jp": "負荷分散オプションで環境を再構築するアクションを選択する。アベイラビリティーゾーンを選択する。合計CPU使用率が85％を5分間超えた場合に実行されるスケールアウトルールを追加する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. It provides an effective solution to mitigate CPU performance issues with minimal operational overhead.",
        "situation_analysis": "The application is consuming more CPU resources than expected, indicating a need for scalability to support increased loads.",
        "option_analysis": "Option A involves creating a new Elastic Beanstalk application with load balancing, which optimally distributes demand across instances. Option C also offers a load-balanced environment, but modifying an existing environment may involve more operational tasks. Options B and D do not directly resolve the CPU-related performance issue effectively.",
        "additional_knowledge": "Using a load-balanced environment not only addresses current CPU limitations but also positions the application for future growth.",
        "key_terminology": "Elastic Beanstalk, Load Balancing, Auto Scaling",
        "overall_assessment": "Choosing option A is the most practical approach as it sets up a foundation for scalable performance while minimizing maintenance tasks."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。これは、最小限の運用負荷でCPUパフォーマンスの問題を軽減する効果的なソリューションを提供する。",
        "situation_analysis": "アプリケーションは、予想以上にCPUリソースを消費しており、増加した負荷を支えるためのスケーラビリティの必要性が示唆される。",
        "option_analysis": "選択肢Aは、負荷分散を使用した新しいElastic Beanstalkアプリケーションを作成するもので、インスタンス間で需要を最適に分散する。選択肢Cも負荷分散環境を提供するが、既存の環境を変更することはより多くの運用タスクを伴う可能性がある。選択肢BとDは、CPUに関連するパフォーマンスの問題を効果的に解決しない。",
        "additional_knowledge": "負荷分散環境を使用することで、現在のCPUの制限を解決するだけでなく、アプリケーションの将来の成長に備えることができる。",
        "key_terminology": "Elastic Beanstalk, 負荷分散, 自動スケーリング",
        "overall_assessment": "選択肢Aを選ぶことは、メンテナンスタスクを最小限に抑えつつ、スケーラブルなパフォーマンスの基盤を設定する最も実用的なアプローチである。"
      }
    ],
    "keywords": [
      "Elastic Beanstalk",
      "Load Balancing",
      "Auto Scaling"
    ]
  },
  {
    "No": "105",
    "question": "A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed\nMySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of trafic during the month.\nHowever, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load\nBalancers and Auto Scaling within its infrastructure to meet the increased demand.\nWhich of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?",
    "question_jp": "ある金融会社は、ビジネスクリティカルなアプリケーションを最新世代のLinux EC2インスタンス上で運用しています。このアプリケーションには、重いI/O操作を行う自己管理型MySQLデータベースが含まれています。アプリケーションは、月内の中程度のトラフィックを処理する際に正常に動作しています。しかし、月末の報告のために、月の最後の3日間は遅くなります。この間、会社はElastic Load BalancersとAuto Scalingを利用して、需要の増加に対応しています。次のうち、データベースが月末の負荷を最小限の性能への影響で処理できるようにするためのアクションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.",
        "text_jp": "Elastic Load Balancersのプリウォーミング、大きなインスタンスタイプの使用、すべてのAmazon EBSボリュームをGP2ボリュームに変更すること。"
      },
      {
        "key": "B",
        "text": "Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.",
        "text_jp": "データベースクラスターをAmazon RDSに一度の移行を行い、月末の負荷を処理するための追加のリードレプリカを作成すること。"
      },
      {
        "key": "C",
        "text": "Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.",
        "text_jp": "特定のCloudWatchメトリクスに基づいて、Amazon EBSボリュームのタイプ、サイズ、またはIOPSを変更するためにAmazon CloudWatchをAWS Lambdaと共に使用すること。"
      },
      {
        "key": "D",
        "text": "Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards.",
        "text_jp": "すべての既存のAmazon EBSボリュームを、新しいPIOPSボリュームに置き換え、最大のストレージサイズとI/O速度を持つスナップショットを取得し、月末の前後で戻すこと。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Migrating to Amazon RDS with additional read replicas allows for better management of heavy I/O during peak reporting periods.",
        "situation_analysis": "The application experiences performance degradation during month-end reporting, despite existing load balancing and scalability mechanisms.",
        "option_analysis": "Option B effectively addresses the I/O performance issues by utilizing Amazon RDS which is optimized for database workloads, and read replicas help distribute the read load. Options A, C, and D do not adequately resolve performance constraints tied to intensive I/O operations.",
        "additional_knowledge": "The use of MySQL within RDS comes with managed features, easing administrative burdens.",
        "key_terminology": "Amazon RDS, read replicas, IOPS, database migration, performance optimization.",
        "overall_assessment": "Option B is strongly supported by community feedback, indicating a consensus on its effectiveness. Other options lack similar community support."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBです。Amazon RDSに移行し、追加のリードレプリカを作成することで、ピーク時の報告期間における重いI/Oの管理が改善されます。",
        "situation_analysis": "アプリケーションは、ロードバランシングやスケーラビリティのメカニズムが存在するにもかかわらず、月末の報告中に性能の低下を経験しています。",
        "option_analysis": "オプションBは、Amazon RDSを利用することでI/Oパフォーマンスの問題に効果的に対処し、リードレプリカにより読み込み負荷を分散します。オプションA、C、Dは、集中的なI/O操作に関連するパフォーマンスの制約に対して十分に解決できません。",
        "additional_knowledge": "RDS内でのMySQLの使用は、管理機能とともに提供され、管理上の負担を軽減します。",
        "key_terminology": "Amazon RDS、リードレプリカ、IOPS、データベース移行、パフォーマンス最適化。",
        "overall_assessment": "オプションBはコミュニティからのフィードバックによって強く支持されており、その効果に対するコンセンサスがあります。他のオプションは同様のコミュニティのサポートを欠いています。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "read replicas",
      "IOPS",
      "database migration",
      "performance optimization"
    ]
  },
  {
    "No": "106",
    "question": "A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable. but\nthe company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative\noverhead to maintain the servers.\nWhich solution will meet these requirements with the LEAST code changes?",
    "question_jp": "ある企業は、データセンター内のVMに複雑な依存関係を持つJavaアプリケーションを運用しています。このアプリケーションは安定していますが、企業は技術スタックを近代化したいと考えています。企業はアプリケーションをAWSに移行し、サーバーの管理オーバーヘッドを最小限に抑えたいと考えています。\nどの解決策が最小限のコード変更でこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application.",
        "text_jp": "AWS App2Containerを使用して、Amazon Elastic Container Service（Amazon ECS）にアプリケーションをAWS Fargateに移行します。コンテナイメージはAmazon Elastic Container Registry（Amazon ECR）に格納します。ECSタスク実行ロールにECRイメージリポジトリへのアクセス権限を付与します。アプリケーションロードバランサー（ALB）を使用するようにAmazon ECSを構成します。ALBを使用してアプリケーションと対話させます。"
      },
      {
        "key": "B",
        "text": "Migrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration. Use API Gateway to interact with the application.",
        "text_jp": "アプリケーションコードをAWS Lambdaで実行されるコンテナに移行します。Lambda統合を使用したAmazon API Gateway REST APIを構築します。API Gatewayを使用してアプリケーションと対話させます。"
      },
      {
        "key": "C",
        "text": "Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Give the EKS nodes permission to access the ECR image repository. Use Amazon API Gateway to interact with the application.",
        "text_jp": "AWS App2Containerを使用してアプリケーションをAmazon Elastic Kubernetes Service（Amazon EKS）に、EKS管理ノードグループで移行します。コンテナイメージはAmazon Elastic Container Registry（Amazon ECR）に格納します。EKSノードにECRイメージリポジトリへのアクセス権限を付与します。アプリケーションと対話するためにAmazon API Gatewayを使用します。"
      },
      {
        "key": "D",
        "text": "Migrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use the ALB to interact with the application.",
        "text_jp": "アプリケーションコードをAWS Lambdaで実行されるコンテナに移行します。Lambdaをアプリケーションロードバランサー（ALB）を使用するように構成します。ALBを使用してアプリケーションと対話させます。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Migrating the application code to a container running in AWS Lambda reduces administrative overhead while requiring minimal changes to the application code.",
        "situation_analysis": "The requirement to modernize the technology stack and lower administrative overhead while keeping the application stable makes containerization in AWS a good choice.",
        "option_analysis": "Option A requires moving to ECS and using an ALB, which involves more changes and management overhead. Option C involves EKS, which is more complex and involves more configuration. Option D still moves to Lambda but introduces an ALB unnecessarily, adding more complexity than needed.",
        "additional_knowledge": "Using AWS Lambda also allows for significant cost savings with its pay-as-you-go model and automatic scaling.",
        "key_terminology": "Serverless, AWS Lambda, API Gateway, Containerization, Minimal changes.",
        "overall_assessment": "Considering the community vote distribution, although it heavily favors choice A, the specifics of the question strongly align with B's requirements for minimal change and low overhead."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。アプリケーションコードをAWS Lambdaで実行されるコンテナに移行することで、管理オーバーヘッドを削減しながら、アプリケーションコードへの最小限の変更を要求する。",
        "situation_analysis": "技術スタックの近代化と管理オーバーヘッドを低下させるという要件を考慮すると、AWSにおけるコンテナ化は良い選択肢である。",
        "option_analysis": "選択肢AはECSへの移行とALBの使用を必要とし、より多くの変更と管理オーバーヘッドを伴う。選択肢CはEKSを使用するもので、より複雑で設定が多くなる。選択肢DはLambdaに移行するが、ALBを導入することになるため、必要以上の複雑さが加わる。",
        "additional_knowledge": "AWS Lambdaを使用することは、従量課金モデルと自動スケーリングによる大幅なコスト削減にもつながる。",
        "key_terminology": "サーバーレス、AWS Lambda、API Gateway、コンテナ化、最小限の変更。",
        "overall_assessment": "コミュニティの投票分布を考慮すると、選択肢Aが圧倒的に支持されているが、質問の具体的な要点は、最小限の変更と低いオーバーヘッドの要件に強く一致している。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "API Gateway",
      "Serverless",
      "Containerization",
      "Minimal changes"
    ]
  },
  {
    "No": "107",
    "question": "A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes\nthe Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign\nthe application to support failover to another AWS Region.\nWhich solution will meet these requirements?",
    "question_jp": "ある会社に、AWS Lambda 関数としてホストされている非同期 HTTP アプリケーションがあります。パブリックな Amazon API Gateway エンドポイントが Lambda 関数を呼び出します。Lambda 関数と API Gateway エンドポイントは us-east-1 リージョンに存在します。ソリューションアーキテクトは、別の AWS リージョンへのフェイルオーバーをサポートするようにアプリケーションを再設計する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an API Gateway endpoint in the us-west-2 Region to direct trafic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route trafic for the two API Gateway endpoints.",
        "text_jp": "us-west-2 リージョンに API Gateway エンドポイントを作成し、traffic を us-east-1 の Lambda 関数に向けます。Amazon Route 53 を設定してフェイルオーバールーティングポリシーを使用し、2 つの API Gateway エンドポイントへの traffic をルーティングします。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct trafic to the SQS queue instead of to the Lambda function. Configure the Lambda function to pull messages from the queue for processing.",
        "text_jp": "Amazon Simple Queue Service (Amazon SQS) キューを作成します。API Gateway を設定して traffic を Lambda 関数ではなく SQS キューに向けます。Lambda 関数を設定して、キューからメッセージを取得して処理します。"
      },
      {
        "key": "C",
        "text": "Deploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 10 direct trafic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage trafic across the two API Gateway endpoints.",
        "text_jp": "Lambda 関数を us-west-2 リージョンにデプロイします。us-west-2 の Lambda 関数に traffic を向ける API Gateway エンドポイントを作成します。AWS Global Accelerator と Application Load Balancer を設定して、2 つの API Gateway エンドポイント間で traffic を管理します。"
      },
      {
        "key": "D",
        "text": "Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route trafic for the two API Gateway endpoints.",
        "text_jp": "Lambda 関数と API Gateway エンドポイントを us-west-2 リージョンにデプロイします。Amazon Route 53 を設定してフェイルオーバールーティングポリシーを使用し、2 つの API Gateway エンドポイントへの traffic をルーティングします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. By creating an SQS queue, API Gateway can route traffic to the queue, allowing the Lambda function to pull messages as needed. This decouples the processing and supports failover across regions.",
        "situation_analysis": "The requirement is to redesign the existing application for failover purposes, which requires a method to queue requests in case of a failure in the primary region.",
        "option_analysis": "Option B enables decoupled processing, which is ideal for resiliency. Options A and C increase complexity and do not inherently provide better failover mechanisms. Option D, while valid for failover, requires deployment in another region, which may not be necessary.",
        "additional_knowledge": "AWS services like SQS are designed to maintain message integrity even during failures.",
        "key_terminology": "Amazon SQS, decoupling, failover, queue-based architecture, Lambda",
        "overall_assessment": "Although community votes significantly favor option D, the technical insight supports that option B is fundamentally more capable of ensuring failover through decoupling."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは B です。SQS キューを作成することにより、API Gateway はトラフィックをキューにルーティングでき、Lambda 関数は必要に応じてメッセージを引き出すことができます。これにより、処理が分離され、リージョン間のフェイルオーバーをサポートします。",
        "situation_analysis": "必要な要件は、フェイルオーバー目的で既存のアプリケーションを再設計することです。これは、プライマリリージョンに障害が発生した場合にリクエストをキューに入れる方法を必要とします。",
        "option_analysis": "B オプションは処理の分離を可能にし、レジリエンシーに最適です。A および C のオプションは複雑さを高め、必然的により良いフェイルオーバー機構を提供するわけではありません。D オプションもフェイルオーバーには適していますが、別のリージョンへのデプロイが必要であり、それが必要とは限りません。",
        "additional_knowledge": "SQS のような AWS サービスは、障害が発生してもメッセージの整合性を維持するように設計されています。",
        "key_terminology": "Amazon SQS、分離、フェイルオーバー、キューアーキテクチャ、Lambda",
        "overall_assessment": "コミュニティの投票は D オプションを著しく好んでいるが、技術的な知見は B オプションが本質的にフェイルオーバーを確保できることを支持している。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "decoupling",
      "failover",
      "queue-based architecture",
      "Lambda"
    ]
  },
  {
    "No": "108",
    "question": "A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated\nbilling and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has\nmultiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and\nproduction.\nThe HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved\nInstances (RIs) in its production AWS account. The HR department will install the new application on this account. The HR department wants to\nmake sure that other departments cannot share the RI discounts.\nWhich solution will meet these requirements?",
    "question_jp": "小売会社は、AWS OrganizationsにおいてAWSアカウントを組織の一部として構築しています。会社は統合請求を設定し、次のように部門をOUにマッピングしました：財務、営業、人事（HR）、マーケティング、業務。それぞれのOUには、部門内の各環境ごとに1つのAWSアカウントがあり、開発、テスト、プレプロダクション、およびプロダクションがあります。HR部門は、3か月後に立ち上げる新しいシステムをリリースします。準備のために、HR部門はプロダクションAWSアカウントでいくつかのリザーブドインスタンス（RI）を購入しました。HR部門は、新しいアプリケーションをこのアカウントにインストールします。HR部門は、他の部門がRIの割引を共有できないようにしたいと考えています。この要件を満たすには、どのソリューションが適切ですか？",
    "choices": [
      {
        "key": "A",
        "text": "In the AWS Billing and Cost Management console for the HR department's production account turn off RI sharing.",
        "text_jp": "HR部門のプロダクションアカウントのAWS Billing and Cost ManagementコンソールでRIの共有をオフにする。"
      },
      {
        "key": "B",
        "text": "Remove the HR department's production AWS account from the organization. Add the account 10 the consolidating billing configuration only.",
        "text_jp": "HR部門のプロダクションAWSアカウントを組織から削除し、そのアカウントを統合請求設定のみに追加する。"
      },
      {
        "key": "C",
        "text": "In the AWS Billing and Cost Management console. use the organization's management account 10 turn off RI Sharing for the HR departments production AWS account.",
        "text_jp": "AWS Billing and Cost Managementコンソールで、組織の管理アカウントを使用してHR部門のプロダクションAWSアカウントのRI共有をオフにする。"
      },
      {
        "key": "D",
        "text": "Create an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments.",
        "text_jp": "組織内にSCPを作成し、RIへのアクセスを制限する。他の部門のOUにSCPを適用する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (82%) D (18%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Use the organization's management account to turn off RI Sharing for the HR department's production AWS account.",
        "situation_analysis": "The HR department needs to ensure that no other departments can share the RI discounts. This can be achieved by managing RI sharing settings at the organizational level.",
        "option_analysis": "Option A only affects the HR department's account and might not fulfill the requirement. Option B removes the account from the organization, which defeats the purpose of managing it under AWS Organizations. Option D does not directly address RI sharing settings but rather imposes SCP restrictions.",
        "additional_knowledge": "SCPs can restrict actions but do not control RI sharing directly.",
        "key_terminology": "AWS Organizations, Reserved Instances, Billing, Cost Management, Service Control Policies (SCP).",
        "overall_assessment": "Answer C is the most effective solution for the HR department's requirement as it controls RI sharing directly from the management account, ensuring complete isolation of RI benefits."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はC：組織の管理アカウントを使用してHR部門のプロダクションAWSアカウントのRI共有をオフにすることである。",
        "situation_analysis": "HR部門は、他の部門がRIの割引を共有できないようにする必要があります。これは、組織レベルでRI共有設定を管理することで実現できます。",
        "option_analysis": "選択肢AはHR部門のアカウントにのみ影響を与え、要件を満たさない可能性があります。選択肢Bはアカウントを組織から削除し、AWS Organizationsの下で管理する目的を達成しません。選択肢Dは、RIへのアクセスを制限するためのSCP制限を課すもので、RI共有設定に直接対処していません。",
        "additional_knowledge": "SCPはアクションを制限できますが、RI共有を直接制御することはできません。",
        "key_terminology": "AWS Organizations、リザーブドインスタンス、請求、コスト管理、サービスコントロールポリシー（SCP）。",
        "overall_assessment": "回答Cは、HR部門の要件に最も効果的なソリューションである。これは、管理アカウントから直接RI共有を制御するため、RIの利点の完全な分離を保証します。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Reserved Instances",
      "Billing",
      "Cost Management",
      "Service Control Policies"
    ]
  },
  {
    "No": "109",
    "question": "A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a\nprivate subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager\nSession Manager is configured, and AWS Systems Manager Agent is running on all the EC2 instances.\nThe company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being\nterminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon\nCloudWatch logs that are collected from the application, but the logs are inconclusive.\nHow should the solutions architect gain access to an EC2 instance to troubleshoot the issue?",
    "question_jp": "大手企業が人気のウェブアプリケーションを運営しています。このアプリケーションは、プライベートサブネット内のAuto ScalingグループにあるいくつかのAmazon EC2 Linuxインスタンスで実行されています。アプリケーションロードバランサーは、プライベートサブネット内のAuto Scalingグループにあるインスタンスを対象としています。AWS Systems Manager Session Managerが設定されており、AWS Systems Manager AgentがすべてのEC2インスタンスで実行されています。企業は最近、新しいバージョンのアプリケーションをリリースしましたが、一部のEC2インスタンスが不健康とマークされ、終了されています。その結果、アプリケーションは能力を減少しています。ソリューションアーキテクトは、Amazon CloudWatchログを分析して問題の根本原因を特定しようとしましたが、ログは決定的ではありませんでした。ソリューションアーキテクトは、問題をトラブルシュートするためにEC2インスタンスにアクセスするにはどうすればよいですか？",
    "choices": [
      {
        "key": "A",
        "text": "Suspend the Auto Scaling group's HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy.",
        "text_jp": "Auto ScalingグループのHealthCheckスケーリングプロセスを一時停止し、不健康とマークされたインスタンスにSession Managerでログインします。"
      },
      {
        "key": "B",
        "text": "Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy.",
        "text_jp": "EC2インスタンスの終了保護を有効にし、不健康とマークされたインスタンスにSession Managerでログインします。"
      },
      {
        "key": "C",
        "text": "Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an unhealthy.",
        "text_jp": "Auto Scalingグループの終了ポリシーをOldestInstanceに設定し、不健康とマークされたインスタンスにSession Managerでログインします。"
      },
      {
        "key": "D",
        "text": "Suspend the Auto Scaling group's Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy.",
        "text_jp": "Auto ScalingグループのTerminateプロセスを一時停止し、不健康とマークされたインスタンスにSession Managerでログインします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (89%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Suspending the Auto Scaling group's Terminate process allows access to an unhealthy EC2 instance using Session Manager for troubleshooting.",
        "situation_analysis": "The scenario involves EC2 instances marked as unhealthy and being terminated by the Auto Scaling group, resulting in a reduced application capacity.",
        "option_analysis": "Option D is the best choice as it directly addresses the need to prevent instances from being terminated. Options A, B, and C do not facilitate access without causing further disruptions.",
        "additional_knowledge": "This approach aligns with AWS best practices for managing instances in an Auto Scaling group.",
        "key_terminology": "Auto Scaling, Terminate, Session Manager, EC2 instances, Application Load Balancer",
        "overall_assessment": "Option D is the optimal solution given the circumstances. By suspending the Terminate process, the architect can troubleshoot effectively without further disrupting application availability."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はD：Auto ScalingグループのTerminateプロセスを一時停止することで、不健康なEC2インスタンスにSession Managerを使用してアクセスし、トラブルシューティングが可能になります。",
        "situation_analysis": "シナリオには、不健康とマークされ、Auto Scalingグループによって終了されているEC2インスタンスが含まれ、アプリケーションの能力が減少しています。",
        "option_analysis": "オプションDが最適な選択であり、インスタンスを終了させないでアクセスを可能にするからです。オプションA、B、Cは、さらなる中断を引き起こすことなくアクセスを容易にはしません。",
        "additional_knowledge": "このアプローチは、Auto Scalingグループ内のインスタンスの管理に関するAWSのベストプラクティスに沿っています。",
        "key_terminology": "Auto Scaling, Terminate, Session Manager, EC2インスタンス, アプリケーションロードバランサー",
        "overall_assessment": "状況から、オプションDが最適な解決策です。Terminateプロセスを一時停止することで、アーキテクトはアプリケーションの可用性をさらに中断することなく、効果的にトラブルシューティングが可能です。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "Terminate",
      "Session Manager",
      "EC2 instances",
      "Application Load Balancer"
    ]
  },
  {
    "No": "110",
    "question": "A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under\ndifferent OUs in AWS Organizations.\nAdministrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the\nability to automatically update and remediate noncompliant AWS WAF rules in all accounts.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
    "question_jp": "ある企業は、AWS WAFソリューションを展開して、複数のAWSアカウントに跨るAWS WAFルールを管理したいと考えています。アカウントはAWS Organizations内の異なるOU（組織単位）で管理されています。管理者は必要に応じて管理対象のAWS WAFルールセットからアカウントやOUを追加または削除できる必要があります。また、管理者はすべてのアカウントの非準拠のAWS WAFルールを自動的に更新および修正する能力も必要です。最も運用上の負荷が少ない要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account.",
        "text_jp": "AWS Firewall Managerを使用して、組織内のアカウント間でAWS WAFルールを管理します。AWS Systems Manager Parameter Storeのパラメータを使用してアカウント番号とOUを管理します。必要に応じてパラメータを更新してアカウントまたはOUを追加または削除します。Amazon EventBridgeのルールを使用して、パラメータの変更を特定し、AWS Lambda関数を呼び出してFirewall Managerの管理アカウントでセキュリティポリシーを更新します。"
      },
      {
        "key": "B",
        "text": "Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied.",
        "text_jp": "選択したOU内のすべてのリソースがAWS WAFルールに関連付けられることを要求する組織全体のAWS Configルールをデプロイします。AWS Lambdaを使用して非準拠のリソースを修正する自動修正アクションをデプロイします。AWS Configルールが適用されているOUを対象とするAWS CloudFormationスタックセットを使用してAWS WAFルールをデプロイします。"
      },
      {
        "key": "C",
        "text": "Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers and OUs to manage. Update environment variables as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts. Assume the roles by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts.",
        "text_jp": "組織の管理アカウントにAWS WAFルールを作成します。AWS Lambdaの環境変数を使用してアカウント番号とOUを管理します。必要に応じて環境変数を更新してアカウントやOUを追加または削除します。メンバーアカウントにクロスアカウントIAMロールを作成します。AWS Security Token Service（AWS STS）を使用してLambda関数内でロールを引き受け、メンバーアカウントにAWS WAFルールを作成および更新します。"
      },
      {
        "key": "D",
        "text": "Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts.",
        "text_jp": "AWS Control Towerを使用して、組織内のアカウント間でAWS WAFルールを管理します。AWS Key Management Service（AWS KMS）を使用してアカウント番号とOUを管理します。必要に応じてAWS KMSを更新してアカウントまたはOUを追加または削除します。メンバーアカウントにIAMユーザーを作成します。管理アカウントのAWS Control Towerがアクセスキーと秘密アクセスキーを使用して、メンバーアカウント内のAWS WAFルールを作成および更新できるようにします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. AWS Control Tower provides a managed environment in AWS Organizations, allowing administrators to efficiently manage AWS resources, including AWS WAF rules, with minimal overhead.",
        "situation_analysis": "The company's need to manage AWS WAF rules across multiple accounts while allowing administrators to add/remove accounts or OUs indicates a requirement for centralized management and automation.",
        "option_analysis": "Option D allows the management of WAF rules with AWS Control Tower, which streamlines operations compared to other options. Option A, while feasible, introduces more manual process with Parameter Store. Option B focuses on compliance and requires more setup. Option C relies on Lambda environment variables and is less streamlined.",
        "additional_knowledge": "AWS Control Tower is beneficial for companies looking to standardize account setups and automate compliance.",
        "key_terminology": "AWS Control Tower, AWS WAF, AWS KMS, IAM users, automation",
        "overall_assessment": "Based on the requirements and analysis, option D is the most effective in terms of reducing operational overhead while meeting security management needs."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。AWS Control TowerはAWS Organizations内の管理された環境を提供し、管理者が運用の負担を最小限に抑えながら、AWS WAFルールを効率的に管理できる。",
        "situation_analysis": "複数のアカウントにわたってAWS WAFルールを管理しつつ、管理者がアカウントやOUを追加・削除できる必要があるため、集中管理と自動化が求められている。",
        "option_analysis": "DオプションはAWS Control Towerを使用してWAFルールを管理できるため、その他のオプションに比べて運用が簡素化される。オプションAは可能であるが、Parameter Storeによる管理は手動プロセスが多くなる。オプションBは準拠性に焦点を当て、より多くの設定が必要である。オプションCはLambda環境変数に依存し、あまり効率的ではない。",
        "additional_knowledge": "AWS Control Towerは、アカウントセットアップの標準化と遵守の自動化を望む企業にとって非常に有益である。",
        "key_terminology": "AWS Control Tower、AWS WAF、AWS KMS、IAMユーザー、自動化",
        "overall_assessment": "要件と分析を考慮すると、Dオプションは運用オーバーヘッドを減らし、セキュリティ管理ニーズを満たすために最も効果的である。"
      }
    ],
    "keywords": [
      "AWS Control Tower",
      "AWS WAF",
      "AWS KMS",
      "IAM users",
      "automation"
    ]
  },
  {
    "No": "111",
    "question": "A solutions architect is auditing the security setup or an AWS Lambda function for a company. The Lambda function retrieves, the latest changes\nfrom an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the\ndatabase credentials to the Lambda function.\nThe Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with\nAWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised,\nthe company needs a solution that minimizes the impact of the compromise.\nWhat should the solutions architect recommend to meet these requirements?",
    "question_jp": "ソリューションアーキテクトが企業のAWS Lambda関数のセキュリティセットアップを監査しています。Lambda関数は、Amazon Auroraデータベースから最新の変更を取得します。Lambda関数とデータベースは同じVPC内で実行されています。Lambda環境変数はデータベースの資格情報をLambda関数に提供しています。\nLambda関数はデータを集約し、Amazon S3バケットにデータを格納します。このS3バケットはAWS KMS管理の暗号化キー(SSE-KMS)によるサーバーサイド暗号化に設定されています。データはインターネットを経由してはいけません。もしデータベースの資格情報が漏洩した場合、企業はその影響を最小限に抑えるソリューションが必要です。\nこれらの要件を満たすために、ソリューションアーキテクトは何を推奨するべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.",
        "text_jp": "Aurora DBクラスターでIAMデータベース認証を有効にします。Lambda関数のIAMロールを変更して、関数がIAMデータベース認証を使用してデータベースにアクセスできるようにします。VPC内でAmazon S3用のゲートウェイVPCエンドポイントをデプロイします。"
      },
      {
        "key": "B",
        "text": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers.",
        "text_jp": "Aurora DBクラスターでIAMデータベース認証を有効にします。Lambda関数のIAMロールを変更して、関数がIAMデータベース認証を使用してデータベースにアクセスできるようにします。データ転送中にAmazon S3への接続でHTTPSを強制します。"
      },
      {
        "key": "C",
        "text": "Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.",
        "text_jp": "データベースの資格情報をAWS Systems Managerパラメータストアに保存します。パラメータストアの資格情報についてパスワードローテーションを設定します。Lambda関数のIAMロールを変更して、関数がパラメータストアにアクセスできるようにします。Lambda関数を変更して、パラメータストアから資格情報を取得します。VPC内でAmazon S3用のゲートウェイVPCエンドポイントをデプロイします。"
      },
      {
        "key": "D",
        "text": "Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers.",
        "text_jp": "データベースの資格情報をAWS Secrets Managerに保存します。Secrets Managerの資格情報についてパスワードローテーションを設定します。Lambda関数のIAMロールを変更して、関数がSecrets Managerにアクセスできるようにします。Lambda関数を変更して、Secrets Managerから資格情報を取得します。データ転送中にAmazon S3への接続でHTTPSを強制します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (84%) D (16%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Utilizing AWS Secrets Manager for storing database credentials provides built-in security features such as automatic password rotation and access control.",
        "situation_analysis": "The security requirements are high due to the sensitive nature of database credentials and the need to limit the impact of potential exposure.",
        "option_analysis": "Option D not only handles credential management effectively but also ensures HTTPS for data transfer. Option A and B do not provide sufficient protection for credentials. Option C uses Parameter Store, which lacks some advanced features like automatic rotation.",
        "additional_knowledge": "Using Secrets Manager significantly reduces the risk associated with hard-coded credentials.",
        "key_terminology": "AWS Secrets Manager, IAM database authentication, parameter store, password rotation, HTTPS.",
        "overall_assessment": "While community vote favors option A significantly, option D is indeed a more secure recommendation given the requirement for credential management, emphasizing proper secret handling practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。AWS Secrets Managerを使用してデータベースの資格情報を保存することにより、自動パスワードローテーションやアクセス制御などの組み込みのセキュリティ機能を提供します。",
        "situation_analysis": "データベースの資格情報の機密性の高い性質と、潜在的な漏洩の影響を制限する必要があるため、セキュリティ要件は高いです。",
        "option_analysis": "選択肢Dは、資格情報の管理を効果的に行うだけでなく、データ転送に対してHTTPSを確保します。選択肢AおよびBは、資格情報の保護が不十分です。選択肢Cはパラメータストアを使用していますが、自動ローテーションなどの高度な機能が不足しています。",
        "additional_knowledge": "Secrets Managerを使用することで、ハードコーディングされた資格情報に関連するリスクが大幅に減少します。",
        "key_terminology": "AWS Secrets Manager、IAMデータベース認証、パラメータストア、パスワードローテーション、HTTPS。",
        "overall_assessment": "コミュニティの投票は選択肢Aを大きく支持していますが、移行における適切なシークレット管理アプローチを強調するため、選択肢Dは実際にはより安全な推奨であると考えられます。"
      }
    ],
    "keywords": [
      "AWS Secrets Manager",
      "IAM database authentication",
      "parameter store",
      "password rotation",
      "HTTPS"
    ]
  },
  {
    "No": "112",
    "question": "A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is\nreviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected\nFramework.\nWhile reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several\nlarge instance types account for a high proportion of the costs. The solutions architect finds out that the company's developers are launching new\nAmazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types.\nThe solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.\nWhich solution will meet these requirements?",
    "question_jp": "大規模なモバイルゲーム会社は、すべてのオンプレミスインフラストラクチャをAWSクラウドに正常に移行しました。ソリューションアーキテクトは、環境が設計通りに構築されており、Well-Architected Frameworkに沿って運用されていることを確認するためにレビューしています。コストエクスプローラーで過去の月次コストを確認していると、いくつかの大型インスタンスタイプの作成及びその後の終了がコストの高い割合を占めていることに気付きました。ソリューションアーキテクトは、会社の開発者がテストの一環として新しいAmazon EC2インスタンスを起動しており、適切なインスタンスタイプを使用していないことを発見します。ソリューションアーキテクトは、開発者が起動できるインスタンスタイプを制限するための管理メカニズムを実装する必要があります。この要求を満たす解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to an event to run each time a new EC2 instance is launched.",
        "text_jp": "AWS Configで希望のインスタンスタイプの管理ルールを作成します。許可されるインスタンスタイプでルールを構成します。新しいEC2インスタンスが起動されるたびに実行されるイベントにルールを添付します。"
      },
      {
        "key": "B",
        "text": "In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the developers' IAM accounts.",
        "text_jp": "EC2コンソールで、許可されるインスタンスタイプを指定した起動テンプレートを作成します。起動テンプレートを開発者のIAMアカウントに割り当てます。"
      },
      {
        "key": "C",
        "text": "Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers",
        "text_jp": "新しいIAMポリシーを作成します。許可されるインスタンスタイプを指定します。そのポリシーを開発者のIAMアカウントを含むIAMグループに添付します。"
      },
      {
        "key": "D",
        "text": "Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image.",
        "text_jp": "EC2 Image Builderを使用して開発者のためにイメージパイプラインを作成し、ゴールデンイメージの作成を支援します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It allows creating an IAM policy that explicitly defines which EC2 instance types can be launched by developers.",
        "situation_analysis": "The company is facing cost issues due to developers launching inappropriate EC2 instance types for testing. There is a need for control over which instance types can be used.",
        "option_analysis": "Option A provides monitoring but does not restrict instance creation. Option B sets a launch template but can be overridden. Option C directly restricts actions via IAM policy, making it the most suitable. Option D does not address instance type restrictions.",
        "additional_knowledge": "Understanding IAM policies is crucial for enforcing security and governance in AWS environments.",
        "key_terminology": "IAM Policy, EC2 Instance Types, AWS Config, Cost Management, Resource Governance",
        "overall_assessment": "Option C is the best choice as it utilizes IAM policies effectively to manage costs and control resources. Community support is unanimous."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。これは、開発者が起動できるEC2インスタンスタイプを明示的に定義するIAMポリシーを作成することを可能にする。",
        "situation_analysis": "この会社は、開発者がテストのために不適切なEC2インスタンスタイプを起動することによるコスト問題に直面している。使用できるインスタンスタイプに対する制御が必要である。",
        "option_analysis": "選択肢Aは監視を提供するが、インスタンスの作成を制限しない。選択肢Bは起動テンプレートを設定するが、上書きされる可能性がある。選択肢CはIAMポリシーを通じて直接アクションを制限するため、最も適している。選択肢Dはインスタンスタイプの制限に対処していない。",
        "additional_knowledge": "IAMポリシーの理解は、AWS環境におけるセキュリティとガバナンスを強化するために重要である。",
        "key_terminology": "IAMポリシー, EC2インスタンスタイプ, AWS Config, コスト管理, リソースガバナンス",
        "overall_assessment": "選択肢Cはコスト管理とリソース制御を効果的に行えるIAMポリシーを利用しており、最良の選択肢である。コミュニティの支持は一致している。"
      }
    ],
    "keywords": [
      "IAM Policy",
      "EC2 Instance Types",
      "AWS Config",
      "Cost Management",
      "Resource Governance"
    ]
  },
  {
    "No": "113",
    "question": "A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the\nsame organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team\nresponsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.\nWhich actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.)",
    "question_jp": "企業はAWSクラウドで複数のプロジェクトを開発およびホスティングしています。プロジェクトはAWS Organizationsで同じ組織内の複数のAWSアカウントで開発されています。企業はクラウドインフラストラクチャのコストを所有するプロジェクトに割り当てる必要があります。すべてのAWSアカウントの責任を負うチームは、コスト割り当てに使用されるProjectタグが欠落しているいくつかのAmazon EC2インスタンスを発見しました。問題を解決し、将来的に同様の問題が発生しないようにするために、ソリューションアーキテクトはどのようなアクションを取るべきですか？（三つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Config rule in each account to find resources with missing tags.",
        "text_jp": "各アカウントでタグが欠落しているリソースを見つけるためにAWS Configルールを作成します。"
      },
      {
        "key": "B",
        "text": "Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.",
        "text_jp": "Projectタグが欠落している場合にec2:RunInstancesの拒否アクションを持つSCPを組織内に作成します。"
      },
      {
        "key": "C",
        "text": "Use Amazon Inspector in the organization to find resources with missing tags.",
        "text_jp": "組織内でAmazon Inspectorを使用して、タグが欠落しているリソースを見つけます。"
      },
      {
        "key": "D",
        "text": "Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing.",
        "text_jp": "各アカウントでProjectタグが欠落している場合にec2:RunInstancesの拒否アクションを持つIAMポリシーを作成します。"
      },
      {
        "key": "E",
        "text": "Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.",
        "text_jp": "組織全体のAWS Configアグリゲーターを作成して、欠落しているProjectタグのEC2インスタンスのリストを収集します。"
      },
      {
        "key": "F",
        "text": "Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag.",
        "text_jp": "AWS Security Hubを使用して、欠落しているProjectタグのEC2インスタンスのリストを集約します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ABE (82%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using Amazon Inspector assists in identifying resources with missing tags and supports security and compliance efforts.",
        "situation_analysis": "The organization is using multiple AWS accounts under AWS Organizations and requires proper cost allocation for resources via tagging. The missing Project tags are hindering this requirement.",
        "option_analysis": "Option C is effective as Amazon Inspector scans for compliance and can help in identifying instances without necessary tags. The other options, while potentially useful (A, E), do not provide the same direct integration for security assessment. Options B and D enforce tag dependencies but do not proactively find missing tags.",
        "additional_knowledge": "It's essential for organizations to ensure all resources are appropriately tagged, as this directly influences cloud cost management.",
        "key_terminology": "Amazon Inspector, AWS Config, Compliance, Tagging, AWS Organizations.",
        "overall_assessment": "Question quality is high, focusing on tagging best practices necessary for cost allocation. There's a discrepancy between the community vote and the correct answer as they may favor options enforcing tagging over identification, illustrating a common preference for prevention strategies."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。Amazon Inspectorを使用すると、タグが欠落しているリソースを特定し、セキュリティおよびコンプライアンスの取り組みをサポートします。",
        "situation_analysis": "組織はAWS Organizationsの下で複数のAWSアカウントを使用しており、リソースの適切なコスト割り当てにはタグ付けが必要です。欠落しているProjectタグはこの要件を妨げています。",
        "option_analysis": "オプションCは効果的で、Amazon Inspectorはコンプライアンスをスキャンし、必要なタグのないインスタンスを特定するのに役立ちます。その他のオプション（A、E）は有用ですが、直接的にセキュリティ評価に統合されているわけではありません。オプションBとDはタグの依存関係を強制しますが、欠落しているタグを積極的に見つけるものではありません。",
        "additional_knowledge": "すべてのリソースが適切にタグ付けされていることを確認することは、クラウドコスト管理に直接影響するため、組織にとって重要です。",
        "key_terminology": "Amazon Inspector, AWS Config, コンプライアンス, タグ付け, AWS Organizations.",
        "overall_assessment": "質問の質は高く、コスト割り当てに必要なタグ付けのベストプラクティスに焦点を当てています。正しい回答とコミュニティ投票の間には相違があります。コミュニティは、確認よりもタグ付けを強制する選択肢を支持する傾向があり、予防策への一般的な好みを示しています。"
      }
    ],
    "keywords": [
      "Amazon Inspector",
      "AWS Config",
      "Compliance",
      "Tagging",
      "AWS Organizations"
    ]
  },
  {
    "No": "114",
    "question": "A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due\nto heavy ingestion and it frequently runs out of storage.\nThe company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should\ninclude the following attributes:\n• Managed AWS services to minimize operational complexity.\n• A buffer that automatically scales to match the throughput of data and requires no ongoing administration.\n• A visualization tool to create dashboards to observe events in near-real time.\n• Support for semi-structured JSON data and dynamic schemas.\nWhich combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)",
    "question_jp": "ある企業は、イベントの持続性のためにPostgreSQLデータベースを使用したオンプレミスの監視ソリューションを持っています。このデータベースは、重い取り込みによりスケーリングできず、頻繁にストレージが不足しています。\nこの企業はハイブリッドソリューションを作成したいと考えており、すでに自社のネットワークとAWS間にVPN接続を設定しています。ソリューションには以下の属性が含まれる必要があります。\n• 運用の複雑さを最小限に抑えるために管理されたAWSサービス。\n• データのスループットに合わせて自動的にスケールし、継続的な管理を必要としないバッファ。\n• イベントをほぼリアルタイムで観察するためのダッシュボードを作成するための可視化ツール。\n• 半構造化JSONデータおよび動的スキーマのサポート。\nこれらの要件を満たす監視ソリューションを作成するために、どのコンポーネントの組み合わせを使用しますか？（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.",
        "text_jp": "Amazon Kinesis Data Firehoseを使用してイベントをバッファリングします。AWS Lambda関数を作成してイベントを処理および変換します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.",
        "text_jp": "Amazon Kinesisデータストリームを作成してイベントをバッファリングします。AWS Lambda関数を作成してイベントを処理および変換します。"
      },
      {
        "key": "C",
        "text": "Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near- real-time visualizations and dashboards.",
        "text_jp": "Amazon Aurora PostgreSQL DBクラスターを構成してイベントを受信します。Amazon QuickSightを使用してデータベースから読み取り、ほぼリアルタイムの可視化およびダッシュボードを作成します。"
      },
      {
        "key": "D",
        "text": "Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near- real-time visualizations and dashboards.",
        "text_jp": "Amazon Elasticsearch Service（Amazon ES）を構成してイベントを受信します。Amazon ESにデプロイされたKibanaエンドポイントを使用して、ほぼリアルタイムの可視化およびダッシュボードを作成します。"
      },
      {
        "key": "E",
        "text": "Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.",
        "text_jp": "Amazon Neptune DBインスタンスを構成してイベントを受信します。Amazon QuickSightを使用してデータベースから読み取り、ほぼリアルタイムの可視化およびダッシュボードを作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D. They leverage managed AWS services that minimize operational complexity with auto-scaling capabilities and visualization tools.",
        "situation_analysis": "The company has a PostgreSQL database that cannot scale, and they seek to implement a hybrid solution that efficiently handles event ingestion and provides real-time monitoring.",
        "option_analysis": "Option A (Kinesis Data Firehose with Lambda) provides a fully managed service that scales automatically and allows for processing and transforming event data. Option D (Elasticsearch with Kibana) allows for near real-time visualizations, thereby addressing the dashboard requirement effectively.",
        "additional_knowledge": "Amazon QuickSight can complement Kinesis and Elasticsearch by providing powerful visualization tools.",
        "key_terminology": "Amazon Kinesis Data Firehose, AWS Lambda, Amazon Elasticsearch Service, Kibana, Amazon QuickSight",
        "overall_assessment": "The combination of A and D presents the most comprehensive solution for the requirements outlined. Each option provides critical functionality that ensures a robust monitoring solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAとDです。これらは運用の複雑さを最小限に抑え、自動スケーリング機能および可視化ツールを備えたAWSの管理サービスを活用しています。",
        "situation_analysis": "この企業はスケールできないPostgreSQLデータベースを持ち、イベントの取り込みを効率的に処理し、リアルタイムでの監視を提供するハイブリッドソリューションを実装したいと考えています。",
        "option_analysis": "A（Kinesis Data Firehose と Lambdaの組み合わせ）は、自動的にスケールし、イベントデータの処理および変換を可能にする完全管理型サービスを提供します。D（Elasticsearch と Kibana）は、ほぼリアルタイムの可視化を可能にし、ダッシュボード要件を効果的に満たしています。",
        "additional_knowledge": "Amazon QuickSightは、KinesisおよびElasticsearchの補完ツールとして、強力な可視化ツールを提供することができます。",
        "key_terminology": "Amazon Kinesis Data Firehose, AWS Lambda, Amazon Elasticsearch Service, Kibana, Amazon QuickSight",
        "overall_assessment": "AとDの組み合わせは、示された要件に対する最も包括的なソリューションを提供します。各オプションは、強力な監視ソリューションを確保するために重要な機能を提供します。"
      }
    ],
    "keywords": [
      "Amazon Kinesis Data Firehose",
      "AWS Lambda",
      "Amazon Elasticsearch Service",
      "Kibana",
      "Amazon QuickSight"
    ]
  },
  {
    "No": "115",
    "question": "A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private\nsubnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company's applications read from and write to\nAmazon Kinesis Data Streams. Most of the workloads run in private subnets.\nA solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications.\nThe solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that\nNatGateway-Bytes charges are increasing the cost in the EC2-Other category.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "あるチームが会社全体の行動データを収集してルーティングしています。この会社は、パブリックサブネット、プライベートサブネット、およびインターネットゲートウェイを備えたMulti-AZ VPC環境を運営しています。各パブリックサブネットにはNATゲートウェイも含まれています。会社のアプリケーションのほとんどは、Amazon Kinesis Data Streamsから読み込み、書き込みを行います。ほとんどのワークロードは、プライベートサブネットで実行されています。\nソリューションアーキテクトはインフラストラクチャをレビューし、コストを削減しながらアプリケーションの機能を維持する必要があります。\nソリューションアーキテクトはCost Explorerを使用し、EC2-Otherカテゴリのコストが常に高いことに気付きます。さらに調査したところ、NatGateway-Bytesの料金がEC2-Otherカテゴリのコストを増加させていることがわかりました。\nソリューションアーキテクトは、これらの要件を満たすために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for trafic that can be removed. Ensure that security groups are blocking trafic that is responsible for high costs.",
        "text_jp": "VPCフローログを有効化します。Amazon Athenaを使用して、削除可能なトラフィックのログを分析します。コストが高いトラフィックをブロックするために、セキュリティグループが正しく設定されていることを確認します。"
      },
      {
        "key": "B",
        "text": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint.",
        "text_jp": "Kinesis Data StreamsのためのインターフェースVPCエンドポイントをVPCに追加します。アプリケーションがインターフェースVPCエンドポイントを使用するために必要なIAM権限を持っていることを確認します。"
      },
      {
        "key": "C",
        "text": "Enable VPC Flow Logs and Amazon Detective. Review Detective findings for trafic that is not related to Kinesis Data Streams. Configure security groups to block that trafic.",
        "text_jp": "VPCフローログとAmazon Detectiveを有効化します。Kinesis Data Streamsに関連しないトラフィックについてのDetectiveの調査結果をレビューします。そのトラフィックをブロックするようにセキュリティグループを設定します。"
      },
      {
        "key": "D",
        "text": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows trafic from the applications.",
        "text_jp": "Kinesis Data StreamsのためのインターフェースVPCエンドポイントをVPCに追加します。アプリケーションからのトラフィックを許可するようにVPCエンドポイントポリシーを設定します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Adding an Interface VPC Endpoint for Kinesis Data Streams can help reduce the traffic routed through the NAT Gateway, leading to lower NAT Gateway charges.",
        "situation_analysis": "The infrastructure consists of a Multi-AZ VPC setup with NAT Gateways, and high costs associated with NAT Gateway bytes indicate inefficient data routing.",
        "option_analysis": "Option D directly addresses the issue by reducing the reliance on NAT Gateways for Kinesis by establishing a private connection. Options A, B, and C do not effectively minimize NAT Gateway usage and associated costs.",
        "additional_knowledge": "",
        "key_terminology": "VPC, NAT Gateway, Kinesis Data Streams, VPC Endpoint, IAM permissions",
        "overall_assessment": "Option D is the best choice according to best practices for minimizing costs in AWS by leveraging private connectivity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。Kinesis Data StreamsのためにインターフェースVPCエンドポイントを追加することで、NATゲートウェイを通過するトラフィックを減少させ、NATゲートウェイの料金を低減できる。",
        "situation_analysis": "インフラストラクチャはMulti-AZ VPC設定とNATゲートウェイを含んでおり、NATゲートウェイバイトに関連する高コストは非効率的なデータルーティングを示している。",
        "option_analysis": "選択肢Dは、Kinesisへの依存を減らすことにより、NATゲートウェイの利用を減少させ、関連コストを軽減することを直接的に解決している。選択肢A、B、CはNATゲートウェイの使用に対する効果的な最小化策を提供していない。",
        "additional_knowledge": "",
        "key_terminology": "VPC, NAT Gateway, Kinesis Data Streams, VPC Endpoint, IAM権限",
        "overall_assessment": "選択肢DはAWSコストを最小化するためのベストプラクティスによると最良の選択肢である。"
      }
    ],
    "keywords": [
      "VPC",
      "NAT Gateway",
      "Kinesis Data Streams",
      "VPC Endpoint",
      "IAM permissions"
    ]
  },
  {
    "No": "116",
    "question": "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and\nus-east-1 Regions. The company wants to be able to route network trafic from its on-premises infrastructure into VPCs in either of those Regions.\nThe company also needs to support trafic that is routed directly between VPCs in those Regions. No single points of failure can exist on the\nnetwork.\nThe company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a\nseparate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a\nsingle AWS Transit Gateway that is configured to route all inter-VPC trafic within that Region.\nWhich solution will meet these requirements?",
    "question_jp": "ある小売企業はヨーロッパにオンプレミスのデータセンターを持っている。この企業は、eu-west-1およびus-east-1リージョンを含むマルチリージョンのAWSプレゼンスも持っている。企業は、オンプレミスインフラからこれらのリージョンのVPCへネットワークトラフィックをルーティングできるようにしたい。その上、これらのリージョン内のVPC間で直接ルーティングされるトラフィックもサポートする必要がある。ネットワークにシングルポイントオブフェイラーが存在してはいけない。企業はすでに、オンプレミスデータセンターからの2つの1 GbpsのAWS Direct Connect接続を作成している。それぞれの接続は、高可用性のためにヨーロッパの異なるDirect Connectロケーションに接続されている。これら2つのロケーションはDX-AとDX-Bと名付けられている。それぞれのリージョンには、同リージョン内の全てのインターヴィPCトラフィックをルーティングするように設定された単一のAWS Transit Gatewayがある。どのソリューションがこれらの要件を満たすのか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.",
        "text_jp": "DX-A接続からDirect ConnectゲートウェイへのプライベートVIFを作成する。高可用性のために、DX-B接続からも同じDirect ConnectゲートウェイへのプライベートVIFを作成する。eu-west-1およびus-east-1の両方のトランジットゲートウェイをDirect Connectゲートウェイに関連付ける。クロスリージョンのルーティングをサポートするためにトランジットゲートウェイを相互接続する。"
      },
      {
        "key": "B",
        "text": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-8 connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross- Region routing.",
        "text_jp": "DX-A接続からDirect ConnectゲートウェイへのトランジットVIFを作成する。このDirect Connectゲートウェイにeu-west-1トランジットゲートウェイを関連付ける。DX-B接続から別のDirect ConnectゲートウェイへのトランジットVIFを作成する。us-east-1トランジットゲートウェイをこの別のDirect Connectゲートウェイに関連付ける。高可用性とクロスリージョンのルーティングをサポートするためにDirect Connectゲートウェイを相互接続する。"
      },
      {
        "key": "C",
        "text": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Configure the Direct Connect gateway to route trafic between the transit gateways.",
        "text_jp": "DX-A接続からDirect ConnectゲートウェイへのトランジットVIFを作成する。高可用性のために、DX-B接続からも同じDirect ConnectゲートウェイへのトランジットVIFを作成する。eu-west-1およびus-east-1の両方のトランジットゲートウェイをこのDirect Connectゲートウェイに関連付ける。Direct Connectゲートウェイを設定してトランジットゲートウェイ間のトラフィックをルーティングする。"
      },
      {
        "key": "D",
        "text": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.",
        "text_jp": "DX-A接続からDirect ConnectゲートウェイへのトランジットVIFを作成する。高可用性のために、DX-B接続からも同じDirect ConnectゲートウェイへのトランジットVIFを作成する。eu-west-1およびus-east-1の両方のトランジットゲートウェイをこのDirect Connectゲートウェイに関連付ける。クロスリージョンのルーティングをサポートするためにトランジットゲートウェイを相互接続する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (93%) 3%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This option leverages AWS Direct Connect and Transit Gateway features to create a robust cross-region connectivity solution.",
        "situation_analysis": "The retail company has strict requirements regarding reliability and network availability, highlighted by their need for high availability and avoidance of single points of failure.",
        "option_analysis": "Option A effectively establishes the necessary connections for both high availability and cross-region routing. Other options do not meet all requirements, particularly regarding the interaction and scalability of Direct Connect gateways.",
        "additional_knowledge": "High availability architectures are crucial in global applications to ensure redundancy and continuity.",
        "key_terminology": "Direct Connect, Transit Gateway, VPC Peering, High Availability, Cross-Region Routing",
        "overall_assessment": "The choice aligns with AWS best practices for implementing a multi-Region architecture without introducing a single point of failure."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。この選択肢は、AWS Direct Connectおよびトランジットゲートウェイの機能を利用して、強力なクロスリージョン接続ソリューションを作成している。",
        "situation_analysis": "小売企業は、高可用性とシングルポイントオブフェイラーを回避するという厳格な要件を持っている。",
        "option_analysis": "選択肢Aは、高可用性とクロスリージョンルーティングに必要な接続を効果的に確立する。他の選択肢は、特にDirect Connectゲートウェイの相互作用とスケーラビリティに関して、全ての要件を満たしていない。",
        "additional_knowledge": "高可用性のアーキテクチャは、グローバルアプリケーションにおいて冗長性と継続性を確保するために重要である。",
        "key_terminology": "Direct Connect, Transit Gateway, VPCピアリング, 高可用性, クロスリージョンルーティング",
        "overall_assessment": "この選択は、シングルポイントオブフェイラーを導入せずにマルチリージョンアーキテクチャを実装するためのAWSのベストプラクティスに合致している。"
      }
    ],
    "keywords": [
      "Direct Connect",
      "Transit Gateway",
      "VPC Peering",
      "High Availability",
      "Cross-Region Routing"
    ]
  },
  {
    "No": "117",
    "question": "A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new\nIAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user.\nThe company has a multi-Region AWS CloudTrail trail in the AWS account.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ある企業がAWSクラウドでアプリケーションを運営しています。この企業のセキュリティチームは、新しいIAMユーザーの作成を承認しなければなりません。新しいIAMユーザーが作成されると、自動的にそのユーザーのすべてのアクセスを削除しなければなりません。次に、セキュリティチームはユーザーを承認するための通知を受け取る必要があります。この企業は、AWSアカウント内でマルチリージョンのAWS CloudTrailトレイルを持っています。これらの要件を満たすステップの組み合わせはどれですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.",
        "text_jp": "Amazon EventBridge (Amazon CloudWatch Events) ルールを作成します。detail-type 値を AWS API Call via CloudTrail、eventName を CreateUser に設定したパターンを定義します。"
      },
      {
        "key": "B",
        "text": "Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "text_jp": "CloudTrail を構成して、CreateUser イベントの通知を Amazon Simple Notification Service (Amazon SNS) トピックに送信します。"
      },
      {
        "key": "C",
        "text": "Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access.",
        "text_jp": "AWS Fargate 技術を使用して、Amazon Elastic Container Service (Amazon ECS) 上で実行されるコンテナを呼び出してアクセスを削除します。"
      },
      {
        "key": "D",
        "text": "Invoke an AWS Step Functions state machine to remove access.",
        "text_jp": "アクセスを削除するために AWS Step Functions 状態マシンを呼び出します。"
      },
      {
        "key": "E",
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.",
        "text_jp": "Amazon Simple Notification Service (Amazon SNS) を使用してセキュリティチームに通知します。"
      },
      {
        "key": "F",
        "text": "Use Amazon Pinpoint to notify the security team.",
        "text_jp": "Amazon Pinpoint を使用してセキュリティチームに通知します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ADE (88%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct steps involve using Amazon EventBridge to detect the CreateUser API call and notify the security team for approval.",
        "situation_analysis": "The company needs an automated approval process post-IAM user creation, ensuring that access is disabled until approved.",
        "option_analysis": "Option A is key for detecting user creation. Option B relates to sending notifications but does not address access removal. Options C and D involve unnecessary complexity for access removal. Option E is about notification but not about the approval process. Option F is another notification service not required.",
        "additional_knowledge": "",
        "key_terminology": "Amazon EventBridge, AWS CloudTrail, IAM, notification, AWS Fargate, AWS Step Functions",
        "overall_assessment": "Option A together with a notification mechanism meets the requirements effectively, while the other options are not necessary or complicate the architecture."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい手順は、Amazon EventBridgeを使用してCreateUser API呼び出しを検知し、承認のためにセキュリティチームに通知することです。",
        "situation_analysis": "企業は、IAMユーザー作成後に自動承認プロセスを必要としており、承認されるまでアクセスが無効化される必要があります。",
        "option_analysis": "Aの選択肢はユーザー作成の検知に重要です。Bの選択肢は通知を送信することに関係していますが、アクセスの削除には対応していません。CとDの選択肢は、アクセス削除のために不必要な複雑さを伴います。Eの選択肢は通知に関するものですが、承認プロセスに関するものではありません。Fの選択肢は必要のない別の通知サービスです。",
        "additional_knowledge": "",
        "key_terminology": "Amazon EventBridge、AWS CloudTrail、IAM、通知、AWS Fargate、AWS Step Functions",
        "overall_assessment": "Aの選択肢と通知メカニズムが効果的に要件を満たしており、他の選択肢は必要がないか、アーキテクチャを複雑にします。"
      }
    ],
    "keywords": [
      "Amazon EventBridge",
      "AWS CloudTrail",
      "IAM",
      "notification",
      "AWS Fargate",
      "AWS Step Functions"
    ]
  },
  {
    "No": "118",
    "question": "A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and\napplications. The company also wants to keep the trafic on a private network. Multi-factor authentication (MFA) is required at login, and specific\nroles are assigned to user groups.\nThe company must create separate accounts for development. staging, production, and shared network. The production account and the shared\nnetwork account must have connectivity to all accounts. The development account and the staging account must have access only to each other.\nWhich combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)",
    "question_jp": "ある企業がAWSへの移行を希望している。この企業は、すべてのアカウントおよびアプリケーションへの中央管理されたアクセスを持つマルチアカウント構造を使用したいと考えている。また、プライベートネットワーク上でのトラフィックを維持したい。ログイン時にはマルチファクター認証 (MFA) が必要であり、特定の役割がユーザーグループに割り当てられる。企業は、開発、ステージング、プロダクション、および共有ネットワークのために別々のアカウントを作成しなければならない。プロダクションアカウントと共有ネットワークアカウントはすべてのアカウントに接続できなければならず、開発アカウントとステージングアカウントは互いにのみアクセスできなければならない。この要件を満たすために、ソリューションアーキテクトが取るべきステップの組み合わせは何か？（3つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations.",
        "text_jp": "AWS Control Towerを使用してランディングゾーン環境を展開する。アカウントに登録し、既存のアカウントをAWS Organizations内の結果として生成された組織に招待する。"
      },
      {
        "key": "B",
        "text": "Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login.",
        "text_jp": "すべてのアカウントでAWS Security Hubを有効にし、クロスアカウントアクセスを管理する。AWS CloudTrailを通じて結果を収集し、MFAログインを強制する。"
      },
      {
        "key": "C",
        "text": "Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables.",
        "text_jp": "各アカウントにトランジットゲートウェイとトランジットゲートウェイVPCアタッチメントを作成する。適切なルートテーブルを構成する。"
      },
      {
        "key": "D",
        "text": "Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts.",
        "text_jp": "AWS IAM Identity Center (AWS Single Sign-On)を設定し、有効にする。既存のアカウントのために必要なMFAを備えた適切な権限セットを作成する。"
      },
      {
        "key": "E",
        "text": "Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA login.",
        "text_jp": "すべてのアカウントでAWS Control Towerを有効にし、アカウント間のルーティングを管理する。AWS CloudTrailを通じて結果を収集し、MFAログインを強制する。"
      },
      {
        "key": "F",
        "text": "Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognoto user pools and Identity pools to manage access to accounts and between accounts.",
        "text_jp": "IAMユーザーとグループを作成する。すべてのユーザーにMFAを構成する。Amazon Cognitoユーザープールとアイデンティティプールを設定し、アカウント間のアクセスを管理する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ACD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login.",
        "situation_analysis": "The company requires a multi-account structure with access management and specific connectivity requirements between accounts. MFA is necessary for logging in.",
        "option_analysis": "Option B meets the requirements by utilizing AWS Security Hub for management and AWS CloudTrail for compliance monitoring. Other options either focus too much on routing or fail to provide centralized access management.",
        "additional_knowledge": "Security Hub integrates with multiple AWS services to provide insights about security posture and ensures that MFA is enforced as per compliance needs.",
        "key_terminology": "AWS Security Hub, AWS CloudTrail, cross-account access, multi-factor authentication (MFA), connectivity.",
        "overall_assessment": "The overall assessment indicates that while community votes suggested ACD, option B aligns best with the central management and compliance requirements stipulated."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はB：すべてのアカウントでAWS Security Hubを有効にし、クロスアカウントアクセスを管理する。AWS CloudTrailを通じて結果を収集し、MFAログインを強制する。",
        "situation_analysis": "この企業は、アクセス管理およびアカウント間の特定の接続要件を持つマルチアカウント構造を必要としている。ログインにはMFAが必要である。",
        "option_analysis": "選択肢Bは、AWS Security Hubを利用した管理とAWS CloudTrailによるコンプライアンスモニタリングを用いることで要件を満たしている。他の選択肢はルーティングに集中しすぎているか、中央集権的なアクセス管理を提供していない。",
        "additional_knowledge": "Security Hubは複数のAWSサービスと統合され、セキュリティ姿勢の洞察を提供し、MFAがコンプライアンスニーズに応じて強制されることを確保する。",
        "key_terminology": "AWS Security Hub、AWS CloudTrail、クロスアカウントアクセス、マルチファクター認証 (MFA)、接続。",
        "overall_assessment": "全体的な評価は、コミュニティの投票がACDを示唆したが、選択肢Bが中心管理およびコンプライアンス要件に最も合致していることを示している。"
      }
    ],
    "keywords": [
      "AWS Security Hub",
      "AWS CloudTrail",
      "cross-account access",
      "multi-factor authentication (MFA)",
      "connectivity"
    ]
  },
  {
    "No": "119",
    "question": "A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production.\nAll the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases.\nThe databases are between 500 GB and 800 GB in size.\nThe development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7\ndays a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or\nproduction as the key.\nWhat should a solutions architect do to reduce costs with the LEAST operational effort?",
    "question_jp": "ある企業は、eu-west-1リージョンでアプリケーションを運用しており、開発、テスト、製品の各環境にそれぞれアカウントを持っています。すべての環境は、状態を保持するAmazon EC2インスタンスとAmazon RDS for MySQLデータベースを使用して、24時間年中無休で稼働しています。データベースのサイズは500 GBから800 GBの間です。開発チームとテストチームは、営業日の営業時間中に作業を行いますが、製品環境は24時間年中無休で運用されています。この企業はコストを削減したいと考えています。すべてのリソースには「environment」タグが付与されており、開発、テスト、製品のいずれかの値が設定されています。運用の手間を最小限に抑えつつコストを削減するために、ソリューションアーキテクトは何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or slops instances based on me tag, day, and time.",
        "text_jp": "Amazon EventBridgeルールを作成して、毎日1回実行します。ルールを設定して、環境に応じたタグ、曜日、時間に基づいてインスタンスを起動または停止するAWS Lambda関数を呼び出すようにします。"
      },
      {
        "key": "B",
        "text": "Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that starts instances based on the tag.",
        "text_jp": "Amazon EventBridgeルールを作成して、営業日の夕方に毎日実行します。ルールを設定して、タグに基づいてインスタンスを停止するAWS Lambda関数を呼び出すようにします。次に、営業日の朝に毎日実行する2つ目のEventBridgeルールを作成します。この2つ目のルールは、タグに基づいてインスタンスを起動する別のLambda関数を呼び出すように設定します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that terminates, instances based on the lag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that restores the instances from their last backup based on the tag.",
        "text_jp": "Amazon EventBridgeルールを作成して、営業日の夕方に毎日実行します。ルールを設定して、タグに基づいてインスタンスを終了するAWS Lambda関数を呼び出すようにします。次に、営業日の朝に毎日実行する2つ目のEventBridgeルールを作成します。この2つ目のルールは、タグに基づいてインスタンスを最後のバックアップから復元する別のLambda関数を呼び出すように設定します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores instances from their last backup based on the tag. day, and time.",
        "text_jp": "Amazon EventBridgeルールを作成して、毎時実行します。ルールを設定して、タグ、曜日、時間に基づいてインスタンスを終了または最後のバックアップから復元するAWS Lambda関数を呼び出すようにします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. It involves creating a single EventBridge rule that runs daily to manage both start and stop actions based on tags, which is efficient.",
        "situation_analysis": "The organization has environments running 24/7, while development and testing only need to be operational during business hours.",
        "option_analysis": "Option A allows automated management of instances based on tag criteria, reducing unnecessary costs. Option B implements daily actions which could be slightly more disruptive. Option C suggests termination which may not preserve state. Option D does not effectively utilize the environment tag strategy.",
        "additional_knowledge": "Employing tags effectively is critical to optimize costs associated with AWS resources.",
        "key_terminology": "Amazon EventBridge, AWS Lambda, stateful instances, automation",
        "overall_assessment": "The community vote heavily favored B, but A provides a lower operational overhead with proper tagging."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。これは、タグに基づいて起動および停止アクションを管理するために毎日実行される単一のEventBridgeルールを作成することを含んでおり、効率的です。",
        "situation_analysis": "組織は24時間年中無休で実行される環境を持っていますが、開発とテストは営業日の営業時間中のみ稼働する必要があります。",
        "option_analysis": "選択肢Aは、タグの条件に基づいてインスタンスを自動的に管理することができるため、不必要なコストを削減します。選択肢Bはとても少し手間のかかる日次のアクションを実施します。選択肢Cは、状態を保持できない終了を提案しています。選択肢Dは、環境タグ戦略を効果的に活用していません。",
        "additional_knowledge": "効果的なタグ付けを行うことは、AWSリソースに関連するコストを最適化するために重要です。",
        "key_terminology": "Amazon EventBridge, AWS Lambda, 状態を保持するインスタンス, 自動化",
        "overall_assessment": "コミュニティの投票はBに強く偏っていましたが、Aは適切なタグ付けにより運用のオーバーヘッドを低減します。"
      }
    ],
    "keywords": [
      "Amazon EventBridge",
      "AWS Lambda",
      "EC2",
      "RDS",
      "Cost Optimization"
    ]
  },
  {
    "No": "120",
    "question": "A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed an Amazon API Gateway REST API with AWS\nLambda integration in multiple AWS Regions and in the same production account.\nThe company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of API calls per second. The\npremium tier offers up to 3,000 calls per second, and customers are identified by a unique API key. Several premium tier customers in various\nRegions report that they receive error responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logs indicate\nthat the Lambda function is never invoked.\nWhat could be the cause of the error messages for these customers?",
    "question_jp": "ある企業がAWS上にソフトウェア・アズ・ア・サービス（SaaS）ソリューションを構築しています。企業は、複数のAWSリージョンおよび同じプロダクションアカウント内に、AWS Lambda統合を持つAmazon API Gateway REST APIを展開しました。企業は、顧客が一定数のAPIコールを毎秒行うための容量を支払うことができる段階的な料金設定を提供しています。プレミアムプランでは、毎秒最大3,000コールが可能で、顧客は一意のAPIキーによって識別されます。複数のリージョンにいる複数のプレミアムプラン顧客から、ピーク使用時間中に複数のAPIメソッドから429 Too Many Requestsのエラーレスポンスを受け取っていると報告されています。ログには、Lambda関数が一度も呼び出されていないことが示されています。この顧客のエラーメッセージの原因は何でしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "The Lambda function reached its concurrency limit.",
        "text_jp": "Lambda関数は同時実行制限に達しました。"
      },
      {
        "key": "B",
        "text": "The Lambda function its Region limit for concurrency.",
        "text_jp": "Lambda関数はリージョン制限の同時実行に達しました。"
      },
      {
        "key": "C",
        "text": "The company reached its API Gateway account limit for calls per second.",
        "text_jp": "企業はAPI Gatewayアカウントの呼び出し制限に達しました。"
      },
      {
        "key": "D",
        "text": "The company reached its API Gateway default per-method limit for calls per second.",
        "text_jp": "企業はAPI Gatewayのメソッドごとのデフォルト呼び出し制限に達しました。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: The company reached its API Gateway account limit for calls per second.",
        "situation_analysis": "The company deployed a SaaS solution with a premium tier allowing up to 3,000 API calls per second. During peak usage, customers began receiving 429 errors, meaning they exceeded the API limit.",
        "option_analysis": "Option A (Lambda concurrency limit) is incorrect as the logs show Lambda was never invoked. Option B also pertains to Lambda, which isn't applicable. Option D focuses on per-method limits, but the account limit would likely provide a broader diagnosis given the symptoms.",
        "additional_knowledge": "Regular monitoring and usage alerts could help prevent future occurrences.",
        "key_terminology": "API Gateway, requests per second limit, concurrency, Lambda, SaaS.",
        "overall_assessment": "The solution's architecture and implementation must consider the API Gateway's account-level limits to effectively handle high traffic, which the premium tier customers exceeded."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はC: 企業はAPI Gatewayアカウントの呼び出し制限に達しました。",
        "situation_analysis": "企業は最大3,000APIコール/秒を許可するプレミアムプランを持つSaaSソリューションを展開しました。ピーク使用中に、顧客は429エラーを受信し、API制限を超過したことを示しています。",
        "option_analysis": "選択肢A（Lambdaの同時実行制限）は、ログにLambdaが一度も呼び出されていないと示されているため、正しくありません。選択肢BもLambdaに関するもので、適用されません。選択肢Dはメソッドごとの制限に関するもので、症状を考慮するとアカウントの制限の方がより広範な診断を提供する可能性があります。",
        "additional_knowledge": "定期的な監視と使用アラートがあれば、将来の発生を防ぐ手助けになるでしょう。",
        "key_terminology": "API Gateway、リクエスト毎秒制限、同時実行、Lambda、SaaS。",
        "overall_assessment": "ソリューションのアーキテクチャと実装は、高トラフィックを効果的に処理するために、API Gatewayのアカウントレベルの制限を考慮する必要があります。プレミアムプラン顧客はこの制限を超過しました。"
      }
    ],
    "keywords": [
      "API Gateway",
      "requests per second limit",
      "concurrency",
      "Lambda",
      "SaaS"
    ]
  },
  {
    "No": "121",
    "question": "A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor\nthe inbound trafic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available\nfrom its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology.\nThe company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a\ndedicated VPC. The company needs to use the security tool to inspect all packets that come in and out of the VPC. This inspection must occur in\nreal time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available\nwithin an AWS Region.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "金融会社は、オンプレミスからAWSへのWebアプリケーションの移行を計画しています。会社は、アプリケーションへのインバウンドトラフィックを監視するために、サードパーティのセキュリティツールを使用しています。この会社は過去15年間そのセキュリティツールを使用しており、そのベンダーからクラウドソリューションは提供されていません。会社のセキュリティチームは、AWS技術とのセキュリティツールの統合方法について懸念を抱いています。会社は、アプリケーションの移行をAWSのAmazon EC2インスタンスにデプロイする予定です。EC2インスタンスは、専用VPC内のオートスケーリンググループで実行されます。会社は、セキュリティツールを使用してVPCの出入りするすべてのパケットを検査する必要があります。この検査はリアルタイムで行われ、アプリケーションのパフォーマンスに影響を与えてはなりません。ソリューションアーキテクトは、AWS上で高可用性のターゲットアーキテクチャを設計する必要があります。要件を満たすためにソリューションアーキテクトが取るべき手順の組み合わせはどれか。（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC",
        "text_jp": "EC2インスタンスにセキュリティツールをデプロイし、既存のVPC内の新しいオートスケーリンググループに配置する"
      },
      {
        "key": "B",
        "text": "Deploy the web application behind a Network Load Balancer",
        "text_jp": "Network Load Balancerの背後にWebアプリケーションをデプロイする"
      },
      {
        "key": "C",
        "text": "Deploy an Application Load Balancer in front of the security tool instances",
        "text_jp": "セキュリティツールインスタンスの前にApplication Load Balancerをデプロイする"
      },
      {
        "key": "D",
        "text": "Provision a Gateway Load Balancer for each Availability Zone to redirect the trafic to the security tool",
        "text_jp": "可用性ゾーンごとにGateway Load Balancerをプロビジョニングし、トラフィックをセキュリティツールにリダイレクトする"
      },
      {
        "key": "E",
        "text": "Provision a transit gateway to facilitate communication between VPCs.",
        "text_jp": "VPC間の通信を促進するためにトランジットゲートウェイをプロビジョニングする"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (54%) DE (40%) 4%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D, as they allow for real-time inspection of traffic while maintaining application performance.",
        "situation_analysis": "The company requires that their existing security tool be implemented in the AWS architecture without affecting performance.",
        "option_analysis": "Option A allows for the deployment of the security tool in a highly available manner in EC2 instances. Option D enables the redirection of traffic through a Gateway Load Balancer, ensuring quick packet inspection.",
        "additional_knowledge": "Without proper traffic management, performance issues could arise.",
        "key_terminology": "Auto Scaling, Gateway Load Balancer, Network Load Balancer, traffic inspection, VPC.",
        "overall_assessment": "Combining both answers aligns with best practices for integrating third-party tools for security on AWS."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAとDであり、これによりトラフィックのリアルタイム検査を行いつつ、アプリケーションのパフォーマンスを維持することができる。",
        "situation_analysis": "会社は、パフォーマンスに影響を与えずに、既存のセキュリティツールをAWSアーキテクチャに実装する必要がある。",
        "option_analysis": "オプションAは、EC2インスタンス内にセキュリティツールを高可用性の形でデプロイすることを可能にする。オプションDは、Gateway Load Balancerを介してトラフィックをリダイレクトし、迅速なパケット検査を確保する。",
        "additional_knowledge": "適切なトラフィック管理がないと、パフォーマンスの問題が生じる可能性がある。",
        "key_terminology": "オートスケーリング、Gateway Load Balancer、Network Load Balancer、トラフィック検査、VPC。",
        "overall_assessment": "両方の答えを組み合わせることで、AWS上のセキュリティツールの統合に関するベストプラクティスに沿ったものになる。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "Gateway Load Balancer",
      "Network Load Balancer",
      "traffic inspection",
      "VPC"
    ]
  },
  {
    "No": "122",
    "question": "A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the\nvendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique\nformat. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis.\nThe company needs to design a new data analysis solution that can deliver faster and optimize costs.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、異なるベンダーから装置を購入しました。これらの装置にはすべてIoTセンサーが搭載されており、センサーは各ベンダーの独自形式で状態情報を送信します。この情報はレガシーアプリケーションによって解析され、JSONに変換されます。解析は簡単ですが、各ベンダーには固有のフォーマットがあります。アプリケーションは、1日に一度すべてのJSONレコードを解析し、分析のためにリレーショナルデータベースに記録を保存します。企業は、より迅速にコストを最適化する新しいデータ分析ソリューションを設計する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon. S3 Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis.",
        "text_jp": "AWS IoT CoreにIoTセンサーを接続します。ルールを設定してAWS Lambda関数を呼び出し、情報を解析してAmazon S3に.csvファイルを保存します。AWS Glueでファイルをカタログ化します。分析にはAmazon AthenaとAmazon QuickSightを使用します。"
      },
      {
        "key": "B",
        "text": "Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a relational format. Save the parsed information to Amazon Redshlft for analysis.",
        "text_jp": "アプリケーションサーバーをAWS Fargateに移行し、IoTセンサーから情報を受信してリレーショナル形式に解析します。解析された情報をAmazon Redshiftに保存し、分析を行います。"
      },
      {
        "key": "C",
        "text": "Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis.",
        "text_jp": "AWS Transfer for SFTPサーバーを作成します。IoTセンサーのコードを更新して情報を.csvファイルとしてSFTPを介してサーバーに送信します。AWS Glueでファイルをカタログ化します。分析にはAmazon Athenaを使用します。"
      },
      {
        "key": "D",
        "text": "Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon Redshift to perform global analysis.",
        "text_jp": "AWS Snowball Edgeを使用してIoTセンサーからデータを直接収集し、ローカル分析を行います。周期的にデータをAmazon Redshiftに集約してグローバル分析を行います。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This solution allows for direct data transmission from IoT devices in a consumable format, which is necessary given the variety of proprietary formats currently in use.",
        "situation_analysis": "The company requires a more efficient way to handle data from IoT sensors with varying proprietary formats, suggesting the need for uniformity in data ingestion and processing.",
        "option_analysis": "Option C allows the sensors to send data as .csv files, which simplifies parsing, while also utilizing AWS Glue and Amazon Athena for effective data management and analysis. Other options like A and D may incur more complexity, and B does not fundamentally change the data ingestion problem.",
        "additional_knowledge": "Moving to a standard file format can streamline integration processes and improve analytical performance.",
        "key_terminology": "IoT, AWS Glue, Amazon S3, Amazon Athena, SFTP",
        "overall_assessment": "With the overwhelming community support leaning towards A, it raises a good point about that solution's potential effectiveness but the technical merits of option C remain robust and logically sound given the situation at hand, thus it's important to validate that C still meets the outlined requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。このソリューションは、IoTデバイスから消費可能な形式で直接データを送信できるため、現在使用されているさまざまな独自形式に対処する必要があることを示しています。",
        "situation_analysis": "企業は、さまざまな独自形式を持つIoTセンサーからのデータをより効率的に処理する方法を必要としているため、データの取り込みと処理の均一性が求められています。",
        "option_analysis": "選択肢Cは、センサーがデータを.csvファイルとして送信することを可能にし、解析を簡素化するとともに、AWS GlueとAmazon Athenaを利用して効果的なデータ管理と分析を行います。他の選択肢AやDは複雑さが増す可能性があり、Bはデータ取り込みの問題に根本的に変化をもたらしません。",
        "additional_knowledge": "標準ファイル形式に移行することで、統合プロセスを効率化し、分析パフォーマンスを向上させることができます。",
        "key_terminology": "IoT, AWS Glue, Amazon S3, Amazon Athena, SFTP",
        "overall_assessment": "コミュニティサポートがAに偏っていることは見逃せず、このソリューションの効果を示す良い点を挙げていますが、Cの技術的メリットは堅実であり、論理的に健全であるため、Cが要件に適合しているかの検証が重要です。"
      }
    ],
    "keywords": [
      "IoT",
      "AWS Glue",
      "Amazon S3",
      "Amazon Athena",
      "SFTP"
    ]
  },
  {
    "No": "123",
    "question": "A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes\nnetworking and security strategies. The company has set up an AWS Direct Connect connection in a central network account.\nThe company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the\nresources on AWS seamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to\nthe internet through its on-premises data center.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ある企業が複数のアプリケーションをAWSに移行しています。この企業は、ネットワーキングとセキュリティ戦略を確定した後、アプリケーションを迅速に移行および近代化したいと考えています。企業は中央のネットワークアカウントにAWS Direct Connect接続を設定しました。企業は近い将来、数百のAWSアカウントとVPCを持つことを期待しています。企業ネットワークはAWS上のリソースにシームレスにアクセスでき、すべてのVPCと通信できる必要があります。また、クラウドリソースをインターネットにルーティングするために、オンプレミスのデータセンターを経由したいと考えています。\nこれらの要件を満たすためには、どの組み合わせのステップを選択しますか？（3つ選んでください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a Direct Connect gateway in the central account. In each of the accounts, create an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway.",
        "text_jp": "中央アカウントにDirect Connectゲートウェイを作成します。各アカウントで、Direct Connectゲートウェイと各バーチャルプライベートゲートウェイのアカウントIDを使用して、関連付け提案を作成します。"
      },
      {
        "key": "B",
        "text": "Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF.",
        "text_jp": "中央ネットワークアカウントにDirect Connectゲートウェイとトランジットゲートウェイを作成します。トランジットゲートウェイをDirect ConnectゲートウェイにトランジットVIFを使用して接続します。"
      },
      {
        "key": "C",
        "text": "Provision an internet gateway. Attach the internet gateway to subnets. Allow internet trafic through the gateway.",
        "text_jp": "インターネットゲートウェイをプロビジョニングします。サブネットにインターネットゲートウェイを接続します。ゲートウェイを通じてインターネットトラフィックを許可します。"
      },
      {
        "key": "D",
        "text": "Share the transit gateway with other accounts. Attach VPCs to the transit gateway.",
        "text_jp": "トランジットゲートウェイを他のアカウントと共有します。VPCをトランジットゲートウェイに接続します。"
      },
      {
        "key": "E",
        "text": "Provision VPC peering as necessary.",
        "text_jp": "必要に応じてVPCピアリングをプロビジョニングします。"
      },
      {
        "key": "F",
        "text": "Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet trafic from AWS to fiow through NAT services that run in the data center.",
        "text_jp": "プライベートサブネットのみをプロビジョニングします。トランジットゲートウェイとカスタマーゲートウェイで必要なルートを開放し、AWSからのアウトバウンドインターネットトラフィックがデータセンターで実行されるNATサービスを通るようにします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BDF (100%)",
    "page_images": [],
    "community_vote_distribution_jp": "BDF (100%)",
    "page_images_jp": [],
    "explanation_en": [
      {
        "answer_and_key_points": "正解はBである。この選択肢は、企業が要求しているネットワークの接続とルーティング要件を満たしている。",
        "situation_analysis": "企業はAWSに数百のアカウントとVPCを持ち、シームレスなアクセスとインターネット経由の通信が必要である。",
        "option_analysis": "Bの選択肢は、Direct Connectゲートウェイとトランジットゲートウェイを使用してネットワークを統一し、他のオプションは部分的な解決を提供するに過ぎない。",
        "additional_knowledge": "このアプローチにより、ネットワークの一貫性が確保され、拡張性の高いデザインに対応することができる。",
        "key_terminology": "Direct Connect, Transit Gateway, VPC",
        "overall_assessment": "Bの選択肢が最も多く支持されており、他の選択肢は明確な解決策を提供しないため、選択肢Bがベストプラクティスである。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。この選択肢は、企業が要求しているネットワークの接続とルーティング要件を満たしている。",
        "situation_analysis": "企業はAWSに数百のアカウントとVPCを持ち、シームレスなアクセスとインターネット経由の通信が必要である。",
        "option_analysis": "Bの選択肢は、Direct Connectゲートウェイとトランジットゲートウェイを使用してネットワークを統一し、他のオプションは部分的な解決を提供するに過ぎない。",
        "additional_knowledge": "このアプローチにより、ネットワークの一貫性が確保され、拡張性の高いデザインに対応することができる。",
        "key_terminology": "Direct Connect, Transit Gateway, VPC",
        "overall_assessment": "Bの選択肢が最も多く支持されており、他の選択肢は明確な解決策を提供しないため、選択肢Bがベストプラクティスである。"
      }
    ],
    "keywords": [
      "Direct Connect",
      "Transit Gateway",
      "VPC"
    ]
  },
  {
    "No": "124",
    "question": "A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved\nInstances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved\nInstances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances\nin their own respective AWS accounts autonomously.\nA solutions architect needs to enforce the new process in the most secure way possible.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "企業は何百ものAWSアカウントを持っています。企業は最近、新しい予約インスタンスを購入し、既存の予約インスタンスを変更するための中央集権的な内部プロセスを実装しました。このプロセスでは、予約インスタンスを購入または変更したいすべてのビジネスユニットが、調達のために専任チームにリクエストを提出する必要があります。以前は、ビジネスユニットがそれぞれのAWSアカウント内で自律的に予約インスタンスを直接購入または変更していました。ソリューションアーキテクトは、可能な限り安全な方法で新しいプロセスを強制する必要があります。これらの要件を満たすためにソリューションアーキテクトが取るべき手順の組み合わせはどれですか？（二つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.",
        "text_jp": "すべてのAWSアカウントがAWS Organizationsの組織の一部であり、すべての機能が有効になっていることを確認します。"
      },
      {
        "key": "B",
        "text": "Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.",
        "text_jp": "AWS Configを使用して、ec2:PurchaseReservedInstancesOfferingアクションおよびec2:ModifyReservedInstancesアクションへのアクセスを拒否するIAMポリシーのアタッチメントを報告します。"
      },
      {
        "key": "C",
        "text": "In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.",
        "text_jp": "各AWSアカウントにec2:PurchaseReservedInstancesOfferingアクションおよびec2:ModifyReservedInstancesアクションを拒否するIAMポリシーを作成します。"
      },
      {
        "key": "D",
        "text": "Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization.",
        "text_jp": "ec2:PurchaseReservedInstancesOfferingアクションおよびec2:ModifyReservedInstancesアクションを拒否するSCPを作成します。このSCPを組織の各OUにアタッチします。"
      },
      {
        "key": "E",
        "text": "Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature.",
        "text_jp": "すべてのAWSアカウントが統合請求機能を使用するAWS Organizationsの組織の一部であることを確認します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D. Ensuring that all AWS accounts are part of an AWS Organization allows for easier management and enforcement of policies across multiple accounts.",
        "situation_analysis": "The scenario indicates a centralized procurement process for Reserved Instances, necessitating tight controls on who can make these purchases.",
        "option_analysis": "Option A and D provide organizational cohesion and security. D uses Service Control Policies (SCPs) to enforce rules at the organizational level, which is essential for centralized governance.",
        "additional_knowledge": "Implementing these practices prevents potential misuse or unapproved purchases.",
        "key_terminology": "AWS Organizations, Service Control Policies (SCP), IAM Policies.",
        "overall_assessment": "Together, these options ensure that only authorized individuals can make purchases related to Reserved Instances while allowing management at the organizational level."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとDである。すべてのAWSアカウントがAWS Organizationsに属することを保証することで、複数のアカウントにわたるポリシーの管理と強制が容易になる。",
        "situation_analysis": "このシナリオは、予約インスタンスのための中央集権的な調達プロセスを示しており、これらの購入を行うことができる者に対して厳しい管理が必要であることを示唆している。",
        "option_analysis": "選択肢AとDは、組織の cohesion とセキュリティを提供する。Dはサービスコントロールポリシー（SCP）を使用して、組織レベルでルールを強制し、中央集権的なガバナンスに必要不可欠である。",
        "additional_knowledge": "これらの実践を実施することで、誤用や未承認の購入を防止する。",
        "key_terminology": "AWS Organizations, サービスコントロールポリシー（SCP）, IAMポリシー。",
        "overall_assessment": "これらの選択肢を組み合わせることで、認可された個人だけが予約インスタンスに関連する購入を行えるようにし、組織レベルで管理を可能にする。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Service Control Policies",
      "IAM Policies"
    ]
  },
  {
    "No": "125",
    "question": "A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-\nAZ mode.\nA recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the\noutage time to less than 20 seconds.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "企業がデータを格納するためにAmazon RDS for MySQLデータベースを使用している重要なアプリケーションを実行しています。RDS DBインスタンスはMulti-AZモードで展開されています。最近のRDSデータベースフェイルオーバーテストにより、アプリケーションは40秒の停止を経験しました。ソリューションアーキテクトは、停止時間を20秒未満に短縮するソリューションを設計する必要があります。要件を満たすためにソリューションアーキテクトが取るべき手順の組み合わせはどれですか？（3つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon ElastiCache for Memcached in front of the database",
        "text_jp": "データベースの前にAmazon ElastiCache for Memcachedを使用する。"
      },
      {
        "key": "B",
        "text": "Use Amazon ElastiCache for Redis in front of the database",
        "text_jp": "データベースの前にAmazon ElastiCache for Redisを使用する。"
      },
      {
        "key": "C",
        "text": "Use RDS Proxy in front of the database.",
        "text_jp": "データベースの前にRDS Proxyを使用する。"
      },
      {
        "key": "D",
        "text": "Migrate the database to Amazon Aurora MySQL.",
        "text_jp": "データベースをAmazon Aurora MySQLに移行する。"
      },
      {
        "key": "E",
        "text": "Create an Amazon Aurora Replica.",
        "text_jp": "Amazon Auroraレプリカを作成する。"
      },
      {
        "key": "F",
        "text": "Create an RDS for MySQL read replica",
        "text_jp": "RDS for MySQLリードレプリカを作成する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CDE (89%) 11%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Use Amazon ElastiCache for Redis in front of the database. This choice, along with others not selected, helps to reduce the effective downtime during RDS failover.",
        "situation_analysis": "The application has critical requirements that demand low latency and high availability, which were not sufficiently met with the Multi-AZ setup as shown by the 40-second outage.",
        "option_analysis": "Option B (Using Amazon ElastiCache for Redis) is chosen to enhance performance and provide caching, thus improving response times. Option C (RDS Proxy) also supports failover handling, while Option D (migrating to Aurora MySQL) may reduce failover times, but it requires more significant changes.",
        "additional_knowledge": "Using these services in conjunction with RDS can provide a robust solution for high availability and improved performance.",
        "key_terminology": "Amazon RDS, Amazon Aurora, Amazon Redis, availability, caching, failover.",
        "overall_assessment": "The question effectively gauges the understanding of solutions that can enhance database resilience and performance, especially under failover conditions. Although the community feedback heavily leans towards options C, D, and E, leveraging ElastiCache effectively addresses the low-latency requirement without a complete overhaul of the existing setup."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はB: データベースの前にAmazon ElastiCache for Redisを使用するです。この選択肢は、選択されなかった他の選択肢と共に、RDSのフェイルオーバー中の実際のダウンタイムを削減するのに役立ちます。",
        "situation_analysis": "このアプリケーションは低遅延と高可用性を要求する重要な要件があり、Multi-AZセットアップによって示された40秒間の停止が十分に満たされませんでした。",
        "option_analysis": "Bの選択肢（Amazon ElastiCache for Redisの使用）は、パフォーマンスを向上させ、キャッシングを提供し、応答時間を改善するために選ばれました。C（RDS Proxyの使用）はフェイルオーバー処理をサポートし、D（Aurora MySQLへの移行）はフェイルオーバー時間を短縮する可能性がありますが、より大きな変更が必要です。",
        "additional_knowledge": "これらのサービスをRDSと併用することで、高可用性とパフォーマンス向上のための強力なソリューションを提供できます。",
        "key_terminology": "Amazon RDS、Amazon Aurora、Amazon Redis、可用性、キャッシング、フェイルオーバー。",
        "overall_assessment": "この質問は、フェイルオーバー条件の下でデータベースの耐障害性とパフォーマンスを向上させるためのソリューションの理解を効果的に測定しています。コミュニティのフィードバックはC、D、Eの選択肢に偏っているものの、ElastiCacheを活用することで、既存のセットアップの大規模なオーバーホールを行うことなく低遅延の要件に対処します。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "Amazon Aurora",
      "Amazon ElastiCache",
      "failover",
      "caching"
    ]
  },
  {
    "No": "126",
    "question": "An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company\nto have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least\nprivilege security access using an API or command line tool to the customer account.\nWhat is the MOST secure way to allow org1 to access resources in org2?",
    "question_jp": "AWSパートナー企業がorg1という名前の組織を使用してAWS Organizationsでサービスを構築しています。このサービスでは、パートナー企業が、別の組織であるorg2にある顧客アカウントのAWSリソースにアクセスする必要があります。この企業はAPIまたはコマンドラインツールを使用して顧客アカウントに最小権限のセキュリティアクセスを確立しなければなりません。org1がorg2のリソースにアクセスするための最も安全な方法は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.",
        "text_jp": "顧客はパートナー企業にAWSアカウントのアクセスキーを提供し、ログインして必要な作業を行います。"
      },
      {
        "key": "B",
        "text": "The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks.",
        "text_jp": "顧客はIAMユーザーを作成し、必要な権限をそのIAMユーザーに割り当てます。顧客はその後、パートナー企業にログインして必要な作業を行うための資格情報を提供します。"
      },
      {
        "key": "C",
        "text": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role's Amazon Resource Name (ARN) when requesting access to perform the required tasks.",
        "text_jp": "顧客はIAMロールを作成し、必要な権限をそのIAMロールに割り当てます。パートナー企業は、そのIAMロールのAmazonリソースネーム（ARN）を使用してアクセスを要求し、必要な作業を行います。"
      },
      {
        "key": "D",
        "text": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role's Amazon Resource Name (ARN), including the external ID in the IAM role's trust policy, when requesting access to perform the required tasks.",
        "text_jp": "顧客はIAMロールを作成し、必要な権限をそのIAMロールに割り当てます。パートナー企業は、要求する際にIAMロールの信頼ポリシーに外部IDを含むIAMロールのAmazonリソースネーム（ARN）を使用します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using an IAM role with a trusted external ID is the most secure method to grant access across AWS Organizations.",
        "situation_analysis": "The partner company needs access to resources in another organization while maintaining security best practices. This requires a trusted approach that minimizes potential unauthorized access.",
        "option_analysis": "Option A is insecure as it shares access keys. Option B may allow too many permissions if not configured correctly. Option C lacks the use of an external ID, which can provide an additional layer of security.",
        "additional_knowledge": "IAM roles facilitate the granting of specific permissions only when required, aiding in security compliance.",
        "key_terminology": "IAM role, external ID, least privilege access, AWS Organizations",
        "overall_assessment": "The question addresses a common scenario in AWS Organizations, and option D is the best practice for secure access management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。IAMロールを外部IDを使用して信頼できる方法で設定することが、AWS Organizations間でアクセスを許可する最も安全な方法である。",
        "situation_analysis": "パートナー企業は、別の組織内のリソースにアクセスする必要があり、セキュリティのベストプラクティスを維持しながら行う必要がある。これには、未承認のアクセスの可能性を最小限に抑える信頼できるアプローチが必要である。",
        "option_analysis": "選択肢Aはアクセスキーを共有するため安全でない。選択肢Bは、適切に設定されていない場合に過剰な権限を与える可能性がある。選択肢Cは、外部IDを使用していないため、追加のセキュリティ層が欠けている。",
        "additional_knowledge": "IAMロールは、必要なときだけ特定の権限を付与する手段として、セキュリティコンプライアンスに役立つ。",
        "key_terminology": "IAMロール、外部ID、最小権限アクセス、AWS Organizations",
        "overall_assessment": "この質問は、AWS Organizationsで一般的に発生するシナリオを扱っており、選択肢Dがセキュリティ管理におけるベストプラクティスである。"
      }
    ],
    "keywords": [
      "IAM role",
      "external ID",
      "least privilege access",
      "AWS Organizations"
    ]
  },
  {
    "No": "127",
    "question": "A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a\npublic registry. The image can run in as many containers as required to generate the route map.\nThe company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the\nhubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom\nconfiguration that processes orders only in the section's area.\nThe company needs the ability to allocate resources cost-effectively based on the number of running containers.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "配送会社は、サードパーティのルートプランニングアプリケーションをAWSに移行する必要があります。サードパーティは、パブリックレジストリからサポートされたDockerイメージを提供しています。このイメージは、ルートマップを生成するために必要な数のコンテナで実行できます。会社は配送エリアを供給ハブで区切り、配送ドライバーがハブから顧客への最短距離で移動できるようにしています。ルートマップを生成するための必要時間を短縮するために、各セクションは、そのセクションのエリア内でのみ注文を処理するカスタム設定のDockerコンテナを独自に使用しています。会社は、実行中のコンテナの数に基づいてコスト効率よくリソースを割り当てる能力が必要です。どのソリューションが最も低い運用負荷でこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning application in pods by using the --tags option to assign a custom tag to the pod.",
        "text_jp": "Amazon EC2上にAmazon Elastic Kubernetes Service (Amazon EKS)クラスタを作成します。Amazon EKS CLIを使用して、--tagsオプションを使用して、ポッドにカスタムタグを割り当てることで計画アプリケーションをポッドで起動します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning application. Use the AWS CLI tag-resource API call to assign a custom tag to the pod.",
        "text_jp": "AWS Fargate上にAmazon Elastic Kubernetes Service (Amazon EKS)クラスタを作成します。Amazon EKS CLIを使用して計画アプリケーションを起動します。AWS CLIのtag-resource API呼び出しを使用してポッドにカスタムタグを割り当てます。"
      },
      {
        "key": "C",
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch the planning application by using the --tags option to assign a custom tag to the task.",
        "text_jp": "Amazon EC2上にAmazon Elastic Container Service (Amazon ECS)クラスタを作成します。--tagsオプションを使用してカスタムタグをタスクに割り当てるために、AWS CLIでrun-tasksをtrueに設定して計画アプリケーションを起動します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task.",
        "text_jp": "AWS Fargate上にAmazon Elastic Container Service (Amazon ECS)クラスタを作成します。AWS CLI run-taskコマンドを使用し、enableECSManagedTagsをtrueに設定して計画アプリケーションを起動します。--tagsオプションを使用してタスクにカスタムタグを割り当てます。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (79%) B (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. The solution using Amazon ECS with AWS Fargate provides the least operational overhead by managing the underlying infrastructure automatically. Fargate allows you to run containers without needing to manage the servers, which aligns with the requirement of minimizing operational workload.",
        "situation_analysis": "The company requires a scalable and cost-effective solution for running multiple instances of Docker containers for route planning, with the flexibility to allocate resources based on the number of running containers.",
        "option_analysis": "Option A requires managing an EKS cluster manually on EC2, leading to higher operational overhead. Option B, while using Fargate, introduces complexity by using EKS, which is not necessary for the task. Option C requires managing EC2 instances and lacks Fargate's serverless capabilities. Thus, option D is optimal as it leverages ECS with Fargate, simplifying management while meeting the company's requirements.",
        "additional_knowledge": "Fargate is suitable for applications with variable workloads where scaling in and out is necessary.",
        "key_terminology": "AWS Fargate, Amazon ECS, Docker containers, operational overhead, serverless computing",
        "overall_assessment": "Option D provides the best combination of simplicity and functionality to meet the requirements with minimal operational overhead. The community vote aligns with this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。Amazon ECSをAWS Fargateで使用することで、基盤となるインフラストラクチャを自動的に管理できるため、最も運用負荷が少なくなる。Fargateでは、サーバーを管理することなくコンテナを実行でき、運用 workload の最小化の要件と一致する。",
        "situation_analysis": "会社は、ルートプランニング用のDockerコンテナの複数インスタンスを実行するためのスケーラブルでコスト効率の良いソリューションを必要としており、実行中のコンテナの数に基づいてリソースを割り当てる柔軟性を求めている。",
        "option_analysis": "選択肢Aは、EC2上に手動でEKSクラスタを管理する必要があり、運用負荷が高くなる。選択肢Bは、Fargateを使用するが、EKSを使用することで不必要な複雑さが生じる。選択肢CはEC2インスタンスの管理が必要であり、Fargateのサーバーレス機能が欠けている。したがって選択肢Dが最適であり、ECSとFargateを活用し、管理を簡素化しつつ、会社の要件を満たしている。",
        "additional_knowledge": "Fargateは、ワークロードが変動するアプリケーションに適しており、スケーリングが必要な場合に有用である。",
        "key_terminology": "AWS Fargate, Amazon ECS, Dockerコンテナ, 運用負荷, サーバーレスコンピューティング",
        "overall_assessment": "選択肢Dは、運用負荷を最小限に抑えつつ、要件を満たすための簡素さと機能性の最良の組み合わせを提供する。コミュニティ票もこの選択に一致している。"
      }
    ],
    "keywords": [
      "AWS Fargate",
      "Amazon ECS",
      "Docker containers",
      "operational overhead",
      "serverless computing"
    ]
  },
  {
    "No": "128",
    "question": "A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of\nAmazon EC2 instances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account,\na shared services VPC is located in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS\nCloudFormation to attempt to peer the application VPC with the shared services VPC, an error message indicates a peering failure.\nWhich factors could cause this error? (Choose two.)",
    "question_jp": "あるソフトウェア会社は、複数のAWSアカウントとリージョンにリソースを持つアプリケーションをAWS上でホストしています。このアプリケーションは、us-east-1リージョンに位置するアプリケーションVPCの一群のAmazon EC2インスタンス上で実行されており、IPv4 CIDRブロックは10.10.0.0/16です。異なるAWSアカウントには、us-east-2リージョンに位置する共有サービスVPCがあり、IPv4 CIDRブロックは10.10.10.0/24です。Cloud engineerがAWS CloudFormationを使用してアプリケーションVPCと共有サービスVPCをピアリングしようとすると、エラーメッセージが表示され、ピアリングの失敗が示されます。このエラーの原因として考えられる要因はどれですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "The IPv4 CIDR ranges of the two VPCs overlap",
        "text_jp": "2つのVPCのIPv4 CIDR範囲が重複している"
      },
      {
        "key": "B",
        "text": "The VPCs are not in the same Region",
        "text_jp": "VPCが同じリージョンにない"
      },
      {
        "key": "C",
        "text": "One or both accounts do not have access to an Internet gateway",
        "text_jp": "1つまたは両方のアカウントがインターネットゲートウェイへのアクセスを持っていない"
      },
      {
        "key": "D",
        "text": "One of the VPCs was not shared through AWS Resource Access Manager",
        "text_jp": "VPCの1つがAWS Resource Access Managerを通じて共有されていない"
      },
      {
        "key": "E",
        "text": "The IAM role in the peer accepter account does not have the correct permissions",
        "text_jp": "ピアアクセプターアカウントのIAMロールに正しい権限がない"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AE (84%) BE (16%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and B. The IPv4 CIDR ranges of the two VPCs must not overlap for VPC peering to work, and they also cannot be in different regions.",
        "situation_analysis": "The problem describes an attempted VPC peering between two VPCs located in different AWS accounts and regions, with specific CIDR blocks assigned.",
        "option_analysis": "Option A is correct as the VPC CIDR blocks do not overlap, while Option B is correct as the VPCs are in different regions.",
        "additional_knowledge": "Inter-region VPC peering does allow peering between VPCs in different regions, but with the given CIDR blocks, an error would arise.",
        "key_terminology": "VPC Peering, CIDR Block, AWS CloudFormation",
        "overall_assessment": "The question effectively assesses knowledge on VPC peering prerequisites, highlighting the critical nature of CIDR block management and regional considerations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAとBです。VPCピアリングが機能するためには、2つのVPCのIPv4 CIDR範囲が重複してはいけないのと、異なるリージョンに存在してはいけません。",
        "situation_analysis": "この問題は、異なるAWSアカウントとリージョンにある2つのVPCの間でのVPCピアリングの試みを説明しており、特定のCIDRブロックが割り当てられています。",
        "option_analysis": "選択肢Aは、VPCのCIDRブロックが重複していないため正解です。一方、選択肢Bも、VPCが異なるリージョンにあるため正しいです。",
        "additional_knowledge": "インターレジョンVPCピアリングは異なるリージョンにあるVPC間でのピアリングを可能にしますが、与えられたCIDRブロックではエラーが発生することになります。",
        "key_terminology": "VPCピアリング、CIDRブロック、AWS CloudFormation",
        "overall_assessment": "この質問は、VPCピアリングの前提条件に関する知識を効果的に評価しており、CIDRブロックの管理と地域の考慮が重要であることを明確にしています。"
      }
    ],
    "keywords": [
      "VPC Peering",
      "CIDR Block",
      "AWS CloudFormation"
    ]
  },
  {
    "No": "129",
    "question": "An external audit of a company's serverless application reveals IAM policies that grant too many permissions. These policies are attached to the\ncompany's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to\nAmazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function\nneeds to complete its task.\nA solutions architect must determine which permissions each Lambda function needs.\nWhat should the solutions architect do to meet this requirement with the LEAST amount of effort?",
    "question_jp": "会社のサーバーレスアプリケーションの外部監査により、IAMポリシーが過剰な権限を付与していることが明らかになりました。これらのポリシーは、会社のAWS Lambda実行ロールに関連付けられています。会社の何百ものLambda関数には、Amazon S3バケットやAmazon DynamoDBテーブルへの完全なアクセス権など、広範なアクセス権があります。会社は、各関数がそのタスクを完了するために必要な最小限の権限のみを持つことを望んでいます。ソリューションアーキテクトは、各Lambda関数にどの権限が必要かを特定する必要があります。最小限の労力でこの要件を満たすために、ソリューションアーキテクトは何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and resources for each Lambda function. Create new IAM access policies for each Lambda function. Review the new policies to ensure that they meet the company's business requirements.",
        "text_jp": "Amazon CodeGuruを設定してLambda関数をプロファイリングし、AWS APIコールを検索します。各Lambda関数に必要なAPIコールとリソースのインベントリを作成します。各Lambda関数用に新しいIAMアクセスポリシーを作成します。新しいポリシーが会社のビジネス要件を満たすことを確認します。"
      },
      {
        "key": "B",
        "text": "Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements.",
        "text_jp": "AWSアカウントのAWS CloudTrailログを有効にします。AWS Identity and Access Management Access Analyzerを使用して、CloudTrailログに記録されたアクティビティに基づいてIAMアクセスポリシーを生成します。生成されたポリシーが会社のビジネス要件を満たすことを確認します。"
      },
      {
        "key": "C",
        "text": "Turn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda execution role, and create a summary report. Review the report. Create IAM access policies that provide more restrictive permissions for each Lambda function.",
        "text_jp": "AWSアカウントのAWS CloudTrailログを有効にします。CloudTrailログを解析し、Lambda実行ロールによるAWS APIコールを検索し、サマリーレポートを作成するスクリプトを作成します。レポートをレビューします。各Lambda関数のために、より制限された権限を持つIAMアクセスポリシーを作成します。"
      },
      {
        "key": "D",
        "text": "Turn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resources used by each execution role. Create a new IAM access policy for each role. Export the generated roles to an S3 bucket. Review the generated policies to ensure that they meet the company's business requirements.",
        "text_jp": "AWSアカウントのAWS CloudTrailログを有効にします。CloudTrailログをAmazon S3にエクスポートします。Amazon EMRを使用してAmazon S3のCloudTrailログを処理し、各実行ロールによって使用されるAPIコールとリソースのレポートを生成します。各ロールのために新しいIAMアクセスポリシーを作成します。生成されたロールをS3バケットにエクスポートします。生成されたポリシーが会社のビジネス要件を満たすことを確認します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves turning on AWS CloudTrail logging and using the Identity and Access Management Access Analyzer. This method is efficient as it automates the generation of IAM policies based on actual usage recorded in the logs, thus ensuring minimal permissions are granted.",
        "situation_analysis": "The company currently has too broad IAM permissions associated with its Lambda functions. The objective is to minimize these permissions while ensuring business requirements are still met.",
        "option_analysis": "Option A requires manual profiling with Amazon CodeGuru, which is more effort-intensive. Option C involves creating a script to summarize CloudTrail logs, introducing significant custom effort. Option D also involves complex processing of logs and generating reports using multiple services.",
        "additional_knowledge": "By automating policy generation based on actual usage, the company can effectively reduce security risks.",
        "key_terminology": "AWS CloudTrail, IAM Access Analyzer, Principle of Least Privilege, AWS Lambda, IAM Policies",
        "overall_assessment": "Option B is the best choice as it provides automation of the process, allowing the solutions architect to fulfill the requirement with minimal effort compared to other options."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、AWS CloudTrailのログを有効にし、Identity and Access Management Access Analyzerを使用することを含みます。この方法は、ログに記録された実際の使用状況に基づいてIAMポリシーの生成を自動化するため、最小の権限を付与して効率的です。",
        "situation_analysis": "会社は現在、Lambda関数に関連付けられたIAM権限が広すぎる状態にあります。目標は、これらの権限を最小限に抑えつつ、ビジネス要件も満たすことです。",
        "option_analysis": "選択肢AはAmazon CodeGuruを使用した手動プロファイリングを必要とし、より多くの労力を要します。選択肢CはCloudTrailログの要約を作成するスクリプトの作成を必要とし、かなりのカスタム労力がかかります。選択肢Dも複数のサービスを使用したログの処理及びレポート生成を含み、複雑です。",
        "additional_knowledge": "実際の使用に基づくポリシー生成を自動化することで、会社はセキュリティリスクを効果的に軽減できます。",
        "key_terminology": "AWS CloudTrail, IAM Access Analyzer, 最小権限の原則, AWS Lambda, IAMポリシー",
        "overall_assessment": "選択肢Bは、プロセスの自動化を提供するため、他の選択肢と比べてソリューションアーキテクトが最小の労力で要件を満たすことができるため、最良の選択肢です。"
      }
    ],
    "keywords": [
      "AWS CloudTrail",
      "IAM Access Analyzer",
      "Principle of Least Privilege",
      "AWS Lambda",
      "IAM Policies"
    ]
  },
  {
    "No": "130",
    "question": "A solutions architect must analyze a company's Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine\nwhether the company is using resources eficiently. The company is running several large, high-memory EC2 instances to host database clusters\nthat are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and\nthe company has not identified a pattern.\nThe solutions architect must analyze the environment and take action based on the findings.\nWhich solution meets these requirements MOST cost-effectively?",
    "question_jp": "ソリューションアーキテクトは、企業のAmazon EC2インスタンスおよびAmazon Elastic Block Store (Amazon EBS) ボリュームを分析し、リソースが効率的に使用されているかどうかを判断する必要があります。この企業は、アクティブ/パッシブ構成でデプロイされたデータベースクラスタをホストするために、いくつかの大規模で高メモリのEC2インスタンスを運用しています。これらのEC2インスタンスの利用状況は、データベースを利用するアプリケーションによって異なり、企業はパターンを特定していません。ソリューションアーキテクトは、環境を分析し、調査結果に基づいて対策を講じる必要があります。どのソリューションが最もコスト効果が高い要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.",
        "text_jp": "AWS Systems Manager OpsCenterを使用してダッシュボードを作成します。EC2インスタンスおよびそのEBSボリュームに関連付けられたAmazon CloudWatchメトリクスの視覚化を構成します。ダッシュボードを定期的にレビューし、使用パターンを特定します。メトリクスのピークに基づいてEC2インスタンスのサイズを適切に調整します。"
      },
      {
        "key": "B",
        "text": "Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.",
        "text_jp": "EC2インスタンスおよびそのEBSボリュームのためにAmazon CloudWatchの詳細監視をオンにします。メトリクスに基づいてダッシュボードを作成し、レビューします。使用パターンを特定します。メトリクスのピークに基づいてEC2インスタンスのサイズを適切に調整します。"
      },
      {
        "key": "C",
        "text": "Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed.",
        "text_jp": "各EC2インスタンスにAmazon CloudWatchエージェントをインストールします。AWS Compute Optimizerをオンにし、少なくとも12時間実行させます。Compute Optimizerからの推奨事項をレビューし、指示に従ってEC2インスタンスのサイズを適切に調整します。"
      },
      {
        "key": "D",
        "text": "Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed.",
        "text_jp": "AWSエンタープライズサポートプランにサインアップします。AWS Trusted Advisorをオンにします。12時間待ち、Trusted Advisorからの推奨をレビューし、指示に従ってEC2インスタンスのサイズを適切に調整します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. By using AWS Compute Optimizer, the architect receives data-driven recommendations for optimizing EC2 instances based on their actual usage.",
        "situation_analysis": "The company is facing uncertain utilization patterns of EC2 instances. They need a reliable solution that automatically analyzes performance data to determine right sizing.",
        "option_analysis": "Option C is the best solution because it leverages AWS Compute Optimizer to provide recommendations based on actual usage over a significant period. Other options either require manual tracking or do not utilize workload-based analysis effectively.",
        "additional_knowledge": "Choosing Compute Optimizer helps in understanding the load and optimizing EC2 resources accordingly.",
        "key_terminology": "AWS Compute Optimizer, EC2, CloudWatch, right-sizing, utilization metrics.",
        "overall_assessment": "Answer C is not only the most effective but also the most cost-efficient method among the options provided, making it a best practice to utilize service offerings that analyze resource usage."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。AWS Compute Optimizerを使用することにより、アーキテクトは実際の使用に基づくEC2インスタンスの最適化に関するデータ駆動の推奨事項を受け取る。",
        "situation_analysis": "この企業はEC2インスタンスの利用状況の不確実なパターンに直面している。信頼性のあるソリューションが必要であり、パフォーマンスデータを自動的に分析してサイズ適正化を判断する必要がある。",
        "option_analysis": "Cの選択肢が最適な理由は、AWS Compute Optimizerを活用して実際の使用状況に基づいて推奨事項を提供するためである。他の選択肢は手動による追跡が必要であったり、ワークロードベースの分析を効果的に活用していない。",
        "additional_knowledge": "Compute Optimizerを選ぶことで、負荷を理解し、EC2リソースをそれに応じて最適化することができる。",
        "key_terminology": "AWS Compute Optimizer, EC2, CloudWatch, サイズ適正化, 利用状況メトリクス。",
        "overall_assessment": "選択肢Cは、提供されたオプションの中で最も効果的かつコスト効率の良い方法であり、リソース使用状況を分析するサービス提供を利用することがベストプラクティスである。"
      }
    ],
    "keywords": [
      "AWS Compute Optimizer",
      "EC2",
      "CloudWatch",
      "right-sizing",
      "utilization metrics"
    ]
  },
  {
    "No": "131",
    "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses\nAWS Transit Gateway for VPC connectivity across accounts.\nIn an AWS application account, the company's application team has deployed a web application that uses AWS Lambda and Amazon RDS. The\ncompany's database administrators have a separate DBA account and use the account to centrally manage all the databases across the\norganization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is\ndeployed m the application account.\nThe application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is\nmanually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in\nthe application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and\neliminates the need to manually share the secrets.\nWhich solution will meet these requirements?",
    "question_jp": "企業はAWS Organizationsを使用してAWSクラウドでのマルチアカウントのセットアップを行っています。企業はガバナンスのためにAWS Control Towerを使用し、AWS Transit Gatewayを利用してアカウント間のVPC接続を行っています。AWSアプリケーションアカウントにおいて、企業のアプリケーションチームはAWS LambdaとAmazon RDSを使用するウェブアプリケーションをデプロイしました。企業のデータベース管理者は別のDBAアカウントを持ち、そのアカウントを使用して組織内のすべてのデータベースを集中管理しています。データベース管理者は、DBAアカウントにデプロイされたAmazon EC2インスタンスを使用して、アプリケーションアカウントにデプロイされたRDSデータベースにアクセスしています。アプリケーションチームは、アプリケーションアカウントのAWS Secrets Managerにデータベースの資格情報をシークレットとして保存しています。アプリケーションチームは手動でシークレットをデータベース管理者と共有しています。シークレットはアプリケーションアカウントのSecrets Manager向けのAWSマネージドキーによって暗号化されています。ソリューションアーキテクトは、データベース管理者にデータベースへのアクセスを提供し、シークレットを手動で共有する必要を排除するソリューションを実装する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA- Admin role to the EC2 instance for access to the cross-account secrets.",
        "text_jp": "AWS Resource Access Manager (AWS RAM)を使用して、アプリケーションアカウントからDBAアカウントにシークレットを共有します。DBAアカウントにDBA-Adminという名前のIAMロールを作成し、そのロールに共有シークレットにアクセスするために必要な権限を付与します。DBA-AdminロールをEC2インスタンスにアタッチして、クロスアカウントシークレットにアクセスします。"
      },
      {
        "key": "B",
        "text": "In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets",
        "text_jp": "アプリケーションアカウントにDBA-Secretという名前のIAMロールを作成し、そのロールにシークレットにアクセスするために必要な権限を付与します。DBAアカウントにDBA-Adminという名前のIAMロールを作成し、DBA-AdminロールにアプリケーションアカウントのDBA-Secretロールを引き受けるための必要な権限を付与します。DBA-AdminロールをEC2インスタンスにアタッチして、クロスアカウントシークレットにアクセスします。"
      },
      {
        "key": "C",
        "text": "In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
        "text_jp": "DBAアカウントにDBA-Adminという名前のIAMロールを作成し、そのロールにシークレットとアプリケーションアカウントのデフォルトのAWSマネージドキーにアクセスするために必要な権限を付与します。アプリケーションアカウント内で、そのキーへのアクセスをDBAアカウントから許可するためにリソースベースのポリシーをアタッチします。DBA-AdminロールをEC2インスタンスにアタッチし、クロスアカウントシークレットにアクセスします。"
      },
      {
        "key": "D",
        "text": "In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
        "text_jp": "DBAアカウントにDBA-Adminという名前のIAMロールを作成し、そのロールにアプリケーションアカウントのシークレットにアクセスするために必要な権限を付与します。アプリケーションアカウントにSCPをアタッチして、DBAアカウントからのシークレットへのアクセスを許可します。DBA-AdminロールをEC2インスタンスにアタッチし、クロスアカウントシークレットにアクセスします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (78%) 8% 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account.",
        "situation_analysis": "The company requires a secure way to grant database administrators access to database credentials stored in AWS Secrets Manager without manual sharing.",
        "option_analysis": "Option A implements AWS RAM, allowing secure and controlled sharing of resources, which is ideal for this scenario. Options B and D introduce complexities without directly sharing the secrets or may require overly complicated cross-account roles. Option C, while valid, does not utilize AWS RAM which simplifies permission management.",
        "additional_knowledge": "Understanding AWS IAM roles and policies is crucial in configuring access to AWS resources appropriately.",
        "key_terminology": "AWS Resource Access Manager, AWS Secrets Manager, IAM roles, cross-account access",
        "overall_assessment": "Option A is optimal as it directly addresses the need for sharing the secrets in a secure manner, while the other options may involve unnecessary complexities or do not directly meet the requirement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAである：AWS Resource Access Manager (AWS RAM)を使用して、アプリケーションアカウントからDBAアカウントにシークレットを共有する。",
        "situation_analysis": "企業は、データベース管理者がAWS Secrets Managerに保存されたデータベースの資格情報に手動で共有せずにアクセスできる安全な方法を必要としている。",
        "option_analysis": "選択肢AはAWS RAMを実装し、リソースの安全で管理された共有を可能にするため、このシナリオに適している。選択肢BおよびDは、シークレットを直接共有せず、過剰な複雑さをもたらす可能性がある。選択肢Cは正当ではあるが、AWS RAMを利用しないため、権限管理が簡素化されない。",
        "additional_knowledge": "AWSのIAMロールとポリシーを理解することは、AWSリソースへのアクセスを適切に構成する上で重要である。",
        "key_terminology": "AWS Resource Access Manager, AWS Secrets Manager, IAMロール, クロスアカウントアクセス",
        "overall_assessment": "選択肢Aはシークレットを安全に共有する必要性に直接対応しており、他の選択肢は不必要な複雑さを含むか、要件を直接満たしていないため最適である。"
      }
    ],
    "keywords": [
      "AWS Resource Access Manager",
      "AWS Secrets Manager",
      "IAM roles",
      "cross-account access"
    ]
  },
  {
    "No": "132",
    "question": "A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs: Research and DataOps.\nBecause of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region.\nAdditionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types.\nA solutions architect must implement a solution that applies these restrictions. The solution must maximize operational eficiency and must\nminimize ongoing maintenance.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "question_jp": "企業はAWS Organizationsを使用して複数のAWSアカウントを管理しています。ルートOUの下には、研究OUとDataOps OUの2つのOUがあります。規制要件により、企業が組織内で展開するすべてのリソースはap-northeast-1リージョンに存在しなければなりません。また、企業がDataOps OUに展開するEC2インスタンスは、あらかじめ定義されたインスタンスタイプのリストを使用する必要があります。ソリューションアーキテクトは、これらの制限を適用するソリューションを実装しなければなりません。このソリューションは運用効率を最大化し、継続的なメンテナンスを最小限に抑える必要があります。これらの要件を満たす手順の組み合わせはどれですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict access to specific instance type.",
        "text_jp": "DataOps OUの1つのアカウントにIAMロールを作成します。インラインポリシーのec2:InstanceType条件キーを使用して、特定のインスタンスタイプへのアクセスを制限します。"
      },
      {
        "key": "B",
        "text": "Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1.",
        "text_jp": "ルートOUのすべてのアカウントにIAMユーザーを作成します。インラインポリシーのaws:RequestedRegion条件キーを使用して、ap-northeast-1以外のすべてのAWSリージョンへのアクセスを制限します。"
      },
      {
        "key": "C",
        "text": "Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.",
        "text_jp": "SCPを作成します。aws:RequestedRegion条件キーを使用して、ap-northeast-1以外のすべてのAWSリージョンへのアクセスを制限します。このSCPをルートOUに適用します。"
      },
      {
        "key": "D",
        "text": "Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU, the DataOps OU, and the Research OU.",
        "text_jp": "SCPを作成します。ec2:Region条件キーを使用して、ap-northeast-1以外のすべてのAWSリージョンへのアクセスを制限します。このSCPをルートOU、DataOps OU、Research OUに適用します。"
      },
      {
        "key": "E",
        "text": "Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.",
        "text_jp": "SCPを作成します。ec2:InstanceType条件キーを使用して、特定のインスタンスタイプへのアクセスを制限します。このSCPをDataOps OUに適用します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "CE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are C and E. Both choices help to ensure compliance with the regulatory requirements by restricting which regions and instance types can be accessed.",
        "situation_analysis": "The company's regulatory constraints require all resources to be deployed in the ap-northeast-1 Region and dictate specific instance types for EC2 instances in the DataOps OU.",
        "option_analysis": "Option C correctly restricts resources at the OU level for all accounts under the organization using a Service Control Policy (SCP). This is a common AWS best practice. Option E also effectively restricts the instance types used in the DataOps OU.",
        "additional_knowledge": "Understanding the structure of AWS Organizations and how SCPs function is crucial for effectively managing permissions in multi-account setups.",
        "key_terminology": "Service Control Policy (SCP), AWS Organizations, regulatory requirements, EC2 Instance Types, IAM Role",
        "overall_assessment": "Community votes also supported choices C and E, suggesting these are widely accepted answers."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCとEです。両方の選択肢は、遵守すべき規制要件を確保するために、アクセス可能なリージョンとインスタンスタイプを制限します。",
        "situation_analysis": "企業の規制上の制約により、すべてのリソースはap-northeast-1リージョンに展開されなければならず、DataOps OUのEC2インスタンスには特定のインスタンスタイプが指定される必要があります。",
        "option_analysis": "選択肢Cは、組織内のすべてのアカウントに対してOUレベルでリソースを適切に制限し、サービス制御ポリシー（SCP）を使用しているため、これは一般的なAWSのベストプラクティスです。選択肢EもDataOps OUで使用されるインスタンスタイプを効果的に制限しています。",
        "additional_knowledge": "AWS Organizationsの構造とSCPの機能を理解することは、マルチアカウント設定での権限管理において重要です。",
        "key_terminology": "サービス制御ポリシー（SCP）、AWS Organizations、規制要件、EC2インスタンスタイプ、IAMロール",
        "overall_assessment": "コミュニティの投票もCとEを支持しており、これらが広く受け入れられた回答であることを示唆しています。"
      }
    ],
    "keywords": [
      "Service Control Policy",
      "AWS Organizations",
      "EC2 Instance Types",
      "IAM Role",
      "regulatory requirements"
    ]
  },
  {
    "No": "133",
    "question": "A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites.\nThe company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon\nSQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an\nAmazon S3 bucket.\nThe company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the\nexisting Region. Results must be written to the existing S3 bucket in the current Region.\nWhich combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)",
    "question_jp": "ある企業が、単一のAWSリージョンでサーバーレスアプリケーションを運用しています。このアプリケーションは外部URLにアクセスし、そのサイトからメタデータを抽出します。企業は、URLをAmazon Simple Queue Service (Amazon SQS) キューに公開するために、Amazon Simple Notification Service (Amazon SNS) トピックを使用しています。AWS Lambda関数はそのキューをイベントソースとして使用し、キューからURLを処理します。結果はAmazon S3バケットに保存されます。企業は、各URLを他のリージョンで処理し、サイトのローカリゼーションの可能な違いを比較したいと考えています。URLは既存のリージョンから公開する必要があります。結果は現在のリージョンの既存のS3バケットに書き込まれる必要があります。この要件を満たすマルチリージョンデプロイメントを実現するために必要な変更の組み合わせはどれですか？（二つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the SQS queue with the Lambda function to other Regions.",
        "text_jp": "SQSキューとLambda関数を他のリージョンに展開する。"
      },
      {
        "key": "B",
        "text": "Subscribe the SNS topic in each Region to the SQS queue.",
        "text_jp": "各リージョンでSNSトピックをSQSキューにサブスクライブする。"
      },
      {
        "key": "C",
        "text": "Subscribe the SQS queue in each Region to the SNS topic.",
        "text_jp": "各リージョンでSQSキューをSNSトピックにサブスクライブする。"
      },
      {
        "key": "D",
        "text": "Configure the SQS queue to publish URLs to SNS topics in each Region.",
        "text_jp": "SQSキューを設定して、各リージョンのSNSトピックにURLを公開する。"
      },
      {
        "key": "E",
        "text": "Deploy the SNS topic and the Lambda function to other Regions.",
        "text_jp": "SNSトピックとLambda関数を他のリージョンに展開する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct choices to enable multi-Region processing while adhering to the constraints of using the existing Region for URL publication and writing results to the existing S3 bucket are A and D.",
        "situation_analysis": "The application architecture requires that it can process URLs in multiple Regions while maintaining a single source for publications and results storage. The approach must ensure that the original Region remains the centralized location for SNS topic publication.",
        "option_analysis": "Choice A suggests deploying the SQS queue and the Lambda function to other Regions which is critical for processing the URLs outside of the original Region. Choice D discusses configuring the SQS queue to publish URLs to SNS topics in each Region, which allows for URL dissemination across them, thus satisfying the application requirement to access multiple Regions.",
        "additional_knowledge": "Lambda functions can be configured with different triggers and event sources, which is imperative in designing a global application setup.",
        "key_terminology": "AWS Lambda, Amazon SQS, Amazon SNS, Amazon S3, Multi-Region Architecture",
        "overall_assessment": "This problem's community vote shows a significant preference for A and C. However, A is essential for deploying Lambda and SQS in multiple Regions, confirming the needed infrastructure. The community's vote on C seems to indicate a misunderstanding, as this doesn't align with the original key requirements: results must be delivered back to the original Region."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "マルチリージョンでの処理を有効にし、URLの公開を既存のリージョンで行い、結果を既存のS3バケットに書き込むという制約に従った場合の正解はAとDである。",
        "situation_analysis": "アプリケーションアーキテクチャは、URLを複数のリージョンで処理できる必要があり、公開および結果の保存の単一のソースを維持する必要がある。このアプローチは、元のリージョンがSNSトピックの公開先として中央の位置に留まることを確実にしなければならない。",
        "option_analysis": "選択肢Aは、SQSキューとLambda関数を他のリージョンに展開することを示唆しており、これは元のリージョンの外でURLを処理するために重要である。選択肢Dは、SQSキューを設定して各リージョンのSNSトピックにURLを公開することを述べており、これにより複数のリージョンでのURL配信が可能になり、アプリケーションの要件を満たす。",
        "additional_knowledge": "Lambda関数は異なるトリガーおよびイベントソースで構成することができ、これはグローバルアプリケーションセットアップを設計する際に重要である。",
        "key_terminology": "AWS Lambda、Amazon SQS、Amazon SNS、Amazon S3、マルチリージョンアーキテクチャ",
        "overall_assessment": "この問題のコミュニティ投票結果は、AとCへの明確な支持を示している。ただし、Aはマルチリージョンでの処理を行うためにLambdaとSQSを展開する必要があるため、不可欠である。コミュニティのCに対する投票は誤解を示しているようで、元の要件には合致していない。結果は元のリージョンに返されるべきである。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon SQS",
      "Amazon SNS",
      "Amazon S3",
      "Multi-Region Architecture"
    ]
  },
  {
    "No": "134",
    "question": "A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code\ncannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4\nhours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.\nWhich strategy should the solutions architect use?",
    "question_jp": "ある企業が、Amazon EC2 Linuxインスタンス上で独自のステートレスETLアプリケーションを実行しています。このアプリケーションはLinuxバイナリであり、ソースコードの修正ができません。アプリケーションはシングルスレッドで、2 GBのRAMを使用し、CPU集約型です。アプリケーションは4時間ごとにスケジュールされ、最大20分間実行されます。ソリューションアーキテクトは、ソリューションのアーキテクチャを見直したいと考えています。どの戦略をソリューションアーキテクトは使用すべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.",
        "text_jp": "AWS Lambdaを使用してアプリケーションを実行します。Amazon CloudWatch Logsを使用して、4時間ごとにLambda関数を呼び出します。"
      },
      {
        "key": "B",
        "text": "Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.",
        "text_jp": "AWS Batchを使用してアプリケーションを実行します。AWS Step Functionsのステートマシンを使用して、4時間ごとにAWS Batchジョブを呼び出します。"
      },
      {
        "key": "C",
        "text": "Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.",
        "text_jp": "AWS Fargateを使用してアプリケーションを実行します。Amazon EventBridge（Amazon CloudWatch Events）を使用して、4時間ごとにFargateタスクを呼び出します。"
      },
      {
        "key": "D",
        "text": "Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours.",
        "text_jp": "Amazon EC2スポットインスタンスを使用してアプリケーションを実行します。AWS CodeDeployを使用して、4時間ごとにアプリケーションをデプロイおよび実行します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Use AWS Fargate to run the application. This is because Fargate allows you to run containers without managing the underlying infrastructure, making it suitable for a stateless application like the one described.",
        "situation_analysis": "The application runs every 4 hours for 20 minutes and is highly CPU-centric, making it crucial to choose a service that can efficiently manage compute resources while accommodating the application constraints.",
        "option_analysis": "Option A is not suitable because AWS Lambda has a maximum execution time limit, which cannot accommodate the 20 minutes runtime. Option B, AWS Batch, is viable but may introduce more complexity for running a stateless application. Option D could work but is not cost-effective for a short runtime and may require management of EC2 instances.",
        "additional_knowledge": "Using AWS Fargate also allows for easier scaling and management of the application without worrying about infrastructure concerns.",
        "key_terminology": "AWS Fargate, stateless application, container management, CPU resources",
        "overall_assessment": "Overall, C is the best option, aligning with AWS best practices for running stateless applications with manageable resources."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはC: AWS Fargateを使用してアプリケーションを実行することです。Fargateを使用すると基盤となるインフラストラクチャを管理することなくコンテナを実行できるため、説明されたようなステートレスアプリケーションに適しています。",
        "situation_analysis": "アプリケーションは4時間ごとに20分間実行され、高いCPU集約型であるため、アプリケーションの制約に適した計算リソースを効率的に管理できるサービスを選択することが重要です。",
        "option_analysis": "選択肢Aは、AWS Lambdaには最大実行時間制限があり、20分のランタイムを許容できないため適していません。選択肢BのAWS Batchは実行できますが、ステートレスアプリケーションを実行するための複雑さを増す可能性があります。選択肢Dは機能すると考えられますが、短時間の実行にはコスト効果が悪く、EC2インスタンスの管理が必要になる可能性があります。",
        "additional_knowledge": "AWS Fargateを使用することで、インフラストラクチャの懸念を気にせず、アプリケーションのスケールや管理を容易に行えるようになります。",
        "key_terminology": "AWS Fargate、ステートレスアプリケーション、コンテナ管理、CPUリソース",
        "overall_assessment": "全体的に見て、CはAWSのベストプラクティスに沿ったステートレスアプリケーションを管理可能なリソースで実行するための最良の選択肢です。"
      }
    ],
    "keywords": [
      "AWS Fargate",
      "stateless application",
      "container management",
      "CPU resources"
    ]
  },
  {
    "No": "135",
    "question": "A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week\nafter launch. Currently, the game consists of the following components deployed in a single AWS Region:\n• Amazon S3 bucket that stores game assets\n• Amazon DynamoDB table that stores player scores\nA solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "企業は人気のあるオンラインゲームの続編を制作しています。世界中の多くのユーザーが、リリース後の最初の週にゲームをプレイします。現在、ゲームは以下のコンポーネントで構成されており、単一のAWSリージョンにデプロイされています：\n• ゲームアセットを格納するAmazon S3バケット\n• プレイヤーのスコアを格納するAmazon DynamoDBテーブル\nソリューションアーキテクトは、レイテンシを低減し、信頼性を向上させるために、実装に最も少ない労力を必要とするマルチリージョンソリューションを設計する必要があります。\nソリューションアーキテクトは、これらの要件を満たすために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、S3バケットからアセットを提供します。S3クロスリージョンレプリケーションを設定します。新しいリージョンに新しいDynamoDBテーブルを作成します。この新しいテーブルをDynamoDBグローバルテーブルのレプリカターゲットとして使用します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC).",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、S3バケットからアセットを提供します。S3同一リージョンレプリケーションを設定します。新しいリージョンに新しいDynamoDBテーブルを作成します。AWS Database Migration Service (AWS DMS)を使用して、DynamoDBテーブル間に非同期レプリケーションを設定します。"
      },
      {
        "key": "C",
        "text": "Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region.",
        "text_jp": "新しいリージョンに別のS3バケットを作成し、バケット間でS3クロスリージョンレプリケーションを設定します。Amazon CloudFrontディストリビューションを作成し、各リージョンのS3バケットにアクセスする2つのオリジンを持つオリジンフェイルオーバーを構成します。Amazon DynamoDBストリームを有効にして、DynamoDBグローバルテーブルを設定し、新しいリージョンにレプリカテーブルを追加します。"
      },
      {
        "key": "D",
        "text": "Create another S3 bucket in the sine Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.",
        "text_jp": "同じリージョンに別のS3バケットを作成し、バケット間でS3同一リージョンレプリケーションを設定します。Amazon CloudFrontディストリビューションを作成し、S3バケットにアクセスする2つのオリジンを持つオリジンフェイルオーバーを構成します。新しいリージョンに新しいDynamoDBテーブルを作成します。この新しいテーブルをDynamoDBグローバルテーブルのレプリカターゲットとして使用します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (88%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This option effectively incorporates multi-region architecture, S3 Cross-Region Replication, and DynamoDB global tables for reduced latency and improved reliability.",
        "situation_analysis": "Given the need for a multi-region solution that minimizes latency and maintains high reliability during a worldwide game launch, the focus should be on strategies that leverage AWS global infrastructure.",
        "option_analysis": "Option C correctly suggests creating both Buckets and configuring replication and CloudFront, utilizing global tables in DynamoDB. Other options either restrict to single region replication or do not optimally leverage CloudFront's capability.",
        "additional_knowledge": "None",
        "key_terminology": "Amazon S3, Amazon DynamoDB, CloudFront, Cross-Region Replication, Global Tables, AWS Database Migration Service",
        "overall_assessment": "This question is well-designed to assess understanding of AWS services that optimize performance and availability in a multi-region architecture. The community vote largely supports the correct answer."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。このオプションは、マルチリージョンアーキテクチャ、S3クロスリージョンレプリケーション、DynamoDBグローバルテーブルを効果的に組み込み、レイテンシを低下させ、信頼性を向上させる。",
        "situation_analysis": "世界中のゲームのリリース中に、レイテンシを最小限に抑え、高い信頼性を維持するためのマルチリージョンソリューションが必要であるため、AWSのグローバルインフラストラクチャを活用する戦略に焦点を当てる必要がある。",
        "option_analysis": "オプションCは、バケットの作成やレプリケーションの設定、CloudFrontの利用、DynamoDBのグローバルテーブルの利用を正しく提案している。他のオプションは、単一リージョンのレプリケーションに制限されているか、CloudFrontの能力を最適に利用できていない。",
        "additional_knowledge": "なし",
        "key_terminology": "Amazon S3, Amazon DynamoDB, CloudFront, クロスリージョンレプリケーション, グローバルテーブル, AWS Database Migration Service",
        "overall_assessment": "この質問は、マルチリージョンアーキテクチャにおけるパフォーマンスと可用性を最適化するAWSサービスの理解を評価するために非常によく設計されている。コミュニティ投票も大部分が正しい答えを支持している。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Amazon DynamoDB",
      "CloudFront",
      "Cross-Region Replication",
      "Global Tables"
    ]
  },
  {
    "No": "136",
    "question": "A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java\nbackend and a NoSQL MongoDB database to store subscriber data.\nThe company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and\nthe company cannot make changes to the application.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、潜在的な賃貸者や購入者に向けた不動産情報を提供するオンプレミスのウェブサイトアプリケーションを持っています。このウェブサイトは、サブスクライバーデータを保存するためにJavaバックエンドとNoSQLのMongoDBデータベースを使用しています。企業は、アプリケーションをAWSに移行する必要がありますが、構造は同様です。アプリケーションは高可用性で展開される必要があり、企業はアプリケーションに変更を加えることはできません。どのソリューションがこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
        "text_jp": "サブスクライバーデータのデータベースとしてAmazon Aurora DBクラスターを使用します。Javaバックエンドアプリケーションのために、複数のアベイラビリティゾーンにわたってAuto ScalingグループにAmazon EC2インスタンスを展開します。"
      },
      {
        "key": "B",
        "text": "Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application.",
        "text_jp": "サブスクライバーデータのデータベースとしてAmazon EC2インスタンス上のMongoDBを使用します。Javaバックエンドアプリケーションのために、単一のアベイラビリティゾーンにAuto ScalingグループでEC2インスタンスを展開します。"
      },
      {
        "key": "C",
        "text": "Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
        "text_jp": "サブスクライバーデータのデータベースとして適切なサイズのAmazon DocumentDB（MongoDB互換）を複数のアベイラビリティゾーンで構成します。JavaバックエンドアプリケーションのためにAuto ScalingグループでAmazon EC2インスタンスを複数のアベイラビリティゾーンに展開します。"
      },
      {
        "key": "D",
        "text": "Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
        "text_jp": "サブスクライバーデータのデータベースとして複数のアベイラビリティゾーンでオンデマンドキャパシティモードのAmazon DocumentDB（MongoDB互換）を構成します。JavaバックエンドアプリケーションのためにAuto ScalingグループでAmazon EC2インスタンスを複数のアベイラビリティゾーンに展開します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (86%) 14%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
        "situation_analysis": "The application must be migrated to AWS without any changes and should maintain high availability. The database must support MongoDB compatibility, and the architecture needs to span multiple Availability Zones.",
        "option_analysis": "Option A uses Amazon Aurora, which is not MongoDB compatible. Option B also does not span multiple Availability Zones and uses MongoDB on EC2, which does not inherently provide high availability. Option C, while using DocumentDB, does not specify on-demand capacity mode, which is essential for scalability without needing to provision instances manually. Option D meets all criteria by using DocumentDB with MongoDB compatibility and specifying the on-demand capacity mode.",
        "additional_knowledge": "Understanding the differences between managed database services like Amazon DocumentDB versus self-managed databases on EC2 instances is crucial for architects.",
        "key_terminology": "Amazon DocumentDB, MongoDB compatibility, auto scaling, availability zone, high availability",
        "overall_assessment": "This question effectively tests the understanding of AWS database services compatible with MongoDB and the importance of high availability through proper architecture design."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです：サブスクライバーデータのデータベースとして、複数のアベイラビリティゾーンでオンデマンドキャパシティモードのAmazon DocumentDB（MongoDB互換）を構成します。また、JavaバックエンドアプリケーションのためにAuto ScalingグループでAmazon EC2インスタンスを複数のアベイラビリティゾーンに展開します。",
        "situation_analysis": "アプリケーションは変更せずにAWSに移行する必要があり、高可用性を維持する必要があります。データベースはMongoDB互換をサポートし、アーキテクチャは複数のアベイラビリティゾーンにまたがる必要があります。",
        "option_analysis": "選択肢AはAmazon Auroraを使用していますが、MongoDB互換ではありません。選択肢Bも複数のアベイラビリティゾーンにまたがらず、MongoDBをEC2で使用していますが、高可用性を保証するものではありません。選択肢CはDocumentDBを使用していますが、オンデマンドキャパシティモードを指定しておらず、手動でインスタンスをプロビジョニングすることなくスケーラビリティを維持するためには不可欠です。選択肢Dは、MongoDB互換のDocumentDBを使用し、オンデマンドキャパシティモードを指定しているため、すべての基準を満たします。",
        "additional_knowledge": "Amazon DocumentDBのようなマネージドデータベースサービスとEC2インスタンス上の自己管理データベースの違いを理解することは、アーキテクトにとって重要です。",
        "key_terminology": "Amazon DocumentDB、MongoDB互換、自動スケーリング、アベイラビリティゾーン、高可用性",
        "overall_assessment": "この問題は、MongoDB互換のAWSデータベースサービス理解と、適切なアーキテクチャ設計による高可用性の重要性をテストする効果的なものです。"
      }
    ],
    "keywords": [
      "Amazon DocumentDB",
      "MongoDB compatibility",
      "auto scaling",
      "availability zone",
      "high availability"
    ]
  },
  {
    "No": "137",
    "question": "A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS\naccount to securely store images and media files that are used as content for the company's marketing campaigns. The creative team wants to\nshare the S3 bucket with the strategy team so that the strategy team can view the objects.\nA solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a\ncustom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when\nusers from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.\nThe solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only\nthe minimum permissions that they need.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "デジタルマーケティング会社は、さまざまなチームに属する複数のAWSアカウントを持っています。クリエイティブチームは、そのAWSアカウントでAmazon S3バケットを使用して、会社のマーケティングキャンペーンのコンテンツとして使用される画像やメディアファイルを安全に保存しています。クリエイティブチームは、戦略チームとS3バケットを共有し、戦略チームがオブジェクトを表示できるようにしたいと考えています。ソリューションアーキテクトは、戦略アカウントに「strategy_reviewer」というIAMロールを作成しました。ソリューションアーキテクトは、クリエイティブアカウントでカスタムAWSキー管理サービス（AWS KMS）キーを設定し、そのキーをS3バケットに関連付けました。しかし、戦略アカウントのユーザーがIAMロールを引き受け、S3バケット内のオブジェクトにアクセスしようとすると、「アクセス拒否」エラーが表示されます。ソリューションアーキテクトは、戦略アカウントのユーザーがS3バケットにアクセスできるようにする必要があります。解決策は、これらのユーザーに必要最低限の権限のみを提供しなければなりません。これらの要件を満たすために、ソリューションアーキテクトはどの組み合わせのステップを実行すべきでしょうか。（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.",
        "text_jp": "S3バケットの読み取り権限を含むバケットポリシーを作成します。バケットポリシーのプリンシパルを戦略アカウントのアカウントIDに設定します。"
      },
      {
        "key": "B",
        "text": "Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.",
        "text_jp": "strategy_reviewer IAMロールを更新して、S3バケットへのフル権限とカスタムKMSキーへの復号権限を付与します。"
      },
      {
        "key": "C",
        "text": "Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.",
        "text_jp": "クリエイティブアカウント内のカスタムKMSキーのポリシーを更新し、strategy_reviewer IAMロールに復号権限を付与します。"
      },
      {
        "key": "D",
        "text": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user.",
        "text_jp": "S3バケットの読み取り権限を含むバケットポリシーを作成します。バケットポリシーのプリンシパルを匿名ユーザーに設定します。"
      },
      {
        "key": "E",
        "text": "Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role.",
        "text_jp": "クリエイティブアカウント内のカスタムKMSキーのポリシーを更新し、strategy_reviewer IAMロールに暗号化権限を付与します。"
      },
      {
        "key": "F",
        "text": "Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.",
        "text_jp": "strategy_reviewer IAMロールを更新して、S3バケットへの読み取り権限とカスタムKMSキーへの復号権限を付与します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ACF (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves granting full permissions for the S3 bucket and decrypt permissions for the custom KMS key to the IAM role used by the Strategy account.",
        "situation_analysis": "The Strategy account needs access to an S3 bucket in the Creative account while ensuring security and minimal permissions. The users from the Strategy account must access the objects without facing access denied errors.",
        "option_analysis": "Option A does not grant sufficient permissions; option C is necessary but does not provide complete access; option D is insecure; option E is unnecessary; option F provides the right permissions but does not address the bucket access directly.",
        "additional_knowledge": "Understanding of both resource and KMS policies is crucial for enabling cross-account access securely.",
        "key_terminology": "IAM Role, S3 Bucket Policy, AWS KMS, Access Denied, Cross-Account Access",
        "overall_assessment": "This question effectively tests knowledge of cross-account access management in AWS and the relationship between IAM roles, S3, and KMS. The community vote distribution supports this conclusion as option B was recognized as the best choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、これは戦略アカウントで使用されるIAMロールにS3バケットへのフル権限とカスタムKMSキーへの復号権限を付与することに関係しています。",
        "situation_analysis": "戦略アカウントは、クリエイティブアカウントのS3バケットにアクセスする必要がありつつ、セキュリティと最小限の権限を確保しなければなりません。戦略アカウントのユーザーは、アクセス拒否のエラーに直面することなくオブジェクトにアクセスする必要があります。",
        "option_analysis": "選択肢Aは十分な権限を付与しない; 選択肢Cは必要だが完全なアクセスを提供しない; 選択肢Dは安全ではない; 選択肢Eは不要; 選択肢Fは適切な権限を提供するが、バケットアクセスには直接関係しない。",
        "additional_knowledge": "リソースポリシーとKMSポリシーの両方の理解が必要であり、アカウント間アクセスを安全に有効にするには重要です。",
        "key_terminology": "IAMロール, S3バケットポリシー, AWS KMS, アクセス拒否, アカウント間アクセス",
        "overall_assessment": "この質問は、AWSでのアカウント間アクセス管理の知識とIAMロール、S3、KMSの関係を効果的にテストしています。コミュニティの投票分布も、その結論を支持しており、選択肢Bが最良の選択肢として認識されています。"
      }
    ],
    "keywords": [
      "IAM Role",
      "S3 Bucket Policy",
      "AWS KMS",
      "Access Denied",
      "Cross-Account Access"
    ]
  },
  {
    "No": "138",
    "question": "A life sciences company is using a combination of open source tools to manage data analysis workfiows and Docker containers running on\nservers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN),\nand then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their\ngenomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days.\nThe company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual\njobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is\nexpecting 10-15 job requests each day.\nWhich solution meets these requirements?",
    "question_jp": "あるライフサイエンス企業が、オープンソースツールとオンプレミスデータセンターのサーバーで稼働するDockerコンテナの組み合わせを使用して、データ分析ワークフローを管理しています。シーケンスデータは、ローカルストレージエリアネットワーク（SAN）に生成されて保存され、その後データが処理されます。研究開発チームはキャパシティに関する問題に直面しており、ワークロードの要求に応じてスケールし、数週間から数日間にターンアラウンドタイムを短縮するために、AWS上にゲノミクス分析プラットフォームを再設計することを決定しました。企業には高速なAWS Direct Connect接続があります。シーケンサーは、各ゲノムに対して約200GBのデータを生成し、個別のジョブは理想的な計算能力で数時間のデータ処理を要します。最終結果はAmazon S3に保存されます。企業は毎日10〜15件のジョブリクエストを期待しています。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.",
        "text_jp": "定期的にスケジュールされたAWS Snowball Edgeデバイスを使用してシーケンシングデータをAWSに転送します。AWSがSnowball Edgeデバイスを受け取った後、データがAmazon S3にロードされると、S3イベントを使用してデータを処理するAWS Lambda関数をトリガーします。"
      },
      {
        "key": "B",
        "text": "Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.",
        "text_jp": "AWS Data Pipelineを使用してシーケンシングデータをAmazon S3に転送します。S3イベントを使用して、カスタムAMI EC2インスタンスを起動するAmazon EC2 Auto Scalingグループをトリガーし、その上でDockerコンテナを実行してデータを処理します。"
      },
      {
        "key": "C",
        "text": "Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workfiow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data.",
        "text_jp": "AWS DataSyncを使用してシーケンシングデータをAmazon S3に転送します。S3イベントを使用してAWS Lambda関数をトリガーし、AWS Step Functionsワークフローを開始します。DockerイメージはAmazon Elastic Container Registry（Amazon ECR）に保存し、AWS Batchをトリガーしてコンテナを実行し、シーケンシングデータを処理します。"
      },
      {
        "key": "D",
        "text": "Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data.",
        "text_jp": "AWS Storage Gatewayファイルゲートウェイを使用してシーケンシングデータをAmazon S3に転送します。S3イベントを使用してAWS Batchジョブをトリガーし、Amazon EC2インスタンス上でDockerコンテナを実行してデータを処理します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (70%) D (30%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This option makes use of AWS DataSync to efficiently transfer data to Amazon S3, utilizing S3 events to trigger AWS Lambda and AWS Step Functions to handle data processing and orchestration effectively.",
        "situation_analysis": "The company needs to process large volumes of genomics data quickly and efficiently. They have high-speed connectivity to AWS and expect a certain volume of daily job requests.",
        "option_analysis": "Option C provides a scalable and efficient method for data transfer and processing using AWS services that integrate well together. Other options do not provide the same level of orchestration and scalability.",
        "additional_knowledge": "Option D might seem viable but lacks the orchestration benefits provided by the combination of Lambda and Step Functions in option C.",
        "key_terminology": "AWS DataSync, Amazon S3, AWS Lambda, AWS Step Functions, Amazon ECR, AWS Batch",
        "overall_assessment": "The majority community supports option C due to its efficient handling of data processing and orchestration combined with AWS's scalability features. Less than optimal choices like D may not utilize the full range of features for handling batches effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。このオプションは、AWS DataSyncを利用してデータをAmazon S3に効率的に転送し、S3イベントを使用してAWS LambdaおよびAWS Step Functionsをトリガーし、データ処理とオーケストレーションを効果的に管理する。",
        "situation_analysis": "企業は、大量のゲノミクスデータを迅速かつ効率的に処理する必要がある。彼らはAWSへの高速接続を持ち、毎日のジョブリクエストの一定量を期待している。",
        "option_analysis": "オプションCは、AWSサービスがうまく統合されており、データ転送と処理のためのスケーラブルで効率的な方法を提供する。他のオプションは同じレベルのオーケストレーションとスケーラビリティを提供しない。",
        "additional_knowledge": "オプションDは有望に見えるかもしれないが、オプションCで提供されるLambdaとStep Functionsの組み合わせによるオーケストレーションの利点に欠けている。",
        "key_terminology": "AWS DataSync, Amazon S3, AWS Lambda, AWS Step Functions, Amazon ECR, AWS Batch",
        "overall_assessment": "コミュニティの大多数は、データ処理とオーケストレーションの効率的な処理とAWSのスケーラビリティ機能を考慮して、オプションCを支持している。オプションDのような最適でない選択肢は、バッチ処理を効果的に処理するための機能を十分に利用していない可能性がある。"
      }
    ],
    "keywords": [
      "AWS DataSync",
      "Amazon S3",
      "AWS Lambda",
      "AWS Step Functions",
      "Amazon ECR",
      "AWS Batch"
    ]
  },
  {
    "No": "139",
    "question": "A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application\nreads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device.\nThe company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2\ninstances across multiple Availability Zones.\nA solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also\nmust implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running\ninstances at any given point in time.\nWhich solution will meet these requirements with the LEAST management overhead?",
    "question_jp": "企業は、開発環境で単一のWindows Amazon EC2インスタンス上でコンテンツ管理アプリケーションを実行しています。このアプリケーションは、ルートデバイスとしてインスタンスに接続された2TBのAmazon Elastic Block Store（Amazon EBS）ボリュームに静的コンテンツを読み書きします。企業は、このアプリケーションを複数のアベイラビリティゾーンにまたがる少なくとも3つのEC2インスタンスで実行する高度に可用性があり、障害耐性のあるソリューションとして本番環境にデプロイする計画を立てています。ソリューションアーキテクトは、アプリケーションを実行するすべてのインスタンスをActive Directoryドメインに参加させるソリューションを設計しなければなりません。また、ファイルコンテンツへのアクセスを制御するためにWindows ACLを実装する必要があります。アプリケーションは、すべての実行中のインスタンスで常に同じコンテンツを維持しなければなりません。どのソリューションが最も管理オーバーヘッドが少ない要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share.",
        "text_jp": "Amazon Elastic File System (Amazon EFS)ファイル共有を作成します。3つのアベイラビリティゾーンにまたがるAuto Scalingグループを作成し、最小サイズを3インスタンスに維持します。ユーザーデータスクリプトを実装してアプリケーションをインストールし、インスタンスをADドメインに参加させてEFSファイル共有をマウントします。"
      },
      {
        "key": "B",
        "text": "Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system.",
        "text_jp": "現在稼働中のEC2インスタンスから新しいAMIを作成します。Amazon FSx for Lustreファイルシステムを作成します。3つのアベイラビリティゾーンにまたがるAuto Scalingグループを作成し、最小サイズを3インスタンスに維持します。ユーザーデータスクリプトを実装してインスタンスをADドメインに参加させ、FSx for Lustreファイルシステムをマウントします。"
      },
      {
        "key": "C",
        "text": "Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain.",
        "text_jp": "Amazon FSx for Windows File Serverファイルシステムを作成します。3つのアベイラビリティゾーンにまたがるAuto Scalingグループを作成し、最小サイズを3インスタンスに維持します。ユーザーデータスクリプトを実装してアプリケーションをインストールし、FSx for Windows File Serverファイルシステムをマウントします。シームレスなドメイン参加を行ってインスタンスをADドメインに参加させます。"
      },
      {
        "key": "D",
        "text": "Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three Instances. Perform a seamless domain join to join the instance to the AD domain.",
        "text_jp": "現在稼働中のEC2インスタンスから新しいAMIを作成します。Amazon Elastic File System（Amazon EFS）ファイルシステムを作成します。3つのアベイラビリティゾーンにまたがるAuto Scalingグループを作成し、最小サイズを3インスタンスに維持します。シームレスなドメイン参加を行ってインスタンスをADドメインに参加させます。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which proposes using FSx for Lustre. This service is well-suited for high-performance applications and provides shared filesystems suitable for your requirements.",
        "situation_analysis": "The requirements are for high availability, fault-tolerance, and consistency of content across multiple instances. Using FSx for Lustre addresses these by providing a scalable file system that can be shared among EC2 instances.",
        "option_analysis": "Option A uses EFS, which is also a valid choice, but B's FSx for Lustre is more optimized for the use-case described. Options C and D introduce unnecessary complexity and do not guarantee the same level of performance as FSx for Lustre.",
        "additional_knowledge": "It's also worth considering that creating AMIs frequently can lead to additional overhead if not managed properly.",
        "key_terminology": "Amazon FSx, Auto Scaling, Active Directory, EFS, Lustre",
        "overall_assessment": "The question effectively tests knowledge of file storage solutions in AWS and highlights the importance of matching performance requirements to the correct service."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、FSx for Lustreを使用することを提案しています。このサービスは高性能アプリケーションに適しており、要件に適した共有ファイルシステムを提供します。",
        "situation_analysis": "要件は、高可用性、障害耐性、および複数のインスタンス間でのコンテンツの一貫性です。FSx for Lustreを使用することで、EC2インスタンス間で共有できるスケーラブルなファイルシステムを提供し、これらを満たします。",
        "option_analysis": "選択肢AはEFSを使用していますが、これも有効な選択肢ですが、BのFSx for Lustreは説明されたユースケースに対して最適化されています。選択肢CとDは不必要な複雑さを引き起こし、FSx for Lustreほどのパフォーマンスを保証しません。",
        "additional_knowledge": "AMIsを頻繁に作成すると、適切に管理しないと追加のオーバーヘッドにつながることも考慮に値します。",
        "key_terminology": "Amazon FSx、Auto Scaling、Active Directory、EFS、Lustre",
        "overall_assessment": "この質問はAWSにおけるファイルストレージソリューションに関する知識を効果的にテストし、パフォーマンス要件に適したサービスを一致させることの重要性を強調しています。"
      }
    ],
    "keywords": [
      "Amazon FSx",
      "Auto Scaling",
      "Active Directory",
      "EFS",
      "Lustre"
    ]
  },
  {
    "No": "140",
    "question": "A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a\nstandalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email\ntemplate for acknowledgement email messages that populate customer data before the application sends the email message to the customer.\nThe company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ソフトウェア・アズ・ア・サービス（SaaS）ベースの会社が顧客にケース管理ソリューションを提供しています。この会社は、アプリケーションからのメールメッセージを送信するために、独立したシンプルメール転送プロトコル（SMTP）サーバーを使用しています。このアプリケーションは、顧客データを埋め込んだ確認メールメッセージのためのメールテンプレートも保存しています。この会社は、AWS Cloud にこのメッセージング機能を移行する計画を立てており、運用オーバーヘッドを最小限に抑える必要があります。どのソリューションが最もコスト効率よくこれらの要件を満たすことができますか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.",
        "text_jp": "AWS MarketplaceのAMIを使用して、Amazon EC2インスタンス上にSMTPサーバーを設定します。メールテンプレートは、Amazon S3バケットに保存します。AWS Lambda関数を作成して、S3バケットからテンプレートを取得し、アプリケーションから顧客データとテンプレートを統合します。Lambda関数でSDKを使用して、メールメッセージを送信します。"
      },
      {
        "key": "B",
        "text": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.",
        "text_jp": "Amazon Simple Email Service（Amazon SES）を設定して、メールメッセージを送信します。メールテンプレートは、Amazon S3バケットに保存します。AWS Lambda関数を作成して、S3バケットからテンプレートを取得し、アプリケーションから顧客データとテンプレートを統合します。Lambda関数でSDKを使用して、メールメッセージを送信します。"
      },
      {
        "key": "C",
        "text": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameters. Use the AWS Marketplace SMTP server to send the email message.",
        "text_jp": "AWS MarketplaceのAMIを使用して、Amazon EC2インスタンス上にSMTPサーバーを設定します。メールテンプレートは、顧客データのパラメータとともにAmazon Simple Email Service（Amazon SES）に保存します。AWS Lambda関数を作成して、SESテンプレートを呼び出し、カスタマーデータを渡してパラメータを置き換えます。AWS MarketplaceのSMTPサーバーを使用して、メールメッセージを送信します。"
      },
      {
        "key": "D",
        "text": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination.",
        "text_jp": "Amazon Simple Email Service（Amazon SES）を設定して、メールメッセージを送信します。メールテンプレートは、顧客データのパラメータとともにAmazon SESに保存します。AWS Lambda関数を作成して、SendTemplatedEmail API操作を呼び出し、パラメータとメール宛先を置き換えるために顧客データを渡します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Configure Amazon SES to send emails while leveraging S3 for template storage and Lambda for data merging.",
        "situation_analysis": "The situation involves migrating email messaging functionality to AWS while minimizing operational overhead and costs.",
        "option_analysis": "Option B directly utilizes AWS managed services (Amazon SES and Lambda) which reduce maintenance and administrative tasks compared to a self-managed SMTP server.",
        "additional_knowledge": "Using Amazon SES allows for automatic scaling and compliance with email protocol standards.",
        "key_terminology": "Amazon SES, AWS Lambda, S3, cost efficiency, operational overhead",
        "overall_assessment": "Although community votes show a strong preference for D, B remains the most suitable option considering operational simplicity and cost-effectiveness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはB: メールを送信するためにAmazon SESを設定し、テンプレートストレージのためにS3を利用し、データを統合するためにLambdaを活用することである。",
        "situation_analysis": "メールメッセージ機能をAWSに移行し、運用オーバーヘッドとコストを最小限に抑える必要がある状況である。",
        "option_analysis": "選択肢Bは、自己管理型SMTPサーバーと比べて、メンテナンスや管理作業を削減できるAWSのマネージドサービス（Amazon SESとLambda）を直接利用している。",
        "additional_knowledge": "Amazon SESを使用することで、自動スケーリングとメールプロトコル標準への準拠が可能になります。",
        "key_terminology": "Amazon SES, AWS Lambda, S3, コスト効率, 運用オーバーヘッド",
        "overall_assessment": "コミュニティの投票はDが強く支持されているが、運用のシンプルさとコスト効率を考慮するとBが最も適切である。"
      }
    ],
    "keywords": [
      "Amazon SES",
      "AWS Lambda",
      "S3",
      "cost efficiency",
      "operational overhead"
    ]
  },
  {
    "No": "141",
    "question": "A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a\nvideo Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.\nThe company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The\ncompany has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the\ndevelopment team when there are messages in the dead-letter queue.\nSeveral times during the day. the development team receives notification that messages are in the dead-letter queue and that videos have not\nbeen processed property. An investigation finds no errors m the application logs.\nHow can the company solve this problem?",
    "question_jp": "会社は、Amazon EC2 インスタンスを使用して、AWS クラウドで動画を処理しています。動画の処理には 30 分かかります。複数の EC2 インスタンスは、Amazon Simple Queue Service (Amazon SQS) キュー内の動画の数に応じてスケールインおよびスケールアウトしています。会社は、ターゲットのデッドレターキューと最大受信回数 (maxReceiveCount) を 1 に設定したリドライブポリシーを持つ SQS キューを構成しました。また、会社は SQS キューの可視性タイムアウトを 1 時間に設定しました。会社は、デッドレターキューにメッセージがあると開発チームに通知する Amazon CloudWatch アラームを設定しました。数回にわたり、開発チームはデッドレターキューにメッセージがあり、動画が適切に処理されていないとの通知を受信します。調査の結果、アプリケーションログにエラーは見つかりませんでした。この問題を会社はどのように解決できますか？",
    "choices": [
      {
        "key": "A",
        "text": "Turn on termination protection tor the EC2 Instances",
        "text_jp": "EC2 インスタンスの終了保護をオンにする"
      },
      {
        "key": "B",
        "text": "Update the visibility timeout for the SQS queue to 3 hours",
        "text_jp": "SQS キューの可視性タイムアウトを 3 時間に更新する"
      },
      {
        "key": "C",
        "text": "Configure scale-in protection for the instances during processing",
        "text_jp": "処理中のインスタンスにスケールイン保護を設定する"
      },
      {
        "key": "D",
        "text": "Update the redrive policy and set maxReceiveCount to 0.",
        "text_jp": "リドライブポリシーを更新し、maxReceiveCount を 0 に設定する"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (76%) D (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Update the redrive policy and set maxReceiveCount to 0. This prevents messages from being sent to the dead-letter queue during processing.",
        "situation_analysis": "The company is encountering issues with videos that are not being processed correctly, as indicated by messages in the dead-letter queue, despite having a visibility timeout set.",
        "option_analysis": "Option A would not resolve the underlying issue of message processing. Option B does not address why messages are being moved to the dead-letter queue in the first place. Option C may help but does not stop instances from being terminated while they are busy. Option D aims to rectify the problem by stopping failed messages from being routed improperly.",
        "additional_knowledge": "It's important to ensure that the visibility timeout aligns with the message processing duration.",
        "key_terminology": "Dead-letter queue, Visibility timeout, maxReceiveCount, Redrive policy, Amazon SQS",
        "overall_assessment": "The question illustrates a common scenario in AWS regarding message processing and handling failures. The community vote suggests there may be some confusion or differing interpretations regarding the processing strategy."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は D: リドライブポリシーを更新し、maxReceiveCount を 0 に設定することです。これにより、処理中にメッセージがデッドレターキューに送信されることを防ぎます。",
        "situation_analysis": "会社は、デッドレターキューにメッセージがあることを示す動画が正しく処理されていない問題に直面していますが、可視性タイムアウトが設定されているにもかかわらずです。",
        "option_analysis": "選択肢 A は、メッセージ処理の根本的な問題を解決することはありません。選択肢 B は、そもそもメッセージがデッドレターキューに移動される理由に対応していません。選択肢 C は助けになるかもしれませんが、インスタンスが忙しい間に終了されるのを防ぐことはできません。選択肢 D は、失敗したメッセージが不適切にルーティングされるのを防ぐことに焦点を当てています。",
        "additional_knowledge": "可視性タイムアウトがメッセージ処理時間と整合することを確認することが重要です。",
        "key_terminology": "デッドレターキュー, 可視性タイムアウト, maxReceiveCount, リドライブポリシー, Amazon SQS",
        "overall_assessment": "この問題は、AWS におけるメッセージ処理および失敗処理の一般的なシナリオを示しています。コミュニティの投票は、処理戦略に関して混乱や異なる解釈があることを示唆しています。"
      }
    ],
    "keywords": [
      "Dead-letter queue",
      "Visibility timeout",
      "maxReceiveCount",
      "Redrive policy",
      "Amazon SQS"
    ]
  },
  {
    "No": "142",
    "question": "A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API\nGateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.\nThe solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an\nauthenticated user\nWhich solution will meet these requirements with the LEAST amount of effort?",
    "question_jp": "ある企業は、Amazon API Gatewayを使用して統合エンドポイントを持つAPIを開発しました。これらのAPIは、API Gatewayの認証メカニズムを使用するAWS Lambda関数を呼び出します。設計レビューの結果、公共アクセスを必要としない一連のAPIが特定されました。ソリューションアーキテクトは、これらのAPIに対して、VPCからのみアクセスできるように設計する必要があります。すべてのAPIは認証されたユーザーによって呼び出される必要があります。最も労力をかけずにこれらの要求を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to call the API from the VPC.",
        "text_jp": "内部アプリケーションロードバランサー（ALB）を作成する。ターゲットグループを作成する。呼び出すLambda関数を選択する。ALBのDNS名を使用して、VPCからAPIを呼び出す。"
      },
      {
        "key": "B",
        "text": "Remove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in the hosted zone. Update the API in API Gateway with the CNAME record. Use the CNAME record to call the API from the VPC.",
        "text_jp": "API Gatewayに関連付けられたDNSエントリを削除する。Amazon Route 53にホステッドゾーンを作成する。ホステッドゾーンにCNAMEレコードを作成する。CNAMEレコードでAPIを呼び出すためにAPI Gatewayを更新する。"
      },
      {
        "key": "C",
        "text": "Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPCreate a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC.",
        "text_jp": "API GatewayのAPIエンドポイントを統合からプライベートに更新する。VPCにインターフェースVPCエンドポイントを作成する。リソースポリシーを作成してAPIに添付する。VPCエンドポイントを使用して、VPCからAPIを呼び出す。"
      },
      {
        "key": "D",
        "text": "Deploy the Lambda functions inside the VPC Provision an EC2 instance, and install an Apache server. From the Apache server, call the Lambda functions. Use the internal CNAME record of the EC2 instance to call the API from the VPC.",
        "text_jp": "Lambda関数をVPC内にデプロイする。EC2インスタンスをプロビジョニングし、Apacheサーバーをインストールする。ApacheサーバーからLambda関数を呼び出す。EC2インスタンスの内部CNAMEレコードを使用して、VPCからAPIを呼び出す。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Deploying the Lambda functions inside the VPC and calling them via an EC2 server leverages private access within the VPC, meeting the requirement of restricting public access.",
        "situation_analysis": "The requirement is to access a set of APIs only from within a VPC while ensuring all API calls are authenticated, which means public access must be restricted.",
        "option_analysis": "Option A requires an ALB but does not inherently restrict access to the APIs from the public internet. Option B involves DNS configuration but still does not provide VPC isolation. Option C moves towards a private endpoint setup but involves additional steps compared to option D, which directly meets the requirements with fewer changes by utilizing existing VPC resources. Therefore, Option D is the most straightforward solution.",
        "additional_knowledge": "Using VPC endpoints can also enhance security and performance.",
        "key_terminology": "API Gateway, VPC, AWS Lambda, EC2, CNAME",
        "overall_assessment": "Despite community voting heavily towards option C (100%), option D fulfills the requirements effectively with the least effort by leveraging existing infrastructure to maintain control over API access."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDです。Lambda関数をVPC内にデプロイし、EC2サーバーを介して呼び出すことは、VPC内でのプライベートアクセスを活用し、公共アクセスを制限する要件を満たします。",
        "situation_analysis": "要件は、一連のAPIにVPC内からのみアクセスできるようにし、すべてのAPI呼び出しが認証されるようにすることです。これにより、公共アクセスを制限する必要があります。",
        "option_analysis": "選択肢AはALBが必要ですが、APIへのアクセスを公共インターネットから制限するわけではありません。選択肢BはDNS設定を伴いますが、依然としてVPC隔離を提供しません。選択肢Cはプライベートエンドポイントの設定に向かっていますが、選択肢Dと比較すると、設定の変更が少ない割にパフォーマンスを犠牲にします。したがって、選択肢Dが最も直接的なソリューションです。",
        "additional_knowledge": "VPCエンドポイントを使用すると、セキュリティとパフォーマンスの向上にもつながります。",
        "key_terminology": "API Gateway, VPC, AWS Lambda, EC2, CNAME",
        "overall_assessment": "コミュニティは選択肢Cに100%投票していますが、選択肢Dは最小の労力で要件を効果的に満たしています。既存のインフラストラクチャを利用してAPIのアクセスを制御します。"
      }
    ],
    "keywords": [
      "API Gateway",
      "VPC",
      "AWS Lambda",
      "EC2",
      "CNAME"
    ]
  },
  {
    "No": "143",
    "question": "A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are\nupdated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.\nThe company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is\nslow from time to time.\nWhich combination of steps will resolve the us-east-1 performance issues? (Choose two.)",
    "question_jp": "ある気象サービスが、eu-west-1リージョンにホストされたウェブアプリケーションから高解像度の天気マップを提供しています。この天気マップは頻繁に更新され、Amazon S3に静的HTMLコンテンツとともに保存されています。ウェブアプリケーションはAmazon CloudFrontによってフロントされています。最近、同社はus-east-1リージョンのユーザーにもサービスを拡大しましたが、新しいユーザーはそれぞれの天気マップの表示が時々遅いと報告しています。us-east-1のパフォーマンス問題を解決するためのステップの組み合わせはどれですか？（2つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us- east-1.",
        "text_jp": "S3バケット（eu-west-1）のAWS Global Acceleratorエンドポイントを構成します。us-east-1でTCPポート80と443のエンドポイントグループを設定します。"
      },
      {
        "key": "B",
        "text": "Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.",
        "text_jp": "us-east-1に新しいS3バケットを作成します。eu-west-1のS3バケットから同期するためにS3クロスリージョンレプリケーションを設定します。"
      },
      {
        "key": "C",
        "text": "Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.",
        "text_jp": "North Americaからのリクエストを変更するためにLambda@Edgeを使用し、us-east-1のS3 Transfer Accelerationエンドポイントを使用させます。"
      },
      {
        "key": "D",
        "text": "Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.",
        "text_jp": "North Americaからのリクエストを変更するためにLambda@Edgeを使用し、us-east-1のS3バケットを使用させます。"
      },
      {
        "key": "E",
        "text": "Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.",
        "text_jp": "CloudFrontディストリビューションのオリジンとしてus-east-1のAWS Global Acceleratorエンドポイントを設定します。Lambda@Edgeを使用してNorth Americaからのリクエストを新しいオリジンを使用させるようにします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BD (96%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are B and E. Setting up an S3 bucket in us-east-1 with cross-region replication allows for faster access to the content for users in that region.",
        "situation_analysis": "Users in us-east-1 are experiencing slow access to weather maps stored in eu-west-1 due to latency issues. Content needs to be closer to users to improve performance.",
        "option_analysis": "Option B directly addresses the need for lower latency by creating an S3 bucket in the same region as the new users. Option E provides a way to accelerate content delivery but is secondary to the storage improvement of option B.",
        "additional_knowledge": "CloudFront caches content but having a local source for updates through cross-region replication will provide users quicker access.",
        "key_terminology": "S3 bucket, cross-region replication, AWS Global Accelerator, CloudFront, latency",
        "overall_assessment": "The community supports B and D, with B being the best choice due to the necessity for duplicate S3 storage in us-east-1. This ensures users can access content with reduced latency."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBとEである。us-east-1に新しいS3バケットを設定し、クロスリージョンレプリケーションを設定することで、その地域のユーザーにとって高速なコンテンツアクセスが可能になる。",
        "situation_analysis": "us-east-1のユーザーは、eu-west-1に保存された天気マップへのアクセスが遅く、レイテンシの問題に直面している。この状況では、コンテンツがユーザーに近いほどパフォーマンスが向上する。",
        "option_analysis": "選択肢Bは、ユーザーの近くにコンテンツがあることでレイテンシを低下させる必要を直接解決する。一方、選択肢Eは、コンテンツ配信を加速する方法を提供するが、選択肢Bのストレージ改善に対しては二次的なものである。",
        "additional_knowledge": "CloudFrontはコンテンツをキャッシュするが、クロスリージョンレプリケーションによる更新のためのローカルソースを持つことで、ユーザーは迅速にアクセスできる。",
        "key_terminology": "S3バケット、クロスリージョンレプリケーション、AWS Global Accelerator、CloudFront、レイテンシ",
        "overall_assessment": "コミュニティはBとDを支持しており、Bはus-east-1に重複したS3ストレージが必要であるため最良の選択肢である。これにより、ユーザーはレイテンシを減少させたコンテンツにアクセスできる。"
      }
    ],
    "keywords": [
      "S3 bucket",
      "cross-region replication",
      "AWS Global Accelerator",
      "CloudFront",
      "latency"
    ]
  },
  {
    "No": "144",
    "question": "A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis\nindicates that the issue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as\nthe profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.\nThe solutions architect discovers that the file system has reached Its maximum capacity. The solutions architect must ensure that users can\nregain access. The solution also must prevent the problem from occurring again.\nWhich solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトが、企業がAmazon Workspacesで新しいセッションを確立できない問題を調査しています。初期分析では、この問題がユーザープロファイルに関係していることが示されています。Amazon Workspaces環境は、プロファイル共有ストレージとしてAmazon FSx for Windows File Serverを使用するように設定されています。FSx for Windows File Serverファイルシステムは、10 TBのストレージで構成されています。ソリューションアーキテクトは、ファイルシステムが最大容量に達していることを発見しました。ソリューションアーキテクトは、ユーザーが再度アクセスできるようにし、問題が再発しないようにする必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system.",
        "text_jp": "古いユーザープロファイルを削除してスペースを作成します。ユーザープロファイルをAmazon FSx for Lustreファイルシステムに移行します。"
      },
      {
        "key": "B",
        "text": "Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required.",
        "text_jp": "update-file-systemコマンドを使用して容量を増加させます。空き容量を監視するAmazon CloudWatchメトリックを実装します。必要に応じてAWS Lambda関数を呼び出すためにAmazon EventBridgeを使用します。"
      },
      {
        "key": "C",
        "text": "Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity as required.",
        "text_jp": "Amazon CloudWatchのFreeStorageCapacityメトリックを使用してファイルシステムを監視します。必要に応じて容量を増加させるためにAWS Step Functionsを使用します。"
      },
      {
        "key": "D",
        "text": "Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection for 50% of the users to use the new file system.",
        "text_jp": "古いユーザープロファイルを削除してスペースを作成します。追加のFSx for Windows File Serverファイルシステムを作成します。ユーザープロファイルのリダイレクトを50%のユーザーが新しいファイルシステムを使用するように更新します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (91%) 4%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which involves monitoring the FreeStorageCapacity metric in Amazon CloudWatch and using AWS Step Functions to increase capacity as required. This solution not only addresses the immediate issue of space but also ensures that capacity can be adjusted dynamically based on usage.",
        "situation_analysis": "The company is facing issues with Amazon Workspaces due to the limitation in the FSx for Windows File Server's storage capacity, which has now reached its maximum. Therefore, resolving this problem involves addressing both the immediate need for space and the prevention of future occurrences.",
        "option_analysis": "Option A, while it provides immediate relief by removing old profiles, does not prevent future issues. Option B involves using EventBridge which, while helpful, centers around manual remediations that could lead to delays. Option D also removes profiles but does not fully address the capacity needs. Hence, C is the most holistic solution.",
        "additional_knowledge": "Continuous monitoring of storage helps maintain the necessary profiling space and can streamline operational efficiency.",
        "key_terminology": "Amazon FSx, Amazon Workspaces, AWS Step Functions, Amazon CloudWatch, FreeStorageCapacity metric",
        "overall_assessment": "Choice C is superior because it effectively combines monitoring and dynamic scaling, aligning with AWS best practices for managing resources efficiently. The community support indicates choice B is popular, but it lacks the scalability that choice C provides."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCで、Amazon CloudWatchのFreeStorageCapacityメトリックを使用して監視し、必要に応じてAWS Step Functionsを使用して容量を増加させることです。このソリューションは、即座のスペース問題に対処するだけでなく、使用状況に基づいて動的に容量を調整することを保証します。",
        "situation_analysis": "企業は、FSx for Windows File Serverのストレージ容量が制限されているためにAmazon Workspacesで問題に直面しています。現在、その最大に達しました。したがって、この問題を解決するには、一時的なスペースの必要性と将来の発生防止の両方に対応する必要があります。",
        "option_analysis": "選択肢Aは、古いプロファイルを削除することで即時の救済を提供しますが、将来の問題を防ぐことはありません。選択肢BはEventBridgeを使用しますが、手動の修正に依存し遅延を招く可能性があります。選択肢Dもプロファイルを削除しますが、容量のニーズに完全には対処できません。したがって、選択肢Cがもっとも包括的なソリューションです。",
        "additional_knowledge": "ストレージの継続的な監視は、必要なプロファイリングスペースを維持し、運用効率をスムーズに進めるのに役立ちます。",
        "key_terminology": "Amazon FSx, Amazon Workspaces, AWS Step Functions, Amazon CloudWatch, FreeStorageCapacityメトリック",
        "overall_assessment": "選択肢Cは効率的なリソース管理においてAWSベストプラクティスに沿った監視と動的スケーリングを効果的に組み合わせているため優れています。コミュニティも選択肢Bを支持しているように見えますが、選択肢Cが提供するスケーラビリティが欠如しています。"
      }
    ],
    "keywords": [
      "Amazon FSx",
      "Amazon Workspaces",
      "AWS Step Functions",
      "Amazon CloudWatch",
      "FreeStorageCapacity metric"
    ]
  },
  {
    "No": "145",
    "question": "An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery.\nConfirmation includes the recipient's signature or a photo of the package with the recipient. The driver's handheld device uploads signatures and\nphotos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file\nname matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information.\nThe file is then placed in Amazon S3 for archiving.\nAs the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped\nconnections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30\nminutes. The billing team reports that files are not always in the archive and that the central system is not always updated.\nA solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems\nare always updated. The handheld devices cannot be modified, so the company cannot deploy a new application.\nWhich solution will meet these requirements?",
    "question_jp": "国際的な配送会社は、AWS上で配送管理システムをホストしている。ドライバーは、このシステムを使用して配送確認をアップロードする。\n確認には、受取人の署名または受取人とパッケージの写真が含まれる。ドライバーのハンドヘルドデバイスは、FTPを介して単一のAmazon EC2インスタンスに署名と写真をアップロードする。各ハンドヘルドデバイスは、サインインしたユーザーに基づいてディレクトリにファイルを保存し、ファイル名は配送番号に一致する。EC2インスタンスは、中央データベースにクエリを実行して配送情報を取得した後、ファイルにメタデータを追加する。\nその後、ファイルはアーカイブ用にAmazon S3に置かれる。\n会社が拡大するにつれて、ドライバーはシステムが接続を拒否していると報告している。FTPサーバーは、接続が切れたりメモリーの問題が発生しており、これに対応するためにシステムエンジニアはEC2インスタンスを30分ごとに再起動するCronタスクをスケジュールする。請求チームは、ファイルがアーカイブに必ずしも存在せず、中央システムが常に更新されていないと報告する。\nソリューションアーキテクトは、アーカイブが常にファイルを受け取り、システムが常に更新されることを最大限にスケーラブルにするソリューションを設計する必要がある。ハンドヘルドデバイスは変更できないため、会社は新しいアプリケーションを展開できない。\nどのソリューションがこれらの要件を満たすことができるか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure the Auto Scaling group to have a minimum of three instances.",
        "text_jp": "既存のEC2インスタンスのAMIを作成する。アプリケーションロードバランサーの背後にEC2インスタンスのオートスケーリンググループを作成する。オートスケーリンググループを最低3つのインスタンスに設定する。"
      },
      {
        "key": "B",
        "text": "Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the new path for file processing.",
        "text_jp": "AWS Transfer Familyを使用して、ファイルをAmazon Elastic File System（Amazon EFS）に配置するFTPサーバーを作成する。EFSボリュームを既存のEC2インスタンスにマウントする。ファイル処理のためにEC2インスタンスに新しいパスを指す。"
      },
      {
        "key": "C",
        "text": "Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.",
        "text_jp": "AWS Transfer Familyを使用して、ファイルをAmazon S3に配置するFTPサーバーを作成する。Amazon S3イベント通知を介してAmazon Simple Notification Service（Amazon SNS）を使用してAWS Lambda関数を呼び出す。Lambda関数を設定してメタデータを追加し、配送システムを更新する。"
      },
      {
        "key": "D",
        "text": "Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.",
        "text_jp": "ハンドヘルドデバイスを更新して、ファイルを直接Amazon S3に配置する。Amazon S3イベント通知を介してAmazon Simple Queue Service（Amazon SQS）を使用してAWS Lambda関数を呼び出す。Lambda関数を設定してメタデータを追加し、配送システムを更新する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Using AWS Transfer Family to create an FTP server that places the files in Amazon EFS is a viable solution to address scalability issues, as EFS provides a shared file system that can handle multiple caller uploads without the memory issues seen with EC2.",
        "situation_analysis": "The delivery management system is facing connection rejection due to scalability issues with the current FTP server architecture and manual reboot tasks being needed due to memory constraints of a single EC2 instance.",
        "option_analysis": "Option A introduces an Auto Scaling group, which can improve connection handling but does not address the shared file issue. Option C, while proposing S3 directly, cannot interface with the existing unmodifiable handheld devices. Option D requires altering the devices which is not possible. Option B uses EFS to maintain a scalable architecture without changing existing devices.",
        "additional_knowledge": "The setup with EFS would also allow for lower maintenance compared to managing instances on EC2, thus aligning well with AWS best practices for scalability and data integrity.",
        "key_terminology": "AWS Transfer Family, Amazon EFS, EC2, scalability, file processing",
        "overall_assessment": "While the community votes show high preference for option C, option B is the most practical solution given the requirement not to alter existing handheld devices and for ensuring file availability and system updates as delivery operations grow."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。AWS Transfer Familyを使用して、ファイルをAmazon EFSに配置するFTPサーバーを作成することは、スケーラビリティの問題に対処するための実行可能なソリューションであり、EFSは複数の呼び出しアップロードを処理できる共有ファイルシステムを提供し、EC2のメモリー問題を避けることができる。",
        "situation_analysis": "配送管理システムは、現在のFTPサーバーアーキテクチャのスケーラビリティの問題と、単一のEC2インスタンスのメモリー制約のために手動再起動タスクが必要であるため、接続拒否の問題に直面している。",
        "option_analysis": "選択肢Aはオートスケーリンググループを導入し、接続処理を改善する可能性があるが、共有ファイルの問題には対処していない。選択肢Cは、S3を直接提案しているが、既存の変更できないハンドヘルドデバイスとのインターフェースができない。選択肢Dは、デバイスの変更を要求するが、これは不可能である。選択肢Bは、EFSを使用して既存のデバイスを変更せずにスケーラブルなアーキテクチャを維持する。",
        "additional_knowledge": "EFSを使用した構成は、EC2上でインスタンスを管理するよりもメンテナンスが少なくて済み、スケーラビリティとデータ整合性のためのAWSのベストプラクティスによく合致する。",
        "key_terminology": "AWS Transfer Family, Amazon EFS, EC2, スケーラビリティ, ファイル処理",
        "overall_assessment": "コミュニティの投票は選択肢Cに高い支持を示しているが、既存のハンドヘルドデバイスを変更せずにファイルの可用性とシステムの更新を確実にするためには、選択肢Bが最も実用的な解決策である。"
      }
    ],
    "keywords": [
      "AWS Transfer Family",
      "Amazon EFS",
      "EC2",
      "scalability",
      "file processing"
    ]
  },
  {
    "No": "146",
    "question": "A company is running an application in the AWS Cloud. The application runs on containers m an Amazon Elastic Container Service (Amazon ECS)\ncluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory\nrequirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data\ncan be lost.\nWhich solution will meet these requirements with the LEAST amount of operational overhead?",
    "question_jp": "企業はAWSクラウドでアプリケーションを稼働させています。そのアプリケーションは、Amazon Elastic Container Service (Amazon ECS) クラスターでコンテナとして実行されています。ECSタスクはFargate起動タイプを使用しています。アプリケーションのデータは関係データベースで、Amazon Aurora MySQLに保存されています。規制要件を満たすために、アプリケーションは障害が発生した場合に別のAWSリージョンに復旧できなければなりません。障害発生時にはデータが失われてはなりません。どのソリューションが、最も運用オーバーヘッドが少なく、これらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Provision an Aurora Replica in a different Region.",
        "text_jp": "異なるリージョンにAuroraレプリカをプロビジョニングする。"
      },
      {
        "key": "B",
        "text": "Set up AWS DataSync for continuous replication of the data to a different Region.",
        "text_jp": "AWS DataSyncを設定し、データを異なるリージョンに継続的に複製する。"
      },
      {
        "key": "C",
        "text": "Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.",
        "text_jp": "AWS Database Migration Service (AWS DMS) を設定し、データを異なるリージョンに継続的に複製する。"
      },
      {
        "key": "D",
        "text": "Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes.",
        "text_jp": "Amazon Data Lifecycle Manager (Amazon DLM)を使用して、5分ごとにスナップショットをスケジュールする。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Provisioning an Aurora Replica in a different Region allows for a direct, straightforward way to meet the requirements of region-based disaster recovery with minimal operational overhead.",
        "situation_analysis": "The application requires a strict recovery mechanism to ensure no data loss, along with a quick recovery time in a different AWS region during a failure incident.",
        "option_analysis": "Option A directly addresses both the need for zero data loss and rapid recovery in another region. Options B and C involve more complexity and operational overhead due to continuous data replication setups. Option D, while creating backups, does not guarantee immediate recovery or avoid data loss if a failure occurs before the next snapshot.",
        "additional_knowledge": "AWS best practices recommend leveraging AWS services that are built for redundancy and recovery.",
        "key_terminology": "Amazon Aurora, Aurora Replica, Disaster Recovery, Fargate, Container Services",
        "overall_assessment": "Choosing Aurora Replica is a best practice for ensuring disaster recovery with minimal overhead. Community voting is 100% aligned with this choice, endorsing its effectiveness for regulatory compliance."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。異なるリージョンにAuroraレプリカをプロビジョニングすることで、運用オーバーヘッドを最小限に抑えつつ、リージョンベースの災害復旧要件を満たすための直接的かつ簡単な方法が提供されます。",
        "situation_analysis": "アプリケーションは厳格な復旧メカニズムを必要としており、障害事象時にデータ損失を避けつつ、異なるAWSリージョンでの迅速な復旧が求められています。",
        "option_analysis": "選択肢Aは、ゼロデータ損失と他のリージョンでの迅速な復旧の両方のニーズに直接応えるものです。選択肢BおよびCは、継続的なデータ複製のセットアップにより、より複雑で運用的なオーバーヘッドが発生します。選択肢Dはバックアップを作成しますが、即時復旧を保証せず、次のスナップショットが作成される前に障害が発生した場合はデータ損失を避けることができません。",
        "additional_knowledge": "AWSのベストプラクティスでは、冗長性と復旧のために構築されたAWSサービスを活用することが推奨されています。",
        "key_terminology": "Amazon Aurora、Auroraレプリカ、災害復旧、Fargate、コンテナサービス",
        "overall_assessment": "Auroraレプリカを選択することは、運用オーバーヘッドを最小限に抑えながら災害復旧を確保するためのベストプラクティスです。コミュニティ投票は100%この選択と一致しており、規制遵守の有効性を支持しています。"
      }
    ],
    "keywords": [
      "Amazon Aurora",
      "Aurora Replica",
      "Disaster Recovery",
      "Fargate",
      "Container Services"
    ]
  },
  {
    "No": "147",
    "question": "A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15\nminutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card\nprimary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for\nadditional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format.\nAdditionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.\nWhich solutions will meet these requirements?",
    "question_jp": "金融サービス会社は、クレジットカードサービスのパートナーから定期的にデータフィードを受信します。約5,000件のレコードが15分ごとに平文で送信され、サーバー側の暗号化が施されたAmazon S3バケットにHTTPS経由で直接配信されます。このフィードには、機密性の高いクレジットカードのプライマリアカウント番号（PAN）が含まれています。同社は、データを別のS3バケットに送信する前にPANを自動的にマスクする必要があります。また、特定のフィールドを削除・統合し、レコードをJSON形式に変換する必要があります。さらに、将来的に追加のフィードが追加される可能性があるため、設計は簡単に拡張可能である必要があります。\nこれらの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Invoke a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.",
        "text_jp": "ファイル配信時にAWS Lambda関数を呼び出し、各レコードを抽出してAmazon SQSキューに書き込みます。新しいメッセージがSQSキューに到着すると、別のLambda関数を呼び出してレコードを処理し、結果をAmazon S3の一時的な場所に書き込みます。SQSキューが空になると、最終的なLambda関数を呼び出してレコードをJSON形式に変換し、結果を別のS3バケットに送信して内部処理を行います。"
      },
      {
        "key": "B",
        "text": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.",
        "text_jp": "ファイル配信時にAWS Lambda関数を呼び出し、各レコードを抽出してAmazon SQSキューに書き込みます。AWS Fargateコンテナアプリケーションを設定して、SQSキューにメッセージがあるときに自動的に単一インスタンスにスケールします。アプリケーションは各レコードを処理し、レコードをJSON形式に変換します。キューが空になると、結果を別のS3バケットに送信し、AWS Fargateインスタンスをスケールダウンします。"
      },
      {
        "key": "C",
        "text": "Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.",
        "text_jp": "AWS Glueクローラーとカスタムクラスifierを作成し、データフィード形式に基づいて一致するテーブル定義を構築します。ファイル配信時にAWS Lambda関数を呼び出して、AWS Glue ETLジョブを開始し、処理および変換要件に従って全レコードを変換します。出力形式をJSONとして定義します。完了後、ETLジョブが結果を別のS3バケットに送信して内部処理を行います。"
      },
      {
        "key": "D",
        "text": "Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster.",
        "text_jp": "データフィード形式に基づいてAWS Glueクローラーとカスタムクラスifierを作成し、一致するテーブル定義を構築します。ファイル配信時にAmazon Athenaクエリを実行して、Amazon EMR ETLジョブを開始し、処理および変換要件に従って全レコードを変換します。出力形式をJSONとして定義します。完了後、結果を別のS3バケットに送信し、EMRクラスターをスケールダウンします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. AWS Glue is a serverless ETL service that is well-suited for transforming large sets of data and can easily scale as needed.",
        "situation_analysis": "The company requires the masking of sensitive PAN data, field transformation, and JSON formatting in an expandable manner. AWS Glue allows for these requirements to be handled seamlessly.",
        "option_analysis": "Option C is the best choice because it leverages AWS Glue's capabilities for data transformation, allowing for scalable and efficient processing of large data sets. Option A relies on multiple Lambda functions, which could introduce complexity, while B introduces Fargate, creating additional operational overhead. Option D adds unnecessary complexity with Athena and EMR.",
        "additional_knowledge": "Additionally, Glue can automatically discover data and update the schema, adding further flexibility.",
        "key_terminology": "AWS Glue, ETL, JSON, Lambda, S3",
        "overall_assessment": "Overall, Option C aligns perfectly with the requirements of the question. It provides a streamlined, serverless approach to data transformation with minimum operational overhead."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCである。AWS Glueは、データの変換に適したサーバーレスETLサービスであり、必要に応じて容易にスケールすることが可能である。",
        "situation_analysis": "会社は機密性の高いPANデータのマスク化、フィールドの変換、JSONフォーマットへの変換を拡張可能な方法で行う必要がある。AWS Glueは、これらの要件をシームレスに処理できる。",
        "option_analysis": "選択肢Cは、データ変換のためにAWS Glueの能力を活用するため、拡張性が高く効率的な大規模データ処理を可能にするため、最適な選択肢である。選択肢Aは複数のLambda関数に依存しており、複雑さをもたらす可能性がある。一方、選択肢BはFargateを導入し、追加の運用負荷を生じる。選択肢DはAthenaとEMRを使用した不必要な複雑さを加える。",
        "additional_knowledge": "さらに、Glueはデータを自動的に発見し、スキーマを更新する機能もあり、さらなる柔軟性を加える。",
        "key_terminology": "AWS Glue, ETL, JSON, Lambda, S3",
        "overall_assessment": "全体として、選択肢Cは質問の要件と完全に一致している。最小限の運用負荷でデータ変換のための効率的なサーバーレスアプローチを提供するからである。"
      }
    ],
    "keywords": [
      "AWS Glue",
      "ETL",
      "JSON",
      "Lambda",
      "S3"
    ]
  },
  {
    "No": "148",
    "question": "A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application\nruns on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL\ndatabase as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.\nWhich solution will achieve the company's goal with the LEAST operational overhead?",
    "question_jp": "ある企業は、主なオンプレミスアプリケーションが失敗した場合に備えて、AWSを使用してビジネス継続性ソリューションを作成したいと考えています。このアプリケーションは、他のアプリケーションも実行している物理サーバー上で動作しています。移行を計画しているオンプレミスアプリケーションは、データストアとしてMySQLデータベースを使用しています。企業の全てのオンプレミスアプリケーションは、Amazon EC2と互換性のあるオペレーティングシステムを使用しています。最小限の運用オーバーヘッドで企業の目標を達成するには、どのソリューションが最適ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail over the workload in the case of a failure event.",
        "text_jp": "ソースサーバー（MySQLサーバーを含む）にAWSレプリケーションエージェントをインストールします。全てのサーバーでレプリケーションを設定します。定期的な演習用にテストインスタンスを起動します。故障イベントが発生した場合に対応するために、テストインスタンスに切り替えます。"
      },
      {
        "key": "B",
        "text": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time.",
        "text_jp": "ソースサーバー（MySQLサーバーを含む）にAWSレプリケーションエージェントをインストールします。ターゲットAWSリージョンでAWS Elastic Disaster Recoveryを初期化します。起動設定を定義します。最近の時点からのフェイルオーバーとフォールバックを頻繁に実行します。"
      },
      {
        "key": "C",
        "text": "Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI.",
        "text_jp": "AWSデータベースマイグレーションサービス（AWS DMS）レプリケーションサーバーと、データベースをホストするためのターゲットAmazon Aurora MySQL DBクラスターを作成します。既存のデータをターゲットDBクラスターにコピーするためのDMSレプリケーションタスクを作成します。データを同期させるためにローカルAWSスキーマ変換ツール（AWS SCT）チェンジデータキャプチャ（CDC）タスクを作成します。互換性のあるベースAMIから起動してEC2インスタンスに残りのソフトウェアをインストールします。"
      },
      {
        "key": "D",
        "text": "Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the MySQL database on the new volumes. Take regular snapshots. Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a Volume Gateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes on the EC2 instances in the case of a failure event.",
        "text_jp": "オンプレミスにAWSストレージゲートウェイボリュームゲートウェイをデプロイします。全てのオンプレミスサーバーでボリュームをマウントします。新しいボリュームにアプリケーションとMySQLデータベースをインストールします。定期的にスナップショットを取得します。互換性のあるベースAMIから起動してEC2インスタンスに全てのソフトウェアをインストールします。故障イベントが発生した場合に、最新のスナップショットからボリュームを復元し、EC2インスタンスに新しいボリュームをマウントします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (79%) C (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using AWS Database Migration Service (AWS DMS) allows for minimal operational overhead while migrating and syncing the MySQL database efficiently.",
        "situation_analysis": "The goal is to create a business continuity solution with the least operational overhead, which involves ensuring data consistency during migration from on-premises to AWS.",
        "option_analysis": "Option C is the best choice because it directly utilizes AWS DMS to handle data migration and synchronization, which reduces the need for extensive manual configuration. Options A and B require additional setup for failover and do not effectively utilize DMS features. Option D involves more complex steps with local installations and is not optimized for minimal operational overhead.",
        "additional_knowledge": "Understanding the capabilities of AWS services can significantly impact the design and operational overhead of cloud solutions.",
        "key_terminology": "AWS Database Migration Service, Amazon Aurora, MySQL, Change Data Capture, Migration.",
        "overall_assessment": "While community votes are heavily in favor of option B, option C is technically superior due to AWS DMS's capabilities. It's essential to evaluate the operational simplicity versus community sentiment."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCである。AWSデータベースマイグレーションサービス（AWS DMS）を使用することで、MySQLデータベースの移行と同期が効率的に行われ、運用オーバーヘッドを最小限に抑えることができる。",
        "situation_analysis": "目標は、運用オーバーヘッドを最小限に抑えたビジネス継続性ソリューションを作成することであり、これにはオンプレミスからAWSへの移行中にデータの整合性を確保することが含まれる。",
        "option_analysis": "選択肢Cは、AWS DMSを直接利用してデータの移行と同期を扱い、広範な手動設定の必要性を削減するため、最適な選択肢である。選択肢AとBはフェイルオーバーのための追加の設定が必要で、DMS機能を効果的に活用していない。選択肢Dは、ローカルインストールを伴うより複雑なステップを含み、運用オーバーヘッドが最小限に抑えられていない。",
        "additional_knowledge": "AWSサービスの能力を理解することが、クラウドソリューションの設計と運用オーバーヘッドに大きな影響を与える。",
        "key_terminology": "AWSデータベースマイグレーションサービス、Amazon Aurora、MySQL、チェンジデータキャプチャ、マイグレーション。",
        "overall_assessment": "コミュニティの投票は選択肢Bに大きく偏っているが、AWS DMSの機能を考慮すると、選択肢Cは技術的に優れている。運用の簡素化 versus コミュニティの感情を評価することが重要である。"
      }
    ],
    "keywords": [
      "AWS Database Migration Service",
      "Amazon Aurora",
      "MySQL",
      "Change Data Capture",
      "Migration"
    ]
  },
  {
    "No": "149",
    "question": "A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the\ncompany's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The\nsolution must comply with AWS security best practices.\nWhich solution will meet these requirements?",
    "question_jp": "企業は財務情報の規制監査の対象となっています。外部監査人は単一のAWSアカウントを使用し、企業のAWSアカウントへアクセスする必要があります。ソリューションアーキテクトは、監査人に企業のAWSアカウントへの安全な読み取り専用アクセスを提供しなければなりません。このソリューションはAWSのセキュリティベストプラクティスに準拠する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "In the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS account. Assign a unique external ID to the resource policy.",
        "text_jp": "企業のAWSアカウント内で、すべてのリソースに対してリソースポリシーを作成し、監査人のAWSアカウントへのアクセスを付与します。リソースポリシーにユニークな外部IDを割り当てます。"
      },
      {
        "key": "B",
        "text": "In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.",
        "text_jp": "企業のAWSアカウント内で、監査人のAWSアカウントを信頼するIAMロールを作成します。必要な権限を持つIAMポリシーを作成し、ロールにそのポリシーをアタッチします。ロールの信頼ポリシーにユニークな外部IDを割り当てます。"
      },
      {
        "key": "C",
        "text": "In the company's AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM user. Share the access keys with the auditors.",
        "text_jp": "企業のAWSアカウント内でIAMユーザーを作成します。必要なIAMポリシーをIAMユーザーにアタッチします。IAMユーザーのAPIアクセスキーを作成し、監査人にアクセスキーを共有します。"
      },
      {
        "key": "D",
        "text": "In the company's AWS account, create an IAM group that has the required permissions. Create an IAM user in the company's account for each auditor. Add the IAM users to the IAM group.",
        "text_jp": "企業のAWSアカウント内で、必要な権限を持つIAMグループを作成します。監査人ごとに企業アカウント内にIAMユーザーを作成し、それらをIAMグループに追加します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct solution is B, which involves creating an IAM role that trusts the auditors' AWS account and attaching the necessary IAM policy for read-only access.",
        "situation_analysis": "The company needs to provide secure access for external auditors while adhering to regulatory practices and AWS security best practices.",
        "option_analysis": "Option A allows access through resource policies but may be less secure. Option C creates an IAM user with API keys, which is not ideal for temporary access. Option D creates IAM users without leveraging role-based access, missing due diligence for external auditors.",
        "additional_knowledge": "This approach allows for easy auditing and monitoring of actions taken by the auditors.",
        "key_terminology": "IAM role, external ID, policy, trust relationship, secure access, read-only permissions.",
        "overall_assessment": "Answer B is well-supported by AWS security best practices and aligns with the need for secure, controlled access while minimizing potential security risks."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい解決策はBであり、これは監査人のAWSアカウントを信頼するIAMロールを作成し、読み取り専用アクセスのために必要なIAMポリシーをアタッチすることを含みます。",
        "situation_analysis": "企業は外部監査人に対して安全なアクセスを提供する必要があり、規制要件およびAWSセキュリティベストプラクティスに従わなければなりません。",
        "option_analysis": "オプションAはリソースポリシーを通じてアクセスを許可しますが、セキュリティが低下する可能性があります。オプションCはIAMユーザーを作成しAPIキーを使用するもので、一時的なアクセスには適していません。オプションDはIAMユーザーを作成しますが、ロールベースのアクセスを活用しておらず、外部監査人に対する適切な注意義務を果たしていません。",
        "additional_knowledge": "このアプローチは、監査人によって行われたアクションの容易な監査とモニタリングを可能にします。",
        "key_terminology": "IAMロール、外部ID、ポリシー、信頼関係、安全なアクセス、読み取り専用権限。",
        "overall_assessment": "回答BはAWSセキュリティベストプラクティスによく支持されており、安全で制御されたアクセスの必要性と整合しています。"
      }
    ],
    "keywords": [
      "IAM role",
      "external ID",
      "trust relationship",
      "policy",
      "read-only access"
    ]
  },
  {
    "No": "150",
    "question": "A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB\ntable to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The\nnew solution must ensure high availability for the trading platform.\nWhich solution will meet these requirements with the LEAST latency?",
    "question_jp": "企業は、Amazon DynamoDBをストレージバックエンドとして使用するレイテンシーに敏感な取引プラットフォームを運営しています。企業はDynamoDBテーブルをオンデマンド容量モードで構成しました。ソリューションアーキテクトは、取引プラットフォームのパフォーマンスを向上させるためのソリューションを設計する必要があります。この新しいソリューションは、取引プラットフォームの高可用性を確保する必要があります。どのソリューションが最も低レイテンシーでこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.",
        "text_jp": "2ノードのDynamoDB Accelerator (DAX) クラスターを作成します。アプリケーションを構成して、DAXを使用してデータを読み書きします。"
      },
      {
        "key": "B",
        "text": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.",
        "text_jp": "3ノードのDynamoDB Accelerator (DAX) クラスターを作成します。アプリケーションを構成して、DAXを使用してデータを読み、DynamoDBテーブルに直接書き込みます。"
      },
      {
        "key": "C",
        "text": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX.",
        "text_jp": "3ノードのDynamoDB Accelerator (DAX) クラスターを作成します。アプリケーションを構成して、DynamoDBテーブルからデータを直接読み、DAXを使用してデータを書き込みます。"
      },
      {
        "key": "D",
        "text": "Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.",
        "text_jp": "1ノードのDynamoDB Accelerator (DAX) クラスターを作成します。アプリケーションを構成して、DAXを使用してデータを読み、DynamoDBテーブルに直接書き込みます。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating a two-node DAX cluster provides improved performance for read and write operations while ensuring high availability with minimal latency.",
        "situation_analysis": "The requirement is to improve performance and ensure high availability for a latency-sensitive trading platform. Using DAX can reduce the latency involved in DynamoDB operations.",
        "option_analysis": "Option A is suitable as it uses a two-node DAX cluster, enhancing availability and response times. Option B introduces unnecessary complexity without additional performance benefits. Option C compromises write operations, which should prefer DAX, as the application reads from the main table. Option D does not offer adequate redundancy.",
        "additional_knowledge": "High availability is critical in transaction processing systems where downtime can lead to significant financial loss.",
        "key_terminology": "DynamoDB, DynamoDB Accelerator (DAX), latency, high availability, read/write operations",
        "overall_assessment": "Option A is the best choice as it provides the necessary performance improvement with high availability while having the least latency. Despite the community voting heavily for option B, option A meets all specified requirements better."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。2ノードのDAXクラスターを作成することで、読み書き操作のパフォーマンスが向上し、高可用性を維持しながら、最小限のレイテンシーを実現します。",
        "situation_analysis": "レイテンシーに敏感な取引プラットフォームのパフォーマンスを向上させ、高可用性を確保する必要があります。DAXを使用することで、DynamoDB操作にかかるレイテンシーを削減できます。",
        "option_analysis": "オプションAは、2ノードのDAXクラスターを使用して、可用性と応答時間を改善するために適しています。オプションBは追加のパフォーマンスメリットがないのに複雑さを増します。オプションCは、アプリケーションが基本テーブルから直接読み込むため、書き込み操作がDAXを利用すべきシナリオを損ないます。オプションDは、適切な冗長性を提供しません。",
        "additional_knowledge": "取引処理システムにおいて高可用性は重要であり、ダウンタイムは重大な経済的損失につながる可能性があります。",
        "key_terminology": "DynamoDB、DynamoDB Accelerator (DAX)、レイテンシー、高可用性、読み書き操作",
        "overall_assessment": "オプションAは、最少のレイテンシーで高可用性を提供し、必要なパフォーマンス向上を実現するため、最良の選択肢です。コミュニティの投票がオプションBに偏っているにもかかわらず、オプションAは指定されたすべての要件をより良く満たしています。"
      }
    ],
    "keywords": [
      "DynamoDB",
      "DynamoDB Accelerator",
      "DAX",
      "latency",
      "high availability",
      "read/write operations"
    ]
  },
  {
    "No": "151",
    "question": "A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2\ninstances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind\nanother ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak\nusage of the application.\nThe application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives\nminimal trafic during the rest of the day.\nA solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "question_jp": "ある企業がオンプレミスからAWSにアプリケーションを移行しました。アプリケーションのフロントエンドは2つのAmazon EC2インスタンスで実行されており、Application Load Balancer (ALB) の背後で動いています。アプリケーションのバックエンドは、別のALBの背後で動くPythonアプリケーションで、3つのEC2インスタンスで実行されています。EC2インスタンスは、大きな汎用性のあるオンデマンドインスタンスで、ピーク使用のためのオンプレミスの仕様に合わせてサイズ設定されています。アプリケーションは月間数十万のリクエストを平均していますが、主にランチ時間に使用され、残りの時間帯は最小限のトラフィックを受けています。ソリューションアーキテクトは、アプリケーションの可用性に悪影響を及ぼすことなく、インフラストラクチャコストを最適化する必要があります。どの組み合わせのステップがこれらの要件を満たしますか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances.",
        "text_jp": "すべてのEC2インスタンスを、既存のEC2インスタンスと同じコア数を持つコンピュート最適化インスタンスに変更します。"
      },
      {
        "key": "B",
        "text": "Move the application frontend to a static website that is hosted on Amazon S3.",
        "text_jp": "アプリケーションのフロントエンドをAmazon S3でホスティングされている静的ウェブサイトに移動します。"
      },
      {
        "key": "C",
        "text": "Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes.",
        "text_jp": "AWS Elastic Beanstalkを使用してアプリケーションのフロントエンドをデプロイします。ノードには同じインスタンスタイプを使用します。"
      },
      {
        "key": "D",
        "text": "Change all the backend EC2 instances to Spot Instances.",
        "text_jp": "すべてのバックエンドEC2インスタンスをスポットインスタンスに変更します。"
      },
      {
        "key": "E",
        "text": "Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances.",
        "text_jp": "バックエンドのPythonアプリケーションを、既存のEC2インスタンスと同じコア数を持つ汎用のバースト可能なEC2インスタンスにデプロイします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BE (88%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are B and D. Moving the frontend to S3 for cost savings and changing backend instances to Spot Instances for cost efficiency.",
        "situation_analysis": "The application runs mostly during lunchtime, suggesting a need for cost-effectiveness outside peak hours.",
        "option_analysis": "Option B significantly reduces costs by using S3, while Option D allows leveraging Spot Instances for lower cost without sacrificing performance.",
        "additional_knowledge": "",
        "key_terminology": "Amazon S3, Spot Instances, Application Load Balancer, EC2, static website hosting.",
        "overall_assessment": "This question effectively tests knowledge of cost optimization strategies in AWS while maintaining application availability."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBとDです。フロントエンドをS3に移行することでコストを削減し、バックエンドのインスタンスをスポットインスタンスに変更してコスト効率を高めます。",
        "situation_analysis": "アプリケーションは主に昼食時間に稼働しており、ピーク時間外のコスト効果が求められます。",
        "option_analysis": "選択肢BはS3を活用することでコストを大幅に削減し、選択肢Dはスポットインスタンスを利用して価格を下げることができ、パフォーマンスを犠牲にしません。",
        "additional_knowledge": "",
        "key_terminology": "Amazon S3、スポットインスタンス、Application Load Balancer、EC2、静的ウェブサイトホスティング。",
        "overall_assessment": "この問題は、AWS内でのコスト最適化戦略の知識を効果的に評価し、アプリケーションの可用性を維持する方法を問います。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Spot Instances",
      "Elastic Load Balancing"
    ]
  },
  {
    "No": "152",
    "question": "A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on\nAmazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is\ndeveloping new application features to run on Amazon EKS with AWS Fargate.\nThe platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.\nWhich solution will provide the MOST cost-effective setup for the platform?",
    "question_jp": "ある企業がAWS上でイベントチケットプラットフォームを運営しており、プラットフォームのコスト効率を最適化したいと考えています。プラットフォームはAmazon Elastic Kubernetes Service（Amazon EKS）でAmazon EC2上にデプロイされており、Amazon RDS for MySQLのDBインスタンスにバックアップされています。企業は、AWS Fargate上で動作する新機能をアプリケーションに開発中です。プラットフォームは、需要の高いピークが稀に発生します。その需要の急増はイベントの日時に依存しています。このプラットフォームにとって、最もコスト効率の良いセットアップを提供するソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year.",
        "text_jp": "EKSクラスターが基準負荷に使用するEC2インスタンスのために標準リザーブドインスタンスを購入します。ピーク時に対応するためにクラスターをスポットインスタンスでスケールさせます。年のピーク負荷を満たすために、データベースのために1年の全額前払いリザーブドインスタンスを購入します。"
      },
      {
        "key": "B",
        "text": "Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale out database read replicas during peaks.",
        "text_jp": "EKSクラスターの予測された中間負荷のためにコンピュートセービングスプランを購入します。ピーク時にイベント日時に基づいてオンデマンドキャパシティ予約でクラスターをスケールさせます。予想される基本負荷を満たすためにデータベースのために1年のノーアップフロントリザーブドインスタンスを購入します。ピーク時にデータベースリードレプリカを一時的にスケールアウトします。"
      },
      {
        "key": "C",
        "text": "Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.",
        "text_jp": "EKSクラスターの予測された基本負荷のためにEC2インスタンスセービングスプランを購入します。ピーク時に対応するためにクラスターをスポットインスタンスでスケールさせます。予測された基本負荷を満たすためにデータベースのために1年の全額前払いリザーブドインスタンスを購入します。ピーク時にDBインスタンスを手動で一時的にスケールアップします。"
      },
      {
        "key": "D",
        "text": "Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.",
        "text_jp": "EKSクラスターの予測された基本負荷のためにコンピュートセービングスプランを購入します。ピーク時に対応するためにクラスターをスポットインスタンスでスケールさせます。予測された基本負荷を満たすためにデータベースのために1年の全額前払いリザーブドインスタンスを購入します。ピーク時にDBインスタンスを手動で一時的にスケールアップします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (80%) 9% 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This solution provides a balance of cost-saving measures while also allowing for scalability based on demand during event peaks.",
        "situation_analysis": "The platform has infrequent high peaks in demand that depend on event dates. To optimize costs while accommodating sudden spikes in usage, a flexible pricing strategy is crucial.",
        "option_analysis": "Option A is costly due to upfront purchases of Standard Reserved Instances for both EC2 and RDS without optimal scaling. Option C, while it provides Savings Plans, also includes an All Upfront Reserved Instance for the DB, which may not be needed. Option D is similar to C but has planned scalability that can be delayed during peaks, which may lead to performance issues. Option B, however, proposes to use Compute Savings Plans, which allows for flexible scaling with On-Demand Capacity Reservations during high demand while investing in a No Upfront option, ensuring cost effectiveness.",
        "additional_knowledge": "Utilization of database read replicas during peak demand enhances availability and performance.",
        "key_terminology": "AWS, EC2, EKS, RDS, Compute Savings Plans, No Upfront Reserved Instances, On-Demand Reservations.",
        "overall_assessment": "Option B aligns best with AWS best practices for cost management and is supported by community vote distribution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。このソリューションは、コスト削減策のバランスを提供しながら、イベントのピーク時の需要に応じたスケーラビリティを可能にする。",
        "situation_analysis": "プラットフォームには、イベントの日時に依存した稀な高需要のピークがある。コストを最適化しつつ、突然の使用量の急増に対応するには、柔軟な料金戦略が不可欠である。",
        "option_analysis": "選択肢Aは、EC2とRDSの一括購入のため、コストが高い。選択肢Cは、セービングスプランを提供するが、DB用の全額前払いリザーブドインスタンスを含んでいるため必要がないことも。選択肢DはCに似ているが、ピーク時の計画的スケーラビリティが遅れ、パフォーマンス問題を引き起こす可能性がある。しかし選択肢Bは、コンピュートセービングスプランを用いて、ピーク時のオンデマンドキャパシティ予約による柔軟なスケーリングを提案しており、ノーアップフロントオプションに投資することでコスト効果を確保している。",
        "additional_knowledge": "ピーク時のデータベースリードレプリカの活用は、可用性およびパフォーマンスを向上させる。",
        "key_terminology": "AWS, EC2, EKS, RDS, Compute Savings Plans, No Upfront Reserved Instances, On-Demand Reservations。",
        "overall_assessment": "選択肢Bは、コスト管理におけるAWSのベストプラクティスと最も一致しており、コミュニティの投票配分によっても支持されている。"
      }
    ],
    "keywords": [
      "AWS",
      "EC2",
      "EKS",
      "RDS",
      "Compute Savings Plans",
      "No Upfront Reserved Instances",
      "On-Demand Reservations"
    ]
  },
  {
    "No": "153",
    "question": "A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon\nCloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured\nwith an alternate domain name that visitors use when they access the application.\nEach week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the\ncompany wants visitors to receive an informational message instead of a CloudFront error message.\nA solutions architect creates an Amazon S3 bucket as the first step in the process.\nWhich combination of steps should the solutions architect take next to meet the requirements? (Choose three.)",
    "question_jp": "ある企業は、AWS Elastic Beanstalkにアプリケーションを展開しています。このアプリケーションはデータベースレイヤーとしてAmazon Auroraを使用しています。Amazon CloudFrontディストリビューションがウェブリクエストを提供し、Elastic Beanstalkのドメイン名をオリジンサーバーとして含んでいます。ディストリビューションには、訪問者がアプリケーションにアクセスする際に使用する代替ドメイン名が設定されています。企業は、毎週アプリケーションを定期メンテナンスのためにサービスを停止します。アプリケーションが利用できない間、企業は訪問者がCloudFrontエラーメッセージの代わりに情報メッセージを受け取ることを望んでいます。ソリューションアーキテクトはプロセスの最初のステップとしてAmazon S3バケットを作成します。要件を満たすために、ソリューションアーキテクトは次にどの組み合わせのステップを実行すべきですか？（3つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Upload static informational content to the S3 bucket.",
        "text_jp": "静的な情報コンテンツをS3バケットにアップロードする。"
      },
      {
        "key": "B",
        "text": "Create a new CloudFront distribution. Set the S3 bucket as the origin.",
        "text_jp": "新しいCloudFrontディストリビューションを作成。S3バケットをオリジンとして設定。"
      },
      {
        "key": "C",
        "text": "Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI).",
        "text_jp": "S3バケットを元のCloudFrontディストリビューションの第二のオリジンとして設定。ディストリビューションとS3バケットがオリジンアクセスアイデンティティ（OAI）を使用するように設定する。"
      },
      {
        "key": "D",
        "text": "During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete.",
        "text_jp": "週次メンテナンス中に、デフォルトキャッシュ動作を編集してS3オリジンを使用する。メンテナンスが完了したら変更を元に戻す。"
      },
      {
        "key": "E",
        "text": "During the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \\ Set the precedence to 0. Delete the cache behavior when the maintenance is complete.",
        "text_jp": "週次メンテナンス中に、新しいディストリビューションのS3オリジン用のキャッシュ動作を作成。パスパターンを\\に設定。優先度を0に設定。メンテナンスが完了したらキャッシュ動作を削除する。"
      },
      {
        "key": "F",
        "text": "During the weekly maintenance, configure Elastic Beanstalk to serve trafic from the S3 bucket.",
        "text_jp": "週次メンテナンス中に、Elastic Beanstalkを設定してS3バケットからトラフィックを提供する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A 'Upload static informational content to the S3 bucket.' This step directly addresses the need to provide visitors with informative content during application downtime.",
        "situation_analysis": "The scenario outlines a requirement for a message to be displayed to users during maintenance periods, indicating that the application is not available.",
        "option_analysis": "Option A is necessary, as it uploads the required content. Option B creates a redundant distribution, option C complicates the configuration unnecessarily, option D involves temporary changes that could lead to errors, option E introduces another complexity with a new distribution, and option F is not a valid approach during maintenance.",
        "additional_knowledge": "Having static content prepared allows for quick and efficient communication with users during times when the main application is not operational.",
        "key_terminology": "Amazon S3, CloudFront, Static Content, Origin Server, Cache Behavior",
        "overall_assessment": "This question tests knowledge on handling application downtime effectively using AWS services and ensures understanding of the appropriate configuration steps."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA「静的な情報コンテンツをS3バケットにアップロードする。」です。このステップは、アプリケーションのダウンタイム中に訪問者に情報を提供する必要性に直接対処します。",
        "situation_analysis": "シナリオは、メンテナンス期間中にユーザーにメッセージを表示する要件を示しており、アプリケーションが利用できないことを示しています。",
        "option_analysis": "選択肢Aは、必要なコンテンツをアップロードするために必須です。選択肢Bは冗長なディストリビューションを作成し、選択肢Cは設定を不必要に複雑にします。選択肢Dはエラーの原因となる可能性のある一時的な変更を含むため不適切です。選択肢Eは、新しいディストリビューションによる別の複雑さを導入し、選択肢Fはメンテナンス中には有効なアプローチではありません。",
        "additional_knowledge": "静的コンテンツを準備しておくことで、主要なアプリケーションが運用されていないときでも迅速かつ効率的にユーザーとコミュニケーションを取ることが可能です。",
        "key_terminology": "Amazon S3, CloudFront, 静的コンテンツ, オリジンサーバー, キャッシュ動作",
        "overall_assessment": "この質問は、AWSサービスを利用してアプリケーションのダウンタイムを効果的に処理する知識を試し、適切な設定手順の理解を確保します。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "CloudFront",
      "Static Content",
      "Origin Server",
      "Cache Behavior"
    ]
  },
  {
    "No": "154",
    "question": "A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that\nprocesses and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN.\nThe Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment\nvariables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new\nfunction version with the updated environment variables after validating results. This update process also requires frequent changes to the\ncustom application to invoke the new function version ARN. These changes cause interruptions for users.\nA solutions architect needs to simplify this process to minimize disruption to users.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "会社はユーザーに独自のアプリケーションから画像をアップロードする機能を提供しています。アップロードプロセスは、画像を処理し、Amazon S3バケットに保存するAWS Lambda関数を呼び出します。アプリケーションは特定の関数バージョンARNを使用してLambda関数を呼び出します。Lambda関数は、環境変数を使用して画像処理のパラメータを受け取ります。会社は、最適な画像処理出力を達成するためにLambda関数の環境変数を頻繁に調整します。会社は異なるパラメータをテストし、結果を検証した後、更新された環境変数を持つ新しい関数バージョンを公開します。この更新プロセスは、新しい関数バージョンARNを呼び出すためにカスタムアプリケーションに頻繁に変更が必要であり、これによりユーザーに中断が生じます。ソリューションアーキテクトは、ユーザーへの影響を最小限に抑えるためにこのプロセスを簡素化する必要があります。どのソリューションが、最小の運用負荷でこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Directly modify the environment variables of the published Lambda function version. Use the SLATEST version to test image processing parameters.",
        "text_jp": "公開されたLambda関数バージョンの環境変数を直接修正します。SLATESTバージョンを使用して画像処理パラメータをテストします。"
      },
      {
        "key": "B",
        "text": "Create an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image processing parameters from the DynamoDB table.",
        "text_jp": "画像処理パラメータを保存するためのAmazon DynamoDBテーブルを作成します。Lambda関数を修正して、DynamoDBテーブルから画像処理パラメータを取得します。"
      },
      {
        "key": "C",
        "text": "Directly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function version when the company updates the parameters.",
        "text_jp": "画像処理パラメータをLambda関数内に直接記述し、環境変数を削除します。会社がパラメータを更新する際に新しい関数バージョンを公開します。"
      },
      {
        "key": "D",
        "text": "Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing.",
        "text_jp": "Lambda関数のエイリアスを作成します。クライアントアプリケーションを修正して関数エイリアスARNを使用します。会社がテストを終了したら、Lambdaエイリアスを新しい関数のバージョンに再構成します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Creating a Lambda function alias allows for easier management of function versions without changing the client application each time.",
        "situation_analysis": "The company often changes Lambda function environment variables to optimize image processing which causes user disruptions due to frequent modifications in the application.",
        "option_analysis": "Option D allows the client to dynamically point to different versions without needing to alter the application each time an update is made. Option A still requires direct changes to Lambda which may cause disruptions. Option B adds unnecessary complexity by introducing DynamoDB, and option C negates the advantage of using functions with environment variables.",
        "additional_knowledge": "It is best practice in AWS to avoid direct modifications in production environments which could lead to instability.",
        "key_terminology": "AWS Lambda, function version, Lambda alias, operational overhead, Amazon S3.",
        "overall_assessment": "Option D is the most efficient solution as it minimizes disruptions while allowing for testing and deployment flexibility."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。Lambda関数のエイリアスを作成すると、クライアントアプリケーションを変更することなく関数のバージョンをより簡単に管理できます。",
        "situation_analysis": "会社は画像処理を最適化するためにLambda関数の環境変数を頻繁に変更しており、これによりアプリケーションの変更によってユーザーに中断が発生しています。",
        "option_analysis": "選択肢Dは、更新のたびにアプリケーションを変更する必要がなく、クライアントが異なるバージョンに動的にポイントできるようにします。選択肢AはLambdaを直接変更する必要があるため中断を引き起こす可能性があります。選択肢BはDynamoDBを導入することで不必要な複雑さを追加し、選択肢Cは環境変数を使用する利点を否定します。",
        "additional_knowledge": "AWSでは、プロダクション環境での直接変更は不安定を引き起こす可能性があるため、避けることがベストプラクティスです。",
        "key_terminology": "AWS Lambda, 関数バージョン, Lambdaエイリアス, 運用負荷, Amazon S3。",
        "overall_assessment": "選択肢Dは中断を最小限に抑えつつ、テストと展開の柔軟性を提供する最も効率的なソリューションです。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "function version",
      "Lambda alias",
      "operational overhead",
      "Amazon S3"
    ]
  },
  {
    "No": "155",
    "question": "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to\nkeep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application\nLoad Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex\ndomain.\nWhich solution will meet these requirements with the LEAST effort?",
    "question_jp": "グローバルメディア企業が、アプリケーションのマルチリージョンデプロイメントを計画しています。Amazon DynamoDBグローバルテーブルがデプロイメントを支え、ユーザーが集中している二つの大陸でのユーザー体験の一貫性を保つ予定です。各デプロイメントにはパブリックアプリケーションロードバランサー（ALB）が設置されます。企業は公的DNSを内部で管理しています。企業はアペックスドメインを介してアプリケーションを利用可能にしたいと考えています。最小限の労力でこの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route trafic based on user location.",
        "text_jp": "パブリックDNSをAmazon Route 53に移行します。アペックスドメインがALBを指すCNAMEレコードを作成します。ユーザーの位置情報に基づいてトラフィックをルーティングするための地理的位置ルーティングポリシーを使用します。"
      },
      {
        "key": "B",
        "text": "Place a Network Load Balancer (NLB) in front of the ALMigrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB's static IP address. Use a geolocation routing policy to route trafic based on user location.",
        "text_jp": "Network Load Balancer（NLB）をALBの前に配置し、パブリックDNSをAmazon Route 53に移行します。アペックスドメインがNLBの静的IPアドレスを指すCNAMEレコードを作成します。ユーザーの位置情報に基づいてトラフィックをルーティングする地理的位置ルーティングポリシーを使用します。"
      },
      {
        "key": "C",
        "text": "Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator's static IP address to create a record in public DNS for the apex domain.",
        "text_jp": "AWS Global Acceleratorを作成し、適切なAWSリージョンのエンドポイントをターゲットとした複数のエンドポイントグループを設定します。Global Acceleratorの静的IPアドレスを使用して、アペックスドメインのパブリックDNSにレコードを作成します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route trafic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL.",
        "text_jp": "AWSリージョンの一つにAWS Lambdaでバックエンドを持つAmazon API Gateway APIを作成します。アプリケーションデプロイメントへのトラフィックをラウンドロビン方式でルーティングするためのLambda関数を構成します。APIのURLを指すCNAMEレコードをアペックスドメインに対して作成します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using AWS Global Accelerator allows for reduced latency and improved performance for global applications by leveraging static IP addresses.",
        "situation_analysis": "The company needs to deploy an application across multiple regions and ensure a consistent user experience. Public DNS management internally adds complexity to the deployment, especially when handling apex domains.",
        "option_analysis": "Option C is the simplest solution as it utilizes AWS Global Accelerator, which is designed for global applications. Options A and B require additional record setups and complexities in DNS management. Option D introduces unnecessary layers with API Gateway and Lambda, which isn't needed for direct user traffic routing.",
        "additional_knowledge": "AWS Global Accelerator provides two static IP addresses that can be used to route traffic to the ALB's endpoints, ensuring high availability and low latency regardless of the user's location.",
        "key_terminology": "Global Accelerator, ALB, static IP, apex domain, global applications.",
        "overall_assessment": "Choosing C simplifies the architecture and configuration while maximizing performance for users in different regions. The community overwhelmingly supports this choice as it best aligns with the requirements and AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。AWS Global Acceleratorを使用することで、静的IPアドレスを活用して全球的なアプリケーションの遅延を減少させ、パフォーマンスを向上させることができます。",
        "situation_analysis": "企業はアプリケーションを複数のリージョンにデプロイし、ユーザー体験の一貫性を確保する必要があります。公的DNSの内部管理は、特にアペックスドメインを扱う際にデプロイメントに複雑さをもたらします。",
        "option_analysis": "CはAWS Global Acceleratorを利用していて、グローバルアプリケーションに特化した簡単なソリューションです。AおよびBは追加のレコード設定やDNS管理の複雑さを伴います。DはAPI GatewayとLambdaという不要な層を追加し、直接ユーザートラフィックのルーティングに必要ありません。",
        "additional_knowledge": "AWS Global Acceleratorは、ALBのエンドポイントにトラフィックをルーティングするために使用可能な二つの静的IPアドレスを提供し、ユーザーの位置に関わらず高い可用性と低い遅延を保証します。",
        "key_terminology": "Global Accelerator、ALB、静的IP、アペックスドメイン、グローバルアプリケーション。",
        "overall_assessment": "Cを選択することで、アーキテクチャと設定が簡素化され、異なるリージョンのユーザーに対してパフォーマンスが最大化されます。コミュニティはこの選択を圧倒的に支持しており、要件やAWSのベストプラクティスに最も合致しています。"
      }
    ],
    "keywords": [
      "Global Accelerator",
      "ALB",
      "static IP",
      "apex domain",
      "global applications"
    ]
  },
  {
    "No": "156",
    "question": "A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions\nwith API Gateway to use several shared libraries and custom classes.\nA solutions architect needs to simplify the deployment of the solution and optimize for code reuse.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、Amazon API GatewayとAWS Lambdaを利用して新しいサーバーレスAPIを開発しています。企業は、API GatewayとLambda関数を統合して、いくつかの共有ライブラリやカスタムクラスを使用しています。ソリューションアーキテクトは、ソリューションの展開を簡素化し、コードの再利用を最適化する必要があります。どのソリューションがこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
        "text_jp": "共有ライブラリとカスタムクラスをDockerイメージにデプロイします。イメージをS3バケットに保存します。Dockerイメージをソースとして使用するLambdaレイヤーを作成します。APIのLambda関数をZipパッケージとしてデプロイします。パッケージをLambdaレイヤーを使用するように設定します。"
      },
      {
        "key": "B",
        "text": "Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
        "text_jp": "共有ライブラリとカスタムクラスをDockerイメージにデプロイします。イメージをAmazon Elastic Container Registry（Amazon ECR）にアップロードします。Dockerイメージをソースとして使用するLambdaレイヤーを作成します。APIのLambda関数をZipパッケージとしてデプロイします。パッケージをLambdaレイヤーを使用するように設定します。"
      },
      {
        "key": "C",
        "text": "Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer.",
        "text_jp": "共有ライブラリとカスタムクラスを、AWS Fargate起動タイプを使用してAmazon Elastic Container Service（Amazon ECS）のDockerコンテナにデプロイします。APIのLambda関数をZipパッケージとしてデプロイします。パッケージをデプロイされたコンテナをLambdaレイヤーとして使用するように設定します。"
      },
      {
        "key": "D",
        "text": "Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.",
        "text_jp": "共有ライブラリ、カスタムクラス、およびAPIのLambda関数のコードをDockerイメージにデプロイします。イメージをAmazon Elastic Container Registry（Amazon ECR）にアップロードします。APIのLambda関数をデプロイメントパッケージとしてDockerイメージを使用するように設定します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (65%) B (35%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This solution optimally uses Docker images alongside AWS services for code reuse and simplifies deployments.",
        "situation_analysis": "The company wants to leverage shared libraries and custom classes while minimizing deployment complexity and maximizing reuse across Lambda functions.",
        "option_analysis": "Option A uses S3, which is less optimal than using ECR for Docker images. Option C utilizes ECS instead of Lambda layers, complicating the architecture. Option D deploys the code together instead of separating reusable components, hindering reuse.",
        "additional_knowledge": "AWS Lambda now supports up to five layers per function, which can enhance the modularity and maintainability of serverless applications.",
        "key_terminology": "Lambda layer, Docker image, Amazon ECR, code reuse, deployment simplification.",
        "overall_assessment": "Despite community preference for option D, option B is the recommended choice based on best practices for serverless applications. Community votes may reflect familiarity rather than optimal practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。このソリューションは、コードの再利用を最適化し、AWSサービスと共にDockerイメージを使用することでデプロイメントを簡素化する。",
        "situation_analysis": "企業は、共有ライブラリやカスタムクラスを活用しつつ、デプロイの複雑さを抑え、Lambda関数間での再利用を最大化したいと考えている。",
        "option_analysis": "選択肢AはS3を使用し、Dockerイメージのための最適な選択肢ではない。選択肢Cは、Lambdaレイヤーの代わりにECSを利用しており、アーキテクチャを複雑化させている。選択肢Dは、コードを一緒にデプロイするため、再利用可能なコンポーネントを分離することができず、再利用が妨げられる。",
        "additional_knowledge": "AWS Lambdaは現在、関数ごとに最大5つのレイヤーをサポートしており、サーバーレスアプリケーションのモジュラリティと保守性を高めることができる。",
        "key_terminology": "Lambdaレイヤー、Dockerイメージ、Amazon ECR、コードの再利用、デプロイの簡素化。",
        "overall_assessment": "コミュニティの選好は選択肢Dであるが、選択肢Bがサーバーレスアプリケーションにおけるベストプラクティスに基づく推奨される選択肢である。コミュニティ投票は、最適な実践に対する親しみを反映している可能性がある。"
      }
    ],
    "keywords": [
      "Lambda layer",
      "Docker image",
      "Amazon ECR",
      "code reuse",
      "deployment simplification"
    ]
  },
  {
    "No": "157",
    "question": "A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The\ncompany has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images.\nThe company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback\neven if the factory's internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the\nworkers.\nHow should the company deploy the ML model to meet these requirements?",
    "question_jp": "製造会社は工場のための検査ソリューションを構築しています。この会社は各組立ラインの端にIPカメラを設置しています。会社は、静止画像から一般的な欠陥を特定するためにAmazon SageMakerを使用して機械学習（ML）モデルを訓練しました。この会社は、欠陥が検出されたときに工場労働者にローカルフィードバックを提供したいと考えています。工場のインターネット接続がダウンしていても、このフィードバックを提供できなければなりません。この会社は、労働者にローカルフィードバックを提供するAPIをホストするローカルLinuxサーバーを持っています。会社はこれらの要件を満たすために、MLモデルをどのようにデプロイすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams. Upload the images to an Amazon S3 bucket. Deploy a SageMaker endpoint with the ML model. Invoke an AWS Lambda function to call the inference endpoint when new images are uploaded. Configure the Lambda function to call the local API when a defect is detected.",
        "text_jp": "それぞれのIPカメラからAWSへのAmazon Kinesisビデオストリームを設定します。Amazon EC2インスタンスを使用してストリームの静止画像を取得し、画像をAmazon S3バケットにアップロードします。MLモデルでSageMakerエンドポイントをデプロイします。新しい画像がアップロードされるときに推論エンドポイントを呼び出すためにAWS Lambda関数を呼び出します。欠陥が検出されたときにローカルAPIを呼び出すようにLambda関数を構成します。"
      },
      {
        "key": "B",
        "text": "Deploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still images from the cameras and run inference. Configure the component to call the local API when a defect is detected.",
        "text_jp": "ローカルサーバーにAWS IoT Greengrassをデプロイします。GreengrassサーバーにMLモデルをデプロイします。カメラから静止画像を取得し、推論を実行するGreengrassコンポーネントを作成します。欠陥が検出されたときにローカルAPIを呼び出すようにコンポーネントを構成します。"
      },
      {
        "key": "C",
        "text": "Order an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still images from the cameras. Run inference from the EC2 instance. Configure the instance to call the local API when a defect is detected.",
        "text_jp": "AWS Snowballデバイスを注文します。Snowballデバイス上にSageMakerエンドポイントとMLモデル、Amazon EC2インスタンスをデプロイします。カメラから静止画像を取得し、EC2インスタンスから推論を実行します。欠陥が検出されたときにローカルAPIを呼び出すようにインスタンスを構成します。"
      },
      {
        "key": "D",
        "text": "Deploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the Amazon Monitron devices. Use Amazon Monitron health state alarms to call the local API from an AWS Lambda function when a defect is detected.",
        "text_jp": "それぞれのIPカメラにAmazon Monitronデバイスをデプロイします。オンプレミスにAmazon Monitron Gatewayをデプロイします。MLモデルをAmazon Monitronデバイスにデプロイします。欠陥が検出されたときにAWS Lambda関数からローカルAPIを呼び出すためにAmazon Monitron健康状態アラームを使用します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using Amazon Monitron devices along with a local Monitron Gateway allows for local processing and immediate feedback to factory workers, even when internet connectivity is down.",
        "situation_analysis": "The company requires a solution that allows for local feedback without internet dependency, utilizing existing local infrastructure.",
        "option_analysis": "Option D is the only solution that meets all requirements by using on-premises devices for detection and local processing. Options A and B rely on internet connectivity, while C introduces additional complexity with the Snowball device.",
        "additional_knowledge": "Utilizing AWS services for edge computing ensures efficient data handling and minimizes latency by processing data closer to its source.",
        "key_terminology": "Amazon Monitron, Monitron Gateway, AWS Lambda, local processing, machine learning, industrial IoT",
        "overall_assessment": "Despite community vote skewing towards option B, option D is more suitable for the stated requirements and ensures local processing which is crucial for the factory's operation."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。Amazon MonitronデバイスとローカルMonitron Gatewayを使用することで、インターネット接続がない場合でも工場労働者へのローカル処理と即時フィードバックが可能になる。",
        "situation_analysis": "会社は、インターネットに依存せずローカルフィードバックを提供できるソリューションを求めており、既存のローカルインフラストラクチャを利用することが求められている。",
        "option_analysis": "オプションDは、検出とローカル処理にオンプレミスデバイスを使用する唯一のソリューションであり、すべての要件を満たしている。オプションAとBはインターネット接続に依存しているのに対し、CはSnowballデバイスにより複雑さを増している。",
        "additional_knowledge": "AWSのエッジコンピューティングサービスを利用することで、データ処理が効率的になり、データソースの近くで処理されることでレイテンシが最小化される。",
        "key_terminology": "Amazon Monitron, Monitron Gateway, AWS Lambda, ローカル処理, 機械学習, 産業IoT",
        "overall_assessment": "コミュニティの投票がオプションBに偏っているにもかかわらず、Dは示された要件に対して適切であり、工場の運用においてはローカル処理が重要である。"
      }
    ],
    "keywords": [
      "Amazon Monitron",
      "Monitron Gateway",
      "AWS Lambda",
      "local processing",
      "machine learning",
      "industrial IoT"
    ]
  },
  {
    "No": "158",
    "question": "A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect\nwill use a configuration management database (CMDB) export of all the company's servers to create the case.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ソリューションアーキテクトは、企業のオンプレミスデータセンターをAWSクラウドに移行するためのビジネスケースを作成する必要があります。ソリューションアーキテクトは、会社の全サーバーの構成管理データベース（CMDB）エクスポートを使用してケースを作成します。どのソリューションが最もコスト効果的にこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations.",
        "text_jp": "AWS Well-Architected Toolを使用してCMDBデータをインポートし、分析を行い推奨事項を生成します。"
      },
      {
        "key": "B",
        "text": "Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.",
        "text_jp": "Migration Evaluatorを使用して分析を行います。データインポートテンプレートを使用してCMDBエクスポートからデータをアップロードします。"
      },
      {
        "key": "C",
        "text": "Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in bulk.",
        "text_jp": "リソースマッチングルールを実装します。CMDBエクスポートとAWS Price List Bulk APIを使用して、CMDBデータをAWSサービスに対して一括でクエリします。"
      },
      {
        "key": "D",
        "text": "Use AWS Application Discovery Service to import the CMDB data to perform an analysis.",
        "text_jp": "AWS Application Discovery Serviceを使用してCMDBデータをインポートし、分析を行います。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (85%) D (15%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. AWS Application Discovery Service is specifically designed to collect and analyze information about on-premises data centers, making it the most suitable and cost-effective solution for creating a business case for migration.",
        "situation_analysis": "The requirement is to migrate an on-premises data center to AWS and analyze existing server configurations, which is directly addressed by AWS Application Discovery Service.",
        "option_analysis": "Option D is correct as it directly imports CMDB data for analysis. Option A does not specifically support CMDB imports. Option B, while useful, may not be as thorough as required. Option C involves additional complexity and might not yield the most cost-effective results.",
        "additional_knowledge": "AWS Application Discovery Service automates the process of gathering and analyzing data about an organization's on-premises servers and applications, which is essential for migration planning.",
        "key_terminology": "AWS Application Discovery Service, CMDB, migration analysis, AWS Cloud, cost-effective solutions",
        "overall_assessment": "In summary, option D is the best choice given the context of the question. Despite the community vote leaning towards option B, a deeper analysis validates option D as optimal for the described scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。AWS Application Discovery Serviceは、オンプレミスデータセンターに関する情報を収集し分析するために特別に設計されており、移行のためのビジネスケースを作成するのに最も適切かつコスト効果の高いソリューションである。",
        "situation_analysis": "オンプレミスデータセンターをAWSに移行し、既存のサーバー構成を分析する必要があるという要件は、AWS Application Discovery Serviceによって直接対処される。",
        "option_analysis": "選択肢Dは、CMDBデータをインポートして分析を行うため、正しい。選択肢AはCMDBインポートを特にサポートしていない。選択肢Bは有用だが、要求されるほど徹底的でない可能性がある。選択肢Cは追加の複雑さを伴い、最もコスト効果的な結果を得られない可能性がある。",
        "additional_knowledge": "AWS Application Discovery Serviceは、組織のオンプレミスサーバーとアプリケーションに関するデータを収集・分析するプロセスを自動化し、移行計画に不可欠である。",
        "key_terminology": "AWS Application Discovery Service, CMDB, 移行分析, AWS Cloud, コスト効果の高いソリューション",
        "overall_assessment": "要約すると、選択肢Dが質問の文脈から見て最良の選択肢である。コミュニティの投票が選択肢Bに傾いているにもかかわらず、より深い分析が選択肢Dを検証し、このシナリオに最適であることを示している。"
      }
    ],
    "keywords": [
      "AWS Application Discovery Service",
      "CMDB",
      "migration analysis",
      "AWS Cloud",
      "cost-effective solutions"
    ]
  },
  {
    "No": "159",
    "question": "A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling\ngroup. The ALB is associated with an AWS WAF web ACL.\nThe website often encounters attacks in the application layer. The attacks produce sudden and significant increases in trafic on the application\nserver. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to\nmitigate these attacks.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業のウェブサイトは、Amazon EC2インスタンス上で動作しており、アプリケーションロードバランサー（ALB）の背後にあります。このインスタンスはオートスケーリンググループに配置されています。ALBにはAWS WAFのWeb ACLが関連付けられています。ウェブサイトはアプリケーション層での攻撃をしばしば受けており、これによりアプリケーションサーバー上でのトラフィックが突然かつ大幅に増加します。アクセスログには、各攻撃が異なるIPアドレスから発生していることが示されています。ソリューションアーキテクトは、これらの攻撃を軽減するためのソリューションを実装する必要があります。運用負荷が最も少ない要件を満たすすべてのソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm action that adds the IP address to the web ACL's deny list.",
        "text_jp": "Amazon CloudWatchアラームを作成してサーバーアクセスを監視します。IPアドレスによるアクセスに基づいてしきい値を設定します。アラームアクションを構成して、IPアドレスをWeb ACLの拒否リストに追加します。"
      },
      {
        "key": "B",
        "text": "Deploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource.",
        "text_jp": "AWS WAFに加えて、AWS Shield Advancedを導入します。ALBを保護されたリソースとして追加します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application server's subnet route table for any IP addresses that activate the alarm.",
        "text_jp": "ユーザーIPアドレスを監視するAmazon CloudWatchアラームを作成します。IPアドレスによるアクセスに基づいてしきい値を設定します。アラームを構成して、アラームをトリガーしたIPアドレスに対してアプリケーションサーバーのサブネットルートテーブルに拒否ルールを追加するAWS Lambda関数を呼び出します。"
      },
      {
        "key": "D",
        "text": "Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny trafic from the countries that host those IP addresses.",
        "text_jp": "アクセスログを検査して攻撃を行ったIPアドレスのパターンを見つけます。Amazon Route 53の地理位置ルーティングポリシーを使用して、これらのIPアドレスをホストしている国からのトラフィックを拒否します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is option C. This approach effectively automates the process of identifying and blocking attackers with minimal operational overhead.",
        "situation_analysis": "The attacks are originating from many different IP addresses, making it challenging to block them manually. The goal is to implement a scalable solution with low operational overhead.",
        "option_analysis": "Option A would require manual intervention to update the deny list as it relies on access logs. Option B enhances DDoS protection but does not specifically address the application layer attacks. Option D, while proactive, still requires manual analysis of logs, which adds operational overhead.",
        "additional_knowledge": "Using architecture patterns that incorporate automation can greatly enhance the security posture of web applications.",
        "key_terminology": "AWS WAF, Amazon CloudWatch, AWS Lambda, DDoS, application security",
        "overall_assessment": "Option C is aligned with best practices for mitigating layer 7 attacks and leverages AWS services to reduce the workload on operations teams. Despite community support for option B, option C remains the optimal choice for the specific scenario described."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は選択肢Cです。このアプローチは、運用負荷を最小限に抑えながら、攻撃者を特定してブロックするプロセスを自動化します。",
        "situation_analysis": "攻撃は多数の異なるIPアドレスから発生しており、手動でこれらをブロックすることは困難です。目標は、運用負荷が少ないスケーラブルなソリューションを実装することです。",
        "option_analysis": "選択肢Aは、アクセスログに依存しており、拒否リストの更新に手動介入が必要です。選択肢BはDDoS保護を強化しますが、アプリケーション層の攻撃には具体的に対処していません。選択肢Dは先手を打つ手法ですが、依然としてログの手動分析が必要で、運用負荷が増加します。",
        "additional_knowledge": "自動化を組み込んだアーキテクチャパターンを使用することで、ウェブアプリケーションのセキュリティ姿勢を大幅に向上させることができます。",
        "key_terminology": "AWS WAF, Amazon CloudWatch, AWS Lambda, DDoS, アプリケーションセキュリティ",
        "overall_assessment": "選択肢Cは、レイヤー7攻撃を軽減するためのベストプラクティスに沿っており、AWSサービスを活用して運用チームの負担を軽減します。コミュニティの支持が選択肢Bに集中していても、特定の状況においては選択肢Cが最良の選択です。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "Amazon CloudWatch",
      "AWS Lambda",
      "DDoS",
      "application security"
    ]
  },
  {
    "No": "160",
    "question": "A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and\nan Amazon Aurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already\ndeployed in two Regions.\nCompany policy states that critical applications must have application tier components and data tier components deployed across two Regions.\nThe RTO and RPO must be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant\nwith company policy.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "question_jp": "ある企業が、データ層が単一のAWSリージョンに展開されている重要なアプリケーションを持っています。データ層は、Amazon DynamoDBテーブルとAmazon Aurora MySQL DBクラスタを使用しています。現在のAurora MySQLエンジンバージョンは、グローバルデータベースをサポートしています。アプリケーション層はすでに二つのリージョンに展開されています。企業の方針では、重要なアプリケーションにはアプリケーション層のコンポーネントとデータ層のコンポーネントが二つのリージョンに展開されなければなりません。RTOとRPOはそれぞれ数分を超えてはなりません。ソリューションアーキテクトは、データ層が企業方針に準拠するためのソリューションを推奨する必要があります。どのステップの組み合わせがこれらの要件を満たすでしょうか？（二つ選んでください）",
    "choices": [
      {
        "key": "A",
        "text": "Add another Region to the Aurora MySQL DB cluster",
        "text_jp": "Aurora MySQL DBクラスタに別のリージョンを追加する"
      },
      {
        "key": "B",
        "text": "Add another Region to each table in the Aurora MySQL DB cluster",
        "text_jp": "Aurora MySQL DBクラスタ内の各テーブルに別のリージョンを追加する"
      },
      {
        "key": "C",
        "text": "Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster",
        "text_jp": "DynamoDBテーブルとAurora MySQL DBクラスタのために、定期的なクロスリージョンバックアップを設定する"
      },
      {
        "key": "D",
        "text": "Convert the existing DynamoDB table to a global table by adding another Region to its configuration",
        "text_jp": "既存のDynamoDBテーブルを構成に別のリージョンを追加することによってグローバルテーブルに変換する"
      },
      {
        "key": "E",
        "text": "Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region",
        "text_jp": "Amazon Route 53 Application Recovery Controllerを使用して、二次リージョンへのデータベースバックアップと復旧を自動化する"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "AD (83%) AC (17%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves adding another region to each table in the Aurora MySQL DB cluster. This step is essential for achieving compliance with the company's policy that mandates data tier components be deployed across multiple regions.",
        "situation_analysis": "The company has a critical application that requires its data tier components to be spread across two regions, and the existing architecture only deploys these components in one region. Therefore, migrating data to additional regions is a necessity.",
        "option_analysis": "Option B correctly aligns with the organizational requirements. Options A, C, D, and E either do not address the need for a multi-region deployment of the data tier or do not meet the requirements regarding RTO and RPO specifications.",
        "additional_knowledge": "Understanding the concepts of RTO and RPO is critical in designing fault-tolerant applications.",
        "key_terminology": "Amazon DynamoDB, Amazon Aurora MySQL, Cross-region Replication, RTO, RPO",
        "overall_assessment": "Overall, option B is the best choice. The community vote suggests that options A and D are also considered but do not comprehensively fulfill the requirement. Adopting a multi-region strategy through options like B, in conjunction with backup solutions, is crucial for maintaining compliance with business continuity policies."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBであり、Aurora MySQL DBクラスタ内の各テーブルに別のリージョンを追加することを含みます。このステップは、データ層コンポーネントを複数のリージョンに展開することを義務づける企業方針に準拠するために不可欠です。",
        "situation_analysis": "この企業は、重要なアプリケーションを持ち、そのデータ層のコンポーネントが二つのリージョンに分散配置される必要がありますが、現在のアーキテクチャは一つのリージョンにしか展開されていません。したがって、データを追加のリージョンに移行する必要があります。",
        "option_analysis": "オプションBは、組織の要件に適切に沿っています。オプションA、C、D、およびEは、データ層のマルチリージョン展開の必要性に対処していないか、RTOおよびRPOの仕様を満たしていません。",
        "additional_knowledge": "RTOとRPOの概念を理解することは、フォールトトレラントアプリケーションの設計において重要です。",
        "key_terminology": "Amazon DynamoDB, Amazon Aurora MySQL, クロスリージョンレプリケーション, RTO, RPO",
        "overall_assessment": "全体として、オプションBが最良の選択です。コミュニティの投票は、オプションAおよびDも考慮されていることを示唆していますが、要件を包括的に満たしていません。Bのようなマルチリージョン戦略を採用し、バックアップソリューションと組み合わせることが、ビジネス継続性ポリシーを遵守する上で重要です。"
      }
    ],
    "keywords": [
      "Amazon DynamoDB",
      "Amazon Aurora MySQL",
      "Cross-region Replication",
      "RTO",
      "RPO"
    ]
  },
  {
    "No": "161",
    "question": "A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the\ncompany's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones\nbehind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS\nterminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path.\nThe company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must\ndevelop a solution to allow trafic fiow to AWS from the on-premises network so that the clients can continue to access the application.\nWhich solution will meet these requirements?",
    "question_jp": "ある通信会社がAWS上でアプリケーションを運用している。この会社は、オンプレミスのデータセンターとAWSとの間にAWS Direct Connect接続をセットアップした。会社は、内部アプリケーションロードバランサー（ALB）の背後に複数のアベイラビリティーゾーンのAmazon EC2インスタンスにアプリケーションを展開している。会社のクライアントは、HTTPSを使用してオンプレミスネットワークから接続している。TLSはALBで終端されている。会社は複数のターゲットグループを持ち、URLパスに基づいてリクエストを転送するためにパスベースのルーティングを使用している。会社は、IPアドレスに基づく許可リストを持つオンプレミスのファイアウォールアプライアンスを展開する予定である。ソリューションアーキテクトは、クライアントがアプリケーションに引き続きアクセスできるように、オンプレミスネットワークからAWSへのトラフィックフローを許可するソリューションを開発する必要がある。どのソリューションがこれらの要件を満たすか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP addresses to the firewall appliance.",
        "text_jp": "既存のALBを構成して静的IPアドレスを使用する。ALBに複数のアベイラビリティーゾーンでIPアドレスを割り当てる。ALBのIPアドレスをファイアウォールアプライアンスに追加する。"
      },
      {
        "key": "B",
        "text": "Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB.",
        "text_jp": "Network Load Balancer（NLB）を作成する。NLBに複数のアベイラビリティーゾーンで1つの静的IPアドレスを関連付ける。NLBのためのALBタイプのターゲットグループを作成し、既存のALBを追加する。NLBのIPアドレスをファイアウォールアプライアンスに追加する。クライアントをNLBに接続するように更新する。"
      },
      {
        "key": "C",
        "text": "Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing target groups to the NLB. Update the clients to connect to the NLB. Delete the ALB Add the NLB IP addresses to the firewall appliance.",
        "text_jp": "Network Load Balancer（NLB）を作成する。NLBに複数のアベイラビリティーゾーンで1つの静的IPアドレスを関連付ける。既存のターゲットグループをNLBに追加する。クライアントをNLBに接続するように更新する。ALBを削除する。NLBのIPアドレスをファイアウォールアプライアンスに追加する。"
      },
      {
        "key": "D",
        "text": "Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB. Add the GWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB.",
        "text_jp": "Gateway Load Balancer（GWLB）を作成する。GWLBに複数のアベイラビリティーゾーンで静的IPアドレスを割り当てる。GWLBのためのALBタイプのターゲットグループを作成し、既存のALBを追加する。GWLBのIPアドレスをファイアウォールアプライアンスに追加する。クライアントをGWLBに接続するように更新する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (93%) 3%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Configuring the existing ALB to use static IP addresses and assigning IP addresses in multiple Availability Zones ensures that the ALB can be recognized by the on-premises firewall, allowing uninterrupted access for clients.",
        "situation_analysis": "The requirement is to enable traffic from the on-premises network to the AWS application while ensuring that the firewall allows this traffic based on static IP addresses.",
        "option_analysis": "Option A provides a direct solution by configuring the ALB with static IP addresses that can be whitelisted in the on-premises firewall. Options B, C, and D either introduce additional layers or require significant changes to the architecture, which would not directly satisfy the requirement.",
        "additional_knowledge": "If the NLB was used instead, it could provide static IP options, but it complicates the architecture and does not leverage the existing ALB capabilities.",
        "key_terminology": "AWS Direct Connect, Application Load Balancer, static IP, firewall, traffic flow",
        "overall_assessment": "Option A targets the specific requirement efficiently. Despite community support leaning towards option B, option A ensures compliance with the firewall's requirements as described."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。既存のALBを静的IPアドレスを使用するように構成し、複数のアベイラビリティーゾーンにIPアドレスを割り当てることで、ALBがオンプレミスのファイアウォールによって認識され、クライアントのアクセスが途切れずに行える。",
        "situation_analysis": "要求は、オンプレミスネットワークからAWSアプリケーションへのトラフィックを有効にし、ファイアウォールがこのトラフィックを静的IPアドレスに基づいて許可する必要がある。",
        "option_analysis": "選択肢Aは、オンプレミスのファイアウォールにホワイトリストされる静的IPアドレスを使用してALBを構成することにより、直接的な解決策を提供する。選択肢B、C、およびDは、追加のレイヤーを導入したり、アーキテクチャに重大な変更を必要としたりするため、要求を直接満たすことにはならない。",
        "additional_knowledge": "もしNLBが使用された場合、静的IPオプションを提供できるが、アーキテクチャが複雑になり、既存のALBの機能を活用できなくなる。",
        "key_terminology": "AWS Direct Connect、アプリケーションロードバランサー、静的IP、ファイアウォール、トラフィックフロー",
        "overall_assessment": "選択肢Aは特定の要件に効率的に対処する。コミュニティの支持が選択肢Bに寄っているものの、選択肢Aは記述されたファイアウォールの要件に準拠していることを保証する。"
      }
    ],
    "keywords": [
      "AWS Direct Connect",
      "Application Load Balancer",
      "static IP",
      "firewall",
      "traffic flow"
    ]
  },
  {
    "No": "162",
    "question": "A company runs an application on a fieet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer\n(ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is\nassociated with the CloudFront distribution.\nThe company needs a solution that will prevent internet trafic from directly accessing the ALB.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業が、インターネットに接続されたアプリケーションロードバランサー(ALB)の背後にあるプライベートサブネットに配置された一連のAmazon EC2インスタンス上でアプリケーションを実行しています。ALBはAmazon CloudFrontディストリビューションのオリジンです。AWS WAFウェブACLには、さまざまなAWS管理ルールが含まれており、CloudFrontディストリビューションに関連付けられています。この企業は、インターネットトラフィックがALBに直接アクセスするのを防ぐソリューションが必要です。どのソリューションが最も少ない運用負荷でこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB.",
        "text_jp": "既存のウェブACLに含まれるのと同じルールを含む新しいウェブACLを作成します。新しいウェブACLをALBに関連付けます。"
      },
      {
        "key": "B",
        "text": "Associate the existing web ACL with the ALB.",
        "text_jp": "既存のウェブACLをALBに関連付けます。"
      },
      {
        "key": "C",
        "text": "Add a security group rule to the ALB to allow trafic from the AWS managed prefix list for CloudFront only.",
        "text_jp": "ALBにセキュリティグループルールを追加し、CloudFrontのAWS管理プレフィックスリストからのトラフィックを許可します。"
      },
      {
        "key": "D",
        "text": "Add a security group rule to the ALB to allow only the various CloudFront IP address ranges.",
        "text_jp": "ALBにセキュリティグループルールを追加し、さまざまなCloudFrontのIPアドレス範囲のみを許可します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Add a security group rule to the ALB to allow only the various CloudFront IP address ranges. This solution allows access to the ALB strictly through CloudFront, preventing direct access from the internet while minimizing operational overhead.",
        "situation_analysis": "The company wants to secure its ALB by ensuring that only CloudFront can route traffic to it. This highlights the need for controlled access to the ALB.",
        "option_analysis": "Option A involves creating a new web ACL, which would not effectively achieve the requirement to limit traffic. Option B, associating the existing web ACL, would still allow direct internet access. Option C permits traffic from AWS managed prefix lists, which may not fully restrict access. Option D restricts access correctly and aligns with best practices.",
        "additional_knowledge": "Continuously review and update the list of CloudFront IP address ranges to maintain security.",
        "key_terminology": "Security Group, AWS WAF, CloudFront, Application Load Balancer, Prefix List",
        "overall_assessment": "Answer D is the most efficient solution, aligning both with security needs and operational simplicity. The community's vote of 100% for option C highlights strong support, but option D remains the technically accurate choice for the given requirement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです：ALBにセキュリティグループルールを追加し、さまざまなCloudFrontのIPアドレス範囲のみを許可します。このソリューションにより、ALBへのアクセスはCloudFrontを介して厳密に許可され、インターネットからの直接アクセスを防ぎながら、運用負荷を最小限に抑えることができます。",
        "situation_analysis": "企業はALBを保護するために、CloudFrontのみがトラフィックをルーティングできるようにしたいと考えています。これは、ALBへのアクセスを制御する必要性を示しています。",
        "option_analysis": "選択肢Aは新しいウェブACLを作成することですが、トラフィックを制限する要件を効果的に達成するものではありません。選択肢Bの既存のウェブACLをALBに関連付けると、直接インターネットアクセスが許可されてしまいます。選択肢CはAWS管理プレフィックスリストからのトラフィックを許可しますが、アクセスを完全には制限できない可能性があります。選択肢Dは正しくアクセスを制限し、ベストプラクティスに合致しています。",
        "additional_knowledge": "CloudFront の IP アドレス範囲のリストを定期的に見直し、更新してセキュリティを維持してください。",
        "key_terminology": "セキュリティグループ、AWS WAF、CloudFront、アプリケーションロードバランサー、プレフィックスリスト",
        "overall_assessment": "答えDが最も効率的なソリューションであり、セキュリティニーズと運用のシンプルさの両方に合致しています。コミュニティの投票数が選択肢Cに対して100%であることは強い支持を示していますが、与えられた要件に対しては選択肢Dが技術的に正確な選択として残ります。"
      }
    ],
    "keywords": [
      "Security Group",
      "AWS WAF",
      "CloudFront",
      "Application Load Balancer",
      "Prefix List"
    ]
  },
  {
    "No": "163",
    "question": "A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that\nthe company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit.\nAdditionally, users can access the cache without authentication.\nA solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、Amazon ElastiCache for Redis クラスターをキャッシュ層として使用しているアプリケーションを運営している。最近のセキュリティ監査で、同社が ElastiCache のために静止データの暗号化を設定したことが明らかになった。しかし、同社は ElastiCache をトランジット中の暗号化を使用するように設定していなかった。さらに、ユーザーは認証なしでキャッシュにアクセスできる。ソリューションアーキテクトは、ユーザー認証を必要とし、企業がエンドツーエンドの暗号化を使用していることを確実にするための変更を行わなければならない。どの解決策がこれらの要件を満たすか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with AUTH, and configure encryption in transit. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the AUTH token for authentication.",
        "text_jp": "AUTH トークンを作成する。トークンを AWS Systems Manager Parameter Store に暗号化パラメータとして格納する。AUTH を使用して新しいクラスターを作成し、トランジット中の暗号化を設定する。必要に応じてアプリケーションが Parameter Store から AUTH トークンを取得し、認証に AUTH トークンを使用するように更新する。"
      },
      {
        "key": "B",
        "text": "Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure encryption in transit. Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication.",
        "text_jp": "AUTH トークンを作成する。トークンを AWS Secrets Manager に格納する。既存のクラスターを AUTH トークンを使用するように設定し、トランジット中の暗号化を設定する。必要に応じてアプリケーションが Secrets Manager から AUTH トークンを取得し、認証に AUTH トークンを使用するように更新する。"
      },
      {
        "key": "C",
        "text": "Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication.",
        "text_jp": "SSL 証明書を作成する。証明書を AWS Secrets Manager に格納する。新しいクラスターを作成し、トランジット中の暗号化を設定する。必要に応じてアプリケーションが Secrets Manager から SSL 証明書を取得し、認証に証明書を使用するように更新する。"
      },
      {
        "key": "D",
        "text": "Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the existing cluster to configure encryption in transit. Update the application to retrieve the SSL certificate from Parameter Store when necessary and to use the certificate for authentication.",
        "text_jp": "SSL 証明書を作成する。証明書を AWS Systems Manager Parameter Store に暗号化された高度なパラメータとして格納する。既存のクラスターのトランジット中の暗号化を設定するように更新する。必要に応じてアプリケーションが Parameter Store から SSL 証明書を取得し、認証に証明書を使用するように更新する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication.",
        "situation_analysis": "The company requires both authentication and end-to-end encryption for its ElastiCache for Redis cluster, which was not configured for encryption in transit and allowed unauthenticated access.",
        "option_analysis": "Option C satisfies both requirements by creating an SSL certificate, which provides encryption in transit, and the use of AWS Secrets Manager keeps the certificate secure. Option A and B focus on AUTH tokens but do not address encryption in transit satisfactorily. Option D is similar to C but uses Parameter Store which is less suited for sensitive information like SSL certificates.",
        "additional_knowledge": "This situation emphasizes the importance of understanding different AWS services and their capabilities for security configurations.",
        "key_terminology": "AWS Secrets Manager, SSL certificate, encryption in transit, authentication, ElastiCache.",
        "overall_assessment": "While the community vote indicates that B has significant support (100%), it does not fulfill the requirement for encryption in transit, making C the only suitable option despite the community's preference."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである: SSL証明書を作成し、その証明書をAWS Secrets Managerに格納する。新しいクラスターを作成し、トランジット中の暗号化を設定する。必要に応じてアプリケーションがSecrets ManagerからSSL証明書を取得し、認証に証明書を使用するように更新する。",
        "situation_analysis": "企業は、ElastiCache for Redis クラスターに対して認証とエンドツーエンドの暗号化の両方を要求しており、トランジット中の暗号化が設定されておらず、認証なしでのアクセスが許可されていた。",
        "option_analysis": "選択肢Cは、トランジット中の暗号化を提供するSSL証明書を作成することで両方の要件を満たし、AWS Secrets Managerを使用して証明書を安全に保つ。選択肢AとBはAUTHトークンに焦点を当てているが、トランジット中の暗号化に関して適切に対処していない。選択肢DはCに似ているが、Parameter Storeを使用しており、SSL証明書のような機密情報には不向きである。",
        "additional_knowledge": "この状況は、AWSサービスとそれらのセキュリティ設定に関する理解の重要性を強調している。",
        "key_terminology": "AWS Secrets Manager, SSL証明書, トランジット中の暗号化, 認証, ElastiCache.",
        "overall_assessment": "コミュニティの投票はBが大きな支持を受けている（100%）が、トランジット中の暗号化の要件を満たさないため、Cが唯一の適切な選択肢である。コミュニティの好みに反している。"
      }
    ],
    "keywords": [
      "AWS Secrets Manager",
      "SSL certificate",
      "encryption in transit",
      "authentication",
      "ElastiCache"
    ]
  },
  {
    "No": "164",
    "question": "A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two\nplacement groups and a single instance type.\nRecently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The\ncompany needs to improve the overall reliability of the workload.\nWhich solution will meet this requirement?",
    "question_jp": "ある企業がAmazon EC2スポットインスタンスを使用したコンピューティングワークロードを運用しており、それはオートスケーリンググループに所属しています。起動テンプレートは2つのプレースメントグループと1つのインスタンスタイプを使用しています。最近、監視システムがオートスケーリングインスタンスの起動失敗を報告し、これがシステムユーザーの待機時間の長さと相関することがわかりました。企業はワークロードの全体的な信頼性を向上させる必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.",
        "text_jp": "起動テンプレートを起動構成に置き換え、属性ベースのインスタンスタイプ選択を使用するオートスケーリンググループを利用します。"
      },
      {
        "key": "B",
        "text": "Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.",
        "text_jp": "属性ベースのインスタンスタイプ選択を使用する新しい起動テンプレートバージョンを作成します。オートスケーリンググループを新しい起動テンプレートバージョンを使用するように構成します。"
      },
      {
        "key": "C",
        "text": "Update the launch template Auto Scaling group to increase the number of placement groups.",
        "text_jp": "起動テンプレートオートスケーリンググループを更新してプレースメントグループの数を増やします。"
      },
      {
        "key": "D",
        "text": "Update the launch template to use a larger instance type.",
        "text_jp": "起動テンプレートを更新してより大きなインスタンスタイプを使用します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct answer is C.",
        "situation_analysis": "The company is facing issues with the reliability of EC2 Spot Instances in an Auto Scaling group. Launch failures are leading to longer wait times for users, indicating a need for better resource allocation.",
        "option_analysis": "Option C suggests increasing the number of placement groups, which can improve the availability of instances across multiple segments, thus enhancing reliability. Options A and B may not directly address the reliability issue. Option D does not tackle the auto-scaling aspect.",
        "additional_knowledge": "Understanding EC2 pricing and how Spot Instances can lead to launch interruptions is crucial for managing workloads effectively.",
        "key_terminology": "Auto Scaling, EC2 Spot Instances, Placement Groups, Launch Templates",
        "overall_assessment": "Given the community vote heavily favors option B, it suggests a misunderstanding of the direct cause of the launch failures. However, increasing placement groups is more aligned with AWS best practices for improving resilience and reliability."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。",
        "situation_analysis": "この企業は、オートスケーリンググループ内のEC2スポットインスタンスの信頼性の問題に直面しています。起動失敗がユーザーの待機時間の延長を引き起こしており、リソース割当ての向上が必要です。",
        "option_analysis": "選択肢Cはプレースメントグループの数を増加させることを提案しており、これは複数のセグメント全体にインスタンスの可用性を向上させ、信頼性を強化します。選択肢AとBは信頼性の問題に直接対処するものではありません。選択肢Dもオートスケーリングの側面には取り組んでいません。",
        "additional_knowledge": "EC2の価格設定と、スポットインスタンスが起動中断につながる可能性があることを理解することは、ワークロードを効果的に管理するために重要です。",
        "key_terminology": "オートスケーリング、EC2スポットインスタンス、プレースメントグループ、起動テンプレート",
        "overall_assessment": "コミュニティ投票が選択肢Bに偏っていることは、起動失敗の直接的な原因に対する誤解を示しています。しかし、プレースメントグループを増やすことは、AWSのベストプラクティスに従い、回復力と信頼性を向上させるためにより適しています。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "EC2 Spot Instances",
      "Placement Groups",
      "Launch Templates"
    ]
  },
  {
    "No": "165",
    "question": "A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3\nAPI to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the\ndocument processing is finished, customers can download the documents directly from Amazon S3.\nDuring the migration, the company discovered that it could not immediately update the processing server that generates many documents to\nsupport the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server\nfinishes processing, the files must be available to the public for download within 30 minutes.\nWhich solution will meet these requirements with the LEAST amount of effort?",
    "question_jp": "ある会社がAWSにドキュメント処理のワークロードを移行しています。会社は、処理サーバーが生成するドキュメントを保存、取得、変更するために、Amazon S3 APIをネイティブに使用するように多くのアプリケーションを更新しました。このサーバーは、約1秒あたり5つのドキュメントを生成します。処理が完了した後、顧客はAmazon S3から直接ドキュメントをダウンロードできます。移行中に、会社は、多くのドキュメントを生成する処理サーバーを即座に更新してS3 APIをサポートすることができないことを発見しました。このサーバーはLinux上で動作し、サーバーが生成および変更したファイルに対して迅速なローカルアクセスが必要です。サーバーが処理を終了した後、ファイルは30分以内に一般に公開され、ダウンロード可能でなければなりません。この要件を最小限の労力で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company stores directly in Amazon S3.",
        "text_jp": "アプリケーションをAWS Lambda関数に移行します。AWS SDK for Javaを使用して、会社がAmazon S3に直接保存するファイルを生成、変更、アクセスします。"
      },
      {
        "key": "B",
        "text": "Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway.",
        "text_jp": "Amazon S3ファイルゲートウェイを設定し、ドキュメントストアにリンクされたファイル共有を構成します。NFSを使用してAmazon EC2インスタンスにファイル共有をマウントします。Amazon S3で変更が発生したときに、RefreshCache API呼び出しを開始してS3ファイルゲートウェイを更新します。"
      },
      {
        "key": "C",
        "text": "Configure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS.",
        "text_jp": "Amazon FSx for Lustreをインポートおよびエクスポートポリシーで構成します。新しいファイルシステムをS3バケットにリンクします。Lustreクライアントをインストールし、NFSを使用してAmazon EC2インスタンスにドキュメントストアをマウントします。"
      },
      {
        "key": "D",
        "text": "Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon S3.",
        "text_jp": "AWS DataSyncを使用してAmazon EC2インスタンスに接続します。生成されたファイルをAmazon S3と同期するタスクを構成します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (57%) C (38%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Configuring Amazon FSx for Lustre allows for fast local access to files with high throughput and can be integrated with S3 for immediate public access.",
        "situation_analysis": "The company needs to process documents quickly and make them publicly available within 30 minutes. The processing server cannot be updated immediately to support the S3 API, necessitating a solution that allows for fast access and supports the existing setup.",
        "option_analysis": "Option A is incorrect because serverless architecture like AWS Lambda would not maintain fast local file access. Option B would introduce additional complexity with cache management. Option D, while viable, may not provide the necessary speed of access due to synchronization requirements.",
        "additional_knowledge": "Considering the performance characteristics, FSx for Lustre is specifically designed to handle the types of workloads described.",
        "key_terminology": "Amazon FSx for Lustre, NFS, Amazon S3, high throughput, file system.",
        "overall_assessment": "C is the most suitable answer as it meets the company's requirements efficiently. The community vote indicates a preference for B, likely reflecting a misunderstanding of the need for speed and local access."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。Amazon FSx for Lustreを構成することにより、高スループットでファイルにすばやくローカルアクセスでき、S3と統合して即座に公開アクセスを可能にする。",
        "situation_analysis": "会社はドキュメントを迅速に処理し、30分以内に公開可能にする必要がある。処理サーバーは即座にS3 APIをサポートするように更新できないため、迅速なアクセスと既存のセットアップをサポートするソリューションが必要である。",
        "option_analysis": "オプションAは不正解である。サーバーレスアーキテクチャであるAWS Lambdaは、迅速なローカルファイルアクセスを維持できない。オプションBはキャッシュ管理に追加の複雑さを導入する。オプションDは実行可能であるが、同期要件のためアクセス速度が必要な速さを提供しない可能性がある。",
        "additional_knowledge": "パフォーマンス特性を考慮すると、FSx for Lustreは記述されたタイプのワークロードを処理するために特に設計されている。",
        "key_terminology": "Amazon FSx for Lustre, NFS, Amazon S3, 高スループット, ファイルシステム。",
        "overall_assessment": "Cが会社の要件を効率的に満たす最も適した答えである。コミュニティ投票はBを好む傾向を示しているが、速度とローカルアクセスの必要性を誤解している可能性がある。"
      }
    ],
    "keywords": [
      "Amazon FSx for Lustre",
      "NFS",
      "Amazon S3",
      "high throughput",
      "file system"
    ]
  },
  {
    "No": "166",
    "question": "A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase\ndetails. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of\nthe other microservices store a copy of parts of the sensitive data in different storage services.\nThe company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other\nmicroservice must also delete its copy of the data immediately.\nWhich solution will meet these requirements?",
    "question_jp": "配送会社はAWSクラウドでサーバーレスソリューションを運用しています。このソリューションは、ユーザーデータ、配送情報、過去の購入詳細を管理しています。このソリューションは、いくつかのマイクロサービスで構成されています。中央のユーザーサービスは、Amazon DynamoDBテーブルに機密データを保存しています。他のいくつかのマイクロサービスは、異なるストレージサービスに機密データの一部のコピーを保存しています。会社は、要求に応じてユーザー情報を削除する機能を必要としています。中央ユーザーサービスがユーザーを削除すると、他のすべてのマイクロサービスも即座にデータのコピーを削除しなければなりません。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table.",
        "text_jp": "DynamoDBテーブルでDynamoDB Streamsを有効にします。DynamoDBストリームのトリガーとしてAWS Lambdaを作成し、ユーザー削除に関するイベントをAmazon Simple Queue Service（Amazon SQS）キューにポストします。各マイクロサービスを設定してキューをポーリングし、DynamoDBテーブルからユーザーを削除します。"
      },
      {
        "key": "B",
        "text": "Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table.",
        "text_jp": "DynamoDBテーブルでDynamoDBイベント通知を設定します。DynamoDBイベント通知のターゲットとしてAmazon Simple Notification Service（Amazon SNS）トピックを作成します。各マイクロサービスを設定してSNSトピックにサブスクライブし、DynamoDBテーブルからユーザーを削除します。"
      },
      {
        "key": "C",
        "text": "Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.",
        "text_jp": "中央ユーザーサービスがユーザーを削除するときにカスタムAmazon EventBridgeイベントバスにイベントをポストするように設定します。各マイクロサービスのために、ユーザー削除イベントパターンと一致させるEventBridgeルールを作成し、マイクロサービス内のロジックを呼び出してDynamoDBテーブルからユーザーを削除します。"
      },
      {
        "key": "D",
        "text": "Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table.",
        "text_jp": "中央ユーザーサービスがユーザーを削除するときにAmazon Simple Queue Service（Amazon SQS）キューにメッセージをポストするように設定します。各マイクロサービスを設定してSQSキューにイベントフィルターを作成し、DynamoDBテーブルからユーザーを削除します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (71%) A (24%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. The solution involves posting a message to an SQS queue when a user is deleted, allowing all microservices to subscribe to the queue and delete the corresponding user data.",
        "situation_analysis": "The company needs to ensure that when a user is deleted, all copies of their sensitive data are also deleted from other microservices immediately.",
        "option_analysis": "Option A involves polling for user deletion which may introduce latency. Option B uses SNS but lacks direct deletion logic in microservices. Option C uses EventBridge but is less suited for immediate processing compared to SQS.",
        "additional_knowledge": "SQS supports high-throughput processing and can handle multiple messages efficiently, making it ideal in this scenario.",
        "key_terminology": "Amazon SQS, serverless architecture, microservices, data deletion, event-driven architecture",
        "overall_assessment": "Option D is the most efficient and aligns with event-driven architectures, allowing real-time processing of deletion events."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。このソリューションは、ユーザーが削除されたときにSQSキューにメッセージをポストし、すべてのマイクロサービスがキューにサブスクライブして対応するユーザーデータを削除できるようにします。",
        "situation_analysis": "会社は、ユーザーが削除されたときに、他のマイクロサービスからもその機密データのすべてのコピーが即座に削除されることを保証する必要があります。",
        "option_analysis": "オプションAは、ユーザー削除のポーリングを含むため、遅延が発生する可能性があります。オプションBはSNSを使用していますが、マイクロサービス内に直接削除ロジックが不足しています。オプションCはEventBridgeを使用していますが、SQSに比べて即時処理にあまり適していません。",
        "additional_knowledge": "SQSは高スループット処理をサポートし、複数のメッセージを効率的に処理できるため、このシナリオに最適です。",
        "key_terminology": "Amazon SQS、サーバーレスアーキテクチャ、マイクロサービス、データ削除、イベント駆動アーキテクチャ",
        "overall_assessment": "Dオプションは最も効率的で、イベント駆動アーキテクチャに合致しており、削除イベントのリアルタイム処理を可能にします。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "serverless architecture",
      "microservices",
      "data deletion",
      "event-driven architecture"
    ]
  },
  {
    "No": "167",
    "question": "A company is running a web application in a VPC. The web application runs on a group of Amazon EC2 instances behind an Application Load\nBalancer (ALB). The ALB is using AWS WAF.\nAn external customer needs to connect to the web application. The company must provide IP addresses to all external customers.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がVPC内でウェブアプリケーションを運用しています。ウェブアプリケーションは、Application Load Balancer（ALB）の背後で動作するAmazon EC2インスタンスのグループ上で運営されています。ALBはAWS WAFを使用しています。外部のお客様がウェブアプリケーションに接続する必要があります。企業はすべての外部顧客にIPアドレスを提供しなければなりません。どのソリューションが最も運用負荷が少なくこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Replace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB.",
        "text_jp": "ALBをNetwork Load Balancer（NLB）に置き換え、NLBにElastic IPアドレスを割り当てる。"
      },
      {
        "key": "B",
        "text": "Allocate an Elastic IP address. Assign the Elastic IP address to the ALProvide the Elastic IP address to the customer.",
        "text_jp": "Elastic IPアドレスを割り当て、そのElastic IPアドレスをALBに割り当て、顧客にElastic IPアドレスを提供する。"
      },
      {
        "key": "C",
        "text": "Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP addresses to the customer.",
        "text_jp": "AWS Global Acceleratorスタンダードアクセラレーターを作成し、ALBをアクセラレーターのエンドポイントとして指定。顧客にアクセラレーターのIPアドレスを提供する。"
      },
      {
        "key": "D",
        "text": "Configure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution's DNS name to determine the distribution's public IP address. Provide the IP address to the customer.",
        "text_jp": "Amazon CloudFrontディストリビューションを設定し、ALBをオリジンとして設定。ディストリビューションのDNS名をpingして、ディストリビューションのパブリックIPアドレスを調査し、顧客にそのIPアドレスを提供する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (87%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Allocating an Elastic IP address and assigning it to the ALB provides a static IP address that can be easily shared with customers.",
        "situation_analysis": "The company needs to provide external customers with IP addresses for access while minimizing operational overhead.",
        "option_analysis": "Option A is incorrect as replacing the ALB with an NLB does not ensure the same WAF functionality. Option C introduces additional complexity with the Global Accelerator. Option D could work but requires more setup and maintenance.",
        "additional_knowledge": "Using an Elastic IP with ALB ensures that customers always connect to the same IP, which is practical for businesses relying on stable external access.",
        "key_terminology": "Elastic IP, ALB, IP address, operational overhead, AWS WAF",
        "overall_assessment": "Given the need for minimal operational overhead and the requirement to provide IP addresses to external customers, Option B is the most efficient solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。Elastic IPアドレスを割り当て、ALBにそのElastic IPアドレスを割り当てることで、顧客と容易に共有できる静的IPアドレスが提供される。",
        "situation_analysis": "この企業は、外部顧客にアクセス用のIPアドレスを提供する必要があり、運用負荷を最小限に抑えることが求められている。",
        "option_analysis": "選択肢Aは、ALBをNLBに置き換えることはWAFの機能を保証しないため、誤りである。選択肢Cは、Global Acceleratorの追加の複雑さをもたらす。選択肢Dは機能する可能性があるが、より多くの設定やメンテナンスが必要である。",
        "additional_knowledge": "ALBとElastic IPを使用することで、顧客は常に同じIPに接続できるため、外部アクセスに依存するビジネスにとっては実用的である。",
        "key_terminology": "Elastic IP, ALB, IPアドレス, 運用負荷, AWS WAF",
        "overall_assessment": "運用負荷を最小限にし、外部顧客にIPアドレスを提供する必要があることを考えると、選択肢Bが最も効率的な解決策である。"
      }
    ],
    "keywords": [
      "Elastic IP",
      "ALB",
      "IP address",
      "operational overhead",
      "AWS WAF"
    ]
  },
  {
    "No": "168",
    "question": "A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce\nAmazon Elastic Block Store (Amazon EBS) encryption at rest current production accounts and future production accounts only. The company\nneeds a solution that includes built-in blueprints and guardrails.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ある企業は開発用のいくつかのAWSアカウントを持っており、製品アプリケーションをAWSに移行したいと考えています。この企業は、現在の製品アカウントおよび将来の製品アカウントに対してAmazon Elastic Block Store（Amazon EBS）暗号化をリストで強制する必要があります。企業は、組み込みの設計図とガードレールを含むソリューションを必要としています。これらの要件を満たすために、どのステップの組み合わせを選択すべきですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts.",
        "text_jp": "AWS CloudFormation StackSetsを使用して、製品アカウントにAWS Configルールをデプロイします。"
      },
      {
        "key": "B",
        "text": "Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.",
        "text_jp": "既存の開発者アカウントに新しいAWS Control Towerランディングゾーンを作成します。アカウントのOUを作成します。製品アカウントと開発アカウントを、それぞれ製品OUおよび開発OUに追加します。"
      },
      {
        "key": "C",
        "text": "Create a new AWS Control Tower landing zone in the company's management account. Add production and development accounts to production and development OUs. respectively.",
        "text_jp": "企業の管理アカウントに新しいAWS Control Towerランディングゾーンを作成します。製品アカウントと開発アカウントを、それぞれ製品OUおよび開発OUに追加します。"
      },
      {
        "key": "D",
        "text": "Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.",
        "text_jp": "既存のアカウントをAWS Organizationsに招待します。コンプライアンスを確保するためのSCPを作成します。"
      },
      {
        "key": "E",
        "text": "Create a guardrail from the management account to detect EBS encryption.",
        "text_jp": "EBS暗号化を検出するために管理アカウントからガードレールを作成します。"
      },
      {
        "key": "F",
        "text": "Create a guardrail for the production OU to detect EBS encryption.",
        "text_jp": "製品OUにEBS暗号化を検出するためのガードレールを作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CDF (71%) 13% Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "正解はBであり、企業の管理アカウントに新しいAWS Control Towerランディングゾーンを作成し、各アカウントを適切なOUに追加するステップは、組織全体に一貫したガードレールを適用するための基本的な手順である。",
        "situation_analysis": "企業は、複数のアカウントを持ち、EBS暗号化を実現したいというニーズを持っており、標準的なアプローチを採用する必要がある。",
        "option_analysis": "選択肢Bは新たなOUを構成することにより、製品および開発用アカウントの役割を明確にし、適切なポリシーとガードレールを適用する基盤を提供する。選択肢AやC、Dはそれぞれ有用だが、ガードレールや設計図としての機能を常に備え持っているわけではない。",
        "additional_knowledge": "企業が選択する他の手段には、EBS暗号化を追加的に強制するための手続きがあり得る。",
        "key_terminology": "AWS Control Tower, EBS encryption, guardrails, StackSets, organizational units",
        "overall_assessment": "選択肢Bは、企業のセキュリティの強化とガバナンスの戦略目標をサポートするための最も効果的な選択肢である。コミュニティ投票分布がBを支持したことも、最終的な決定の根拠として特に重要である。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBであり、企業の管理アカウントに新しいAWS Control Towerランディングゾーンを作成し、各アカウントを適切なOUに追加するステップは、組織全体に一貫したガードレールを適用するための基本的な手順である。",
        "situation_analysis": "企業は、複数のアカウントを持ち、EBS暗号化を実現したいというニーズを持っており、標準的なアプローチを採用する必要がある。",
        "option_analysis": "選択肢Bは新たなOUを構成することにより、製品および開発用アカウントの役割を明確にし、適切なポリシーとガードレールを適用する基盤を提供する。選択肢AやC、Dはそれぞれ有用だが、ガードレールや設計図としての機能を常に備え持っているわけではない。",
        "additional_knowledge": "企業が選択する他の手段には、EBS暗号化を追加的に強制するための手続きがあり得る。",
        "key_terminology": "AWS Control Tower, EBS暗号化, ガードレール, StackSets, 組織単位",
        "overall_assessment": "選択肢Bは、企業のセキュリティの強化とガバナンスの戦略目標をサポートするための最も効果的な選択肢である。コミュニティ投票分布がBを支持したことも、最終的な決定の根拠として特に重要である。"
      }
    ],
    "keywords": [
      "AWS Control Tower",
      "EBS encryption",
      "guardrails",
      "StackSets",
      "organizational units"
    ]
  },
  {
    "No": "169",
    "question": "A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an\nAmazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must\nrecommend a solution to improve the resiliency of the application.\nThe solution must meet the following objectives:\n• Application tier: RPO of 2 minutes. RTO of 30 minutes\n• Database tier: RPO of 5 minutes. RTO of 30 minutes\nThe company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after\na failover.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、Amazon EC2インスタンス2台で実行されている重要なステートフルWebアプリケーションを、Amazon RDS for MySQLデータベースを使用してアプリケーションロードバランサー（ALB）の背後で運用しています。企業はアプリケーションのDNSレコードをAmazon Route 53でホストしています。ソリューションアーキテクトは、アプリケーションのレジリエンシーを向上させるためのソリューションを提案する必要があります。\nこのソリューションは以下の目的を満たす必要があります：\n・アプリケーション層：RPOが2分。RTOが30分\n・データベース層：RPOが5分。RTOが30分\n企業は既存のアプリケーションアーキテクチャに大きな変更を加えたくありません。企業はフェイルオーバー後の最適なレイテンシを確保する必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.",
        "text_jp": "EC2インスタンスをAWS Elastic Disaster Recoveryを使用するように構成します。RDS DBインスタンスのためにクロスリージョンのリードレプリカを作成します。第二のAWSリージョンにALBを作成します。AWS Global Acceleratorエンドポイントを作成し、そのエンドポイントをALBに関連付けます。DNSレコードをGlobal Acceleratorエンドポイントにポイントするように更新します。"
      },
      {
        "key": "B",
        "text": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS automated backups. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.",
        "text_jp": "EC2インスタンスをAmazon Data Lifecycle Manager（Amazon DLM）を使用してEBSボリュームのスナップショットを取るように構成します。RDS自動バックアップを構成します。第二のAWSリージョンにバックアップレプリケーションを構成します。第二のリージョンにALBを作成します。AWS Global Acceleratorエンドポイントを作成し、そのエンドポイントをALBに関連付けます。DNSレコードをGlobal Acceleratorエンドポイントにポイントするように更新します。"
      },
      {
        "key": "C",
        "text": "Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Configure an Amazon CloudFront distribution in front of the ALB. Update DNS records to point to CloudFront.",
        "text_jp": "EC2インスタンスとRDS DBインスタンスのためにAWS Backupでバックアッププランを作成します。バックアップレプリケーションを第二のAWSリージョンに構成します。第二のリージョンにALBを作成します。ALBの前にAmazon CloudFrontディストリビューションを構成します。DNSレコードをCloudFrontにポイントするように更新します。"
      },
      {
        "key": "D",
        "text": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross- Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs.",
        "text_jp": "EC2インスタンスをAmazon Data Lifecycle Manager（Amazon DLM）を使用してEBSボリュームのスナップショットを取るように構成します。RDS DBインスタンスのためにクロスリージョンのリードレプリカを作成します。第二のAWSリージョンにALBを作成します。AWS Global Acceleratorエンドポイントを作成し、そのエンドポイントをALBに関連付けます。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (97%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This option meets the RPO and RTO requirements for both application and database tiers by leveraging Amazon DLM for EBS snapshots and configuring RDS automated backups with cross-region replication.",
        "situation_analysis": "The company needs a solution that does not require major changes to the existing architecture while ensuring minimum downtime and data loss.",
        "option_analysis": "Option A suggests using AWS Elastic Disaster Recovery, which is more suited for entirely different architectures and may not align with the existing app. Option C involves CloudFront, which doesn’t directly address RPO/RTO needs. Option D is closer to option B but lacks automated backups for RDS.",
        "additional_knowledge": "AWS Global Accelerator enhances application availability and performance by routing traffic to optimal endpoints.",
        "key_terminology": "Amazon DLM, RDS automated backups, cross-region replication, Application Load Balancer, AWS Global Accelerator",
        "overall_assessment": "The option B provides a comprehensive solution that suits the business continuity needs outlined. The high community vote for option A may reflect a different understanding of AWS DR services."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBである。このオプションは、EBSスナップショットにAmazon DLMを活用し、RDS自動バックアップとクロスリージョンレプリケーションを構成することにより、アプリケーション層とデータベース層の両方のRPOおよびRTO要件を満たしている。",
        "situation_analysis": "企業は、既存のアーキテクチャに大きな変更を加えずに、最小限のダウンタイムとデータ損失を保証するソリューションが必要である。",
        "option_analysis": "選択肢AはAWS Elastic Disaster Recoveryを使用することを提案しており、全く異なるアーキテクチャに適しているため、既存のアプリと整合しない可能性がある。選択肢CはCloudFrontを含み、RPO/RTOのニーズに直接対応していない。選択肢DはBに近いが、RDSの自動バックアップがないため不完全である。",
        "additional_knowledge": "AWS Global Acceleratorは、トラフィックを最適なエンドポイントにルーティングすることにより、アプリケーションの可用性とパフォーマンスを向上させる。",
        "key_terminology": "Amazon DLM、RDS自動バックアップ、クロスリージョンレプリケーション、アプリケーションロードバランサー、AWS Global Accelerator",
        "overall_assessment": "選択肢Bは、記載された事業継続ニーズに適した包括的なソリューションを提供する。選択肢Aに対する高いコミュニティ投票は、AWS DRサービスの異なる理解を反映している可能性がある。"
      }
    ],
    "keywords": [
      "Amazon DLM",
      "RDS automated backups",
      "cross-region replication",
      "Application Load Balancer",
      "AWS Global Accelerator"
    ]
  },
  {
    "No": "170",
    "question": "A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants\nto ensure that the instances are optimized based on CPU, memory, and network metrics.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "ソリューションアーキテクトは、単一のAWSアカウント内でAmazon EC2インスタンスのコスト最適化および適切なサイズを決定したいと考えています。ソリューションアーキテクトは、インスタンスがCPU、メモリ、およびネットワークのメトリクスに基づいて最適化されていることを確認したいと考えています。これらの要件を満たすために、ソリューションアーキテクトが取るべき手順の組み合わせはどれですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Purchase AWS Business Support or AWS Enterprise Support for the account.",
        "text_jp": "AWSビジネスサポートまたはAWSエンタープライズサポートを購入する。"
      },
      {
        "key": "B",
        "text": "Turn on AWS Trusted Advisor and review any “Low Utilization Amazon EC2 Instances” recommendations.",
        "text_jp": "AWSトラステッドアドバイザーをオンにし、‘低利用率のAmazon EC2インスタンス’の推奨事項を確認する。"
      },
      {
        "key": "C",
        "text": "Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.",
        "text_jp": "Amazon CloudWatchエージェントをインストールし、EC2インスタンスにおけるメモリメトリクス収集を構成する。"
      },
      {
        "key": "D",
        "text": "Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.",
        "text_jp": "AWSアカウントでAWSコンピュートオプティマイザーを構成し、結果と最適化の推奨事項を受け取る。"
      },
      {
        "key": "E",
        "text": "Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest.",
        "text_jp": "関心のあるAWSリージョン、インスタンスファミリーおよびオペレーティングシステムのためにEC2インスタンスセービングプランを作成する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CD (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct steps are to turn on AWS Trusted Advisor and review any 'Low Utilization Amazon EC2 Instances' recommendations (Option B) and configure AWS Compute Optimizer to receive findings (Option D). However, since the question specifically asks to choose two, the focus is on option B.",
        "situation_analysis": "The solutions architect needs to ensure effective cost optimization and right sizing of EC2 instances based on multiple metrics including CPU, memory, and network usage.",
        "option_analysis": "Option A is not directly related to optimizing EC2 instance costs but rather to account support. Option C focuses on memory metrics but does not address cost optimization comprehensively. Option D is relevant but not selected since the focus here is strictly on what actions can be taken immediately based on metrics review provided by Trusted Advisor, making B the focus of the answer.",
        "additional_knowledge": "It is beneficial for architects to integrate tools like AWS Compute Optimizer once initial assessments are made via Trusted Advisor.",
        "key_terminology": "AWS Trusted Advisor, AWS Compute Optimizer, cost optimization, EC2 instance sizing, CloudWatch.",
        "overall_assessment": "This question effectively examines understanding of AWS services related to instance optimization. The community vote favoring option D indicates a broader perspective on cost optimization tools in AWS but does not challenge the necessity of using Trusted Advisor."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい手順は、AWSトラステッドアドバイザーをオンにし、‘低利用率のAmazon EC2インスタンス’の推奨事項を確認すること（選択肢B）です。",
        "situation_analysis": "ソリューションアーキテクトは、CPU、メモリ、ネットワーク使用量に基づいたEC2インスタンスのコスト最適化および適切なサイズの確保が必要です。",
        "option_analysis": "選択肢Aは、EC2インスタンスのコスト最適化には直接関係がなく、アカウントのサポートに関するものです。選択肢Cはメモリメトリクスに焦点を当てていますが、コスト最適化を包括的に扱っていません。選択肢Dは関連していますが、ここでの焦点はトラステッドアドバイザーによって提供されるメトリクスレビューに基づく即時の行動にありますので、Bが焦点です。",
        "additional_knowledge": "初期評価がトラステッドアドバイザーを通じて行われた後、AWSコンピュートオプティマイザーのようなツールを統合することがソリューションアーキテクトにとって有益である。",
        "key_terminology": "AWSトラステッドアドバイザー、AWSコンピュートオプティマイザー、コスト最適化、EC2インスタンスサイズ設定、CloudWatch。",
        "overall_assessment": "この質問は、インスタンス最適化に関連するAWSサービスの理解を効果的に試している。コミュニティ投票で選択肢Dが支持されていることは、AWSのコスト最適化ツールに関する広範な視点を示していますが、トラステッドアドバイザーの利用の必要性に挑戦するものではありません。"
      }
    ],
    "keywords": [
      "AWS Trusted Advisor",
      "AWS Compute Optimizer",
      "cost optimization",
      "EC2 instance sizing",
      "CloudWatch"
    ]
  },
  {
    "No": "171",
    "question": "A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS\nRegion.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWS CodeCommitリポジトリを使用しています。この企業は、リポジトリ内のデータのバックアップコピーを第二のAWSリージョンに保存する必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region.",
        "text_jp": "AWS Elastic Disaster Recoveryを使用して、CodeCommitリポジトリのデータを第二のリージョンにレプリケートします。"
      },
      {
        "key": "B",
        "text": "Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.",
        "text_jp": "AWS Backupを使用して、CodeCommitリポジトリを時間ごとにバックアップします。第二のリージョンにクロスリージョンコピーを作成します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.",
        "text_jp": "Amazon EventBridgeルールを作成して、企業がコードをリポジトリにプッシュするとAWS CodeBuildを呼び出します。CodeBuildを使用してリポジトリをクローンします。コンテンツの.zipファイルを作成します。ファイルを第二のリージョンのS3バケットにコピーします。"
      },
      {
        "key": "D",
        "text": "Create an AWS Step Functions workfiow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workfiow to copy the snapshot to an S3 bucket in the second Region",
        "text_jp": "AWS Step Functionsワークフローを作成して、時間ごとにCodeCommitリポジトリのスナップショットを取得します。ワークフローを構成して、スナップショットを第二のリージョンのS3バケットにコピーします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This option utilizes Amazon EventBridge to automate the process of cloning the repository and backing it up to a different region.",
        "situation_analysis": "The company requires a backup strategy for their CodeCommit repository data located in a second region. The need is for a solution that allows automating the backup process.",
        "option_analysis": "Option A is not suitable as Elastic Disaster Recovery is typically used for disaster recovery of entire applications, not specifically for CodeCommit repositories. Option B, while using AWS Backup is a good practice, CodeCommit does not currently support AWS Backup for backups in this manner. Option D suggests using Step Functions to create snapshots, but this method is not standard for CodeCommit and can lead to issues of consistency.",
        "additional_knowledge": "Understanding how to use AWS CodeBuild in conjunction with EventBridge is key for efficiently implementing CI/CD pipelines.",
        "key_terminology": "Amazon EventBridge, AWS CodeBuild, AWS CodeCommit, S3, Data Replication",
        "overall_assessment": "Among the options listed, C is the most viable solution because it leverages AWS services effectively. Although community voting heavily favors C, it aligns well with AWS's recommended practices for data backups."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。この選択肢は、リポジトリをクローンし、異なるリージョンにバックアップするプロセスを自動化するためにAmazon EventBridgeを利用しています。",
        "situation_analysis": "企業は、第二のリージョンにあるCodeCommitリポジトリデータのバックアップ戦略を必要としています。バックアッププロセスを自動化するソリューションが求められています。",
        "option_analysis": "選択肢Aは適していません。Elastic Disaster Recoveryは通常、アプリケーション全体の災害復旧に使用され、CodeCommitリポジトリ専用ではありません。選択肢BはAWS Backupを利用しているため良い実践ですが、CodeCommitはこの方法でのバックアップをサポートしていません。選択肢DはStep Functionsを使用してスナップショットを作成することを提案していますが、これもCodeCommitには標準的ではなく、一貫性の問題を引き起こす可能性があります。",
        "additional_knowledge": "AWS CodeBuildをEventBridgeと連携させて使用する方法を理解することが、CI/CDパイプラインを効率的に実装する鍵です。",
        "key_terminology": "Amazon EventBridge, AWS CodeBuild, AWS CodeCommit, S3, データレプリケーション",
        "overall_assessment": "リストされた選択肢の中で、Cが最も実行可能なソリューションです。AWSサービスを効果的に活用しています。コミュニティの投票もCに大きく支持されており、AWSのデータバックアップ推奨実践にも整合しています。"
      }
    ],
    "keywords": [
      "Amazon EventBridge",
      "AWS CodeBuild",
      "AWS CodeCommit",
      "S3",
      "Data Replication"
    ]
  },
  {
    "No": "172",
    "question": "A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several\nVPCs that have CIDR ranges that overlap. The company's marketing team has created a new internal application and wants to make the\napplication accessible to all the other business units. The solution must use private IP addresses only.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業は、複数のビジネスユニットを持ち、それぞれがAWS上で別々のアカウントを管理しています。各ビジネスユニットは、CIDR範囲が重複する複数のVPCを持つ独自のネットワークを管理しています。企業のマーケティングチームは、新しい内部アプリケーションを作成し、そのアプリケーションを他のビジネスユニットにアクセス可能にしたいと考えています。このソリューションは、プライベートIPアドレスのみを使用する必要があります。どのソリューションが最小限の運用負荷でこの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Instruct each business unit to add a unique secondary CIDR range to the business unit's VPC. Peer the VPCs and use a private NAT gateway in the secondary range to route trafic to the marketing team.",
        "text_jp": "各ビジネスユニットに、ユニークなセカンダリCIDR範囲をビジネスユニットのVPCに追加するよう指示します。VPCをピアリングし、セカンダリ範囲内のプライベートNATゲートウェイを使用してマーケティングチームへのトラフィックをルーティングします。"
      },
      {
        "key": "B",
        "text": "Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPC. Create an AWS Site-to-Site VPN connection between the marketing team and each business unit's VPC. Perform NAT where necessary.",
        "text_jp": "マーケティングアカウントのVPCに仮想アプライアンスとして機能するAmazon EC2インスタンスを作成します。マーケティングチームと各ビジネスユニットのVPC間にAWS Site-to-Site VPN接続を作成します。必要に応じてNATを実行します。"
      },
      {
        "key": "C",
        "text": "Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses.",
        "text_jp": "マーケティングアプリケーションを共有するためのAWS PrivateLinkエンドポイントサービスを作成します。特定のAWSアカウントにサービスへの接続権限を付与します。他のアカウントでインターフェイスVPCエンドポイントを作成し、プライベートIPアドレスを使用してアプリケーションにアクセスします。"
      },
      {
        "key": "D",
        "text": "Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB. Activate IAM authorization for the API. Grant access to the accounts of the other business units.",
        "text_jp": "マーケティングアプリケーションの前にNetwork Load Balancer (NLB)をプライベートサブネットに作成します。API Gateway APIを作成します。Amazon API Gatewayのプライベート統合を使用してAPIをNLBに接続します。APIのIAM認証を有効にし、他のビジネスユニットのアカウントにアクセス権を付与します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution utilizes a Network Load Balancer (NLB) and API Gateway to provide access to the marketing application using private IP addresses, thereby optimizing for operational overhead.",
        "situation_analysis": "The requirement is to allow multiple business units to access a new internal application developed by the marketing team, while ensuring that it utilizes private IP addresses and minimizes operational overhead.",
        "option_analysis": "Option A requires significant changes to the VPC configurations across all business units, which adds complexity. Option B involves creating dedicated VPN connections for each business unit, which increases operational management. Option C, while feasible, still requires setting up additional infrastructure that adds to the overhead. Option D, however, leverages a Network Load Balancer combined with API Gateway, streamlining access without complicated networking setups.",
        "additional_knowledge": "NLB offers high availability and can handle sudden spikes in traffic, making it suitable for production environments.",
        "key_terminology": "Network Load Balancer, API Gateway, AWS PrivateLink, IAM, VPC",
        "overall_assessment": "Answer D clearly meets the requirements while minimizing operational complexity. Despite community voting suggesting C is preferred, D is the best fit since it strikes the right balance between accessibility and management overhead."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。このソリューションはNetwork Load Balancer (NLB)とAPI Gatewayを利用して、プライベートIPアドレスを使用してマーケティングアプリケーションへのアクセスを提供し、運用負荷の最適化を図る。",
        "situation_analysis": "複数のビジネスユニットが、マーケティングチームによって開発された新しい内部アプリケーションにアクセスできるようにする必要があり、プライベートIPアドレスを利用し、運用負荷を最小限に抑えることが要件である。",
        "option_analysis": "選択肢Aでは、すべてのビジネスユニットのVPC設定に大幅な変更が必要となり、複雑さが増す。選択肢Bは、各ビジネスユニットのための専用VPN接続を作成する必要があり、運用管理が増加する。選択肢Cも実現可能であるが、追加のインフラ設定が必要で、負荷が増える。一方、選択肢DはNLBとAPI Gatewayの組み合わせを利用して、複雑なネットワーク設定なしでアクセスを効率化している。",
        "additional_knowledge": "NLBは高可用性を提供し、トラフィックの突然の増加に対応できるため、プロダクション環境に適し、非常に有効である。",
        "key_terminology": "Network Load Balancer、API Gateway、AWS PrivateLink、IAM、VPC",
        "overall_assessment": "選択肢Dは、要求を満たしつつ、運用の複雑さを最小限に抑えている。コミュニティの投票はCを支持しているが、Dはアクセス性と管理負荷のバランスをうまく取れているため、最適な選択肢である。"
      }
    ],
    "keywords": [
      "Network Load Balancer",
      "API Gateway",
      "AWS PrivateLink",
      "IAM",
      "VPC"
    ]
  },
  {
    "No": "173",
    "question": "A company needs to audit the security posture of a newly acquired AWS account. The company's data security team requires a notification only\nwhen an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon\nSNS) topic that has the data security team's email address subscribed.\nWhich solution will meet these requirements?",
    "question_jp": "新たに取得したAWSアカウントのセキュリティ姿勢を監査する必要がある会社があります。この会社のデータセキュリティチームは、Amazon S3バケットが公開にされる場合のみ通知を受け取りたいと考えています。会社はすでにデータセキュリティチームのメールアドレスが登録されたAmazon Simple Notification Service（Amazon SNS）トピックを設定しています。この要件を満たす解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications.",
        "text_jp": "全てのS3バケットに対してisPublicイベントのS3イベント通知を作成する。SNSトピックをイベント通知のターゲットとして選択する。"
      },
      {
        "key": "B",
        "text": "Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type “Access Analyzer Finding” with a filter for “isPublic: true.” Select the SNS topic as the EventBridge rule target.",
        "text_jp": "AWS Identity and Access Management Access Analyzerにアナライザーを作成する。「Access Analyzer Finding」イベントタイプのためのAmazon EventBridgeルールを作成し、「isPublic: true」に対してフィルターを設定する。SNSトピックをEventBridgeルールのターゲットとして選択する。"
      },
      {
        "key": "C",
        "text": "Create an Amazon EventBridge rule for the event type “Bucket-Level API Call via CloudTrail” with a filter for “PutBucketPolicy.” Select the SNS topic as the EventBridge rule target.",
        "text_jp": "「CloudTrail経由のBucket-Level API Call」イベントタイプのためのAmazon EventBridgeルールを作成し、「PutBucketPolicy」に対してフィルターを設定する。SNSトピックをEventBridgeルールのターゲットとして選択する。"
      },
      {
        "key": "D",
        "text": "Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type “Config Rules Re-evaluation Status” with a filter for “NON_COMPLIANT.” Select the SNS topic as the EventBridge rule target.",
        "text_jp": "AWS Configを有効化し、cloudtrail-s3-dataevents-enabledルールを追加する。「Config Rules Re-evaluation Status」イベントタイプのためのAmazon EventBridgeルールを作成し、「NON_COMPLIANT」に対してフィルターを設定する。SNSトピックをEventBridgeルールのターゲットとして選択する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating an S3 event notification for the isPublic event will alert the defined SNS topic whenever an S3 bucket is publicly accessible.",
        "situation_analysis": "The company needs a way to be notified specifically when any Amazon S3 bucket is exposed to the public.",
        "option_analysis": "Option A directly addresses the requirement of notifying only on public exposure. Option B, while it utilizes Access Analyzer, is more complex and does not directly give notifications for public exposure based specifically on S3 buckets. Options C and D involve monitoring API calls or compliance which is not directly related to public exposure notification.",
        "additional_knowledge": "",
        "key_terminology": "Amazon S3, event notification, public access, Amazon SNS.",
        "overall_assessment": "While the community leans towards option B based on the voting distribution, option A is indeed the most straightforward and effective solution to meet the requirements directly related to public exposure notifications."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。isPublicイベントのS3イベント通知を作成することで、S3バケットが公開アクセス可能になるたびに、指定されたSNSトピックへアラートが送信される。",
        "situation_analysis": "会社は、任意のAmazon S3バケットが公開されたときのみ通知を受け取れる方法を必要としている。",
        "option_analysis": "選択肢Aは、公開露出に対する通知の要件を直接満たしている。選択肢BはAccess Analyzerを利用しているが、S3バケットに特化した公開露出通知を提供するものではなく、より複雑である。選択肢CとDはAPIコールやコンプライアンスを監視するものであり、公開露出通知の要件には直接関連していない。",
        "additional_knowledge": "",
        "key_terminology": "Amazon S3, イベント通知, 公開アクセス, Amazon SNS。",
        "overall_assessment": "コミュニティの投票分布では選択肢Bに傾いているが、選択肢Aは公開露出通知に直接関連する要件を満たす最も単純かつ効果的な解決策である。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Amazon SNS",
      "event notification",
      "public access"
    ]
  },
  {
    "No": "174",
    "question": "A solutions architect needs to assess a newly acquired company's portfolio of applications and databases. The solutions architect must create a\nbusiness case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is\nnot well documented. The solutions architect cannot immediately determine how many applications and databases exist. Trafic for the\napplications is variable. Some applications are batch processes that run at the end of each month.\nThe solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.\nWhich solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、 newly acquired companyのアプリケーションとデータベースのポートフォリオを評価する必要があります。ソリューションアーキテクトは、ポートフォリオをAWSに移行するためのビジネスケースを作成しなければなりません。新しく取得した会社は、オンプレミスのデータセンターでアプリケーションを実行しています。データセンターは、十分に文書化されていません。ソリューションアーキテクトは、どれだけのアプリケーションとデータベースが存在するかをすぐに判断できません。アプリケーションのトラフィックは変動的であり、一部のアプリケーションは月末に実行されるバッチプロセスです。ソリューションアーキテクトは、AWSへの移行を開始する前に、ポートフォリオをよりよく理解する必要があります。どのソリューションがこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service Catalog to understand application and database dependencies.",
        "text_jp": "AWS Server Migration Service（AWS SMS）およびAWS Database Migration Service（AWS DMS）を使用して移行を評価します。AWS Service Catalogを使用してアプリケーションおよびデータベースの依存関係を理解します。"
      },
      {
        "key": "B",
        "text": "Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs and database dependencies.",
        "text_jp": "AWS Application Migration Serviceを使用します。オンプレミスのインフラストラクチャにエージェントを実行します。AWS Migration Hubを使用してエージェントを管理し、AWS Storage Gatewayを使用してローカルストレージのニーズとデータベースの依存関係を評価します。"
      },
      {
        "key": "C",
        "text": "Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies.",
        "text_jp": "Migration Evaluatorを使用してサーバーのリストを生成します。ビジネスケースのためのレポートを作成します。AWS Migration Hubを使用してポートフォリオを表示し、AWS Application Discovery Serviceを使用してアプリケーションの依存関係を理解します。"
      },
      {
        "key": "D",
        "text": "Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone for core accounts and resources.",
        "text_jp": "ターゲットアカウントでAWS Control Towerを使用してアプリケーションポートフォリオを生成します。AWS Server Migration Service（AWS SMS）を使用して詳細なレポートとビジネスケースを生成します。コアアカウントとリソースのためのランディングゾーンを使用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (95%) 5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. AWS Application Migration Service allows for better visibility into applications and their dependencies by running agents on the on-premises infrastructure.",
        "situation_analysis": "The company has a poorly documented data center and a variable traffic pattern of applications, making it imperative to gather information systematically before migration.",
        "option_analysis": "Using option B will provide insights into local storage needs and database dependencies, which is critical for developing a robust migration strategy.",
        "additional_knowledge": "Alternative assessments can include AWS Application Discovery Service for detailed application dependency mapping.",
        "key_terminology": "AWS Application Migration Service, AWS Migration Hub, AWS Storage Gateway, on-premises infrastructure",
        "overall_assessment": "Given that community support for option C is significantly high, it may indicate that more users favor a broader toolset; however, option B's focus on direct application assessment and migration aligns with the situation."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。AWS Application Migration Serviceは、オンプレミスのインフラストラクチャ上でエージェントを実行することにより、アプリケーションおよびその依存関係についてのより良い可視性を提供する。",
        "situation_analysis": "この会社は十分に文書化されていないデータセンターを持っており、アプリケーションのトラフィックパターンが変動的であるため、移行前に体系的に情報を収集することが重要である。",
        "option_analysis": "選択肢Bを使用することで、ローカルストレージのニーズやデータベースの依存関係についての洞察を得ることができ、堅牢な移行戦略を開発するために不可欠である。",
        "additional_knowledge": "代替評価には、アプリケーション依存関係の詳細なマッピングのためにAWS Application Discovery Serviceを含めることができる。",
        "key_terminology": "AWS Application Migration Service, AWS Migration Hub, AWS Storage Gateway, オンプレミス インフラストラクチャ",
        "overall_assessment": "コミュニティのC選択肢に対する支持が非常に高いことから、より多くのユーザーが広範なツールセットを好んでいる可能性があるが、選択肢Bの直接的なアプリケーション評価と移行に対する焦点は状況に合致している。"
      }
    ],
    "keywords": [
      "AWS Application Migration Service",
      "AWS Migration Hub",
      "AWS Storage Gateway",
      "on-premises infrastructure"
    ]
  },
  {
    "No": "175",
    "question": "A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS\ncluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances\nof the application. The company needs to back up the files and retain the backups for 1 year.\nWhich solution will meet these requirements while providing the FASTEST storage performance?",
    "question_jp": "ある企業が、Amazon Elastic Kubernetes Service（Amazon EKS）クラスターの複数のポッドのReplicaSetとして動作するアプリケーションを持っています。このEKSクラスターには、複数のアベイラビリティゾーンにノードがあります。アプリケーションは多くの小さなファイルを生成し、これらはアプリケーションのすべての実行インスタンスでアクセス可能でなければなりません。企業はファイルをバックアップし、バックアップを1年間保持する必要があります。どのソリューションがこれらの要件を満たし、最も高速なストレージパフォーマンスを提供するでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct the application to store files in the file system. Configure AWS Backup to back up and retain copies of the data for 1 year.",
        "text_jp": "Amazon Elastic File System（Amazon EFS）ファイルシステムを作成し、EKSクラスター内のノードが存在する各サブネットにマウントターゲットを作成する。ReplicaSetを構成してファイルシステムをマウントする。アプリケーションにファイルをファイルシステムに保存するよう指示する。AWS Backupを構成して、データのコピーを1年間バックアップして保持する。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume. Configure AWS Backup to back up and retain copies of the data for 1 year.",
        "text_jp": "Amazon Elastic Block Store（Amazon EBS）ボリュームを作成する。EBSマルチアタッチ機能を有効にする。ReplicaSetを構成してEBSボリュームをマウントする。アプリケーションにファイルをEBSボリュームに保存するよう指示する。AWS Backupを構成して、データのコピーを1年間バックアップして保持する。"
      },
      {
        "key": "C",
        "text": "Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecycle policy to delete objects after 1 year.",
        "text_jp": "Amazon S3バケットを作成する。ReplicaSetを構成してS3バケットをマウントする。アプリケーションにファイルをS3バケットに保存するよう指示する。S3バージョン管理を構成して、データのコピーを保持する。S3ライフサイクルポリシーを構成して、オブジェクトを1年後に削除する。"
      },
      {
        "key": "D",
        "text": "Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool to back up the EKS cluster for 1 year.",
        "text_jp": "ReplicaSetを構成して、実行中のアプリケーションポッドの各自に利用可能なストレージを使用してファイルをローカルに保存する。サードパーティツールを使用して、EKSクラスターを1年間バックアップする。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Create an Amazon Elastic File System (Amazon EFS).",
        "situation_analysis": "The application generates many small files that need to be accessed by multiple pods running in separate availability zones within the EKS cluster. These files require persistent storage that supports scalability and quick access.",
        "option_analysis": "Option A uses Amazon EFS, which is designed for shared access across multiple EC2 instances and provides high throughput and IOPS, making it suitable for applications generating many small files. Option B (EBS) does not support multi-node access and is not optimal for this use case. Option C (S3) offers durability but not the fastest access needed, and Option D lacks built-in backup solutions.",
        "additional_knowledge": "Using AWS Backup with EFS also ensures that the data can be reliably backed up and retained as required.",
        "key_terminology": "Amazon EFS, EKS, Availability Zones, AWS Backup",
        "overall_assessment": "Given the requirement for fast access and backup of small files across multiple application instances, Amazon EFS represents the best solution. It aligns with recognized AWS best practices for shared storage requirements in a containerized environment."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はA: Amazon Elastic File System（Amazon EFS）を作成することです。",
        "situation_analysis": "アプリケーションは多くの小さなファイルを生成し、これらはEKSクラスター内の異なるアベイラビリティゾーンで実行される複数のポッドによってアクセスされる必要があります。これらのファイルは、持続的なストレージを必要とし、拡張性と迅速なアクセスをサポートする必要があります。",
        "option_analysis": "選択肢AはAmazon EFSを使用しており、複数のEC2インスタンスからの共有アクセスを目的として設計されているため、スループットとIOPSが高く、多くの小さなファイルを生成するアプリケーションに適しています。選択肢B（EBS）は複数ノードのアクセスをサポートせず、このユースケースでは最適ではありません。選択肢C（S3）は耐久性を提供しますが、必要な迅速なアクセスを提供せず、選択肢Dは内蔵のバックアップソリューションが欠如しています。",
        "additional_knowledge": "EFSとともにAWS Backupを使用することで、データを信頼性高くバックアップし、必要なときに保持することができます。",
        "key_terminology": "Amazon EFS, EKS, アベイラビリティゾーン, AWS Backup",
        "overall_assessment": "小さなファイルの迅速なアクセスとバックアップが要求される中で、Amazon EFSは最良のソリューションであることを示しています。これは、コンテナ化された環境における共有ストレージ要件に対する認識されたAWSのベストプラクティスとも一致します。"
      }
    ],
    "keywords": [
      "Amazon EFS",
      "EKS",
      "Availability Zones",
      "AWS Backup"
    ]
  },
  {
    "No": "176",
    "question": "A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience\nsurvey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises\ndata center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to\nmigrate the system to AWS to improve reliability.\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?",
    "question_jp": "ある会社は、顧客サービスセンターを運営しており、通話を受け付け、すべての顧客に管理されたインタラクティブな双方向体験調査をテキストメッセージで自動的に送信しています。顧客サービスセンターをサポートするアプリケーションは、会社が自社データセンターにホストしているマシン上で実行されています。会社が使用しているハードウェアは古く、システムのダウンタイムが発生しています。会社は、信頼性を向上させるためにAWSにシステムを移行したいと考えています。この要件を最小限の運用オーバーヘッドで満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.",
        "text_jp": "Amazon Connectを使用して古いコールセンターハードウェアを置き換えます。Amazon Pinpointを使用して顧客にテキストメッセージの調査を送信します。"
      },
      {
        "key": "B",
        "text": "Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers.",
        "text_jp": "Amazon Connectを使用して古いコールセンターハードウェアを置き換えます。Amazon Simple Notification Service (Amazon SNS)を使用して顧客にテキストメッセージの調査を送信します。"
      },
      {
        "key": "C",
        "text": "Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers.",
        "text_jp": "コールセンターソフトウェアをAuto Scalingグループ内のAmazon EC2インスタンスに移行します。EC2インスタンスを使用して顧客にテキストメッセージの調査を送信します。"
      },
      {
        "key": "D",
        "text": "Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers.",
        "text_jp": "Amazon Pinpointを使用して古いコールセンターハードウェアを置き換え、顧客にテキストメッセージの調査を送信します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using Amazon Connect will replace traditional call center hardware and integrating with Amazon Pinpoint allows the sending of interactive two-way surveys via text message, which meets the requirements efficiently.",
        "situation_analysis": "The company needs to improve the reliability of its customer service operations due to old hardware and frequent downtimes, while minimizing operational overhead after migration to AWS.",
        "option_analysis": "Option A is correct as it leverages AWS managed services, reducing the need for ongoing maintenance. Option B involves Amazon SNS, which is not as feature-rich for interactive messaging compared to Pinpoint. Option C increases complexity and maintenance due to the need to manage EC2 instances. Option D replaces hardware with Pinpoint but does not cover call center capabilities.",
        "additional_knowledge": "The use of Amazon Connect and Pinpoint aligns with AWS best practices for building resilient and scalable applications.",
        "key_terminology": "Amazon Connect, Amazon Pinpoint, Auto Scaling, EC2, Amazon SNS",
        "overall_assessment": "There is strong community support for Option A, as indicated by 100% voting. This suggests that the community recognizes the operational ease that comes from using fully managed services."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAである。Amazon Connectを使用することで、従来のコールセンターハードウェアを置き換え、Amazon Pinpointと統合することで、テキストメッセージを通じてインタラクティブな双方向調査を送信することができ、要件を効率的に満たすことができる。",
        "situation_analysis": "会社は、古いハードウェアと頻繁なダウンタイムのために、顧客サービス業務の信頼性を向上させる必要があり、AWSへの移行後に運用オーバーヘッドを最小化したいと考えている。",
        "option_analysis": "Aは正しい選択であり、AWSのマネージドサービスを活用しているため、運用メンテナンスの必要が減少する。BはAmazon SNSを使用しているが、Pinpointに比べてインタラクティブなメッセージングの機能が不足している。CはEC2インスタンスの管理が必要となるため、複雑性とメンテナンスが増加する。DはハードウェアをPinpointで置き換えるが、コールセンターの機能をカバーしていない。",
        "additional_knowledge": "Amazon ConnectとPinpointの利用は、頑健でスケーラブルなアプリケーションの構築に関するAWSのベストプラクティスに沿っている。",
        "key_terminology": "Amazon Connect, Amazon Pinpoint, Auto Scaling, EC2, Amazon SNS",
        "overall_assessment": "コミュニティのサポートが非常に強いAの選択肢は、100%の投票によって示されている。これは、コミュニティが完全管理サービスを使用することによる運用の容易さを認識していることを示唆している。"
      }
    ],
    "keywords": [
      "Amazon Connect",
      "Amazon Pinpoint",
      "Auto Scaling",
      "EC2",
      "Amazon SNS"
    ]
  },
  {
    "No": "177",
    "question": "A company is building a call center by using Amazon Connect. The company's operations team is defining a disaster recovery (DR) strategy across\nAWS Regions. The contact center has dozens of contact fiows, hundreds of users, and dozens of claimed phone numbers.\nWhich solution will provide DR with the LOWEST RTO?",
    "question_jp": "A company is building a call center by using Amazon Connect. The company's operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact fiows, hundreds of users, and dozens of claimed phone numbers. Which solution will provide DR with the LOWEST RTO?",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact fiows, users, and claimed phone numbers by using an AWS CloudFormation template.",
        "text_jp": "Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact fiows, users, and claimed phone numbers by using an AWS CloudFormation template."
      },
      {
        "key": "B",
        "text": "Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact fiows and claimed numbers in the second Region.",
        "text_jp": "Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact fiows and claimed numbers in the second Region."
      },
      {
        "key": "C",
        "text": "Provision a new Amazon Connect instance with all existing contact fiows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function.",
        "text_jp": "Provision a new Amazon Connect instance with all existing contact fiows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function."
      },
      {
        "key": "D",
        "text": "Provision a new Amazon Connect instance with all existing users and contact fiows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function.",
        "text_jp": "Provision a new Amazon Connect instance with all existing users and contact fiows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (82%) B (18%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This option provisions a new Amazon Connect instance with all existing users and contact flows in a second Region, which ensures rapid recovery.",
        "situation_analysis": "The company needs a disaster recovery strategy that minimizes the Recovery Time Objective (RTO) for the Amazon Connect instance. All users and configurations should be ready to maintain continuity in operations.",
        "option_analysis": "Option A involves manual intervention after a notification, leading to increased RTO. Option B also requires Lambda functions to provision contact flows post-failure, which may delay recovery. Option C suggests provisioning users after an incident, which can cause further delays. Option D mitigates these issues by preparing a full setup in advance.",
        "additional_knowledge": "In practice, organizations often implement such strategies to ensure high availability and business continuity.",
        "key_terminology": "Amazon Connect, Disaster Recovery, Recovery Time Objective (RTO), AWS Lambda, AWS CloudFormation, Amazon EventBridge, Amazon Route 53, Amazon CloudWatch",
        "overall_assessment": "Overall, Option D shows best practices in DR strategy by ensuring users and contact flows are already provisioned, significantly reducing downtime during an operational failure."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。このオプションは、既存のすべてのユーザーとコンタクトフローを含む新しいAmazon Connectインスタンスを2番目のリージョンにプロビジョニングし、迅速な回復を確保する。",
        "situation_analysis": "企業は、Amazon Connectインスタンスのリカバリータイム目標（RTO）を最小限に抑える災害復旧戦略を必要としている。すべてのユーザーと設定が運用の継続性を維持するために準備されている必要がある。",
        "option_analysis": "オプションAは通知後に手動介入が必要であり、RTOが増加する。オプションBも事故後にコンタクトフローをプロビジョニングするためのLambda関数が必要であり、回復が遅れる可能性がある。オプションCは事故後にユーザーをプロビジョニングすることを提案しており、さらなる遅延を引き起こす可能性がある。オプションDはこれらの問題を軽減し、事前に完全なセットアップを準備することで、ダウンタイムを最小限に抑えている。",
        "additional_knowledge": "実際には、組織は高可用性とビジネス継続性を確保するためにこのような戦略を実装することが多い。",
        "key_terminology": "Amazon Connect、災害復旧、リカバリータイム目標（RTO）、AWS Lambda、AWS CloudFormation、Amazon EventBridge、Amazon Route 53、Amazon CloudWatch",
        "overall_assessment": "全体として、オプションDはユーザーとコンタクトフローが事前にプロビジョニングされていることを確保し、運用障害時のダウンタイムを大幅に削減することでDR戦略のベストプラクティスを示している。"
      }
    ],
    "keywords": [
      "Amazon Connect",
      "Disaster Recovery",
      "Recovery Time Objective",
      "AWS Lambda",
      "AWS CloudFormation",
      "Amazon EventBridge",
      "Amazon Route 53",
      "Amazon CloudWatch"
    ]
  },
  {
    "No": "178",
    "question": "A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to\nperform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift\ntables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits\nthe files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has\nbecome dificult.\nThe company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants\nto confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the\ncompany publishes the data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がAWS上でアプリケーションを運営しています。この企業は、いくつかの異なるソースからデータを収集しています。この企業は、独自のアルゴリズムを使用してデータの変換と集計を行います。企業がETLプロセスを実行した後、結果をAmazon Redshiftのテーブルに保存します。この企業は、他の企業にこのデータを販売しています。企業はAmazon Redshiftのテーブルからデータをファイルとしてダウンロードし、FTPを使用していくつかのデータ顧客にファイルを送信します。データ顧客の数が大幅に増加したため、データ顧客の管理が困難になっています。企業は、AWS Data Exchangeを使用して、顧客とデータを共有するためのデータ製品を作成します。企業は、データを共有する前に顧客の身元を確認したいと考えています。顧客はまた、企業がデータを公開するときに最新のデータにアクセスする必要があります。運用のオーバーヘッドが最も少なくなるようにするためには、どの解決策が要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company that produces the data, create an Amazon API Gateway Data API service integration with Amazon Redshift. Require the data customers to subscribe to the data product.",
        "text_jp": "AWS Data ExchangeのAPIを使用して顧客とデータを共有します。サブスクリプションの確認を設定します。データを生成する企業のAWSアカウントにて、Amazon API Gateway Data APIサービス統合をAmazon Redshiftと作成します。データ顧客にデータ製品をサブスクライブすることを要求します。"
      },
      {
        "key": "B",
        "text": "In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster. Configure subscription verification. Require the data customers to subscribe to the data product.",
        "text_jp": "データを生成する企業のAWSアカウントで、AWS Data Exchangeのデータ共有を作成し、AWS Data ExchangeをRedshiftクラスターに接続します。サブスクリプションの確認を設定します。データ顧客にデータ製品をサブスクライブすることを要求します。"
      },
      {
        "key": "C",
        "text": "Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product.",
        "text_jp": "定期的にAmazon RedshiftのテーブルからデータをAmazon S3バケットにダウンロードします。AWS Data Exchange for S3を使用して顧客とデータを共有します。サブスクリプションの確認を設定します。データ顧客にデータ製品をサブスクライブすることを要求します。"
      },
      {
        "key": "D",
        "text": "Publish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS Data Exchange. In the AWS account of the company that produces the data, attach IAM resource-based policies to the Amazon Redshift tables to allow access only to verified AWS accounts.",
        "text_jp": "Amazon RedshiftのデータをOpen Data on AWS Data Exchangeに公開します。顧客にAWS Data Exchangeでデータ製品をサブスクライブすることを要求します。データを生成する企業のAWSアカウントにて、IAMリソースベースのポリシーをAmazon Redshiftのテーブルに付与して確認済みのAWSアカウントのみにアクセスを許可します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (83%) C (17%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B because it directly utilizes AWS Data Exchange connected to Amazon Redshift, allowing for seamless data sharing with customer subscription verification.",
        "situation_analysis": "The company is transitioning from manual FTP sharing to a more automated process using AWS Data Exchange, indicating a need for streamlined access management and customer verification.",
        "option_analysis": "Option A introduces additional complexity by requiring API Gateway integration, which increases operational overhead. Option C, while automated, still involves periodic downloads, adding unnecessary steps. Option D complicates access management by relying on IAM policies for verified accounts after publishing data openly.",
        "additional_knowledge": "Utilizing AWS Data Exchange helps comply with data governance and privacy regulations, ensuring that customer identities are verified before data sharing.",
        "key_terminology": "AWS Data Exchange, Amazon Redshift, ETL, subscription verification, data sharing.",
        "overall_assessment": "The choice of B is further validated by community votes, suggesting broad agreement on its effectiveness in fulfilling the requirements with minimal operational complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、AWS Data ExchangeがAmazon Redshiftと接続されているため、顧客のサブスクリプション確認とともにデータ共有をシームレスに行うことができます。",
        "situation_analysis": "企業は手動のFTP共有からAWS Data Exchangeを使用したより自動化されたプロセスに移行しており、アクセス管理と顧客確認の効率化が必要です。",
        "option_analysis": "選択肢AはAPI Gateway統合を必要とし、運用に対するオーバーヘッドを増加させるため複雑です。選択肢Cは自動化されていますが、定期的なダウンロードを含むため、余分な手順が発生します。選択肢Dはデータを公開した後にIAMポリシーに依存し、確認済みのアカウントへのアクセス管理を複雑にします。",
        "additional_knowledge": "AWS Data Exchangeの利用は、データガバナンスやプライバシー規制を遵守するのに役立ち、データ共有前に顧客の身元を確認することを確実にします。",
        "key_terminology": "AWS Data Exchange, Amazon Redshift, ETL, サブスクリプション確認, データ共有。",
        "overall_assessment": "選択肢Bが選ばれたのは、運用の複雑さを最小限に抑えて要件を満たすのに効果的であるとの広範な合意があることから、コミュニティの投票によっても裏付けられています。"
      }
    ],
    "keywords": [
      "AWS Data Exchange",
      "Amazon Redshift",
      "ETL",
      "subscription verification",
      "data sharing"
    ]
  },
  {
    "No": "179",
    "question": "A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of\nevents that the solution receives. If a processing error occurs, the event must move into a separate queue for review.\nWhich solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトが、イベントを処理するソリューションを設計しています。このソリューションは、受信するイベントの数に基づいてスケールインおよびスケールアウトする能力を持たなければなりません。処理エラーが発生した場合、イベントはレビューのために別のキューに移動しなければなりません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Send event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Add an on-failure destination to the function. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target.",
        "text_jp": "Amazon Simple Notification Service (Amazon SNS) トピックにイベントの詳細を送信します。SNS トピックのサブスクライバーとして AWS Lambda 関数を設定し、イベントを処理します。関数に失敗時の宛先を追加します。ターゲットとして Amazon Simple Queue Service (Amazon SQS) キューを設定します。"
      },
      {
        "key": "B",
        "text": "Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messages to a dead-letter queue.",
        "text_jp": "Amazon Simple Queue Service (Amazon SQS) キューにイベントを公開します。Amazon EC2 Auto Scaling グループを作成します。キューの ApproximateAgeOfOldestMessage メトリックに基づいて、Auto Scaling グループがスケールインおよびスケールアウトするように設定します。アプリケーションが失敗したメッセージをデッドレターキューに書き込むように設定します。"
      },
      {
        "key": "C",
        "text": "Write events to an Amazon DynamoDB table. Configure a DynamoDB stream for the table. Configure the stream to invoke an AWS Lambda function. Configure the Lambda function to process the events.",
        "text_jp": "イベントを Amazon DynamoDB テーブルに書き込みます。テーブルの DynamoDB ストリームを設定します。ストリームが AWS Lambda 関数を呼び出すように設定します。Lambda 関数がイベントを処理します。"
      },
      {
        "key": "D",
        "text": "Publish events to an Amazon EventBndge event bus. Create and run an application on an Amazon EC2 instance with an Auto Scaling group that is behind an Application Load Balancer (ALB). Set the ALB as the event bus target. Configure the event bus to retry events. Write messages to a dead-letter queue if the application cannot process the messages.",
        "text_jp": "Amazon EventBridge イベントバスにイベントを公開します。Application Load Balancer (ALB) の背後にある Amazon EC2 インスタンス上でアプリケーションを作成および実行します。ALB をイベントバスのターゲットとして設定します。イベントバスがイベントを再試行するように設定します。アプリケーションがメッセージを処理できない場合は、デッドレターキューにメッセージを書き込みます。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (53%) A (47%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which effectively uses Amazon SQS and EC2 Auto Scaling to handle events dynamically.",
        "situation_analysis": "The requirement is for a scalable solution that processes events and handles failures. SQS fits these requirements as it queues messages, allowing for automatic scaling of processing resources according to the message load.",
        "option_analysis": "Option B is the best fit as it uses SQS, which supports scaling through Auto Scaling. Options A, C, and D do not effectively utilize scalable models for processing error handling in conjunction with event management.",
        "additional_knowledge": "Understanding how SQS integrates with Auto Scaling is crucial, particularly the metrics that trigger scaling actions.",
        "key_terminology": "Amazon SQS, Amazon EC2 Auto Scaling, Dead-Letter Queue, ApproximateAgeOfOldestMessage, Event Processing.",
        "overall_assessment": "B is supported by 53% of the community votes, indicating its acceptance as a viable solution. Other options, while valid, do not meet the requirements as effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、Amazon SQSとEC2 Auto Scalingを効果的に使用して、イベントを動的に処理します。",
        "situation_analysis": "要件は、イベントを処理し、障害を処理できるスケーラブルなソリューションです。SQSはメッセージをキューイングし、メッセージの負荷に応じて処理リソースの自動スケーリングを可能にするため、これらの要件に適しています。",
        "option_analysis": "BオプションはSQSを使用しており、Auto Scalingを通じてスケーリングをサポートしているため、最適です。A、C、Dのオプションは、イベント管理との連携で処理エラーを効果的に利用していません。",
        "additional_knowledge": "特にスケーリングアクションをトリガーするメトリックについて、SQSがAuto Scalingと統合する方法を理解することが重要です。",
        "key_terminology": "Amazon SQS, Amazon EC2 Auto Scaling, デッドレターキュー, ApproximateAgeOfOldestMessage, イベント処理。",
        "overall_assessment": "Bは53%のコミュニティ投票によって支持されており、実現可能なソリューションとして受け入れられています。他のオプションも有効ですが、要件をあまり効果的に満たしていません。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "Amazon EC2 Auto Scaling",
      "Dead-Letter Queue",
      "Event Processing",
      "ApproximateAgeOfOldestMessage"
    ]
  },
  {
    "No": "180",
    "question": "A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a\nsustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the\nprocessing engine through a RESTful API.\nThe API experiences unpredictable bursts of trafic. The company must implement a solution to process all data that the devices send to the\nprocessing engine. Data loss is unacceptable.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWSクラウドで処理エンジンを運営しています。このエンジンは、物流センターから環境データを処理して持続可能性指数を計算します。企業は、ヨーロッパ中に分散している数百万のデバイスを持つ物流センターを運営しています。デバイスは、RESTful APIを介して処理エンジンに情報を送信します。このAPIは予測不可能なトラフィックの急増を経験しています。企業は、デバイスが処理エンジンに送信するすべてのデータを処理するソリューションを実装しなければなりません。データの損失は許容されません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a listener and a target group for the ALB Add the SQS queue as the target. Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to process messages in the queue.",
        "text_jp": "RESTful API用のアプリケーションロードバランサー（ALB）を作成します。Amazon Simple Queue Service（Amazon SQS）キューを作成します。ALBのリスナーとターゲットグループを作成します。SQSキューをターゲットとして追加します。メッセージをキューで処理するために、Amazon Elastic Container Service（Amazon ECS）でFargate起動タイプのコンテナを使用します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue.",
        "text_jp": "RESTful APIを実装するAmazon API Gateway HTTP APIを作成します。Amazon Simple Queue Service（Amazon SQS）キューを作成します。SQSキューとのAPI Gatewayサービス統合を作成します。SQSキューのメッセージを処理するためのAWS Lambda関数を作成します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon API Gateway REST API that implements the RESTful API. Create a fieet of Amazon EC2 instances in an Auto Scaling group. Create an API Gateway Auto Scaling group proxy integration. Use the EC2 instances to process incoming data.",
        "text_jp": "RESTful APIを実装するAmazon API Gateway REST APIを作成します。Auto Scalingグループ内にAmazon EC2インスタンスの fleetを作成します。API Gateway Auto Scalingグループプロキシ統合を作成します。EC2インスタンスを使用して、受信データを処理します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream as the origin for the distribution. Create an AWS Lambda function to consume and process data in the data stream.",
        "text_jp": "RESTful API用のAmazon CloudFrontディストリビューションを作成します。Amazon Kinesis Data Streamsのデータストリームを作成します。ディストリビューションのオリジンとしてデータストリームを設定します。データストリーム内のデータを消費し処理するAWS Lambda関数を作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (84%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. By using API Gateway with SQS, the solution can handle unpredictable spikes in traffic without data loss.",
        "situation_analysis": "The company needs a scalable solution to process data from millions of devices without losing any information.",
        "option_analysis": "Option B is the best choice as it integrates API Gateway with SQS, allowing messages to be queued and processed asynchronously. Options A and C rely on EC2, which may not scale as easily. Option D introduces CloudFront and Kinesis, which are more complex for this requirement.",
        "additional_knowledge": "It’s crucial to configure the SQS with sufficient visibility timeout and maximum message size settings to ensure reliable processing.",
        "key_terminology": "Amazon API Gateway, AWS Lambda, Amazon SQS, RESTful API, Asynchronous Processing",
        "overall_assessment": "Option B receives significant community support, validating its effectiveness for high-throughput scenarios."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。API GatewayとSQSを使用することで、ソリューションは予測不可能なトラフィックの急増を処理し、データ損失を防ぐことができる。",
        "situation_analysis": "企業は、数百万のデバイスからのデータを損失なく処理するためのスケーラブルなソリューションを必要としている。",
        "option_analysis": "選択肢BはAPI GatewayとSQSを統合しているため、メッセージをキューに入れ非同期に処理できるので最良の選択肢である。選択肢AとCはEC2に依存しており、スケールが難しい可能性がある。選択肢DはCloudFrontとKinesisを導入するが、この要件に対してはより複雑である。",
        "additional_knowledge": "SQSの可視性タイムアウトや最大メッセージサイズ設定を適切に構成することが、信頼性のある処理を保証するために重要である。",
        "key_terminology": "Amazon API Gateway, AWS Lambda, Amazon SQS, RESTful API, 非同期処理",
        "overall_assessment": "選択肢Bはコミュニティから高い支持を得ており、高スループットシナリオにおける効果を検証している。"
      }
    ],
    "keywords": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon SQS",
      "RESTful API",
      "Asynchronous Processing"
    ]
  },
  {
    "No": "181",
    "question": "A company is designing its network configuration in the AWS Cloud. The company uses AWS Organizations to manage a multi-account setup. The\ncompany has three OUs. Each OU contains more than 100 AWS accounts. Each account has a single VPC, and all the VPCs in each OU are in the\nsame AWS Region.\nThe CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution in which VPCs in the same OU can\ncommunicate with each other but cannot communicate with VPCs in other OUs.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "企業はAWSクラウドにおけるネットワーク構成を設計しています。企業はAWS Organizationsを使用してマルチアカウントのセットアップを管理しています。企業には3つのOUがあり、それぞれのOUには100以上のAWSアカウントがあります。各アカウントには単一のVPCがあり、各OU内のすべてのVPCは同じAWSリージョンにあります。すべてのAWSアカウントのCIDR範囲は重複していません。企業は、同じOU内のVPC同士が相互に通信できる一方で、他のOUのVPCとは通信できないソリューションを実装する必要があります。運用のオーバーヘッドが最も少ない要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS CloudFormation stack set that establishes VPC peering between accounts in each OU. Provision the stack set in each OU.",
        "text_jp": "AWS CloudFormationスタックセットを作成し、各OUのアカウント間でVPCピアリングを確立します。スタックセットを各OUにプロビジョニングします。"
      },
      {
        "key": "B",
        "text": "In each OU, create a dedicated networking account that has a single VPC. Share this VPC with all the other accounts in the OU by using AWS Resource Access Manager (AWS RAM). Create a VPC peering connection between the networking account and each account in the OU.",
        "text_jp": "各OUに、単一のVPCを持つ専用のネットワーキングアカウントを作成します。このVPCをAWSリソースアクセスマネージャー（AWS RAM）を使用してOU内の他のすべてのアカウントと共有します。ネットワーキングアカウントとOU内の各アカウントとの間にVPCピアリング接続を作成します。"
      },
      {
        "key": "C",
        "text": "Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM). Create transit gateway VPC attachments for each VPC.",
        "text_jp": "各OUのアカウントにトランジットゲートウェイをプロビジョニングします。AWSリソースアクセスマネージャー（AWS RAM）を使用してトランジットゲートウェイを組織全体で共有します。各VPCに対してトランジットゲートウェイVPCアタッチメントを作成します。"
      },
      {
        "key": "D",
        "text": "In each OU, create a dedicated networking account that has a single VPC. Establish a VPN connection between the networking account and the other accounts in the OU. Use third-party routing software to route transitive trafic between the VPCs.",
        "text_jp": "各OUに、単一のVPCを持つ専用のネットワーキングアカウントを作成します。ネットワーキングアカウントとOU内の他のアカウントとの間にVPN接続を確立します。サードパーティのルーティングソフトウェアを使用してVPC間の遷移トラフィックをルーティングします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (70%) B (17%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, which provides a dedicated networking account with a VPN connection to route transitive traffic efficiently.",
        "situation_analysis": "The company's requirement is to enable VPC communication within the same OU without external communication, and the least operational overhead is also a critical aspect.",
        "option_analysis": "Option D allows dedicated management through a VPN without complicating the architecture with additional peering or transit gateways, which could be more operationally intensive.",
        "additional_knowledge": "Implementing a VPN is aligned with AWS best practices for managing multi-account strategies.",
        "key_terminology": "AWS Organizations, VPC, VPN, transitive routing, AWS Resource Access Manager.",
        "overall_assessment": "While other options may seem appealing, option D ensures effective management and meets the requirement for minimal operational overhead."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はDであり、専用のネットワーキングアカウントを持ち、遷移トラフィックを効率的にルーティングするためのVPN接続を提供します。",
        "situation_analysis": "企業の要件は、同じOU内のVPC間で通信を可能にし、外部との通信を行わないことであり、運用のオーバーヘッドが最小であることも重要な要素です。",
        "option_analysis": "選択肢Dは、追加のピアリングやトランジットゲートウェイでアーキテクチャを複雑にすることなく、VPNによって専用の管理を可能にします。",
        "additional_knowledge": "VPNの実装は、マルチアカウント戦略を管理するためのAWSのベストプラクティスに沿っています。",
        "key_terminology": "AWS Organizations、VPC、VPN、遷移ルーティング、AWSリソースアクセスマネージャー。",
        "overall_assessment": "他のオプションも魅力的に見えるかもしれませんが、選択肢Dは効果的な管理を保証し、最小の運用オーバーヘッドの要件を満たします。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "VPC",
      "VPN",
      "transitive routing",
      "AWS Resource Access Manager"
    ]
  },
  {
    "No": "182",
    "question": "A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company\nneeds to store large important documents within the application with the following requirements:",
    "question_jp": "企業がアプリケーションをAWSに移行しています。移行中に可能な限り完全に管理されたサービスを使用したいと考えています。企業は、以下の要件を持つ重要な文書をアプリケーション内に保存する必要があります。",
    "choices": [
      {
        "key": "1",
        "text": "The data must be highly durable and available",
        "text_jp": "データは高い耐久性と可用性を持たなければならない"
      },
      {
        "key": "2",
        "text": "The data must always be encrypted at rest and in transit",
        "text_jp": "データは常に静止時と転送時に暗号化されていなければならない"
      },
      {
        "key": "3",
        "text": "The encryption key must be managed by the company and rotated periodically Which of the following solutions should the solutions architect recommend?",
        "text_jp": "暗号化キーは企業によって管理され、定期的にローテーションされる必要がある      次のソリューションの中で、ソリューションアーキテクトが推奨すべきものはどれですか？"
      },
      {
        "key": "A",
        "text": "Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.",
        "text_jp": "ストレージゲートウェイをAWSにファイルゲートウェイモードで展開します。AWS KMSキーを使用して、ストレージゲートウェイボリュームを暗号化するためにAmazon EBSボリューム暗号化を使用します。"
      },
      {
        "key": "B",
        "text": "Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.",
        "text_jp": "Amazon S3を使用し、バケットポリシーを使用してバケットへの接続のHTTPSを強制し、サーバーサイド暗号化とオブジェクト暗号化のためのAWS KMSを強制します。"
      },
      {
        "key": "C",
        "text": "Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.",
        "text_jp": "SSLを使用してAmazon DynamoDBに接続します。AWS KMSキーを使用してDynamoDBオブジェクトを静止時に暗号化します。"
      },
      {
        "key": "D",
        "text": "Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data.",
        "text_jp": "データを保存するためにAmazon EBSボリュームがアタッチされたインスタンスを展開します。データを暗号化するためにAWS KMSキーを使用してEBSボリューム暗号化を使用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct answer: B. Using Amazon S3 with a bucket policy enables HTTPS and enforces encryption requirements.",
        "situation_analysis": "The company needs a solution that is fully managed, highly durable, available, and meets the encryption requirements.",
        "option_analysis": "Option B meets all requirements: durability, HTTPS for data in transit, and supports server-side encryption with AWS KMS. Other options either do not fully manage the encryption or lack necessary features.",
        "additional_knowledge": "Using bucket policies in S3 improves data security by enforcing mandatory encryption.",
        "key_terminology": "Amazon S3, KMS, encryption, HTTPS, server-side encryption",
        "overall_assessment": "Option B is the most suitable solution for the provided requirements, and the community vote supports this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解: B。Amazon S3を使用し、バケットポリシーを使ってHTTPSを強制し、暗号化要件を遵守させることができます。",
        "situation_analysis": "企業は、完全に管理されており、高い耐久性と可用性を持ち、暗号化要件を満たすソリューションを必要としています。",
        "option_analysis": "選択肢Bは、全ての要件を満たしています: 耐久性、伝送中のデータに対するHTTPS、およびAWS KMSによるサーバーサイド暗号化のサポート。他の選択肢は、暗号化が完全に管理されていないか、必要な機能を欠いています。",
        "additional_knowledge": "S3のバケットポリシーを使用することで、強制的な暗号化を実施し、データのセキュリティが向上します。",
        "key_terminology": "Amazon S3, KMS, 暗号化, HTTPS, サーバーサイド暗号化",
        "overall_assessment": "選択肢Bは、提供された要件に最も適したソリューションであり、コミュニティの投票もこの選択を支持しています。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "KMS",
      "encryption",
      "HTTPS",
      "server-side encryption"
    ]
  },
  {
    "No": "183",
    "question": "A company's public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application\nLoad Balancer (ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for\nseveral months.\nRecently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection\nattacks had occurred against the API and that the API service had scaled to its maximum amount.\nA solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must\nallow legitimate trafic through and must maximize operational eficiency.\nWhich solution meets these requirements?",
    "question_jp": "企業の公開APIは、Amazon Elastic Container Service (Amazon ECS) 上のタスクとして実行されています。タスクはAWS Fargate上で実行され、アプリケーションロードバランサー (ALB) の背後に配置され、サービスオートスケーリングがCPU使用率に基づいて構成されています。このサービスは、数ヶ月間正常に稼働していました。最近、APIのパフォーマンスが低下し、アプリケーションが使用不可能になりました。企業は、APIに対して多くのSQLインジェクション攻撃が発生し、APIサービスが最大数までスケールしてしまったことを発見しました。ソリューションアーキテクトは、SQLインジェクション攻撃がECS APIサービスに到達しないようにするソリューションを実装する必要があります。このソリューションは、正当なトラフィックを通し、運用効率を最大化する必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks.",
        "text_jp": "ALBの前にあるECSタスクに転送されるHTTPリクエストおよびHTTPSリクエストを監視するAWS WAFウェブACLを新たに作成する。"
      },
      {
        "key": "B",
        "text": "Create a new AWS WAF Bot Control implementation. Add a rule in the AWS WAF Bot Control managed rule group to monitor trafic and allow only legitimate trafic to the ALB in front of the ECS tasks.",
        "text_jp": "新たにAWS WAFボットコントロールの実装を作成する。AWS WAFボットコントロール管理ルールグループにルールを追加し、ALBの前にあるECSタスクに対して正当なトラフィックのみを許可する。"
      },
      {
        "key": "C",
        "text": "Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other trafic that does not match those rules. Attach the web ACL to the ALB in front of the ECS tasks.",
        "text_jp": "新たにAWS WAFウェブACLを作成する。SQLデータベースルールグループに一致するリクエストをブロックする新しいルールを追加する。それ以外のすべてのトラフィックはそのルールに一致しない限り許可するようにウェブACLを設定する。ウェブACLをALBにアタッチする。"
      },
      {
        "key": "D",
        "text": "Create a new AWS WAF web ACL. Create a new empty IP set in AWS WAF. Add a new rule to the web ACL to block requests that originate from IP addresses in the new IP set. Create an AWS Lambda function that scrapes the API logs for IP addresses that send SQL injection attacks, and add those IP addresses to the IP set. Attach the web ACL to the ALB in front of the ECS tasks.",
        "text_jp": "新たにAWS WAFウェブACLを作成する。AWS WAFに新しい空のIPセットを作成する。新しいルールをウェブACLに追加し、新しいIPセット内のIPアドレスからのリクエストをブロックする。SQLインジェクション攻撃を送信するIPアドレスをAPIログからスクリーピングし、それらのIPアドレスをIPセットに追加するAWS Lambda関数を作成する。ウェブACLをALBにアタッチする。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which involves creating a new AWS WAF web ACL to block SQL injection attempts while allowing legitimate traffic.",
        "situation_analysis": "The API has been experiencing performance issues due to an increase in SQL injection attacks. The existing configurations have reached their scaling limits.",
        "option_analysis": "Option C correctly implements an AWS WAF web ACL to block requests matching SQL injection patterns, safeguarding the API while allowing normal use. Option A lacks specificity in dealing with SQL attacks, Option B does not directly address SQL injection, and Option D introduces maintenance overhead with IP sets.",
        "additional_knowledge": "Regular updates to WAF rules can help adapt to emerging threats.",
        "key_terminology": "AWS WAF, SQL injection, web ACL, ALB, traffic management",
        "overall_assessment": "Given the need to address SQL injection directly and efficiently, Option C is the most suitable choice based on its targeted approach."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCであり、SQLインジェクションの試みをブロックしつつ、正当なトラフィックを許可するために新しいAWS WAFウェブACLを作成する内容である。",
        "situation_analysis": "APIはSQLインジェクション攻撃の増加によりパフォーマンスの問題に直面している。既存の構成はスケーリングの限界に達している。",
        "option_analysis": "選択肢Cは、SQLインジェクションパターンに一致するリクエストをブロックするAWS WAFウェブACLを正しく実装し、APIを保護しながら通常の使用を許可する。選択肢AはSQL攻撃への対策が具体的ではなく、選択肢BはSQLインジェクションに直接対処していない。また、選択肢DはIPセットの維持管理のオーバーヘッドを伴う。",
        "additional_knowledge": "WAFルールの定期的な更新は、新たな脅威への適応に役立つ。",
        "key_terminology": "AWS WAF, SQLインジェクション, ウェブACL, ALB, トラフィック管理",
        "overall_assessment": "SQLインジェクションに直接効率的に対処する必要があるため、選択肢Cが最も適切な選択肢である。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "SQL injection",
      "web ACL",
      "ALB",
      "traffic management"
    ]
  },
  {
    "No": "184",
    "question": "An environmental company is deploying sensors in major cities throughout a country to measure air quality. The sensors connect to AWS IoT Core\nto ingest timeseries data readings. The company stores the data in Amazon DynamoDB.\nFor business continuity, the company must have the ability to ingest and store data in two AWS Regions.\nWhich solution will meet these requirements?",
    "question_jp": "環境会社が国内の主要都市にセンサーを展開し、空気質を測定する。そのセンサーはAWS IoT Coreに接続し、時系列データの読み取りを取り込む。会社はデータをAmazon DynamoDBに保存する。ビジネス継続性のため、会社は2つのAWSリージョンでデータを取り込み、保存する能力を持つ必要がある。どのソリューションがこれらの要件を満たすか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core data endpoints in both Regions Migrate data to Amazon Aurora global tables.",
        "text_jp": "Amazon Route 53エイリアスフォールオーバールーティングポリシーを作成し、両方のリージョンでのAWS IoT Coreデータエンドポイントの値を設定する。データをAmazon Auroraグローバルテーブルに移行する。"
      },
      {
        "key": "B",
        "text": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Migrate the data to Amazon MemoryDB for Redis and configure cross-Region replication.",
        "text_jp": "各リージョンに対してAWS IoT Coreのドメイン構成を作成する。Amazon Route 53遅延ベースルーティングポリシーを作成する。両方のリージョンのAWS IoT Coreデータエンドポイントを値として使用する。データをAmazon MemoryDB for Redisに移行し、クロスリージョンレプリケーションを設定する。"
      },
      {
        "key": "C",
        "text": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations. Update the DynamoDB table to a global table.",
        "text_jp": "各リージョンに対してAWS IoT Coreのドメイン構成を作成する。ドメイン構成の正常性を評価するAmazon Route 53ヘルスチェックを作成する。AWS IoT Coreドメイン構成からのドメイン名の値を持つフォールオーバールーティングポリシーを作成する。DynamoDBテーブルをグローバルテーブルに更新する。"
      },
      {
        "key": "D",
        "text": "Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Configure DynamoDB streams and cross-Region data replication.",
        "text_jp": "Amazon Route 53遅延ベースルーティングポリシーを作成する。両方のリージョンのAWS IoT Coreデータエンドポイントを値として使用する。DynamoDBストリームとクロスリージョンデータレプリケーションを設定する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It ensures that data is ingested and processed in both regions while providing a failover mechanism for health checks.",
        "situation_analysis": "The company needs a solution that ensures data redundancy and availability across two AWS regions, which is essential for business continuity in environmental monitoring.",
        "option_analysis": "Option A does not explicitly address the need for health checks. Option B involves AWS MemoryDB, which adds unnecessary complexity. Option D does not ensure structured failover based on health check evaluations.",
        "additional_knowledge": "Implementing health checks is a best practice in AWS for ensuring application availability.",
        "key_terminology": "AWS IoT Core, Amazon DynamoDB, global tables, Amazon Route 53, health check.",
        "overall_assessment": "Option C directly aligns with the architecture best practices for availability and disaster recovery. The community vote confirms this choice as the most supported."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。これは、データが両方のリージョンで取り込まれ、処理されることを確保し、正常性チェックのためのフォールオーバーメカニズムを提供する。",
        "situation_analysis": "会社は、環境モニタリングにおけるビジネス継続性のために、2つのAWSリージョンを通じてデータ冗長性と可用性を確保する解決策が必要である。",
        "option_analysis": "選択肢Aは、正常性チェックの必要性を明示的に示していない。選択肢Bは、余分な複雑さを追加するAWS MemoryDBを含んでいる。選択肢Dは、正常性評価に基づく構造化フォールオーバーを確保していない。",
        "additional_knowledge": "ヘルスチェックを実装することは、AWSのアプリケーション可用性を確保するためのベストプラクティスである。",
        "key_terminology": "AWS IoT Core、Amazon DynamoDB、グローバルテーブル、Amazon Route 53、ヘルスチェック。",
        "overall_assessment": "選択肢Cは、可用性と災害復旧のためのアーキテクチャベストプラクティスに直接一致している。コミュニティの投票は、この選択肢が最も支持されていることを確認している。"
      }
    ],
    "keywords": [
      "AWS IoT Core",
      "Amazon DynamoDB",
      "global tables",
      "Amazon Route 53",
      "health check"
    ]
  },
  {
    "No": "185",
    "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application\nthat uses AWS Lambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table.\nThe DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table.\nThe finance team and the marketing team have separate AWS accounts.\nWhat should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?",
    "question_jp": "ある企業は、AWSクラウド上でマルチアカウント設定のためにAWS Organizationsを使用しています。企業の財務チームは、AWS LambdaとAmazon DynamoDBを使用したデータ処理アプリケーションを持っています。企業のマーケティングチームは、DynamoDBテーブルに保存されているデータにアクセスする必要があります。このDynamoDBテーブルには機密データが含まれており、マーケティングチームはDynamoDBテーブル内の特定の属性のデータにのみアクセスできる必要があります。財務チームとマーケティングチームは別々のAWSアカウントを持っています。ソリューションアーキテクトは、マーケティングチームにDynamoDBテーブルへの適切なアクセスを提供するために何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an SCP to grant the marketing team's AWS account access to the specific attributes of the DynamoDB table. Attach the SCP to the OU of the finance team.",
        "text_jp": "マーケティングチームのAWSアカウントにDynamoDBテーブルの特定の属性へのアクセスを付与するためのSCPを作成します。SCPを財務チームのOUにアタッチします。"
      },
      {
        "key": "B",
        "text": "Create an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access control). Establish trust with the marketing team's account. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.",
        "text_jp": "財務チームのアカウントに特定のDynamoDB属性（きめ細かなアクセス制御）のためのIAMポリシー条件を使用してIAMロールを作成します。マーケティングチームのアカウントと信頼関係を確立します。マーケティングチームのアカウントで、財務チームのアカウントでIAMロールを引き受ける権限を持つIAMロールを作成します。"
      },
      {
        "key": "C",
        "text": "Create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). Attach the policy to the DynamoDB table. In the marketing team's account, create an IAM role that has permissions to access the DynamoDB table in the finance team's account.",
        "text_jp": "特定のDynamoDB属性（きめ細かなアクセス制御）の条件を含むリソースベースのIAMポリシーを作成します。ポリシーをDynamoDBテーブルにアタッチします。マーケティングチームのアカウントで、財務チームのアカウントのDynamoDBテーブルにアクセスする権限を持つIAMロールを作成します。"
      },
      {
        "key": "D",
        "text": "Create an IAM role in the finance team's account to access the DynamoDB table. Use an IAM permissions boundary to limit the access to the specific attributes. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.",
        "text_jp": "DynamoDBテーブルにアクセスするためのIAMロールを財務チームのアカウントに作成します。特定の属性へのアクセスを制限するためにIAM権限境界を使用します。マーケティングチームのアカウントで、財務チームのアカウントでIAMロールを引き受ける権限を持つIAMロールを作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. It involves creating an IAM role in the finance account with conditions for specific DynamoDB attributes, establishing trust with the marketing account, and allowing the marketing account to assume this role.",
        "situation_analysis": "The marketing team needs limited access to specific attributes of sensitive data in DynamoDB. The solution must ensure data confidentiality while allowing necessary access.",
        "option_analysis": "Option A incorrectly suggests using SCP, which is not suitable for attribute-level access. Option C also does not directly address the required role assumption. Option D involves permissions boundaries but does not leverage cross-account role assumption clearly.",
        "additional_knowledge": "",
        "key_terminology": "AWS Organizations, AWS IAM, DynamoDB, fine-grained access control, trust relationship",
        "overall_assessment": "The question effectively tests knowledge of cross-account access and fine-grained permissions in AWS. The community has strongly validated option B as the best choice, aligning with AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。DynamoDBの特定の属性に対する条件を持つIAMロールを財務チームのアカウントに作成し、マーケティングチームのアカウントとの信頼関係を確立し、マーケティングアカウントがこのロールを引き受けることを可能にする。",
        "situation_analysis": "マーケティングチームはDynamoDB内の機密データの特定の属性に限られたアクセスを必要としている。このソリューションはデータの機密性を確保しつつ、必要なアクセスを許可しなければならない。",
        "option_analysis": "選択肢AはSCPを使用することを提案しているが、属性レベルのアクセスには適していない。選択肢Cもロールの引き受けを明確にaddressしていない。選択肢Dは権限境界を使用しているが、クロスアカウントのロールの引き受けを明確にしない。",
        "additional_knowledge": "",
        "key_terminology": "AWS Organizations, AWS IAM, DynamoDB, きめ細かなアクセス制御, 信頼関係",
        "overall_assessment": "この質問はAWSにおけるクロスアカウントアクセスときめ細かな権限に関する知識を効果的にテストしている。コミュニティは強く選択肢Bを最良の選択肢として検証しており、AWSのベストプラクティスにも整合している。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS IAM",
      "DynamoDB",
      "fine-grained access control",
      "trust relationship"
    ]
  },
  {
    "No": "186",
    "question": "A solutions architect is creating an application that stores objects in an Amazon S3 bucket. The solutions architect must deploy the application in\ntwo AWS Regions that will be used simultaneously. The objects in the two S3 buckets must remain synchronized with each other.\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "question_jp": "ソリューションアーキテクトは、Amazon S3バケットにオブジェクトを保存するアプリケーションを作成しています。ソリューションアーキテクトは、同時に使用する2つのAWSリージョンにアプリケーションを展開する必要があります。2つのS3バケット内のオブジェクトは、お互いに同期した状態を保つ必要があります。この要件を最小の運用オーバーヘッドで満たす手順の組み合わせはどれですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create an S3 Multi-Region Access Point Change the application to refer to the Multi-Region Access Point",
        "text_jp": "S3マルチリージョンアクセスポイントを作成し、アプリケーションをマルチリージョンアクセスポイントを参照するように変更します"
      },
      {
        "key": "B",
        "text": "Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets",
        "text_jp": "2つのS3バケット間で双方向S3クロスリージョンレプリケーション（CRR）を構成します"
      },
      {
        "key": "C",
        "text": "Modify the application to store objects in each S3 bucket",
        "text_jp": "アプリケーションを変更して、各S3バケットにオブジェクトを保存します"
      },
      {
        "key": "D",
        "text": "Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket",
        "text_jp": "各S3バケットのライフサイクルルールを作成して、一方のS3バケットから他方のS3バケットにオブジェクトをコピーします"
      },
      {
        "key": "E",
        "text": "Enable S3 Versioning for each S3 bucket",
        "text_jp": "各S3バケットのS3バージョニングを有効にします"
      },
      {
        "key": "F",
        "text": "Configure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3 bucket",
        "text_jp": "各S3バケットのイベント通知を構成して、AWS Lambda関数を呼び出して、一方のS3バケットから他方のS3バケットにオブジェクトをコピーします"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ABE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating an S3 Multi-Region Access Point significantly reduces operational overhead by allowing the application to access the objects in both regions seamlessly.",
        "situation_analysis": "The application requires high availability and synchronization of objects between two AWS Regions.",
        "option_analysis": "Option A utilizes S3 Multi-Region Access Points to manage the storage location and synchronization automatically. Option B involves configuring two-way CRR which adds complexity and operational overhead. Option C requires separate storage management for each bucket, increasing maintenance. Option D requires lifecycle rules but is not optimal for real-time sync. Option E is related to version management, which isn't directly related to synchronization. Option F involves setting up Lambda functions, introducing additional complexity.",
        "additional_knowledge": "Implementing S3 Multi-Region Access Points is now recommended in scenarios where applications need to access data in multiple AWS regions seamlessly.",
        "key_terminology": "S3 Multi-Region Access Point, Cross-Region Replication, S3 Lifecycle rules, Versioning, AWS Lambda",
        "overall_assessment": "Given the requirements, option A is the simplest and most effective solution for reducing operational overhead while ensuring the application remains highly available across two regions."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。S3マルチリージョンアクセスポイントを作成することは、アプリケーションがシームレスに両方のリージョンのオブジェクトにアクセスできるようにし、運用オーバーヘッドを大幅に削減する。",
        "situation_analysis": "アプリケーションは、2つのAWSリージョン間でオブジェクトの高可用性と同期を必要としている。",
        "option_analysis": "選択肢Aは、S3マルチリージョンアクセスポイントを利用して、自動的にストレージロケーションと同期を管理する。選択肢Bは、双方向CRRを構成することは複雑さと運用オーバーヘッドを加える。選択肢Cは、各バケットに別々のストレージ管理を必要とし、メンテナンスを増加させる。選択肢Dはライフサイクルルールを必要とするが、リアルタイム同期には最適ではない。選択肢Eはバージョン管理に関連するが、同期には直接関係しない。選択肢FはLambda関数を設定することを含み、複雑さを増す。",
        "additional_knowledge": "S3マルチリージョンアクセスポイントは、アプリケーションが複数のAWSリージョンでデータにシームレスにアクセスする必要があるシナリオでは推奨される実装である。",
        "key_terminology": "S3マルチリージョンアクセスポイント、クロスリージョンレプリケーション、S3ライフサイクルルール、バージョニング、AWS Lambda",
        "overall_assessment": "要件を考慮すると、選択肢Aは運用オーバーヘッドを最小限に抑えつつ、アプリケーションを2つのリージョンで高可用性に保つための最も簡単で効果的な解決策である。"
      }
    ],
    "keywords": [
      "S3 Multi-Region Access Point",
      "Cross-Region Replication",
      "S3 Lifecycle rules",
      "Versioning",
      "AWS Lambda"
    ]
  },
  {
    "No": "187",
    "question": "A company has an IoT platform that runs in an on-premises environment. The platform consists of a server that connects to IoT devices by using\nthe MQTT protocol. The platform collects telemetry data from the devices at least once every 5 minutes. The platform also stores device\nmetadata in a MongoDB cluster.\nAn application that is installed on an on-premises machine runs periodic jobs to aggregate and transform the telemetry and device metadata. The\napplication creates reports that users view by using another web application that runs on the same on-premises machine. The periodic jobs take\n120-600 seconds to run. However, the web application is always running.\nThe company is moving the platform to AWS and must reduce the operational overhead of the stack.\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "question_jp": "ある企業は、オンプレミス環境で動作するIoTプラットフォームを持っている。このプラットフォームは、MQTTプロトコルを使用してIoTデバイスに接続するサーバーから構成されている。プラットフォームは、少なくとも5分ごとにデバイスからのテレメトリーデータを収集する。また、プラットフォームはデバイスメタデータをMongoDBクラスタに保存している。オンプレミスマシンにインストールされたアプリケーションは、定期的なジョブを実行してテレメトリーとデバイスメタデータを集約・変換する。アプリケーションは、ユーザーが同じオンプレミスマシン上で実行される別のWebアプリケーションを通じて表示するレポートを作成する。定期的なジョブの実行には120〜600秒かかる。しかし、Webアプリケーションは常に稼働している。企業はプラットフォームをAWSに移行し、スタックの運用負荷を軽減しなければならない。運用負荷が最も少ない組み合わせの手順はどれか？（三つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Lambda functions to connect to the IoT devices",
        "text_jp": "AWS Lambda関数を使用してIoTデバイスに接続する"
      },
      {
        "key": "B",
        "text": "Configure the IoT devices to publish to AWS IoT Core",
        "text_jp": "IoTデバイスをAWS IoT Coreに公開するように設定する"
      },
      {
        "key": "C",
        "text": "Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance",
        "text_jp": "メタデータをAmazon EC2インスタンス上の自己管理のMongoDBデータベースに書き込む"
      },
      {
        "key": "D",
        "text": "Write the metadata to Amazon DocumentDB (with MongoDB compatibility)",
        "text_jp": "メタデータをAmazon DocumentDB（MongoDB互換）に書き込む"
      },
      {
        "key": "E",
        "text": "Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use Amazon CloudFront with an S3 origin to serve the reports",
        "text_jp": "AWS Step FunctionsステートマシンとAWS Lambdaタスクを使用してレポートを準備し、レポートをAmazon S3に書き込み、S3オリジンを使用してレポートを提供するためにAmazon CloudFrontを使用する"
      },
      {
        "key": "F",
        "text": "Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress controller in the EKS cluster to serve the reports",
        "text_jp": "Amazon EKSクラスターを使用してレポートを準備し、EKSクラスター内のインゲストコントローラーを使用してレポートを提供する"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BDE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Configuring the IoT devices to publish to AWS IoT Core will reduce operational overhead by leveraging AWS-managed services.",
        "situation_analysis": "The company is migrating to AWS and must minimize the operational costs and complexities associated with the IoT platform.",
        "option_analysis": "Option B allows for automatic ingestion of telemetry data into AWS, reducing the need for manual intervention. Options A, C, D, E, and F either require more configuration or management or do not streamline the process as effectively as B.",
        "additional_knowledge": "Utilizing AWS services enables automatic scaling and reduced maintenance efforts.",
        "key_terminology": "AWS IoT Core, telemetry, operational overhead, managed services, cloud migration",
        "overall_assessment": "B is the most efficient choice for reducing operational overhead. While options D and E could be considered depending on specific use cases, they add unnecessary complexity compared to the simplicity offered by B."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。IoTデバイスをAWS IoT Coreに公開するように設定することで、AWS管理サービスを活用し、運用負荷を軽減できる。",
        "situation_analysis": "企業はAWSに移行しており、IoTプラットフォームに関連する運用コストと複雑さを最小限に抑えなければならない。",
        "option_analysis": "選択肢Bは、テレメトリーデータの自動取り込みを可能にし、手動介入の必要性を減少させる。選択肢A、C、D、E、Fは、より多くの設定や管理が必要であり、Bほど効率的にプロセスを簡素化しない。",
        "additional_knowledge": "AWSサービスの活用により、自動スケーリングやメンテナンス作業の削減が可能となる。",
        "key_terminology": "AWS IoT Core、テレメトリー、運用負荷、管理サービス、クラウド移行",
        "overall_assessment": "Bは運用負荷を削減するための最も効率的な選択である。選択肢DやEは特定のユースケースに依存する場合があるが、Bが提供するシンプルさと比較すると不必要な複雑さを追加する。"
      }
    ],
    "keywords": [
      "AWS IoT Core",
      "telemetry",
      "operational overhead",
      "managed services",
      "cloud migration"
    ]
  },
  {
    "No": "188",
    "question": "A global manufacturing company plans to migrate the majority of its applications to AWS. However, the company is concerned about applications\nthat need to remain within a specific country or in the company's central on-premises data center because of data regulatory requirements or\nrequirements for latency of single-digit milliseconds. The company also is concerned about the applications that it hosts in some of its factory\nsites, where limited network infrastructure exists.\nThe company wants a consistent developer experience so that its developers can build applications once and deploy on premises, in the cloud, or\nin a hybrid architecture. The developers must be able to use the same tools, APIs, and services that are familiar to them.\nWhich solution will provide a consistent hybrid experience to meet these requirements?",
    "question_jp": "グローバル製造会社が、ほとんどのアプリケーションをAWSに移行する計画を立てている。しかし、データ規制の要件や単桁ミリ秒のレイテンシ要件のために、特定の国や会社の中央のオンプレミスデータセンター内に留める必要があるアプリケーションについて懸念している。また、限られたネットワークインフラが存在する工場サイトでホストしているアプリケーションについても懸念している。会社は、開発者が一度アプリケーションを構築し、オンプレミス、クラウド、またはハイブリッドアーキテクチャで展開できるよう、コンサステントな開発者体験を希望している。開発者は、慣れ親しんだ同じツール、API、サービスを使用する必要がある。この要件を満たすために、一貫したハイブリッド体験を提供するソリューションはどれか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on- premises data center and AWS. Deploy a Direct Connect gateway.",
        "text_jp": "すべてのアプリケーションを準拠している最寄りのAWSリージョンに移行する。中央のオンプレミスデータセンターとAWSの間にAWS Direct Connect接続を設定する。Direct Connectゲートウェイを展開する。"
      },
      {
        "key": "B",
        "text": "Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Retain the devices on premises. Deploy AWS Wavelength to host the workloads in the factory sites.",
        "text_jp": "データ規制の要件や単桁ミリ秒のレイテンシ要件のあるアプリケーションには、AWS Snowball Edge Storage Optimizedデバイスを使用する。デバイスはオンプレミスに保持する。工場サイトで作業負荷をホストするためにAWS Wavelengthを展開する。"
      },
      {
        "key": "C",
        "text": "Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites.",
        "text_jp": "データ規制の要件や単桁ミリ秒のレイテンシ要件のあるアプリケーションには、AWS Outpostsをインストールする。工場サイトで作業負荷をホストするためにAWS Snowball Edge Compute Optimizedデバイスを使用する。"
      },
      {
        "key": "D",
        "text": "Migrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local Zone. Deploy AWS Wavelength to host the workloads in the factory sites.",
        "text_jp": "データ規制の要件や単桁ミリ秒のレイテンシ要件のあるアプリケーションをAWS Local Zoneに移行する。工場サイトで作業負荷をホストするためにAWS Wavelengthを展開する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (86%) 14%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Install AWS Outposts for applications with data regulatory and latency requirements. This choice supports a hybrid architecture and allows local processing where needed.",
        "situation_analysis": "The company must comply with data regulations and keep some applications on-premises to satisfy latency requirements. Moreover, they want a hybrid approach.",
        "option_analysis": "Option C is appropriate as AWS Outposts provides a fully managed service that extends AWS infrastructure and services to on-premises locations which is critical for regulatory compliance and low latency. Other options do not provide the same level of consistency in operations.",
        "additional_knowledge": "Furthermore, option C provides a seamless developer experience as the same AWS services and APIs can be used both on-premises and in the cloud.",
        "key_terminology": "AWS Outposts, AWS Snowball Edge, hybrid architecture, data regulations, low latency.",
        "overall_assessment": "Overall, option C aligns best with the company's needs for consistent developer experiences while maintaining compliance and low latency. The community's strong support for C (86%) reflects widespread agreement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はC: データ規制の要件やレイテンシ要件があるアプリケーションには、AWS Outpostsをインストールすることである。この選択はハイブリッドアーキテクチャをサポートし、必要な場所でのローカル処理を可能にする。",
        "situation_analysis": "会社はデータ規制に準拠し、レイテンシ要件を満たすために一部のアプリケーションをオンプレミスに保持する必要がある。また、ハイブリッドアプローチを希望している。",
        "option_analysis": "オプションCは適切である。AWS Outpostsは、データ規制の遵守と低レイテンシに重要な、オンプレミスの場所にAWSのインフラストラクチャーとサービスを拡張する完全に管理されたサービスを提供する。他のオプションは、同じ運用の一貫性を提供しない。",
        "additional_knowledge": "さらに、オプションCは、オンプレミスとクラウドの両方で同じAWSサービスやAPIを使用できるため、シームレスな開発者体験を提供する。",
        "key_terminology": "AWS Outposts, AWS Snowball Edge, ハイブリッドアーキテクチャ, データ規制, 低レイテンシ。",
        "overall_assessment": "全体として、オプションCは、一貫した開発者体験を維持しつつ遵守と低レイテンシを確保する会社のニーズに最も合致している。コミュニティにおけるCの強い支持（86%）は、広く合意されていることを反映している。"
      }
    ],
    "keywords": [
      "AWS Outposts",
      "AWS Snowball Edge",
      "hybrid architecture",
      "data regulations",
      "low latency"
    ]
  },
  {
    "No": "189",
    "question": "A company is updating an application that customers use to make online orders. The number of attacks on the application by bad actors has\nincreased recently.\nThe company will host the updated application on an Amazon Elastic Container Service (Amazon ECS) cluster. The company will use Amazon\nDynamoDB to store application data. A public Application Load Balancer (ALB) will provide end users with access to the application. The company\nmust prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "question_jp": "企業は、顧客がオンライン注文を行うために使用するアプリケーションを更新しています。最近、悪意のある攻撃者によるアプリケーションへの攻撃が増加しています。\n企業は、更新されたアプリケーションをAmazon Elastic Container Service（Amazon ECS）クラスタにホストします。アプリケーションデータの保存にはAmazon DynamoDBを使用します。パブリックアプリケーションロードバランサー（ALB）がエンドユーザーにアプリケーションへのアクセスを提供します。企業は、攻撃を防ぎ、サービス中断を最小限に抑えながら業務の継続性を確保しなければなりません。\nこれらの要件を最も費用効果の高い方法で満たすために、どの組み合わせのステップを選択しますか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain. Configure the ALB to conditionally forward trafic if the header and value match.",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、ALBをオリジンとします。CloudFrontドメインにカスタムヘッダーとランダム値を追加します。ヘッダーと値が一致する場合にトラフィックを条件付きでALBに転送するように設定します。"
      },
      {
        "key": "B",
        "text": "Deploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight.",
        "text_jp": "アプリケーションを2つのAWSリージョンにデプロイします。Amazon Route 53を使用して、両方のリージョンに等しい重みでルーティングします。"
      },
      {
        "key": "C",
        "text": "Configure auto scaling for Amazon ECS tasks Create a DynamoDB Accelerator (DAX) cluster.",
        "text_jp": "Amazon ECSタスクの自動スケーリングを構成します。DynamoDB Accelerator（DAX）クラスタを作成します。"
      },
      {
        "key": "D",
        "text": "Configure Amazon ElastiCache to reduce overhead on DynamoDB.",
        "text_jp": "DynamoDBのオーバーヘッドを削減するためにAmazon ElastiCacheを構成します。"
      },
      {
        "key": "E",
        "text": "Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution.",
        "text_jp": "AWS WAFウェブACLをデプロイし、適切なルールグループを含めます。ウェブACLをAmazon CloudFrontディストリビューションに関連付けます。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct options for addressing the requirements are A and E.",
        "situation_analysis": "The company is facing an increase in attacks on its application and must ensure business continuity with minimal interruptions.",
        "option_analysis": "Option A introduces a CloudFront distribution where a custom header helps in identifying legitimate requests, while Option E involves deploying AWS WAF to add an additional layer of security against attacks.",
        "additional_knowledge": "Cost effectiveness is achieved as both services are managed and scale automatically, allowing companies to pay only for what they use.",
        "key_terminology": "Amazon CloudFront, AWS WAF, Application Load Balancer, Security, Business Continuity.",
        "overall_assessment": "Options A and E together form a strong strategy for preventing attacks while maintaining service availability, complying with AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "要求を満たすための正しい選択肢はAとEである。",
        "situation_analysis": "企業はアプリケーションへの攻撃の増加に直面しており、業務の継続性を確保し、最小限の中断で運営しなければならない。",
        "option_analysis": "選択肢AはCloudFrontディストリビューションを導入し、カスタムヘッダーを使用して正当なリクエストを特定する助けとなる。一方で選択肢EはAWS WAFを展開し、攻撃に対する追加のセキュリティ層を提供する。",
        "additional_knowledge": "コスト効率は、これらの両サービスが管理され、自動的にスケールするため、企業は使用した分だけ支払うことができることによって達成される。",
        "key_terminology": "Amazon CloudFront、AWS WAF、アプリケーションロードバランサー、セキュリティ、業務の継続性。",
        "overall_assessment": "選択肢AとEは、攻撃を防ぎつつサービスの可用性を維持する強力な戦略を形成しており、AWSのベストプラクティスを遵守している。"
      }
    ],
    "keywords": [
      "Amazon CloudFront",
      "AWS WAF",
      "Application Load Balancer",
      "Security",
      "Business Continuity"
    ]
  },
  {
    "No": "190",
    "question": "A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon\nCloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fieet of\nAmazon EC2 instances in Auto Scaling groups. The application uses a domain name setup in Amazon Route 53.\nSome users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the\nALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors\noccur. The page should be displayed immediately for this error code.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がAWS上でWebアプリケーションを運用しています。このWebアプリケーションは、Amazon S3バケットから静的コンテンツを配信し、Amazon CloudFrontディストリビューションの背後にあります。アプリケーションは、アプリケーションロードバランサー（ALB）を使用して動的コンテンツを配信し、要求をオートスケーリンググループ内のAmazon EC2インスタンスに分散させています。アプリケーションは、Amazon Route 53で設定されたドメイン名を使用しています。特定のユーザーは、ピーク時にウェブサイトにアクセスしようとした際に、時折問題を報告しました。運用チームは、ALBが時折HTTP 503 サービス利用不可エラーを返すことを発見しました。企業は、これらのエラーが発生した際にカスタムエラーメッセージページを表示したいと考えています。このエラーメッセージページは、エラーコードに対して即座に表示される必要があります。運用負荷が最も少ないソリューションはどれでしょうか?",
    "choices": [
      {
        "key": "A",
        "text": "Set up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the failover S3 bucket endpoint.",
        "text_jp": "Route 53のフェイルオーバールーティングポリシーを設定します。ALBエンドポイントの状態を確認し、フェイルオーバー用のS3バケットエンドポイントに切り替えるためのヘルスチェックを構成します。"
      },
      {
        "key": "B",
        "text": "Create a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy. Use an active-passive configuration between the two distributions.",
        "text_jp": "カスタムエラーページをホストするためのS3静的ウェブサイトと、2つ目のCloudFrontディストリビューションを作成します。Route 53のフェイルオーバールーティングポリシーを設定し、2つのディストリビューション間でアクティブ-パッシブ構成を使用します。"
      },
      {
        "key": "C",
        "text": "Create a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket that is configured to host a static website Set up origin failover for the CloudFront distribution. Update the S3 static website to incorporate the custom error page.",
        "text_jp": "2つのオリジンを持つCloudFrontオリジングループを作成します。ALBエンドポイントをプライマリオリジンとして設定します。セカンダリオリジンには、静的ウェブサイトをホストするように設定されたS3バケットを設定します。CloudFrontディストリビューションにオリジンフェイルオーバーを設定します。S3静的ウェブサイトを更新してカスタムエラーページを組み込みます。"
      },
      {
        "key": "D",
        "text": "Create a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket. Upload the custom error page to the S3 bucket as a failover. Update the function to read the S3 bucket and to serve the error page to the end users.",
        "text_jp": "ALBが返す各HTTPレスポンスコードを検証するCloudFront関数を作成します。S3バケット内に静的ウェブサイトを作成し、S3バケットにカスタムエラーページをアップロードします。関数を更新してS3バケットを読み取り、エラーページをエンドユーザーに提供します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating a CloudFront origin group with two origins and setting up origin failover allows for immediate display of a custom error page hosted on S3 during ALB downtime.",
        "situation_analysis": "The requirements indicate the need for minimal operational overhead while providing a custom error response during 503 errors from the ALB.",
        "option_analysis": "Option C is effective as it allows for seamless failover to the S3 bucket hosting the custom error page, whereas options A, B, and D introduce more complexity and delay in the error handling process.",
        "additional_knowledge": "This setup not only provides a custom error page but also leverages AWS services efficiently.",
        "key_terminology": "CloudFront, ALB, S3, origin failover, static website",
        "overall_assessment": "Considering the operational simplicity and direct solution to the stated problem of custom error handling with minimal delays, option C is the most efficient and clean solution as it aligns well with AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。2つのオリジンを持つCloudFrontオリジングループを作成し、オリジンフェイルオーバーを設定することで、ALBのダウンタイム中にS3にホストされたカスタムエラーページを即座に表示できる。",
        "situation_analysis": "要件は、ALBからの503エラー時にカスタムエラーレスポンスを提供しながら、最小限の運用負荷が必要であることを示している。",
        "option_analysis": "選択肢Cは、カスタムエラーページをホストするS3バケットへのシームレスなフェイルオーバーを可能にするため、ALBのエラー処理プロセスに複雑さや遅延をもたらす選択肢A、B、Dよりも効果的である。",
        "additional_knowledge": "この設定により、カスタムエラーページが提供されるだけでなく、AWSサービスを効率的に活用することができる。",
        "key_terminology": "CloudFront, ALB, S3, オリジンフェイルオーバー, 静的ウェブサイト",
        "overall_assessment": "カスタムエラー処理の問題に対する解決策を最小遅延で提供するという運用のシンプルさを考慮すると、選択肢CはAWSのベストプラクティスに適合し、最も効率的である。"
      }
    ],
    "keywords": [
      "CloudFront",
      "ALB",
      "S3",
      "origin failover",
      "static website"
    ]
  },
  {
    "No": "191",
    "question": "A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.\nA solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the\nunderlying infrastructure.\nWhich solution will meet these requirements?",
    "question_jp": "A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.\nA solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the\nunderlying infrastructure.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition.",
        "text_jp": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition."
      },
      {
        "key": "B",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx for Lustre for shared storage. Reference the FSx for Lustre file system ID, container mount point, and FSx for Lustre authorization IAM role in the ECS task definition.",
        "text_jp": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx for Lustre for shared storage. Reference the FSx for Lustre file system ID, container mount point, and FSx for Lustre authorization IAM role in the ECS task definition."
      },
      {
        "key": "C",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic File System (Amazon EFS) for shared storage. Mount the EFS file system on the ECS container instances. Add the EFS authorization IAM role to the EC2 instance profile.",
        "text_jp": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic File System (Amazon EFS) for shared storage. Mount the EFS file system on the ECS container instances. Add the EFS authorization IAM role to the EC2 instance profile."
      },
      {
        "key": "D",
        "text": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic Block Store (Amazon EBS) volumes with Multi-Attach enabled for shared storage. Attach the EBS volumes to ECS container instances. Add the EBS authorization IAM role to an EC2 instance profile.",
        "text_jp": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic Block Store (Amazon EBS) volumes with Multi-Attach enabled for shared storage. Attach the EBS volumes to ECS container instances. Add the EBS authorization IAM role to an EC2 instance profile."
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This solution utilizes Amazon ECS with Fargate, which allows for serverless container management without the need to manage the underlying infrastructure.",
        "situation_analysis": "The application moves to AWS and requires a secure and scalable containerized solution. It must handle a Docker container and an NFS version 4 file share while being serverless.",
        "option_analysis": "Option A enables the use of Amazon EFS for shared storage, suitable for this use case. Options B, C, and D introduce unnecessary complexity or do not meet the serverless requirement.",
        "additional_knowledge": "This approach also leverages IAM roles for secure access, enhancing the security of the overall application architecture.",
        "key_terminology": "Amazon ECS, Fargate, Amazon Elastic File System, serverless architecture, Docker containers.",
        "overall_assessment": "Answer A is fully aligned with the stated requirements. The community vote confirms unanimous support for this choice."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。このソリューションはAmazon ECSをFargateと共に利用し、基盤となるインフラを管理する必要がないサーバーレスなコンテナ管理を実現する。",
        "situation_analysis": "アプリケーションはAWSに移行し、セキュアでスケーラブルなコンテナ化されたソリューションを必要としている。DockerコンテナとNFSバージョン4ファイル共有を扱い、サーバーレスである必要がある。",
        "option_analysis": "選択肢Aは共有ストレージにAmazon EFSを使用するため、このユースケースに適している。選択肢B、C、Dは不必要な複雑さを導入するか、サーバーレス要件に合致しない。",
        "additional_knowledge": "このアプローチは、セキュアなアクセスのためにIAMロールを活用し、全体のアプリケーションアーキテクチャのセキュリティを向上させる。",
        "key_terminology": "Amazon ECS、Fargate、Amazon Elastic File System、サーバーレスアーキテクチャ、Dockerコンテナ。",
        "overall_assessment": "選択肢Aは明確に要件に合致している。コミュニティの投票もこの選択肢を支持していることを確認する。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "Fargate",
      "Amazon Elastic File System"
    ]
  },
  {
    "No": "192",
    "question": "A company is running an application in the AWS Cloud. The core business logic is running on a set of Amazon EC2 instances in an Auto Scaling\ngroup. An Application Load Balancer (ALB) distributes trafic to the EC2 instances. Amazon Route 53 record api.example.com is pointing to the\nALB.\nThe company's development team makes major updates to the business logic. The company has a rule that when changes are deployed, only 10%\nof customers can receive the new logic during a testing window. A customer must use the same version of the business logic during the testing\nwindow.\nHow should the company deploy the updates to meet these requirements?",
    "question_jp": "ある会社がAWSクラウドでアプリケーションを運用しています。ビジネスロジックの核心は、オートスケーリンググループ内の一連のAmazon EC2インスタンス上で実行されています。アプリケーションロードバランサー（ALB）がEC2インスタンスにトラフィックを分散します。Amazon Route 53のレコードapi.example.comはALBを指しています。会社の開発チームはビジネスロジックに大きな更新を行います。会社には、変更がデプロイされるとき、テストウィンドウ中に新しいロジックを受け取ることができる顧客は10%だけであるというルールがあります。テストウィンドウ中、顧客は同じバージョンのビジネスロジックを使用する必要があります。会社はこれらの要件を満たすために、どのように更新をデプロイすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling group. Configure the ALB to distribute trafic to the EC2 instances. Update the Route 53 record to use weighted routing, and point the record to both of the ALBs.",
        "text_jp": "2つ目のALBを作成し、新しいロジックを新しいオートスケーリンググループ内のEC2インスタンスにデプロイします。ALBを構成してトラフィックをEC2インスタンスに分散し、Route 53のレコードをウェイトルーティングを使用して更新し、2つのALBの両方を指すようにします。"
      },
      {
        "key": "B",
        "text": "Create a second target group that is referenced by the ALDeploy the new logic to EC2 instances in this new target group. Update the ALB listener rule to use weighted target groups. Configure ALB target group stickiness.",
        "text_jp": "新しいターゲットグループを作成し、この新しいターゲットグループによって参照されるALBをデプロイし、新しいロジックをこの新しいターゲットグループ内のEC2インスタンスにデプロイします。ALBリスナールールを更新してウェイト付きターゲットグループを使用するようにし、ALBターゲットグループのスティッキーを構成します。"
      },
      {
        "key": "C",
        "text": "Create a new launch configuration for the Auto Scaling group. Specify the launch configuration to use the AutoScalingRollingUpdate policy, and set the MaxBatchSize option to 10. Replace the launch configuration on the Auto Scaling group. Deploy the changes.",
        "text_jp": "オートスケーリンググループの新しい起動構成を作成します。起動構成をAutoScalingRollingUpdateポリシーを使用するように指定し、MaxBatchSizeオプションを10に設定します。オートスケーリンググループの起動構成を置き換え、変更をデプロイします。"
      },
      {
        "key": "D",
        "text": "Create a second Auto Scaling group that is referenced by the ALB. Deploy the new logic on a set of EC2 instances in this new Auto Scaling group. Change the ALB routing algorithm to least outstanding requests (LOR). Configure ALB session stickiness.",
        "text_jp": "ALBによって参照される2つ目のオートスケーリンググループを作成します。新しいロジックをこの新しいオートスケーリンググループ内のEC2インスタンスにデプロイします。ALBルーティングアルゴリズムを最小未処理リクエスト（LOR）に変更し、ALBセッションのスティッキーを構成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. By creating a new target group and updating the ALB listener to use weighted target groups with stickiness enabled, only 10% of the customers will receive the new version during the testing window while ensuring session affinity.",
        "situation_analysis": "The requirement is to deploy updates such that only 10% of customers are affected during testing, and they should use the same version throughout that window.",
        "option_analysis": "Option B fulfills the requirement of weighted routing and session stickiness. Other options either do not support session persistence or require more complex configurations.",
        "additional_knowledge": "This approach ensures that user sessions remain stable during transitions.",
        "key_terminology": "Application Load Balancer, target group, stickiness, weighted routing, Auto Scaling.",
        "overall_assessment": "Option B is the most efficient and simple solution to meet the deployment requirements described, while other options introduce unnecessary complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。新しいターゲットグループを作成し、ALBリスナーを更新してウェイト付きターゲットグループを使用し、スティッキーを有効にすることにより、テストウィンドウ中に新しいバージョンを受け取る顧客は10%だけになり、セッションの親和性を確保できる。",
        "situation_analysis": "更新をデプロイする要件は、テスト中に影響を受ける顧客が10%だけであり、同じバージョンをそのウィンドウ間使用する必要があるというものである。",
        "option_analysis": "選択肢Bは、ウェイト付きルーティングとセッションのスティッキーをサポートし、要件を満たしている。他の選択肢は、セッションの持続性をサポートしないか、より複雑な構成を必要とする。",
        "additional_knowledge": "このアプローチは、移行中にユーザーセッションが安定したまま維持されることを確保する。",
        "key_terminology": "アプリケーションロードバランサー、ターゲットグループ、スティッキー、ウェイト付きルーティング、オートスケーリング。",
        "overall_assessment": "選択肢Bは、記載されているデプロイ要件を満たすための最も効率的でシンプルなソリューションであり、他の選択肢は不必要な複雑性を導入する。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "target group",
      "stickiness",
      "weighted routing",
      "Auto Scaling"
    ]
  },
  {
    "No": "193",
    "question": "A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The\ncompany is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is\nconnected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels.\nAn investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput\nof 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.\nWhat should the solutions architect do to meet these requirements with the LEAST administrative effort?",
    "question_jp": "大手教育企業が最近、複数大学で内部アプリケーションへのアクセスを提供するために Amazon Workspaces を導入しました。この企業は、Amazon FSx for Windows File Server ファイルシステムにユーザープロファイルを保存しています。ファイルシステムは DNS エイリアスを使用して構成されており、自己管理型 Active Directory に接続されています。より多くのユーザーが Workspaces を使用し始めたところ、ログイン時間が許容できないレベルに増加しました。調査の結果、ファイルシステムのパフォーマンスが低下していることが明らかになりました。企業は HDD ストレージでスループット 16 MBps のファイルシステムを作成しました。ソリューションアーキテクトは、定義されたメンテナンスウィンドウ中にファイルシステムのパフォーマンスを改善する必要があります。管理作業が最も少なくて済む方法は何でしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system. Select SSD as the storage type. Select 32 MBps as the throughput capacity. When the backup and restore process is completed, adjust the DNS alias accordingly. Delete the original file system.",
        "text_jp": "AWS Backup を使用して、ファイルシステムのポイントインタイムバックアップを作成します。バックアップを新しい FSx for Windows File Server ファイルシステムに復元します。ストレージタイプとして SSD を選択します。スループットキャパシティとして 32 MBps を選択します。バックアップと復元のプロセスが完了したら、DNS エイリアスを調整します。元のファイルシステムを削除します。"
      },
      {
        "key": "B",
        "text": "Disconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to SSD. Reconnect users to the file system.",
        "text_jp": "ファイルシステムからユーザーを切断します。Amazon FSx コンソールで、スループットキャパシティを 32 MBps に更新します。ストレージタイプを SSD に更新します。ユーザーをファイルシステムに再接続します。"
      },
      {
        "key": "C",
        "text": "Deploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location. Configure a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput as the target location. Schedule the task. When the task is completed, adjust the DNS alias accordingly. Delete the original file system.",
        "text_jp": "新しい Amazon EC2 インスタンスに AWS DataSync エージェントを展開します。タスクを作成します。既存のファイルシステムをソースロケーションとして構成します。SSD ストレージと 32 MBps のスループットを持つ新しい FSx for Windows File Server ファイルシステムをターゲットロケーションとして構成します。タスクをスケジュールします。タスクが完了したら、DNS エイリアスを調整します。元のファイルシステムを削除します。"
      },
      {
        "key": "D",
        "text": "Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a point-in-time backup of the file system. Choose to restore previous versions. Create a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput. When the copy job is completed, adjust the DNS alias. Delete the original file system.",
        "text_jp": "Windows PowerShell コマンドを使用して既存のファイルシステムで シャドウ コピーを有効にします。シャドウコピー ジョブをスケジュールして、ファイルシステムのポイントインタイムバックアップを作成します。以前のバージョンを復元することを選択します。SSD ストレージと 32 MBps のスループットを持つ新しい FSx for Windows File Server ファイルシステムを作成します。コピー ジョブが完了したら、DNS エイリアスを調整します。元のファイルシステムを削除します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (63%) B (37%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. The method described minimizes administrative effort while ensuring performance improvement by utilizing AWS Backup to transition to a better storage type.",
        "situation_analysis": "The scenario involves an education company experiencing performance degradation due to increased login times with a file system on HDD storage. Key requirements include enhancing performance with minimal administrative intervention.",
        "option_analysis": "Option A is the preferable solution as it involves creating a backup and restoring it to a new file system with SSD storage and higher throughput. Option B requires disconnecting users which could cause interruptions, whereas enabling SSD and increasing throughput would require downtime. Option C involves more complex operations with DataSync without significant downtime benefit. Option D adds unnecessary complexity by enabling shadow copies, which is not needed in this case.",
        "additional_knowledge": "The utilization of SSD storage significantly enhances read/write speeds, positively impacting user experience.",
        "key_terminology": "AWS Backup, FSx for Windows File Server, SSD, Performance Improvement, Throughput Capacity",
        "overall_assessment": "Answer A is supported by community votes and is aligned with AWS best practices for ease of management and system performance enhancement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は A である。説明された方法は、AWS Backup を利用してより良いストレージタイプに移行することで、管理労力を最小限に抑えつつ、パフォーマンス改善を確実にする。",
        "situation_analysis": "シナリオは、HDD ストレージ上のファイルシステムでログイン時間の増加によるパフォーマンスの劣化を経験している教育企業に関するものである。主要な要件は、最小限の管理介入でパフォーマンスを向上させることである。",
        "option_analysis": "選択肢 A は、新しいファイルシステムに SSD ストレージとより高いスループットでバックアップを作成・復元するため、望ましい解決策である。選択肢 B はユーザーを切断する必要があり、混乱を招く可能性がある。一方、SSD の有効化とスループットの増加はダウンタイムを伴う。選択肢 C は DataSync を使用する複雑な操作が含まれ、重要なダウンタイムの利益をもたらさない。選択肢 D はシャドウコピーを有効化することで、不要な複雑さを追加している。",
        "additional_knowledge": "SSD ストレージの利用は、読み書き速度を大幅に向上させ、ユーザー体験に良い影響を与える。",
        "key_terminology": "AWS Backup, FSx for Windows File Server, SSD, パフォーマンス改善, スループットキャパシティ",
        "overall_assessment": "回答 A はコミュニティの投票によって支持されており、管理の容易さおよびシステムパフォーマンスの向上において AWS のベストプラクティスに合致している。"
      }
    ],
    "keywords": [
      "AWS Backup",
      "FSx for Windows File Server",
      "SSD",
      "Performance Improvement",
      "Throughput Capacity"
    ]
  },
  {
    "No": "194",
    "question": "A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company\nmust modify the application to deploy the application in two AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がAWS上でアプリケーションをホストしています。このアプリケーションは、単一のAmazon S3バケットに保存されたオブジェクトを読み書きします。企業は、アプリケーションを2つのAWSリージョンにデプロイするためにアプリケーションを変更する必要があります。この要件を最小限の運用負荷で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region Modify the application to use the CloudFront distribution. Use AWS Global Accelerator to access the data in the S3 bucket.",
        "text_jp": "Amazon CloudFrontディストリビューションを設定し、S3バケットをオリジンとして使用します。アプリケーションを2番目のリージョンにデプロイし、アプリケーションをCloudFrontディストリビューションで使用するように変更します。AWS Global Acceleratorを使用してS3バケットのデータにアクセスします。"
      },
      {
        "key": "B",
        "text": "Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions.",
        "text_jp": "2番目のリージョンに新しいS3バケットを作成します。元のS3バケットと新しいS3バケットの間で双方向のS3クロスリージョンレプリケーション（CRR）を設定します。両方のS3バケットを使用するS3マルチリージョンアクセスポイントを設定します。修正されたアプリケーションを両方のリージョンにデプロイします。"
      },
      {
        "key": "C",
        "text": "Create a new S3 bucket in a second Region Deploy the application in the second Region. Configure the application to use the new S3 bucket. Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket.",
        "text_jp": "2番目のリージョンに新しいS3バケットを作成し、アプリケーションを2番目のリージョンにデプロイします。アプリケーションを新しいS3バケットを使用するように設定します。元のS3バケットから新しいS3バケットへのS3クロスリージョンレプリケーション（CRR）を設定します。"
      },
      {
        "key": "D",
        "text": "Set up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the new S3 gateway endpoint. Use S3 Intelligent-Tiering on the S3 bucket.",
        "text_jp": "S3バケットをオリジンとして使用するS3ゲートウェイエンドポイントを設定します。アプリケーションを2番目のリージョンにデプロイします。アプリケーションを新しいS3ゲートウェイエンドポイントを使用するように変更します。S3インテリジェントタイアリングをS3バケットで使用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (83%) C (17%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. The use of S3 Cross-Region Replication (CRR) combined with a Multi-Region Access Point allows for minimal operational overhead and ensures that the S3 bucket is available across different regions.",
        "situation_analysis": "The requirement is to deploy the application across two AWS Regions while reading and writing objects to an S3 bucket. The ideal solution should minimize operational complexity.",
        "option_analysis": "Option B stands out because it not only creates a replicated bucket but also allows access to both buckets through a Multi-Region Access Point. Option A includes unnecessary components like CloudFront and Global Accelerator. Option C does not fully utilize the advantages of a Multi-Region Access Point, and option D introduces additional complexity with gateway endpoints and intelligent tiering.",
        "additional_knowledge": "In addition, using transparent replication helps maintain a seamless experience for the application users.",
        "key_terminology": "S3 Cross-Region Replication, Multi-Region Access Point, AWS Regions",
        "overall_assessment": "Answer B is highly supported by the community (83%) indicating its correctness. The simplicity and lower operational overhead is in line with best practices for multi-region deployments."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。S3クロスリージョンレプリケーション（CRR）を用いることで、マルチリージョンアクセスポイントと組み合わせられ、最小限の運用負荷でS3バケットを異なるリージョンで利用できるようになる。",
        "situation_analysis": "アプリケーションを2つのAWSリージョンにデプロイしながら、オブジェクトをS3バケットに読み書きする必要がある。理想的なソリューションは、運用の複雑さを最小限に抑えるべきである。",
        "option_analysis": "選択肢Bは、単にレプリケートされたバケットを作成するだけでなく、マルチリージョンアクセスポイントを介して両方のバケットへのアクセスを可能にする点で優れている。選択肢AはCloudFrontやGlobal Acceleratorのような不要なコンポーネントを含む。選択肢Cは、マルチリージョンアクセスポイントの利点を十分に活用できていない。選択肢Dは、ゲートウェイエンドポイントやインテリジェントタイアリングによる追加の複雑さをもたらす。",
        "additional_knowledge": "透明なレプリケーションの利用は、アプリケーションユーザーにシームレスな体験を維持するのに役立つ。",
        "key_terminology": "S3クロスリージョンレプリケーション, マルチリージョンアクセスポイント, AWSリージョン",
        "overall_assessment": "選択肢Bはコミュニティから高い支持を得ており（83%）、その正当性が示されている。シンプルさと低運用負荷は、マルチリージョンデプロイメントのベストプラクティスに沿ったものである。"
      }
    ],
    "keywords": [
      "S3 Cross-Region Replication",
      "Multi-Region Access Point",
      "AWS Regions"
    ]
  },
  {
    "No": "195",
    "question": "An online gaming company needs to rehost its gaming platform on AWS. The company's gaming application requires high performance computing\n(HPC) processing and has a leaderboard that changes frequently. An Ubuntu instance that is optimized for compute generation hosts a Node.js\napplication for game display. Game state is tracked in an on-premises Redis instance.\nThe company needs a migration strategy that optimizes application performance.\nWhich solution will meet these requirements?",
    "question_jp": "オンラインゲーム会社は、ゲームプラットフォームをAWSにリホストする必要があります。会社のゲームアプリケーションは高性能コンピューティング（HPC）処理を必要とし、頻繁に変わるリーダーボードがあります。コンピュート世代に最適化されたUbuntuインスタンスでNode.jsアプリケーションがゲーム表示用にホストされています。ゲームの状態は、オンプレミスのRedisインスタンスで追跡されています。会社はアプリケーションのパフォーマンスを最適化するマイグレーション戦略が必要です。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Auto Scaling group of m5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon ElastlCache for Redis cluster to maintain the leaderboard.",
        "text_jp": "m5.largeのAmazon EC2スポットインスタンスのオートスケーリンググループをアプリケーションロードバランサーの背後に作成し、リーダーボードを保持するためにAmazon ElastiCache for Redisクラスターを使用します。"
      },
      {
        "key": "B",
        "text": "Create an Auto Scaling group of c5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon OpenSearch Service cluster to maintain the leaderboard.",
        "text_jp": "c5.largeのAmazon EC2スポットインスタンスのオートスケーリンググループをアプリケーションロードバランサーの背後に作成し、リーダーボードを保持するためにAmazon OpenSearch Serviceクラスターを使用します。"
      },
      {
        "key": "C",
        "text": "Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard.",
        "text_jp": "c5.largeのAmazon EC2オンデマンドインスタンスのオートスケーリンググループをアプリケーションロードバランサーの背後に作成し、リーダーボードを保持するためにAmazon ElastiCache for Redisクラスターを使用します。"
      },
      {
        "key": "D",
        "text": "Create an Auto Scaling group of m5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon DynamoDB table to maintain the leaderboard.",
        "text_jp": "m5.largeのAmazon EC2オンデマンドインスタンスのオートスケーリンググループをアプリケーションロードバランサーの背後に作成し、リーダーボードを保持するためにAmazon DynamoDBテーブルを使用します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. The requirement for high-performance computing and using a Redis cluster for the leaderboard aligns well with this option.",
        "situation_analysis": "The gaming application operates with high HPC demands, requiring instances optimized for compute performance. Additionally, the need for real-time leaderboard updates points towards the importance of using a fast caching solution like ElastiCache for Redis.",
        "option_analysis": "Option C provides the best combination of high-performance c5 instances with on-demand pricing, facilitating robust computing power for the application while ensuring low latency through ElastiCache for Redis, which is suitable for maintaining the leaderboard. Option A, while using Spot Instances, may introduce variability in availability. Option B employs OpenSearch Service, which is unnecessary for leaderboard maintenance. Option D introduces DynamoDB, which does not support the rapid updates required for the leaderboard as effectively as ElastiCache.",
        "additional_knowledge": "Focusing on resilient architecture is vital in gaming to ensure minimal downtime and optimal performance under various load conditions.",
        "key_terminology": "High Performance Computing, Auto Scaling, EC2, ElastiCache, Redis",
        "overall_assessment": "Option C is an optimal choice, providing the required performance characteristics and reliability for a gaming application with a frequently updated leaderboard. The community supports this choice entirely."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。高性能コンピューティングの要件とリーダーボードの管理にElastiCacheクラスターを使用することがこの選択肢に適しています。",
        "situation_analysis": "ゲームアプリケーションは高いHPC要件を持ち、コンピュートパフォーマンスに最適化されたインスタンスが必要です。また、リアルタイムのリーダーボード更新の必要性は、ElastiCache for Redisのような高速キャッシングソリューションを使用することの重要性を示しています。",
        "option_analysis": "選択肢Cは、オンデマンド価格で高性能のc5インスタンスの最良の組み合わせを提供し、アプリケーションに堅牢なコンピューティングパワーを提供しつつ、ElastiCache for Redisを使用することで低遅延を確保し、リーダーボードを維持するのに適しています。選択肢Aはスポットインスタンスを使用していますが、可用性に変動をもたらす可能性があります。選択肢Bはリーダーボードの維持に不必要なOpenSearch Serviceを使用しています。選択肢DはDynamoDBを導入していますが、リーダーボードの迅速な更新をサポートするのはElastiCacheほど効果的ではありません。",
        "additional_knowledge": "ゲームにおけるレジリエントなアーキテクチャへの焦点は、さまざまな負荷条件下でのダウンタイムを最小限に抑え、最適なパフォーマンスを確保するうえで重要です。",
        "key_terminology": "高性能コンピューティング、オートスケーリング、EC2、ElastiCache、Redis",
        "overall_assessment": "選択肢Cは必要なパフォーマンス特性と信頼性を提供し、頻繁に更新されるリーダーボードを持つゲームアプリケーションに最適な選択です。コミュニティもこの選択を完全に支持しています。"
      }
    ],
    "keywords": [
      "High Performance Computing",
      "Auto Scaling",
      "EC2",
      "ElastiCache",
      "Redis"
    ]
  },
  {
    "No": "196",
    "question": "A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be\nsubmitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run\nmonthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.\nWhich combination of steps meets these requirements while minimizing operational overhead? (Choose two.)",
    "question_jp": "ソリューションアーキテクトが、従業員がモバイルデバイス上でタイムシートエントリを受け付けるアプリケーションを設計しています。タイムシートは毎週提出され、提出の大部分は金曜日に行われます。データは、給与管理者が月次レポートを実行できる形式で保存する必要があります。インフラストラクチャは高可用性であり、受信データとレポートリクエストのレートに合わせてスケールしなければなりません。運用オーバーヘッドを最小限に抑えながら、これらの要件を満たす手順の組み合わせはどれですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume of submissions on Fridays.",
        "text_jp": "アプリケーションをAmazon EC2のオンデマンドインスタンスにデプロイし、複数のアベイラビリティゾーンにわたってロードバランシングを行います。金曜日の高ボリュームな提出の前に、スケジュールされたAmazon EC2 Auto Scalingを使用して容量を追加します。"
      },
      {
        "key": "B",
        "text": "Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability Zones. Use scheduled Service Auto Scaling to add capacity before the high volume of submissions on Fridays.",
        "text_jp": "アプリケーションをコンテナでAmazon Elastic Container Service (Amazon ECS)を使用してデプロイし、複数のアベイラビリティゾーンにわたってロードバランシングを行います。金曜日の高ボリュームな提出の前に、スケジュールされたサービスオートスケーリングを使用して容量を追加します。"
      },
      {
        "key": "C",
        "text": "Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration.",
        "text_jp": "アプリケーションのフロントエンドをAmazon S3バケットにデプロイし、Amazon CloudFrontで提供します。アプリケーションのバックエンドをAmazon API GatewayとAWS Lambdaプロキシ統合を使用してデプロイします。"
      },
      {
        "key": "D",
        "text": "Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the data source.",
        "text_jp": "タイムシート提出データをAmazon Redshiftに保存します。レポートを生成するためにAmazon QuickSightを使用し、データソースとしてAmazon Redshiftを使用します。"
      },
      {
        "key": "E",
        "text": "Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source.",
        "text_jp": "タイムシート提出データをAmazon S3に保存します。レポートを生成するためにAmazon AthenaとAmazon QuickSightを使用し、データソースとしてAmazon S3を使用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "CE (57%) AE (17%) 13% 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, as it suggests deploying the application using Amazon ECS with service auto scaling which aligns with the requirement for high availability and operational efficiency.",
        "situation_analysis": "The application requires high availability and the ability to scale efficiently due to peaks in submissions on Fridays. Additionally, there is the need for streamlined management and operational overhead reduction.",
        "option_analysis": "Option B is the best choice as it uses containerization which simplifies deployment, while also leveraging service auto scaling for handling unpredictable traffic. Option A would increase operational overhead with managing EC2 instances. Option C focuses on front-end deployment and doesn’t address high availability and scaling effectively. Option D and E require significant management of data sources and reports, making them less optimal for this scenario.",
        "additional_knowledge": "Using a container-based architecture typically leads to improved resource utilization and flexibility.",
        "key_terminology": "Amazon ECS, Service Auto Scaling, high availability, operational overhead, Amazon CloudFront",
        "overall_assessment": "The selected answer corresponds well to the requirements of the scenario, emphasizing reduced operational overhead and effective scaling capabilities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBであり、これはAmazon ECSを使用してアプリケーションをデプロイし、サービスオートスケーリングを活用することを提案しており、高可用性と運用効率に合致しています。",
        "situation_analysis": "アプリケーションは高可用性と金曜日の提出のピークに対応する効率的なスケーリング能力を必要とします。さらに、管理を効率化し、運用オーバーヘッドを削減する必要があります。",
        "option_analysis": "選択肢Bは、コンテナ化を利用してデプロイを簡素化し、トラフィックの不確実性に対応するためにサービスオートスケーリングを活用できるため、最良の選択肢です。選択肢AはEC2インスタンスの管理を伴い、運用オーバーヘッドを増加させます。選択肢Cはフロントエンドのデプロイに焦点を当てており、高可用性とスケーリングには効果的に対応できません。選択肢DおよびEはデータソースとレポートの管理に多くの手間がかかるため、このシナリオには最適ではありません。",
        "additional_knowledge": "コンテナベースのアーキテクチャを使用することで、リソース利用率や柔軟性が向上する傾向があります。",
        "key_terminology": "Amazon ECS, サービスオートスケーリング, 高可用性, 運用オーバーヘッド, Amazon CloudFront",
        "overall_assessment": "選択した回答は、シナリオの要件に非常に適しており、運用のオーバーヘッド削減と効果的なスケーリング能力を強調しています。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "Service Auto Scaling",
      "high availability",
      "operational overhead",
      "Amazon CloudFront"
    ]
  },
  {
    "No": "197",
    "question": "A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the\nlogs for 5 years. The company's security team also must receive an email notification every time there is an attempt to delete data in the S3\nbucket.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "question_jp": "ある企業がAmazon S3バケットに機密データを保存しています。企業はS3バケット内のオブジェクトのすべてのアクティビティをログに記録し、5年間保持する必要があります。また、データの削除を試みるたびに、企業のセキュリティチームがメール通知を受け取る必要があります。\nこれらの要件を最もコスト効率よく満たすためには、どの組み合わせの手順を選択すべきでしょうか？（3つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "Configure AWS CloudTrail to log S3 data events.",
        "text_jp": "AWS CloudTrailを設定してS3データイベントをログに記録する。"
      },
      {
        "key": "B",
        "text": "Configure S3 server access logging for the S3 bucket.",
        "text_jp": "S3バケットのサーバーアクセスログを設定する。"
      },
      {
        "key": "C",
        "text": "Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES).",
        "text_jp": "Amazon S3がオブジェクト削除イベントをAmazon Simple Email Service (Amazon SES)に送信するよう設定する。"
      },
      {
        "key": "D",
        "text": "Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "text_jp": "Amazon S3がオブジェクト削除イベントをAmazon EventBridgeイベントバスに送信し、Amazon Simple Notification Service (Amazon SNS)トピックに公開するよう設定する。"
      },
      {
        "key": "E",
        "text": "Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering.",
        "text_jp": "Amazon S3がログをAmazon Timestreamに保存するよう設定し、データストレージ階層を使用する。"
      },
      {
        "key": "F",
        "text": "Configure a new S3 bucket to store the logs with an S3 Lifecycle policy.",
        "text_jp": "新しいS3バケットを設定してログを保存し、S3ライフサイクルポリシーを適用する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDF (53%) ADF (45%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, B, and D. These options allow for logging, notification, and cost-effective storage management.",
        "situation_analysis": "The company needs to log all activities related to sensitive data in S3 and also needs to notify security on deletion attempts.",
        "option_analysis": "Option A ensures that all S3 events are logged. Option B provides access logs but not specific object activity. Option D ensures notifications are sent to the security team.",
        "additional_knowledge": "Lifecycle policies help manage log retention costs.",
        "key_terminology": "AWS CloudTrail, Amazon SNS, Amazon EventBridge",
        "overall_assessment": "Options A, B, and D are the best combination for achieving the logging and notification requirements efficiently."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はA、B、Dである。これらのオプションは、ログ記録、通知、およびコスト効率の良いストレージ管理を可能にする。",
        "situation_analysis": "企業はS3に関連するすべての活動をログに記録し、削除を試みた際にセキュリティチームに通知する必要がある。",
        "option_analysis": "選択肢Aは、すべてのS3イベントがログに記録されることを保証する。選択肢Bはアクセスログを提供するが、特定のオブジェクト活動を含まない。選択肢Dはセキュリティチームへの通知が行われるようにする。",
        "additional_knowledge": "ライフサイクルポリシーは、ログ保持コストの管理に役立つ。",
        "key_terminology": "AWS CloudTrail、Amazon SNS、Amazon EventBridge",
        "overall_assessment": "オプションA、B、Dは、ログ記録および通知の要件を効率的に達成するための最適な組み合わせである。"
      }
    ],
    "keywords": [
      "AWS CloudTrail",
      "Amazon SNS",
      "Amazon EventBridge"
    ]
  },
  {
    "No": "198",
    "question": "A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed\nAmazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct. Connect connection to\nthe data center from the Region that is closest to the data center.\nThe company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-\npremises data center also must have access to AWS public services.\nWhich combination of steps will meet these requirements with the LEAST cost? (Choose two.)",
    "question_jp": "企業は、オンプレミスのデータセンターおよびAWSクラウド内のサーバーを含むハイブリッド環境を構築しています。企業は、3つのVPCにAmazon EC2インスタンスを展開しています。各VPCは異なるAWSリージョンにあります。企業は、データセンターに最も近いリージョンからデータセンターへのAWS Direct Connect接続を確立しています。企業は、オンプレミスデータセンター内のサーバーがすべての3つのVPC内のEC2インスタンスにアクセスできる必要があります。また、オンプレミスデータセンター内のサーバーは、AWSのパブリックサービスにもアクセスできなければなりません。最もコストがかからない方法でこれらの要件を満たすためのステップの組み合わせはどれですか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions.",
        "text_jp": "最も近いリージョンにDirect Connectゲートウェイを作成します。Direct Connect接続をDirect Connectゲートウェイに接続し、Direct Connectゲートウェイを使用して他の2つのリージョンのVPCに接続します。"
      },
      {
        "key": "B",
        "text": "Set up additional Direct Connect connections from the on-premises data center to the other two Regions.",
        "text_jp": "オンプレミスデータセンターから他の2つのリージョンへの追加のDirect Connect接続をセットアップします。"
      },
      {
        "key": "C",
        "text": "Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions.",
        "text_jp": "プライベートVIFを作成します。プライベートVIFを介して他の2つのリージョンのVPCにAWS Site-to-Site VPN接続を確立します。"
      },
      {
        "key": "D",
        "text": "Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions.",
        "text_jp": "パブリックVIFを作成します。パブリックVIFを介して他の2つのリージョンのVPCにAWS Site-to-Site VPN接続を確立します。"
      },
      {
        "key": "E",
        "text": "Use VPC peering to establish a connection between the VPCs across the Regions Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs.",
        "text_jp": "VPCピアリングを使用して、リージョン間のVPC間の接続を確立します。既存のDirect Connect接続を使用してプライベートVIFを作成し、ピアリングしたVPCに接続します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. By creating a Direct Connect gateway and attaching it to the existing Direct Connect connection, the company can connect its VPCs in other regions cheaply and efficiently.",
        "situation_analysis": "The company requires on-premises server access to EC2 instances across multiple regions and public AWS services. Direct Connect minimizes latency and data transfer costs.",
        "option_analysis": "Option A enables connectivity through a Direct Connect gateway, which is the efficient solution for connecting multiple VPCs across regions. Option B is more expensive due to increased connections. Option C and D would incur costs for VPN connections, and VPC peering (option E) complicates the network topology without a cost-effective solution.",
        "additional_knowledge": "The Direct Connect can interconnect up to five VPCs across different regions through a single gateway.",
        "key_terminology": "Direct Connect, VPC, AWS Site-to-Site VPN, VIF, hybrid environment",
        "overall_assessment": "The question effectively assesses understanding of AWS networking, including Direct Connect and VPC management. It reflects scenarios encountered in hybrid environments, making it relevant for practical applications."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。Direct Connectゲートウェイを作成し、既存のDirect Connect接続に接続することにより、企業は他のリージョンのVPCに安価かつ効率的に接続できる。",
        "situation_analysis": "企業は、オンプレミスサーバーが複数のリージョンにわたるEC2インスタンスおよびAWSのパブリックサービスにアクセスする必要がある。Direct Connectはレイテンシやデータ転送コストを最小限に抑える。",
        "option_analysis": "選択肢Aは、Direct Connectゲートウェイを介しての接続を可能にし、複数のリージョン間のVPCを接続するための効率的な解決策である。選択肢Bは、接続数が増えるため、より高価である。選択肢CおよびDはVPN接続にコストがかかり、VPCピアリング（選択肢E）はネットワークトポロジーを複雑にし、コスト効果の高い解決策にはならない。",
        "additional_knowledge": "Direct Connectは、単一のゲートウェイを介して異なるリージョンの5つのVPCまで相互接続できる。",
        "key_terminology": "Direct Connect, VPC, AWS Site-to-Site VPN, VIF, ハイブリッド環境",
        "overall_assessment": "この質問は、AWSネットワーキング、特にDirect ConnectおよびVPC管理の理解を評価する効果的なものである。ハイブリッド環境で遭遇するシナリオを反映しており、実用的な適用性がある。"
      }
    ],
    "keywords": [
      "Direct Connect",
      "VPC",
      "AWS Site-to-Site VPN",
      "VIF",
      "hybrid environment"
    ]
  },
  {
    "No": "199",
    "question": "A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to\nprovide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect\nis using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.\nWhich combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)",
    "question_jp": "ある企業がAWS Organizationsを使用して何百ものAWSアカウントを管理しています。ソリューションアーキテクトは、Open Web Application Security Project (OWASP) のトップ10のWebアプリケーションの脆弱性に対してベースライン保護を提供するソリューションに取り組んでいます。ソリューションアーキテクトは、組織内で展開されているすべての既存および新しいAmazon CloudFrontディストリビューションに対してAWS WAFを使用しています。ベースライン保護を提供するために、ソリューションアーキテクトが取るべきステップの組み合わせはどれか。 （3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Enable AWS Config in all accounts",
        "text_jp": "すべてのアカウントでAWS Configを有効にする"
      },
      {
        "key": "B",
        "text": "Enable Amazon GuardDuty in all accounts",
        "text_jp": "すべてのアカウントでAmazon GuardDutyを有効にする"
      },
      {
        "key": "C",
        "text": "Enable all features for the organization",
        "text_jp": "組織のすべての機能を有効にする"
      },
      {
        "key": "D",
        "text": "Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions",
        "text_jp": "AWS Firewall Managerを使用して、すべてのアカウントのすべてのCloudFrontディストリビューションにAWS WAFルールを展開する"
      },
      {
        "key": "E",
        "text": "Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions",
        "text_jp": "AWS Shield Advancedを使用して、すべてのアカウントのすべてのCloudFrontディストリビューションにAWS WAFルールを展開する"
      },
      {
        "key": "F",
        "text": "Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions",
        "text_jp": "AWS Security Hubを使用して、すべてのアカウントのすべてのCloudFrontディストリビューションにAWS WAFルールを展開する"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ACD (64%) 8% 8% Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Enable all features for the organization, which provides the necessary framework to manage AWS WAF effectively across multiple accounts.",
        "situation_analysis": "The organization is using AWS Organizations to manage multiple accounts, which necessitates a centralized approach for security and compliance management.",
        "option_analysis": "Option C enables features that facilitate the use of AWS WAF. While other options might enhance security, they do not specifically ensure the baseline protection for OWASP vulnerabilities in the context provided.",
        "additional_knowledge": "Properly configuring AWS WAF along with these organizational settings ensures a robust defense against OWASP top 10 vulnerabilities.",
        "key_terminology": "AWS Organizations, AWS WAF, OWASP, AWS Firewall Manager, CloudFront, security framework",
        "overall_assessment": "The best choice aligns with AWS best practices by leveraging AWS Organizations' capabilities, although integrating AWS Firewall Manager could augment this strategy."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はC: 組織のすべての機能を有効にすることであり、これにより、複数のアカウント間でAWS WAFを効果的に管理するために必要なフレームワークが提供される。",
        "situation_analysis": "この組織はAWS Organizationsを使用して複数のアカウントを管理しているため、セキュリティとコンプライアンス管理のための集中管理アプローチが必要である。",
        "option_analysis": "選択肢Cは、AWS WAFの使用を容易にする機能を有効にする。他の選択肢はセキュリティを強化する可能性があるが、提供された文脈においてOWASPの脆弱性に対するベースライン保護を特に保証するものではない。",
        "additional_knowledge": "AWS WAFを適切に構成することで、これらの組織設定とともにOWASPトップ10の脆弱性に対する堅牢な防御が確保できる。",
        "key_terminology": "AWS Organizations、AWS WAF、OWASP、AWS Firewall Manager、CloudFront、セキュリティフレームワーク",
        "overall_assessment": "この選択肢は、AWSのベストプラクティスに合致しており、AWS Organizationsの機能を活用している。AWS Firewall Managerを統合することで、この戦略を強化することも可能である。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS WAF",
      "OWASP",
      "AWS Firewall Manager",
      "CloudFront",
      "security framework"
    ]
  },
  {
    "No": "200",
    "question": "A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to\nauthenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal,\naccess to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are\nnot able to access the AWS environment.\nWhich items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)",
    "question_jp": "ソリューションアーキテクトは、会社のオンプレミスのアイデンティティプロバイダー（IdP）を使用して、ユーザーのAWS環境へのアクセスを認証するSAML 2.0のフェデレーテッドアイデンティティソリューションを実装した。\nソリューションアーキテクトがフェデレーテッドアイデンティティのウェブポータルを通じて認証をテストすると、AWS環境へのアクセスが許可される。しかし、テストユーザーがフェデレーテッドアイデンティティのウェブポータルを通じて認証を試みると、AWS環境へのアクセスができない。\nソリューションアーキテクトは、アイデンティティフェデレーションが正しく構成されていることを確認するために、どの項目を確認する必要があるか？（3つ選択）",
    "choices": [
      {
        "key": "A",
        "text": "The IAM user's permissions policy has allowed the use of SAML federation for that user.",
        "text_jp": "IAMユーザーの権限ポリシーが、そのユーザーに対してSAMLフェデレーションの使用を許可している。"
      },
      {
        "key": "B",
        "text": "The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.",
        "text_jp": "フェデレーテッドユーザーまたはフェデレーテッドグループのために作成されたIAMロールのトラストポリシーがSAMLプロバイダーをプリンシパルとして設定している。"
      },
      {
        "key": "B",
        "text": "Test users are not in the AWSFederatedUsers group in the company's IdP.",
        "text_jp": "フェデレーテッドユーザーまたはフェデレーテッドグループのために作成されたIAMロールのトラストポリシーがSAMLプロバイダーをプリンシパルとして設定している。"
      },
      {
        "key": "C",
        "text": "The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.",
        "text_jp": "ウェブポータルがAWS STS AssumeRoleWithSAML APIを、SAMLプロバイダーのARN、IAMロールのARN、IdPからのSAMLアサーションを使用して呼び出している。"
      },
      {
        "key": "D",
        "text": "The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.",
        "text_jp": "オンプレミスのIdPのDNSホスト名がAWS環境のVPCから到達可能である。"
      },
      {
        "key": "E",
        "text": "The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions.",
        "text_jp": "会社のIdPが、適切な権限を持つIAMロールにユーザーまたはグループを正しくマッピングするSAMLアサーションを定義している。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BCE (43%) B (29%) BD (29%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. It identifies the need for IAM roles' trust policy to include the SAML provider.",
        "situation_analysis": "The question presents a scenario where federated users cannot access the AWS environment despite successful initial testing by the solutions architect.",
        "option_analysis": "Option B is correct because it ensures that IAM roles trust the SAML provider, allowing federated users to assume the role and gain access. Other options may not directly address the trust relationship needed for access.",
        "additional_knowledge": "Understanding SAML assertions and IAM roles is key to configuring effective identity federation.",
        "key_terminology": "SAML, IAM Roles, Trust Policy, Federated Identity, AWS STS, AssumeRoleWithSAML",
        "overall_assessment": "The question emphasizes the importance of correctly configuring trust policies for federated access. Addressing the trust policy is crucial for enabling access for federated users."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。これは、IAMロールのトラストポリシーにSAMLプロバイダーを含める必要性を示している。",
        "situation_analysis": "この質問は、ソリューションアーキテクトが最初にテストを成功させたにもかかわらず、フェデレーテッドユーザーがAWS環境にアクセスできないというシナリオを提示している。",
        "option_analysis": "選択肢Bは正しい。なぜなら、IAMロールがSAMLプロバイダーを信頼することを確実にすることで、フェデレーテッドユーザーがロールを引き受け、アクセスを得ることができるからである。他の選択肢は、アクセスに必要な信頼関係を直接的に扱っていないかもしれない。",
        "additional_knowledge": "SAMLアサーションとIAMロールを理解することは、効果的なアイデンティティフェデレーションの構成において重要である。",
        "key_terminology": "SAML, IAMロール, トラストポリシー, フェデレーテッドアイデンティティ, AWS STS, AssumeRoleWithSAML",
        "overall_assessment": "この質問は、フェデレーションアクセスのためのトラストポリシーの正しい構成の重要性を強調している。フェデレーテッドユーザーへのアクセスを有効にするためには、トラストポリシーに対処することが重要である。"
      }
    ],
    "keywords": [
      "SAML",
      "IAM Roles",
      "Trust Policy",
      "Federated Identity",
      "AWS STS",
      "AssumeRoleWithSAML"
    ]
  },
  {
    "No": "201",
    "question": "A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB\ninstance that is experiencing overloaded connections. Most of the application's operations insert records into the database. The application\ncurrently stores credentials in a text-based configuration file.\nThe solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the\ncredentials secure and must provide the ability to rotate the credentials automatically on a regular basis.\nWhich solution will meet these requirements?",
    "question_jp": "AWS Cloudにホストされているアプリケーションを改善する必要があるソリューションアーキテクトがいます。このアプリケーションは、過負荷の接続を抱えているAmazon Aurora MySQL DBインスタンスを使用しています。アプリケーションのほとんどの操作は、データベースにレコードを挿入します。現在、アプリケーションはテキストベースの設定ファイルに資格情報を保存しています。ソリューションアーキテクトは、アプリケーションが現在の接続負荷を処理できるようにするソリューションを実装する必要があります。このソリューションは、資格情報を安全に保つ必要があり、定期的に資格情報を自動的にローテーションできる機能を提供する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy an Amazon RDS Proxy layer. In front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager.",
        "text_jp": "Amazon RDS Proxy層をDBインスタンスの前にデプロイします。接続資格情報をAWS Secrets Managerのシークレットとして保存します。"
      },
      {
        "key": "B",
        "text": "Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials in AWS Systems Manager Parameter Store",
        "text_jp": "DBインスタンスの前にAmazon RDS Proxy層をデプロイします。接続資格情報をAWS Systems Manager Parameter Storeに保存します。"
      },
      {
        "key": "C",
        "text": "Create an Aurora Replica. Store the connection credentials as a secret in AWS Secrets Manager",
        "text_jp": "Auroraレプリカを作成します。接続資格情報をAWS Secrets Managerのシークレットとして保存します。"
      },
      {
        "key": "D",
        "text": "Create an Aurora Replica. Store the connection credentials in AWS Systems Manager Parameter Store.",
        "text_jp": "Auroraレプリカを作成します。接続資格情報をAWS Systems Manager Parameter Storeに保存します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Deploying an Amazon RDS Proxy allows the application to handle increased connection loads efficiently. Storing credentials in AWS Secrets Manager ensures security and allows for automatic rotation.",
        "situation_analysis": "The application is overloaded with connections, necessitating an efficient way to manage database access while ensuring credential security.",
        "option_analysis": "Option A is correct because it combines RDS Proxy for connection management and Secrets Manager for credential security. Option B lacks the effective credential rotation capabilities. Option C and D do not provide connection management benefits.",
        "additional_knowledge": "Utilizing Secrets Manager allows developers to rotate database credentials without downtime.",
        "key_terminology": "Amazon RDS Proxy, AWS Secrets Manager, credentials rotation",
        "overall_assessment": "The proposed solution in Option A is optimal for improving application performance and ensuring security. Community support is unanimously in favor of this option."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。Amazon RDS Proxyをデプロイすることで、アプリケーションは増加した接続負荷を効率的に処理できます。資格情報をAWS Secrets Managerに保存することで、セキュリティが確保され、自動ローテーションが可能になります。",
        "situation_analysis": "アプリケーションは接続の過負荷に悩まされており、データベースへのアクセスを効率的に管理しつつ、資格情報のセキュリティを確保する必要があります。",
        "option_analysis": "選択肢Aは、接続管理のためにRDS Proxyを利用し、資格情報のセキュリティのためにSecrets Managerを使用するため正しいです。選択肢Bは、効果的な資格情報のローテーション機能が欠けています。選択肢CおよびDは、接続管理の利点を提供しません。",
        "additional_knowledge": "Secrets Managerを使用することで、開発者はダウンタイムなしにデータベースの資格情報をローテーションすることができます。",
        "key_terminology": "Amazon RDS Proxy, AWS Secrets Manager, 資格情報ローテーション",
        "overall_assessment": "選択肢Aに示された解決策は、アプリケーションのパフォーマンスを向上させ、セキュリティを確保するための最適な選択です。コミュニティの支持はこの選択肢に対して全会一致です。"
      }
    ],
    "keywords": [
      "Amazon RDS Proxy",
      "AWS Secrets Manager",
      "credentials rotation"
    ]
  },
  {
    "No": "202",
    "question": "A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fieet of t3.large\nAmazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across\nmultiple Availability Zones.\nIn the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業が、そのeコマースウェブサイトの災害復旧（DR）ソリューションを構築する必要があります。このWebアプリケーションは、t3.largeのAmazon EC2インスタンスの群れ上にホストされており、Amazon RDS for MySQL DBインスタンスを使用しています。EC2インスタンスは、複数のアベイラビリティゾーンにまたがるオートスケーリンググループに配置されています。災害が発生した場合、Webアプリケーションは、30秒のRPOおよび10分のRTOで、セカンダリ環境にフェイルオーバーする必要があります。どのソリューションが最もコスト効率良くこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Recover the EC2 instances from the latest EC2 backup. Use an Amazon Route 53 geolocation routing policy to automatically fail over to the DR Region in the event of a disaster.",
        "text_jp": "グローバルな災害復旧地点で、新しいインフラストラクチャをプロビジョニングするためにインフラストラクチャをコードとして（IaC）使用します。DBインスタンスのクロスリージョン読み取りレプリカを作成します。AWS BackupでEC2インスタンスとDBインスタンスのクロスリージョンバックアップを作成するバックアッププランを設定します。30秒ごとにEC2インスタンスとDBインスタンスをDRリージョンにバックアップするクリンクススクリプトを作成します。最新のEC2バックアップからEC2インスタンスを復旧します。災害が発生した場合にDRリージョンに自動でフェイルオーバーするために、Amazon Route 53の地理位置情報ルーティングポリシーを使用します。"
      },
      {
        "key": "B",
        "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the EC2 instances at the minimum capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. Increase the desired capacity of the Auto Scaling group.",
        "text_jp": "グローバルな災害復旧地点で、新しいインフラストラクチャをプロビジョニングするためにインフラストラクチャをコードとして（IaC）使用します。DBインスタンスのクロスリージョン読み取りレプリカを作成します。AWS Elastic Disaster Recoveryを使用して、EC2インスタンスをDRリージョンに継続的にレプリケートします。DRリージョンでEC2インスタンスを最小容量で稼働させます。災害が発生した場合にDRリージョンに自動でフェイルオーバーするために、Amazon Route 53のフェイルオーバールーティングポリシーを使用します。オートスケーリンググループの希望数を増加させます。"
      },
      {
        "key": "C",
        "text": "Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Manually restore the backed-up data on new instances. Use an Amazon Route 53 simple routing policy to automatically fail over to the DR Region in the event of a disaster.",
        "text_jp": "AWS BackupでEC2インスタンスとDBインスタンスのクロスリージョンバックアップを作成するバックアッププランを設定します。30秒ごとにEC2インスタンスとDBインスタンスをDRリージョンにバックアップするクリンクススクリプトを作成します。新しいインフラストラクチャをDRリージョンにプロビジョニングするためにインフラストラクチャをコードとして（IaC）使用します。バックアップされたデータを新しいインスタンスに手動で復元します。災害が発生した場合にDRリージョンに自動でフェイルオーバーするために、Amazon Route 53のシンプルルーティングポリシーを使用します。"
      },
      {
        "key": "D",
        "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the Auto Scaling group of EC2 instances at full capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster.",
        "text_jp": "グローバルな災害復旧地点で、新しいインフラストラクチャをプロビジョニングするためにインフラストラクチャをコードとして（IaC）使用します。Amazon Auroraのグローバルデータベースを作成します。AWS Elastic Disaster Recoveryを使用してEC2インスタンスをDRリージョンに継続的にレプリケートします。DRリージョンでEC2インスタンスのオートスケーリンググループをフルキャパシティで稼働させます。災害が発生した場合にDRリージョンに自動でフェイルオーバーするために、Amazon Route 53のフェイルオーバールーティングポリシーを使用します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (83%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This option effectively uses AWS services to meet the RPO and RTO requirements while keeping costs low.",
        "situation_analysis": "The question requires a disaster recovery solution for an ecommerce website hosted on EC2 and RDS, with specific recovery time objectives (RTO) and recovery point objectives (RPO).",
        "option_analysis": "Option B is the best because it includes AWS Elastic Disaster Recovery for continuous replication, ensuring minimal data loss (RPO of 30 seconds) and makes use of a failover routing policy for quicker failover. Other options either do not meet RTO requirements or involve unnecessary complexities/costs.",
        "additional_knowledge": "There is a focus on keeping operational costs low without sacrificing recovery speed.",
        "key_terminology": "AWS Elastic Disaster Recovery, RPO, RTO, Amazon Route 53, Auto Scaling",
        "overall_assessment": "The question is well-constructed, presenting realistic scenarios with multiple viable solutions. Choice B aligns best with AWS best practices for disaster recovery, as indicated by community vote."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。この選択肢は、コストを抑えながらRPOおよびRTOの要件を満たすためにAWSサービスを効果的に利用している。",
        "situation_analysis": "この問題は、特定の回復時間目標（RTO）および回復ポイント目標（RPO）を持つEC2およびRDS上にホストされているeコマースウェブサイトの災害復旧ソリューションを要求している。",
        "option_analysis": "選択肢Bは最良であり、AWS Elastic Disaster Recoveryを使用して継続的なレプリケーションを行うため、最小限のデータ損失（RPO 30秒）を確保し、迅速なフェイルオーバーのためにフェイルオーバールーティングポリシーを利用している。他の選択肢は、RTO要件を満たしていないか、不要な複雑さやコストが含まれている。",
        "additional_knowledge": "運用コストを抑えつつ、復旧速度を犠牲にしないことに焦点を当てている。",
        "key_terminology": "AWS Elastic Disaster Recovery, RPO, RTO, Amazon Route 53, オートスケーリング",
        "overall_assessment": "この質問はよく構成されており、複数の実行可能なソリューションを持つ現実的なシナリオを提示している。選択肢Bはコミュニティの投票からも示されるように、災害復旧のためのAWSの最良のプラクティスに最も合致している。"
      }
    ],
    "keywords": [
      "AWS Elastic Disaster Recovery",
      "RPO",
      "RTO",
      "Amazon Route 53",
      "Auto Scaling"
    ]
  },
  {
    "No": "203",
    "question": "A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's\ncurrent internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a\nmonth to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database\nmore quickly.\nWhich solution will migrate the database in the LEAST amount of time?",
    "question_jp": "ある会社は、オンプレミスのMySQLデータベースをus-east-1リージョンのAmazon Aurora MySQLに一度限りのマイグレーションを計画しています。会社の現在のインターネット接続には限られた帯域幅があります。オンプレミスのMySQLデータベースは60 TBのサイズです。会社は、現在のインターネット接続を介してデータをAWSに転送するのに1か月かかると見積もっています。会社は、データベースをより迅速に移行する解決策を必要としています。最も短時間でデータベースを移行するソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises MySQL database to Aurora MySQL.",
        "text_jp": "オンプレミスのデータセンターとAWSの間に1 GbpsのAWS Direct Connect接続を要求します。AWS Database Migration Service (AWS DMS)を使用して、オンプレミスのMySQLデータベースをAurora MySQLに移行します。"
      },
      {
        "key": "B",
        "text": "Use AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use AWS Application Migration Service to migrate the on-premises MySQL database to Aurora MySQL.",
        "text_jp": "現在のインターネット接続を使用してAWS DataSyncでデータ転送を加速し、AWS Application Migration Serviceを使用してオンプレミスのMySQLデータベースをAurora MySQLに移行します。"
      },
      {
        "key": "C",
        "text": "Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL.",
        "text_jp": "AWS Snowball Edgeデバイスを注文します。S3インターフェースを使用してデータをAmazon S3バケットにロードし、AWS Database Migration Service (AWS DMS)を使用してデータをAmazon S3からAurora MySQLに移行します。"
      },
      {
        "key": "D",
        "text": "Order an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application Migration Service to migrate the data from Amazon S3 to Aurora MySQL.",
        "text_jp": "AWS Snowballデバイスを注文します。S3 Adapter for Snowballを使用してデータをAmazon S3バケットにロードし、AWS Application Migration Serviceを使用してデータをAmazon S3からAurora MySQLに移行します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (94%) 6%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using an AWS Snowball Edge device allows for efficient transfer of large amounts of data, significantly reducing the time needed for migration.",
        "situation_analysis": "The company has a 60 TB MySQL database and limited internet bandwidth, making traditional transfer methods too slow.",
        "option_analysis": "Option A would still rely on existing bandwidth, which is insufficient. Option B is also limited by the current internet connection speed. Option D uses Snowball but involves Application Migration Service, which is not optimized for direct S3 transfers.",
        "additional_knowledge": "Using AWS services optimally ensures fast and cost-effective migrations.",
        "key_terminology": "AWS Snowball, AWS Database Migration Service, Amazon S3, data migration, bandwidth constraints",
        "overall_assessment": "C was strongly supported by community votes (94%), indicating broad agreement on its effectiveness for this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCである。AWS Snowball Edgeデバイスを使用することで、大量のデータを効率的に転送でき、移行に必要な時間を大幅に短縮できる。",
        "situation_analysis": "会社は60 TBのMySQLデータベースを持ち、限られたインターネット帯域幅があるため、従来の転送方法では遅すぎる。",
        "option_analysis": "選択肢Aは依然として既存の帯域幅に依存しており、十分ではない。選択肢Bも現在のインターネット接続速度に制限される。選択肢DはSnowballを使用するが、Application Migration Serviceを含んでおり、S3への直接転送には最適化されていない。",
        "additional_knowledge": "AWSサービスを最適に活用することで、迅速かつコスト効果の高い移行が実現できる。",
        "key_terminology": "AWS Snowball, AWS Database Migration Service, Amazon S3, データ移行, 帯域幅制約",
        "overall_assessment": "Cはコミュニティの投票（94%）によって強力に支持されており、このシナリオに対する効果的な選択肢であることを示している。"
      }
    ],
    "keywords": [
      "AWS Snowball",
      "AWS Database Migration Service",
      "Amazon S3",
      "data migration",
      "bandwidth constraints"
    ]
  },
  {
    "No": "204",
    "question": "A company has an application in the AWS Cloud. The application runs on a fieet of 20 Amazon EC2 instances. The EC2 instances are persistent\nand store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.\nThe company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration\nwithin 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes\noperational eficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network\nconfiguration in a secondary Region.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWSクラウドでアプリケーションを運用しています。このアプリケーションは、20台のAmazon EC2インスタンスの集団で実行されています。EC2インスタンスは永続的で、複数の接続されたAmazon Elastic Block Store（Amazon EBS）ボリュームにデータを保存しています。企業は別のAWSリージョンにバックアップを維持する必要があります。企業は、1営業日以内にEC2インスタンスとその構成を復元でき、1日分以上のデータの損失がないようにする必要があります。企業には限られた人員がおり、運用効率とコストを最適化したバックアップソリューションが必要です。企業はすでに、セカンダリーリージョンで必要なネットワーク構成をデプロイできるAWS CloudFormationテンプレートを作成しています。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots by using AWS Systems Manager Automation runbooks. Copy the snapshots to the secondary Region. In the event of a failure launch the CloudFormation templates, restore the EBS volumes from snapshots, and transfer usage to the secondary Region.",
        "text_jp": "別のCloudFormationテンプレートを作成して、セカンダリーリージョンでEC2インスタンスを再作成できるようにします。AWS Systems Manager Automationのランブックを使用して、毎日マルチボリュームスナップショットを実行します。スナップショットをセカンダリーリージョンにコピーします。障害が発生した場合は、CloudFormationテンプレートを起動し、スナップショットからEBSボリュームを復元し、使用をセカンダリーリージョンに移行します。"
      },
      {
        "key": "B",
        "text": "Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure, launch the CloudFormation template and use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region.",
        "text_jp": "Amazon Data Lifecycle Manager（Amazon DLM）を使用して、EBSボリュームの毎日のマルチボリュームスナップショットを作成します。障害が発生した場合は、CloudFormationテンプレートを起動し、Amazon DLMを使用してEBSボリュームを復元し、使用をセカンダリーリージョンに移行します。"
      },
      {
        "key": "C",
        "text": "Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in the secondary Region. In the event of a failure, launch the CloudFormation template, restore the instance volumes and configurations from the backup vault, and transfer usage to the secondary Region.",
        "text_jp": "AWS Backupを使用して、EC2インスタンスの定期的な毎日のバックアッププランを作成します。バックアップタスクを構成して、バックアップをセカンダリーリージョンのボールトにコピーします。障害が発生した場合は、CloudFormationテンプレートを起動し、バックアップボールトからインスタンスボリュームと構成を復元し、使用をセカンダリーリージョンに移行します。"
      },
      {
        "key": "D",
        "text": "Deploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the primary Region to the secondary Region. In the event of a failure, launch the CloudFormation template and transfer usage to the secondary Region.",
        "text_jp": "同じサイズと構成のEC2インスタンスをセカンダリーリージョンにデプロイします。AWS DataSyncを使用して、毎日プライマリーリージョンからセカンダリーリージョンにデータをコピーするように設定します。障害が発生した場合は、CloudFormationテンプレートを起動し、使用をセカンダリーリージョンに移行します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (74%) B (26%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Using AWS Backup to create a scheduled daily backup plan specifically addresses the requirements for backing up EC2 instances and EBS volumes efficiently and cost-effectively.",
        "situation_analysis": "The company requires a reliable backup solution that can recover EC2 instances and their configurations within one business day, with minimal data loss. The CloudFormation template for deployment in another region is already available, adding efficiency to the recovery process.",
        "option_analysis": "Option A involves additional steps of creating templates and using Systems Manager, which complicates the backup and recovery process. Option B is simpler, but Data Lifecycle Manager is typically better for automated lifecycle policies rather than for instant recovery. Option D focuses on data synchronization rather than backup, failing the requirement for a reliable snapshot-and-recovery plan.",
        "additional_knowledge": "Using AWS Backup enables granular control over backup policies, enhancing recovery time and reducing potential data loss.",
        "key_terminology": "AWS Backup, EBS, CloudFormation, snapshot, recovery process.",
        "overall_assessment": "Answer C is supported by 74% of the community vote, indicating a strong alignment with best practices for backup and recovery on AWS."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです：AWS Backupを使用して定期的な毎日のバックアッププランを作成することは、EC2インスタンスとEBSボリュームの効率的かつコスト効果の高いバックアップ要件に特に対応しています。",
        "situation_analysis": "この企業は、1営業日以内にEC2インスタンスとその構成を復元でき、データ損失が最小限である信頼できるバックアップソリューションを必要としています。また、別のリージョンでデプロイするためのCloudFormationテンプレートがすでに利用可能であり、復旧プロセスの効率を高めています。",
        "option_analysis": "オプションAは、テンプレートを作成し、Systems Managerを使用する追加の手順が必要であり、バックアップと復旧プロセスを複雑にします。オプションBは単純ですが、Data Lifecycle Managerは即時復旧よりも自動化されたライフサイクルポリシーに適しているため、要件を満たすことができません。オプションDはデータの同期に焦点を当てており、バックアップの信頼できるスナップショットおよび復旧プランの要件を満たしていません。",
        "additional_knowledge": "AWS Backupを使用することで、バックアップポリシーに対する粒度の高い制御が可能になり、復旧時間が短縮され、データ損失の可能性が低下します。",
        "key_terminology": "AWS Backup, EBS, CloudFormation, スナップショット, 復旧プロセス。",
        "overall_assessment": "回答Cはコミュニティの投票の74％の支持を受けており、AWSにおけるバックアップと復旧に関するベストプラクティスとの強い整合性を示しています。"
      }
    ],
    "keywords": [
      "AWS Backup",
      "EBS",
      "CloudFormation",
      "snapshot",
      "recovery process"
    ]
  },
  {
    "No": "205",
    "question": "A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files.\nAccording to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using\nAmazon S3 and Amazon CloudFront.\nWhich combination of steps will meet the encryption requirements? (Choose three.)",
    "question_jp": "ある企業が静的コンテンツをホストする新しいウェブサイトを設計しています。このウェブサイトは、ユーザーが大きなファイルをアップロードおよびダウンロードできる機能を提供します。企業の要件によれば、すべてのデータは転送中および静止中に暗号化されなければなりません。ソリューションアーキテクトは、Amazon S3とAmazon CloudFrontを使用してソリューションを構築しています。どの組み合わせの手順が暗号化要件を満たすでしょうか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Turn on S3 server-side encryption for the S3 bucket that the web application uses.",
        "text_jp": "ウェブアプリケーションが使用するS3バケットに対してS3サーバーサイド暗号化をオンにする。"
      },
      {
        "key": "B",
        "text": "Add a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations in the S3 ACLs.",
        "text_jp": "S3 ACLの読み取りおよび書き込み操作のために\"aws:SecureTransport\": \"true\"のポリシー属性を追加する。"
      },
      {
        "key": "C",
        "text": "Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses.",
        "text_jp": "ウェブアプリケーションが使用するS3バケット内の未暗号化操作を拒否するバケットポリシーを作成する。"
      },
      {
        "key": "D",
        "text": "Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).",
        "text_jp": "AWS KMSキー（SSE-KMS）を使用してCloudFrontで静止時の暗号化を設定する。"
      },
      {
        "key": "E",
        "text": "Configure redirection of HTTP requests to HTTPS requests in CloudFront.",
        "text_jp": "CloudFrontでHTTPリクエストをHTTPSリクエストにリダイレクトするように設定する。"
      },
      {
        "key": "F",
        "text": "Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses.",
        "text_jp": "ウェブアプリケーションが使用するS3バケットのプリサイン付きURLを作成する際にRequireSSLオプションを使用する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACE (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is 'A'. To meet the encryption requirements, server-side encryption must be enabled for the S3 bucket, ensuring that all stored data is encrypted at rest.",
        "situation_analysis": "The company needs to ensure that data is encrypted both during transfer and when stored. Since S3 is being used, enabling server-side encryption is a default measure to secure data at rest.",
        "option_analysis": "Option A is correct because it directly addresses data at rest encryption requirements. Option B supports data in transit but is not sufficient alone. Option C is a good practice but does not provide a mechanism for encrypting data at rest. Option D refers to CloudFront, which isn’t about S3 encryption. Option E addresses data in transit, and Option F also supports secure data transmission, making them supplementary but not central.",
        "additional_knowledge": "Understanding AWS identity and access management policies could further enhance security.",
        "key_terminology": "Amazon S3, Amazon CloudFront, encryption, server-side encryption, HTTPS",
        "overall_assessment": "The question effectively assesses the understanding of encryption requirements related to AWS services. However, the community vote distribution shows a significant preference for multiple choices, suggesting a slight ambiguity in what is considered essential."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは「A」です。暗号化要件を満たすためには、S3バケットのサーバーサイド暗号化を有効にし、保存されているすべてのデータが静止中に暗号化されるようにします。",
        "situation_analysis": "企業は、データが転送中と保存中の両方で暗号化されていることを確認する必要があります。S3が使用されているため、サーバーサイド暗号化を有効にすることは、データを静止中に安全にするための標準的な措置です。",
        "option_analysis": "選択肢Aは、データの静止中の暗号化要件に直接取り組んでいるため、正しいです。選択肢Bは転送中のデータを支援しますが、それだけでは不十分です。選択肢Cは良いプラクティスの一つですが、静止中のデータ暗号化のメカニズムを提供しません。選択肢DはCloudFrontに関するもので、S3の暗号化とは関係ありません。選択肢Eは転送中のデータに対応し、選択肢Fも安全なデータ転送をサポートするため、補助的ですが中心的ではありません。",
        "additional_knowledge": "AWSのアイデンティティとアクセス管理ポリシーを理解することで、さらにセキュリティを向上させることができます。",
        "key_terminology": "Amazon S3, Amazon CloudFront, 暗号化, サーバーサイド暗号化, HTTPS",
        "overall_assessment": "この質問は、AWSサービスに関する暗号化要件の理解を効果的に評価しています。ただし、コミュニティ投票分配は複数の選択肢に対する大きな支持を示しており、何が本質的と見なされるかにわずかなあいまいさがあることを示唆しています。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Amazon CloudFront",
      "encryption",
      "server-side encryption",
      "HTTPS"
    ]
  },
  {
    "No": "206",
    "question": "A company is implementing a serverless architecture by using AWS Lambda functions that need to access a Microsoft SQL Server DB instance on\nAmazon RDS. The company has separate environments for development and production, including a clone of the database system.\nThe company's developers are allowed to access the credentials for the development database. However, the credentials for the production\ndatabase must be encrypted with a key that only members of the IT security team's IAM user group can access. This key must be rotated on a\nregular basis.\nWhat should a solutions architect do in the production environment to meet these requirements?",
    "question_jp": "ある企業が、Microsoft SQL Server DBインスタンスにアクセスする必要があるAWS Lambda関数を使用してサーバーレスアーキテクチャを実装しています。企業には開発と本番のための別個の環境があり、データベースシステムのクローンも含まれています。企業の開発者は、開発用データベースの資格情報へのアクセスを許可されています。ただし、本番用データベースの資格情報は、ITセキュリティチームのIAMユーザーグループのメンバーのみがアクセスできる鍵で暗号化する必要があります。この鍵は定期的にローテーションする必要があります。本番環境でこれらの要件を満たすために、ソリューションアーキテクトは何を行うべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Store the database credentials in AWS Systems Manager Parameter Store by using a SecureString parameter that is encrypted by an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the SecureString parameter. Restrict access to the SecureString parameter and the customer managed key so that only the IT security team can access the parameter and the key.",
        "text_jp": "AWS Systems Manager Parameter Storeにデータベースの資格情報をSecureStringパラメータとして保存し、AWS Key Management Service (AWS KMS) の顧客管理鍵によって暗号化します。各Lambda関数にSecureStringパラメータへのアクセスを提供するロールをアタッチします。SecureStringパラメータと顧客管理鍵へのアクセスを制限し、ITセキュリティチームのみがパラメータと鍵にアクセスできるようにします。"
      },
      {
        "key": "B",
        "text": "Encrypt the database credentials by using the AWS Key Management Service (AWS KMS) default Lambda key. Store the credentials in the environment variables of each Lambda function. Load the credentials from the environment variables in the Lambda code. Restrict access to the KMS key so that only the IT security team can access the key.",
        "text_jp": "AWS Key Management Service (AWS KMS) のデフォルトのLambda鍵を使用して、データベースの資格情報を暗号化します。資格情報を各Lambda関数の環境変数に保存します。Lambdaコードで環境変数から資格情報をロードします。KMS鍵へのアクセスを制限し、ITセキュリティチームのみが鍵にアクセスできるようにします。"
      },
      {
        "key": "C",
        "text": "Store the database credentials in the environment variables of each Lambda function. Encrypt the environment variables by using an AWS Key Management Service (AWS KMS) customer managed key. Restrict access to the customer managed key so that only the IT security team can access the key.",
        "text_jp": "データベースの資格情報を各Lambda関数の環境変数に保存します。環境変数をAWS Key Management Service (AWS KMS) の顧客管理鍵を使用して暗号化します。顧客管理鍵へのアクセスを制限し、ITセキュリティチームのみが鍵にアクセスできるようにします。"
      },
      {
        "key": "D",
        "text": "Store the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the secret. Restrict access to the secret and the customer managed key so that only the IT security team can access the secret and the key.",
        "text_jp": "AWS Secrets ManagerにAWS Key Management Service (AWS KMS) の顧客管理鍵に関連付けられた秘密としてデータベースの資格情報を保存します。各Lambda関数に秘密へのアクセスを提供するロールをアタッチします。秘密と顧客管理鍵へのアクセスを制限し、ITセキュリティチームのみが秘密と鍵にアクセスできるようにします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (77%) A (23%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. It uses AWS Secrets Manager to securely store and manage database credentials, ensuring that only the IT security team has access to the secret and that it is encrypted by a customer-managed KMS key. This approach meets the requirement for encryption and access limitations effectively.",
        "situation_analysis": "The company requires secure storage of production database credentials, which must be accessible only by the IT security team. Regular key rotation is also necessary for maintaining security.",
        "option_analysis": "Options A, B, and C do not meet all requirements. Option A involves Parameter Store, which is suitable but less secure than Secrets Manager for sensitive data. Option B stores credentials in environment variables and doesn't provide adequate security. Option C similarly uses environment variables without the security benefits of Secrets Manager. Therefore, Option D is the best choice as it uses Secrets Manager with proper access controls.",
        "additional_knowledge": "Using AWS Secrets Manager allows applications to retrieve secrets seamlessly, enhancing security through managed policies and automatic key rotation features.",
        "key_terminology": "AWS Secrets Manager, AWS Key Management Service, IAM roles, encryption, secure storage.",
        "overall_assessment": "The question is well-structured, and the correct answer (D) aligns with best practices for managing sensitive credentials. The community vote showing strong support for option D confirms its validity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。AWS Secrets Managerを使用してデータベースの資格情報を安全に保存および管理し、ITセキュリティチームのみが秘密にアクセスできること、また顧客管理のKMS鍵によって暗号化されることを保証する。このアプローチは、暗号化およびアクセス制限の要件を効果的に満たす。",
        "situation_analysis": "この企業は、本番データベース資格情報の安全な保存を必要とし、ITセキュリティチームだけがアクセスできる必要がある。定期的な鍵のローテーションもセキュリティ維持に必要である。",
        "option_analysis": "選択肢A、B、Cはすべての要件を満たしていない。選択肢AはParameter Storeを利用するが、Sensitiveデータを扱うにはSecrets Managerに比べて安全性が劣る。選択肢Bは環境変数に資格情報を直接保存し、安全性が不十分である。選択肢Cも環境変数を使用するが、Secrets Managerが提供するセキュリティの利点が不足している。したがって、選択肢DはSecrets Managerと適切なアクセス制御を使用しているため、最良の選択である。",
        "additional_knowledge": "AWS Secrets Managerを使用すると、アプリケーションはシームレスに秘密を取得でき、管理ポリシーや自動的な鍵のローテーション機能を通じてセキュリティが向上する。",
        "key_terminology": "AWS Secrets Manager、AWS Key Management Service、IAMロール、暗号化、安全な保存。",
        "overall_assessment": "問題はよく構成されており、正解(D)は機密資格情報の管理に関するベストプラクティスに合致している。コミュニティ投票でも選択肢Dの強い支持が得られており、その正当性を確認できる。"
      }
    ],
    "keywords": [
      "AWS Secrets Manager",
      "AWS Key Management Service",
      "IAM roles",
      "encryption",
      "secure storage"
    ]
  },
  {
    "No": "207",
    "question": "An online retail company is migrating its legacy on-premises .NET application to AWS. The application runs on load-balanced frontend web\nservers, load-balanced application servers, and a Microsoft SQL Server database.\nThe company wants to use AWS managed services where possible and does not want to rewrite the application. A solutions architect needs to\nimplement a solution to resolve scaling issues and minimize licensing costs as the application scales.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "オンライン小売会社が、レガシーなオンプレミスの.NETアプリケーションをAWSに移行しています。このアプリケーションは、ロードバランスされたフロントエンドのウェブサーバー、ロードバランスされたアプリケーションサーバー、およびMicrosoft SQL Serverデータベースで実行されています。会社は可能な限りAWSのマネージドサービスを使用したいと考えており、アプリケーションを再構築したくありません。ソリューションアーキテクトは、スケールの問題を解決し、アプリケーションがスケールする際のライセンスコストを最小限に抑えるソリューションを実装する必要があります。どのソリューションが最もコスト効果的にこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database.",
        "text_jp": "ウェブティアとアプリケーションティア用に、アプリケーションロードバランサーの背後にAuto Scalingグループ内にAmazon EC2インスタンスを展開します。SQL Serverデータベースを再プラットフォームするためにBabelfishをオンにしたAmazon Aurora PostgreSQLを使用します。"
      },
      {
        "key": "B",
        "text": "Create images of all the servers by using AWS Database Migration Service (AWS DMS). Deploy Amazon EC2 instances that are based on the on-premises imports. Deploy the instances in an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon DynamoDB as the database tier.",
        "text_jp": "AWS Database Migration Service (AWS DMS)を使用してすべてのサーバーのイメージを作成します。オンプレミスのインポートを基にしたAmazon EC2インスタンスを展開します。このインスタンスをウェブティアとアプリケーションティア用にNetwork Load Balancerの背後にあるAuto Scalingグループに配置します。データベースティアとしてAmazon DynamoDBを使用します。"
      },
      {
        "key": "C",
        "text": "Containerize the web frontend tier and the application tier. Provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon RDS for SQL Server to host the database.",
        "text_jp": "ウェブフロントエンドティアとアプリケーションティアをコンテナ化します。Amazon Elastic Kubernetes Service (Amazon EKS)クラスターをプロビジョニングします。ウェブティアとアプリケーションティア用にNetwork Load Balancerの背後にAuto Scalingグループを作成します。データベースとしてAmazon RDS for SQL Serverを使用します。"
      },
      {
        "key": "D",
        "text": "Separate the application functions into AWS Lambda functions. Use Amazon API Gateway for the web frontend tier and the application tier. Migrate the data to Amazon S3. Use Amazon Athena to query the data.",
        "text_jp": "アプリケーション機能をAWS Lambda関数に分離します。ウェブフロントエンドティアとアプリケーションティア用にAmazon API Gatewayを使用します。データをAmazon S3に移行します。データをクエリするためにAmazon Athenaを使用します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This solution utilizes AWS managed services, specifically Amazon EC2 instances in an Auto Scaling group and Amazon Aurora with Babelfish for SQL Server compatibility, optimizing costs and scalability.",
        "situation_analysis": "The company needs a scalable solution to migrate an existing application to AWS without major rewrites, while minimizing licensing costs associated with SQL Server.",
        "option_analysis": "Option A is the best choice because it effectively uses managed services and doesn't require a complete rewrite of the application. Option B introduces DynamoDB, which requires significant changes to the application. Option C involves operational overhead with Kubernetes, which may not be ideal for a migration goal. Option D drastically changes the application architecture using serverless functions, which may not be optimal in this context.",
        "additional_knowledge": "AWS provides a suite of services tailored for legacy application migration, which can reduce overhead and improve performance.",
        "key_terminology": "Amazon EC2, Auto Scaling, Application Load Balancer, Amazon Aurora, Babelfish",
        "overall_assessment": "Option A is the most cost-effective and simplest path for migrating the application to AWS, aligning with the company's goal of using managed services and minimizing changes."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。このソリューションは、AWSのマネージドサービス、具体的にはAuto Scalingグループ内のAmazon EC2インスタンスとSQL Serverとの互換性のあるBabelfishを搭載したAmazon Auroraを利用し、コストとスケーラビリティを最適化している。",
        "situation_analysis": "会社は、既存のアプリケーションをAWSにマイグレーションするためにスケーラブルなソリューションが必要であり、大きな再構築をせずにSQL Serverに関連するライセンスコストを最小限に抑えたいと考えている。",
        "option_analysis": "選択肢Aは、マネージドサービスを効果的に利用し、アプリケーションの全面的な再構築を必要としないため、最良の選択肢となる。選択肢BはDynamoDBを導入し、アプリケーションに大幅な変更を必要とする。選択肢CはKubernetesによる運用上の負荷がかかり、マイグレーションの目標には適していない可能性がある。選択肢Dはアプリケーションアーキテクチャを大幅に変更し、サーバーレス機能を使用するが、この文脈では最適とは言えない。",
        "additional_knowledge": "AWSは、レガシーアプリケーションのマイグレーションに特化した一連のサービスを提供しており、オーバーヘッドを削減し、パフォーマンスを向上させることができる。",
        "key_terminology": "Amazon EC2, Auto Scaling, Application Load Balancer, Amazon Aurora, Babelfish",
        "overall_assessment": "選択肢Aは、アプリケーションをAWSにマイグレーションするための最もコスト効果的かつシンプルな道であり、会社のマネージドサービスを利用し、変更を最小限に抑えるという目標に合致している。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Amazon Aurora",
      "Babelfish"
    ]
  },
  {
    "No": "208",
    "question": "A software-as-a-service (SaaS) provider exposes APIs through an Application Load Balancer (ALB). The ALB connects to an Amazon Elastic\nKubernetes Service (Amazon EKS) cluster that is deployed in the us-east-1 Region. The exposed APIs contain usage of a few non-standard REST\nmethods: LINK, UNLINK, LOCK, and UNLOCK.\nUsers outside the United States are reporting long and inconsistent response times for these APIs. A solutions architect needs to resolve this\nproblem with a solution that minimizes operational overhead.\nWhich solution meets these requirements?",
    "question_jp": "ソフトウェア・アズ・ア・サービス（SaaS）プロバイダーは、アプリケーションロードバランサー（ALB）を介してAPIを公開しています。このALBは、米国東部（us-east-1）リージョンにデプロイされたAmazon Elastic Kubernetes Service（Amazon EKS）クラスターに接続しています。公開されたAPIには、いくつかの非標準RESTメソッド（LINK、UNLINK、LOCK、UNLOCK）が含まれています。米国外のユーザーから、これらのAPIに対して、長い応答時間と一貫性のない応答が報告されています。ソリューションアーキテクトは、運用オーバーヘッドを最小限に抑えるソリューションでこの問題を解決する必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Add an Amazon CloudFront distribution. Configure the ALB as the origin.",
        "text_jp": "Amazon CloudFrontディストリビューションを追加し、ALBをオリジンとして設定します。"
      },
      {
        "key": "B",
        "text": "Add an Amazon API Gateway edge-optimized API endpoint to expose the APIs. Configure the ALB as the target.",
        "text_jp": "APIを公開するためにAmazon API Gatewayエッジ最適化APIエンドポイントを追加し、ALBをターゲットとして設定します。"
      },
      {
        "key": "C",
        "text": "Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin.",
        "text_jp": "AWS Global Acceleratorにアクセラレーターを追加し、ALBをオリジンとして設定します。"
      },
      {
        "key": "D",
        "text": "Deploy the APIs to two additional AWS Regions: eu-west-1 and ap-southeast-2. Add latency-based routing records in Amazon Route 53.",
        "text_jp": "APIを2つの追加AWSリージョン（eu-west-1およびap-southeast-2）にデプロイし、Amazon Route 53にレイテンシーベースのルーティングレコードを追加します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (66%) B (31%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Adding an accelerator in AWS Global Accelerator and configuring the ALB as the origin will enhance the API response times for users outside the United States because it optimizes the network routing to improve the overall availability and performance.",
        "situation_analysis": "Users outside of the United States are experiencing long and inconsistent response times for APIs that employ non-standard REST methods. The architectural setup includes an ALB and Amazon EKS in a specific region (us-east-1), which can cause increased latency for international users.",
        "option_analysis": "Option C is effective because AWS Global Accelerator uses AWS's global network to direct user traffic to the optimal endpoint, effectively minimizing latency. Option A (CloudFront) may not be as effective because it primarily benefits static content and cacheable responses. Option B (API Gateway) can introduce additional latency due to the different service operation. Option D would increase complexity by deploying additional regions but could address latency.",
        "additional_knowledge": "In high-traffic scenarios, leveraging techniques like caching and edge-optimized services can offer further performance benefits.",
        "key_terminology": "AWS Global Accelerator, Application Load Balancer, latency, optimization, network routing",
        "overall_assessment": "The recommendation to use AWS Global Accelerator is sound as it is designed to improve application performance for end-users globally. The community vote aligns with the recommended solution, indicating consensus on its effectiveness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCである: AWS Global Acceleratorにアクセラレーターを追加し、ALBをオリジンとして設定することによって、米国外のユーザーに対するAPI応答時間を改善し、全体の可用性とパフォーマンスを向上させることができる。",
        "situation_analysis": "米国外のユーザーが非標準RESTメソッドを使用するAPIに対して長い応答時間と一貫性のない応答を経験している。アーキテクチャはALBとAmazon EKSを含んでおり、特定のリージョン（us-east-1）に配置されているため、国際的なユーザーにはレイテンシが増加しやすい。",
        "option_analysis": "選択肢Cは効果的である。AWS Global AcceleratorはAWSのグローバルネットワークを使用してユーザートラフィックを最適なエンドポイントに誘導し、レイテンシを最小限に抑える。選択肢A（CloudFront）は主に静的コンテンツやキャッシュ可能な応答に利益をもたらすため、あまり効果的ではないかもしれない。選択肢B（API Gateway）は異なるサービス動作のために追加のレイテンシを引き起こす可能性がある。選択肢Dは追加のリージョンへのデプロイを増やすので複雑さが増すが、レイテンシには対処できる。",
        "additional_knowledge": "トラフィックが多いシナリオでは、キャッシングやエッジ最適化サービスを利用することでさらなるパフォーマンス向上が期待できる。",
        "key_terminology": "AWS Global Accelerator, アプリケーションロードバランサー, レイテンシ, 最適化, ネットワークルーティング",
        "overall_assessment": "AWS Global Acceleratorを使用する推薦は適切であり、エンドユーザーのためのアプリケーションパフォーマンスを向上させることを目的としている。コミュニティの投票は推奨されたソリューションと一致しており、その効果に関する合意を示している。"
      }
    ],
    "keywords": [
      "AWS Global Accelerator",
      "Application Load Balancer",
      "latency",
      "optimization",
      "network routing"
    ]
  },
  {
    "No": "209",
    "question": "A company runs an IoT application in the AWS Cloud. The company has millions of sensors that collect data from houses in the United States. The\nsensors use the MQTT protocol to connect and send data to a custom MQTT broker. The MQTT broker stores the data on a single Amazon EC2\ninstance. The sensors connect to the broker through the domain named iot.example.com. The company uses Amazon Route 53 as its DNS\nservice. The company stores the data in Amazon DynamoDB.\nOn several occasions, the amount of data has overloaded the MQTT broker and has resulted in lost sensor data. The company must improve the\nreliability of the solution.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業はAWSクラウド上でIoTアプリケーションを運営しています。この企業は、アメリカの家庭からデータを収集する数百万のセンサーを持っています。センサーはMQTTプロトコルを使用して接続し、カスタムMQTTブローカーにデータを送信します。MQTTブローカーは、単一のAmazon EC2インスタンスにデータを保存します。センサーは、iot.example.comというドメインを通じてブローカーに接続します。企業はAmazon Route 53をDNSサービスとして利用しています。企業はデータをAmazon DynamoDBに保存します。データ量がMQTTブローカーを過負荷にし、センサーのデータが失われることが何度もありました。企業はソリューションの信頼性を向上させる必要があります。どのソリューションがこの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. Use the Auto Scaling group as the target for the ALB. Update the DNS record in Route 53 to an alias record. Point the alias record to the ALB. Use the MQTT broker to store the data.",
        "text_jp": "Application Load Balancer (ALB)とMQTTブローカー用のAuto Scalingグループを作成します。Auto ScalingグループをALBのターゲットとして使用します。Route 53のDNSレコードをエイリアスレコードに更新し、ALBを指すようにします。MQTTブローカーを使用してデータを保存します。"
      },
      {
        "key": "B",
        "text": "Set up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record in Route 53 to point to the AWS IoT Core Data-ATS endpoint. Configure an AWS IoT rule to store the data.",
        "text_jp": "AWS IoT Coreをセットアップしてセンサーデータを受信します。AWS IoT Coreに接続するためのカスタムドメインを作成および設定します。Route 53のDNSレコードをAWS IoT Core Data-ATSエンドポイントに指すように更新します。AWS IoTルールを設定してデータを保存します。"
      },
      {
        "key": "C",
        "text": "Create a Network Load Balancer (NLB). Set the MQTT broker as the target. Create an AWS Global Accelerator accelerator. Set the NLB as the endpoint for the accelerator. Update the DNS record in Route 53 to a multivalue answer record. Set the Global Accelerator IP addresses as values. Use the MQTT broker to store the data.",
        "text_jp": "Network Load Balancer (NLB)を作成します。MQTTブローカーをターゲットとして設定します。AWS Global Acceleratorアクセラレータを作成します。NLBをアクセラレータのエンドポイントとして設定します。Route 53のDNSレコードをマルチバリュー応答レコードに更新します。Global AcceleratorのIPアドレスを値として設定します。MQTTブローカーを使用してデータを保存します。"
      },
      {
        "key": "D",
        "text": "Set up AWS IoT Greengrass to receive the sensor data. Update the DNS record in Route 53 to point to the AWS IoT Greengrass endpoint. Configure an AWS IoT rule to invoke an AWS Lambda function to store the data.",
        "text_jp": "AWS IoT Greengrassをセットアップしてセンサーデータを受信します。Route 53のDNSレコードをAWS IoT Greengrassエンドポイントを指すように更新します。AWS IoTルールを設定してAWS Lambda関数を呼び出しデータを保存します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating a Network Load Balancer (NLB) provides better handling of the incoming MQTT connections, reducing the load on the MQTT broker.",
        "situation_analysis": "The company has a large number of sensors sending data simultaneously to a single MQTT broker, leading to overload and data loss.",
        "option_analysis": "Option A suggests using an ALB, which is suitable for HTTP traffic but may not provide the optimal performance for MQTT. Option B utilizes AWS IoT Core, which is a viable approach but does not match the answer key. Option D involves AWS IoT Greengrass, which is used for edge computing but does not directly address the issue of overload at the broker level.",
        "additional_knowledge": "Exploring other AWS IoT management solutions could also benefit from incorporating multiple regions or other redundancy measures.",
        "key_terminology": "Network Load Balancer, AWS Global Accelerator, MQTT, AWS IoT Core, AWS IoT Greengrass.",
        "overall_assessment": "The question presents a scenario where scalability and reliability are crucial. While option C aligns with improving the architecture, community votes primarily favor option B, indicating a potential educational aspect or misunderstanding of the questions by the testers."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。Network Load Balancer (NLB)を作成すると、着信MQTT接続の処理が向上し、MQTTブローカーへの負荷が軽減されます。",
        "situation_analysis": "この企業は、同時にデータを送信する多くのセンサーを持ち、単一のMQTTブローカーに負荷がかかり、データ損失が発生しています。",
        "option_analysis": "オプションAはALBの使用を提案していますが、これはHTTPトラフィックに適していて、MQTTの最適なパフォーマンスを提供しない可能性があります。オプションBはAWS IoT Coreを利用しており、これは有効なアプローチですが、回答キーには一致しません。オプションDではAWS IoT Greengrassが関与しますが、これはエッジコンピューティングに使用され、ブローカーレベルでの過負荷の問題に直接対処するものではありません。",
        "additional_knowledge": "他のAWS IoT管理ソリューションを探ることで、複数のリージョンや他の冗長性対策を組み合わせることも有益かもしれません。",
        "key_terminology": "Network Load Balancer, AWS Global Accelerator, MQTT, AWS IoT Core, AWS IoT Greengrass。",
        "overall_assessment": "この質問はスケーラビリティと信頼性が重要である状況を提示しています。Cの選択肢がアーキテクチャの改善と一致する一方で、コミュニティ票は主にBに集中しており、テスターによる教育的な側面や誤解の可能性を示唆しています。"
      }
    ],
    "keywords": [
      "Network Load Balancer",
      "AWS Global Accelerator",
      "MQTT",
      "AWS IoT Core",
      "AWS IoT Greengrass"
    ]
  },
  {
    "No": "210",
    "question": "A company has Linux-based Amazon EC2 instances. Users must access the instances by using SSH with EC2 SSH key pairs. Each machine\nrequires a unique EC2 key pair.\nThe company wants to implement a key rotation policy that will, upon request, automatically rotate all the EC2 key pairs and keep the keys in a\nsecurely encrypted place. The company will accept less than 1 minute of downtime during key rotation.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がLinuxベースのAmazon EC2インスタンスを保有している。ユーザーはEC2 SSHキー ペアを使用してインスタンスにSSHでアクセスしなければならない。各マシンには固有のEC2キー ペアが必要である。企業は、要求に基づいてすべてのEC2キー ペアを自動的にローテーションし、キーを安全に暗号化された場所に保管するキー ローテーションポリシーを実装したいと考えている。キー ローテーション中のダウンタイムは1分未満とする。どのソリューションがこれらの要件を満たすか。",
    "choices": [
      {
        "key": "A",
        "text": "Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Secrets Manager.",
        "text_jp": "すべてのキーをAWS Secrets Managerに保存する。Secrets Managerのローテーションスケジュールを定義し、新しいキー ペアを生成するAWS Lambda関数を呼び出す。EC2インスタンスの公開キーを置き換える。Secrets Managerにプライベートキーを更新する。"
      },
      {
        "key": "B",
        "text": "Store all the keys in Parameter Store, a capability of AWS Systems Manager, as a string. Define a Systems Manager maintenance window to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Parameter Store.",
        "text_jp": "すべてのキーをAWS Systems Managerの機能であるParameter Storeに文字列として保存する。Systems Managerのメンテナンスウィンドウを定義し、新しいキー ペアを生成するAWS Lambda関数を呼び出す。EC2インスタンスの公開キーを置き換える。Parameter Storeにプライベートキーを更新する。"
      },
      {
        "key": "C",
        "text": "Import the EC2 key pairs into AWS Key Management Service (AWS KMS). Configure automatic key rotation for these key pairs. Create an Amazon EventBridge scheduled rule to invoke an AWS Lambda function to initiate the key rotation in AWS KMS.",
        "text_jp": "EC2キー ペアをAWS Key Management Service (AWS KMS)にインポートする。これらのキー ペアの自動キー ローテーションを設定する。Amazon EventBridgeのスケジュールルールを作成し、AWS KMSでキー ローテーションを開始するAWS Lambda関数を呼び出す。"
      },
      {
        "key": "D",
        "text": "Add all the EC2 instances to Fleet Manager, a capability of AWS Systems Manager. Define a Systems Manager maintenance window to issue a Systems Manager Run Command document to generate new key pairs and to rotate public keys to all the instances in Fleet Manager.",
        "text_jp": "すべてのEC2インスタンスをAWS Systems Managerの機能であるFleet Managerに追加する。Systems Managerメンテナンスウィンドウを定義し、新しいキー ペアを生成し、Fleet Manager内のすべてのインスタンスに公開キーをローテーションするためのSystems Manager Run Commandドキュメントを発行する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (69%) D (31%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using AWS Secrets Manager allows for secure storage and management of SSH keys, with the ability to automate the rotation process through Lambda functions.",
        "situation_analysis": "The company needs a solution for managing EC2 SSH key pairs that supports key rotation and ensures that keys are securely stored and easily accessible.",
        "option_analysis": "Option A offers a complete solution for key management and rotation, while options B, C, and D do not provide the same level of secure storage or automated management of the keys.",
        "additional_knowledge": "",
        "key_terminology": "AWS Secrets Manager, AWS Lambda, EC2, key rotation",
        "overall_assessment": "Overall, Option A aligns with AWS best practices for security and automation, providing the necessary features to meet the company's requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。AWS Secrets Managerを使用することで、SSHキーを安全に保存し管理でき、自動的にローテーションするプロセスをLambda関数を介して自動化できる。",
        "situation_analysis": "企業は、EC2 SSHキー ペアの管理を行うためのソリューションが必要であり、キー ローテーションをサポートし、キーが安全に保存され、簡単にアクセスできることを求めている。",
        "option_analysis": "選択肢Aはキー管理とローテーションに対する完全なソリューションを提供し、選択肢B、C、Dはキーの安全な保存や自動管理のレベルが劣る。",
        "additional_knowledge": "",
        "key_terminology": "AWS Secrets Manager、AWS Lambda、EC2、キー ローテーション",
        "overall_assessment": "全体として、選択肢Aはセキュリティと自動化のためのAWSのベストプラクティスに沿っており、企業の要件を満たすために必要な機能を提供する。"
      }
    ],
    "keywords": [
      "AWS Secrets Manager",
      "AWS Lambda",
      "EC2",
      "key rotation"
    ]
  },
  {
    "No": "211",
    "question": "A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no\nconfiguration management database and has little knowledge about the utilization of the VMware portfolio.\nA solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no configuration management database and has little knowledge about the utilization of the VMware portfolio. A solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration. Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight. Identify servers that have high utilization. Remove the servers that have high utilization from the migration list. Import the data to AWS Migration Hub.",
        "text_jp": "Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight. Identify servers that have high utilization. Remove the servers that have high utilization from the migration list. Import the data to AWS Migration Hub."
      },
      {
        "key": "B",
        "text": "Export the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the data to AWS Application Migration Service. Use AWS Server Migration Service (AWS SMS) to migrate the remaining servers.",
        "text_jp": "Export the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the data to AWS Application Migration Service. Use AWS Server Migration Service (AWS SMS) to migrate the remaining servers."
      },
      {
        "key": "C",
        "text": "Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub.",
        "text_jp": "Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub."
      },
      {
        "key": "D",
        "text": "Deploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze the data. Use Amazon QuickSight for data visualization.",
        "text_jp": "Deploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze the data. Use Amazon QuickSight for data visualization."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Deploying the Migration Evaluator agentless collector to the ESXi hypervisor allows for easy data collection without needing to install agents on each VM, minimizing operational overhead.",
        "situation_analysis": "The company needs to gather an accurate inventory of their VMware environment with minimal effort since they lack a configuration management database.",
        "option_analysis": "Option A requires installing software on each VM, which increases operational tasks. Option B involves manual exporting and checking, which adds complexity. Option D requires installing an agent on each VM and additional data processing, which also increases workload.",
        "additional_knowledge": "Using the Migration Evaluator simplifies the inventory process and integrates seamlessly with AWS tools for a smooth migration.",
        "key_terminology": "Migration Evaluator, AWS Migration Hub, ESXi, agentless collector, operational overhead.",
        "overall_assessment": "Considering operational efficiency and the company's current limitations, option C meets the requirements best by reducing the amount of manual work and complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。Migration EvaluatorのエージェントレスコレクターをESXiハイパーバイザーに導入することで、各VMにエージェントをインストールすることなくデータ収集が可能となり、運用負荷を最小限に抑えます。",
        "situation_analysis": "この会社は、設定管理データベースを欠いているため、最小限の労力でVMware環境の正確なインベントリを収集する必要があります。",
        "option_analysis": "オプションAは各VMにソフトウェアをインストールする必要があるため、運用タスクが増えます。オプションBは手動でエクスポートや確認を行う必要があり、複雑さが増します。オプションDは各VMにエージェントをインストールし、追加のデータ処理が必要になるため、作業負荷が増加します。",
        "additional_knowledge": "Migration Evaluatorを使用することで、インベントリプロセスが簡素化され、AWSツールとの統合がスムーズに行われ、スムーズな移行が可能になります。",
        "key_terminology": "Migration Evaluator、AWS Migration Hub、ESXi、エージェントレスコレクター、運用負荷。",
        "overall_assessment": "運用効率と会社の現状の制限を考慮すると、オプションCが手動作業と複雑さを減らすため、最も要求に適合しています。"
      }
    ],
    "keywords": [
      "Migration Evaluator",
      "AWS Migration Hub",
      "ESXi",
      "agentless collector",
      "operational overhead"
    ]
  },
  {
    "No": "212",
    "question": "A company runs a microservice as an AWS Lambda function. The microservice writes data to an on-premises SQL database that supports a\nlimited number of concurrent connections. When the number of Lambda function invocations is too high, the database crashes and causes\napplication downtime. The company has an AWS Direct Connect connection between the company's VPC and the on-premises data center. The\ncompany wants to protect the database from crashes.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がAWS Lambda関数としてマイクロサービスを運用しています。このマイクロサービスは、同時接続数が限られているオンプレミスのSQLデータベースにデータを書き込みます。Lambda関数の呼び出し回数が多すぎると、データベースがクラッシュし、アプリケーションがダウンしてしまいます。企業は、企業のVPCとオンプレミスデータセンター間にAWS Direct Connect接続を持っています。このデータベースをクラッシュから保護するための要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write to the existing database. Set a reserved concurrency limit on the Lambda function that is less than the number of connections that the database supports.",
        "text_jp": "データをAmazon Simple Queue Service（Amazon SQS）キューに書き込みます。Lambda関数を構成して、このキューからデータを読み取り、既存のデータベースに書き込みます。データベースがサポートする接続数未満の予約済み同時実行制限をLambda関数に設定します。"
      },
      {
        "key": "B",
        "text": "Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the data from the existing database to Aurora Serverless. Reconfigure the Lambda function to write to Aurora.",
        "text_jp": "新しいAmazon Aurora Serverless DBクラスターを作成します。AWS DataSyncを使用して、既存のデータベースからAurora Serverlessにデータを移行します。Lambda関数を再構成してAuroraに書き込みます。"
      },
      {
        "key": "C",
        "text": "Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the Amazon RDS DB instance. Reconfigure the Lambda function to write to the RDS Proxy DB instance.",
        "text_jp": "Amazon RDS Proxy DBインスタンスを作成します。RDS Proxy DBインスタンスをAmazon RDS DBインスタンスにアタッチします。Lambda関数を再構成してRDS Proxy DBインスタンスに書き込みます。"
      },
      {
        "key": "D",
        "text": "Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the Lambda function to write to the existing database when the topic receives new messages. Configure provisioned concurrency for the Lambda function to be equal to the number of connections that the database supports.",
        "text_jp": "データをAmazon Simple Notification Service（Amazon SNS）トピックに書き込みます。トピックが新しいメッセージを受信したときに、Lambda関数を呼び出して既存のデータベースに書き込みます。Lambda関数のプロビジョニング同時実行設定をデータベースがサポートする接続数と等しくします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Using Amazon SNS allows the system to decouple the process of writing data to the database, thereby managing the connection load more effectively.",
        "situation_analysis": "The company needs a solution to prevent the database from crashing due to excessive Lambda function invocations. The key requirement is to limit direct connections to the database while still allowing data to be written reliably.",
        "option_analysis": "Answer A proposes using SQS. However, it still imposes a direct load on the database and may not adequately protect it from crashes. Answer B introduces a new database, which may not be feasible without downtime. Answer C suggests RDS Proxy, which helps manage connections but still risks overloading if not configured properly. Answer D addresses the requirement well by using SNS to handle events asynchronously.",
        "additional_knowledge": "The choice of a message-based architecture aligns with microservices principles, promoting decoupled development and scalability.",
        "key_terminology": "AWS Lambda, Amazon SNS, concurrency limits, database connections, event-driven architecture.",
        "overall_assessment": "Answer D is well-supported by AWS best practices in designing resilient architectures that deal with throughput and connection limits effectively. The community vote suggests that the majority believe it's a better solution than the others."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。Amazon SNSを使用することで、データベースへの書き込みプロセスを分離でき、接続負荷をより効果的に管理できます。",
        "situation_analysis": "企業は、Lambda関数の過剰な呼び出しによるデータベースのクラッシュを防ぐためのソリューションが必要です。重要な要件は、データベースへの直接接続を制限しながら、データを信頼性をもって書き込むことです。",
        "option_analysis": "選択肢AはSQSの使用を提案していますが、データベースへの直接負荷を伴い、十分に保護できない可能性があります。選択肢Bは新しいデータベースの導入を提案しますが、ダウンタイムなしでは実行できない可能性があります。選択肢CはRDS Proxyを提案しますが、正しく構成されていないとオーバーロードのリスクがあります。選択肢DはSNSを使用してイベントを非同期に処理するため、要求を適切に満たしています。",
        "additional_knowledge": "メッセージベースのアーキテクチャの選択は、マイクロサービスの原則と一致しており、分離開発とスケーラビリティを促進します。",
        "key_terminology": "AWS Lambda、Amazon SNS、同時実行制限、データベース接続、イベント駆動アーキテクチャ。",
        "overall_assessment": "選択肢Dは、スループットと接続制限に効果的に対処するためのAWSのベストプラクティスに基づいて非常によく支持されています。コミュニティの投票は、他の選択肢よりもこの解決策が良いと多くの人が信じていることを示しています。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon SNS",
      "concurrency limits",
      "database connections",
      "event-driven architecture"
    ]
  },
  {
    "No": "213",
    "question": "A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS\nworkloads. The company has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be\nhighly available and cannot be down for longer than 10 minutes. The company needs to minimize ongoing maintenance.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業が、単一のAmazon EC2インスタンス上で動作するGrafanaデータビジュアライゼーションソリューションを使用して、同社のAWSワークロードの健康を監視しています。企業はダッシュボードの作成に時間と労力を投資しており、それらを保持したいと考えています。ダッシュボードは高い可用性が必要で、10分以上ダウンすることはできません。また、運用上のメンテナンスを最小限に抑える必要があります。どのソリューションが最も運用オーバーヘッドが少なく、これらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate to Amazon CloudWatch dashboards. Recreate the dashboards to match the existing Grafana dashboards. Use automatic dashboards where possible.",
        "text_jp": "Amazon CloudWatchダッシュボードに移行します。既存のGrafanaダッシュボードに合わせてダッシュボードを再作成します。可能な場合には自動ダッシュボードを使用します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing Grafana instance. Import the dashboards into the new workspace.",
        "text_jp": "Amazon Managed Grafanaワークスペースを作成します。新しいAmazon CloudWatchデータソースを構成します。既存のGrafanaインスタンスからダッシュボードをエクスポートします。ダッシュボードを新しいワークスペースにインポートします。"
      },
      {
        "key": "C",
        "text": "Create an AMI that has Grafana pre-installed. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto Scaling group that uses the new AMI. Set the Auto Scaling group's minimum, desired, and maximum number of instances to one. Create an Application Load Balancer that serves at least two Availability Zones.",
        "text_jp": "Grafanaが事前にインストールされたAMIを作成します。既存のダッシュボードをAmazon Elastic File System (Amazon EFS)に保存します。新しいAMIを使用するAuto Scalingグループを作成します。Auto Scalingグループの最小、所望、最大インスタンス数を1に設定します。少なくとも2つのアベイラビリティゾーンにサービスを提供するApplication Load Balancerを作成します。"
      },
      {
        "key": "D",
        "text": "Configure AWS Backup to back up the EC2 instance that runs Grafana once each hour. Restore the EC2 instance from the most recent snapshot in an alternate Availability Zone when required.",
        "text_jp": "AWS Backupを構成して、Grafanaを実行しているEC2インスタンスを毎時バックアップします。必要に応じて、最近のスナップショットから別のアベイラビリティゾーンにEC2インスタンスを復元します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Creating an Amazon Managed Grafana workspace provides built-in high availability and reduces operational overhead by leveraging managed infrastructure.",
        "situation_analysis": "The company requires high availability for Grafana dashboards with minimal downtime. An efficient solution must preserve existing dashboards and provide reliable service.",
        "option_analysis": "Option B is the best as it leverages AWS's managed service, which handles scaling and availability. Option A requires re-creation of dashboards, and options C and D increase operational complexity.",
        "additional_knowledge": "Numerous enterprises use managed services to enhance resilience and decrease management demands, illustrating industry trends.",
        "key_terminology": "Amazon Managed Grafana, Amazon CloudWatch, High Availability",
        "overall_assessment": "The answer aligns with AWS best practices for maximizing uptime and minimizing maintenance overhead via management services."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。Amazon Managed Grafanaワークスペースを作成することで、管理されたインフラを活用して高可用性を確保し、運用オーバーヘッドを減少させることができる。",
        "situation_analysis": "企業は、Grafanaダッシュボードの高可用性を要求しており、ダウンタイムを最小限に抑える必要がある。既存のダッシュボードを保持し、信頼性のあるサービスを提供する効率的なソリューションが必要である。",
        "option_analysis": "選択肢BはAWSの管理サービスを利用するため、スケーリングと可用性を扱う。また、選択肢Aはダッシュボードの再作成が必要であり、選択肢CとDは運用の複雑さを増加させる。",
        "additional_knowledge": "多くの企業が管理サービスを利用して耐障害性を向上させ、管理の要求を軽減していることを示す業界の傾向.",
        "key_terminology": "Amazon Managed Grafana, Amazon CloudWatch, 高可用性",
        "overall_assessment": "この回答は、AWSのベストプラクティスに沿って、稼働時間を最大化し、管理の手間を最小化することを目的としている。"
      }
    ],
    "keywords": [
      "Amazon Managed Grafana",
      "Amazon CloudWatch",
      "High Availability"
    ]
  },
  {
    "No": "214",
    "question": "A company needs to migrate its customer transactions database from on premises to AWS. The database resides on an Oracle DB instance that\nruns on a Linux server. According to a new security requirement, the company must rotate the database password each year.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業が自社の顧客取引データベースをオンプレミスからAWSに移行する必要があります。このデータベースは、Linuxサーバー上で実行されているOracle DBインスタンスに存在しています。新しいセキュリティ要件に従い、企業はデータベースのパスワードを毎年ローテーションする必要があります。どのソリューションが、最も運用負荷が少ない状態でこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool (AWS SCT). Store the password in AWS Systems Manager Parameter Store. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly passtard rotation.",
        "text_jp": "AWS Schema Conversion Tool (AWS SCT)を使用してデータベースをAmazon DynamoDBに変換します。パスワードをAWS Systems Manager Parameter Storeに保存します。Amazon CloudWatchアラームを作成して、毎年のパスワードローテーションのためにAWS Lambda関数を呼び出します。"
      },
      {
        "key": "B",
        "text": "Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a yearly rotation schedule.",
        "text_jp": "データベースをAmazon RDS for Oracleに移行します。パスワードをAWS Secrets Managerに保存します。自動ローテーションをオンにします。年間ローテーションスケジュールを設定します。"
      },
      {
        "key": "C",
        "text": "Migrate the database to an Amazon EC2 instance. Use AWS Systems Manager Parameter Store to keep and rotate the connection string by using an AWS Lambda function on a yearly schedule.",
        "text_jp": "データベースをAmazon EC2インスタンスに移行します。AWS Systems Manager Parameter Storeを使用して接続文字列を保持し、AWS Lambda関数を使用して毎年のスケジュールでローテーションします。"
      },
      {
        "key": "D",
        "text": "Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool (AWS SCT). Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly password rotation.",
        "text_jp": "AWS Schema Conversion Tool (AWS SCT)を使用してデータベースをAmazon Neptuneに移行します。Amazon CloudWatchアラームを作成して、毎年のパスワードローテーションのためにAWS Lambda関数を呼び出します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, as migrating the database to Amazon EC2 allows for custom management of the database, while using AWS Systems Manager Parameter Store provides a secure way to store and rotate the connection string.",
        "situation_analysis": "The company has specific requirements for migrating its database and for password management due to compliance and security standards.",
        "option_analysis": "Option C is the most efficient as it provides flexibility in managing the database instance and password rotation. Options A and D involve additional complexity of converting to a different database service, which may not be necessary. Option B, while also suitable, relies on RDS which may come with its own operational overhead and cost considerations.",
        "additional_knowledge": "The option of using AWS Lambda for scheduling allows for automatic processes to handle password rotations without manual intervention.",
        "key_terminology": "AWS Systems Manager, EC2, Parameter Store, RDS, Secrets Manager",
        "overall_assessment": "Although community support is behind option B, option C provides the necessary flexibility and control for the specific requirements presented in the question. The community's voting patterns may reflect a preference for managed services over EC2."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。データベースをAmazon EC2に移行することで、データベースのカスタム管理が可能になり、AWS Systems Manager Parameter Storeを使用することで接続文字列を安全に保存・ローテーションできます。",
        "situation_analysis": "企業はデータベースの移行と、コンプライアンスおよびセキュリティ基準に基づくパスワード管理について特定の要件があります。",
        "option_analysis": "選択肢Cが最も効率的で、データベースインスタンスとパスワードローテーションの管理に柔軟性を提供します。選択肢AとDは、異なるデータベースサービスへの変換を伴い、必要ない場合が多いため、追加の複雑さがあります。選択肢Bは適切ではあるものの、RDSに依存するため、独自の運用負荷やコストの考慮が必要です。",
        "additional_knowledge": "AWS Lambdaを使用してスケジュールを設定することで、手動介入なしでパスワードローテーションを扱う自動プロセスが可能になります。",
        "key_terminology": "AWS Systems Manager, EC2, Parameter Store, RDS, Secrets Manager",
        "overall_assessment": "選択肢Bに対するコミュニティの支持が高い一方で、選択肢Cは質問に提示された特定の要件に対する必要な柔軟性と制御を提供します。コミュニティの投票パターンは、管理されたサービスがEC2よりも好まれる傾向を反映しているかもしれません。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "EC2",
      "Parameter Store",
      "RDS",
      "Secrets Manager"
    ]
  },
  {
    "No": "215",
    "question": "A solutions architect is designing an AWS account structure for a company that consists of multiple teams. All the teams will work in the same\nAWS Region. The company needs a VPC that is connected to the on-premises network. The company expects less than 50 Mbps of total trafic to\nand from the on-premises network.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "question_jp": "複数のチームで構成される会社のために、ソリューションアーキテクトがAWSアカウント構造を設計しています。すべてのチームは同じAWSリージョンで作業を行います。会社はオンプレミスネットワークに接続されたVPCを必要としています。会社はオンプレミスネットワークとの間の合計トラフィックが50Mbps未満と予想しています。この要件を最もコスト効果の高い方法で満たすためのステップの組み合わせはどれですか？（2つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account.",
        "text_jp": "AWS CloudFormationテンプレートを作成して、VPCおよび必要なサブネットをプロビジョニングします。このテンプレートを各AWSアカウントにデプロイします。"
      },
      {
        "key": "B",
        "text": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager.",
        "text_jp": "AWS CloudFormationテンプレートを作成して、VPCおよび必要なサブネットをプロビジョニングします。このテンプレートを共有サービスアカウントにデプロイします。AWSリソースアクセスマネージャーを使用してサブネットを共有します。"
      },
      {
        "key": "C",
        "text": "Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by using AWS Resource Access Manager.",
        "text_jp": "AWS Transit Gatewayを使用して、オンプレミスネットワークへの接続にAWS Site-to-Site VPNを利用します。AWSリソースアクセスマネージャーを使用してトランジットゲートウェイを共有します。"
      },
      {
        "key": "D",
        "text": "Use AWS Site-to-Site VPN for connectivity to the on-premises network.",
        "text_jp": "オンプレミスネットワークへの接続にAWS Site-to-Site VPNを使用します。"
      },
      {
        "key": "E",
        "text": "Use AWS Direct Connect for connectivity to the on-premises network.",
        "text_jp": "オンプレミスネットワークへの接続にAWS Direct Connectを使用します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BD (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A and D. Option A provides a standardized way to provision VPCs, while option D offers a low-cost connectivity solution.",
        "situation_analysis": "The organization operates in a single AWS Region and requires connecting a VPC to its on-premises network while maintaining low total traffic.",
        "option_analysis": "Option A is effective for provisioning VPCs across multiple accounts. Option B introduces unnecessary complexity. Option C, while viable, adds cost with Transit Gateway. Option D is a straightforward and cost-effective VPN solution. Option E is more expensive and not suitable given the low bandwidth requirements.",
        "additional_knowledge": "Networking best practices encourage understanding cost structures and traffic patterns when designing solutions.",
        "key_terminology": "VPC, AWS CloudFormation, Site-to-Site VPN, AWS Resource Access Manager, Transit Gateway",
        "overall_assessment": "The chosen steps sufficiently meet the requirements with a focus on cost-effectiveness and simplicity, despite community votes favoring options B and D which could imply a misunderstanding of the cost implications."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAとDである。選択肢AはVPCを標準化してプロビジョニングする方法を提供し、選択肢Dは低コストの接続ソリューションを提供する。",
        "situation_analysis": "組織は単一のAWSリージョンで運営し、VPCをオンプレミスネットワークに接続する必要があり、総トラフィックを低く保つ必要がある。",
        "option_analysis": "選択肢Aは複数のアカウントにわたってVPCをプロビジョニングするのに効果的である。選択肢Bは不要な複雑さを導入する。選択肢Cは有効であるが、トランジットゲートウェイによってコストが増加する。選択肢Dはストレートフォワードでコスト効果の高いVPNソリューションである。選択肢Eは高価であり、低帯域幅要件を考慮すると適切でない。",
        "additional_knowledge": "ネットワーキングのベストプラクティスは、ソリューション設計の際にコスト構造とトラフィックパターンを理解することを奨励する。",
        "key_terminology": "VPC, AWS CloudFormation, Site-to-Site VPN, AWSリソースアクセスマネージャー, トランジットゲートウェイ",
        "overall_assessment": "選択されたステップはコスト効果とシンプルさに焦点を当てて要件を十分に満たしており、コミュニティ投票が選択肢BとDを好む結果となったが、コストの影響を誤解している可能性がある。"
      }
    ],
    "keywords": [
      "VPC",
      "AWS CloudFormation",
      "Site-to-Site VPN",
      "AWS Resource Access Manager",
      "Transit Gateway"
    ]
  },
  {
    "No": "216",
    "question": "A solutions architect at a large company needs to set up network security for outbound trafic to the internet from all AWS accounts within an\norganization in AWS Organizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a\ncentralized AWS Transit Gateway. Each account has both an internet gateway and a NAT gateway for outbound trafic to the internet. The company\ndeploys resources only into a single AWS Region.\nThe company needs the ability to add centrally managed rule-based filtering on all outbound trafic to the internet for all AWS accounts in the\norganization. The peak load of outbound trafic will not exceed 25 Gbps in each Availability Zone.\nWhich solution meets these requirements?",
    "question_jp": "大規模企業のソリューションアーキテクトは、AWS Organizations内のすべてのAWSアカウントからのインターネットへのアウトバウンドトラフィックのネットワークセキュリティを設定する必要があります。組織は100以上のAWSアカウントを持ち、アカウントは中央集権的なAWS Transit Gatewayを使用して相互にルーティングします。各アカウントにはインターネットゲートウェイとNATゲートウェイがあり、インターネットへのアウトバウンドトラフィックを処理しています。企業は単一のAWSリージョンにのみリソースを展開します。企業は、組織内のすべてのAWSアカウントに対して、すべてのアウトバウンドトラフィックに対して中央管理されたルールベースのフィルタリングを追加する機能を必要としています。アウトバウンドトラフィックのピークロードは、各アベイラビリティゾーンで25Gbpsを超えません。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new VPC for outbound trafic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Create an Auto Scaling group of Amazon EC2 instances that run an open-source internet proxy for rule-based filtering across all Availability Zones in the Region. Modify all default routes to point to the proxy's Auto Scaling group.",
        "text_jp": "インターネットへのアウトバウンドトラフィック用の新しいVPCを作成します。既存のトランジットゲートウェイを新しいVPCに接続します。新しいNATゲートウェイを構成します。オープンソースのインターネットプロキシを実行するAmazon EC2インスタンスのAuto Scalingグループを作成し、リージョン内のすべてのアベイラビリティゾーンにわたってルールベースのフィルタリングを行います。すべてのデフォルトルートをプロキシのAuto Scalingグループを指すように変更します。"
      },
      {
        "key": "B",
        "text": "Create a new VPC for outbound trafic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Use an AWS Network Firewall firewall for rule-based filtering. Create Network Firewall endpoints in each Availability Zone. Modify all default routes to point to the Network Firewall endpoints.",
        "text_jp": "インターネットへのアウトバウンドトラフィック用の新しいVPCを作成します。既存のトランジットゲートウェイを新しいVPCに接続します。新しいNATゲートウェイを構成します。AWS Network Firewallを使用してルールベースのフィルタリングを行います。各アベイラビリティゾーンにNetwork Firewallエンドポイントを作成します。すべてのデフォルトルートをNetwork Firewallエンドポイントを指すように変更します。"
      },
      {
        "key": "C",
        "text": "Create an AWS Network Firewall firewall for rule-based filtering in each AWS account. Modify all default routes to point to the Network Firewall firewalls in each account.",
        "text_jp": "各AWSアカウントに対してルールベースのフィルタリング用のAWS Network Firewallを作成します。すべてのデフォルトルートを各アカウント内のNetwork Firewallを指すように変更します。"
      },
      {
        "key": "D",
        "text": "In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for rule-based filtering. Modify all default routes to point to the proxy's Auto Scaling group.",
        "text_jp": "各AWSアカウントにおいて、ルールベースのフィルタリングを実行するためにネットワーク最適化されたAmazon EC2インスタンスのAuto Scalingグループを作成します。すべてのデフォルトルートをプロキシのAuto Scalingグループを指すように変更します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This option ensures centralized management of the proxy instances across all accounts, allowing consistent rule-based filtering.",
        "situation_analysis": "The organization has more than 100 AWS accounts with specific outbound traffic requirements and utilizes a centralized AWS Transit Gateway.",
        "option_analysis": "Option D creates a centralized Auto Scaling group to handle filtering, which is scalable and manageable for high outbound traffic. Other options (A, B, and C) either create unnecessary complexity or decentralize the filtering approach.",
        "additional_knowledge": "Utilizing a centralized proxy solution is generally preferred for uniform policy enforcement and can help reduce operational overhead.",
        "key_terminology": "AWS EC2, Auto Scaling, Rule-based filtering, Network Security, AWS Transit Gateway",
        "overall_assessment": "The answer D is well-supported by AWS best practices for managing outbound traffic filtering in a multi-account setup. The voting discrepancy suggests some confusion about the routing and filtering approaches."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。この選択肢は、すべてのアカウントにわたってプロキシインスタンスの中央管理を確保し、一貫したルールベースのフィルタリングを可能にする。",
        "situation_analysis": "この組織は100以上のAWSアカウントを持ち、特定のアウトバウンドトラフィック要件があり、中央集権的なAWS Transit Gatewayを利用している。",
        "option_analysis": "選択肢Dはフィルタリングを処理するための中央集権的なAuto Scalingグループを作成し、大規模なアウトバウンドトラフィックに対してスケーラブルで管理可能である。他の選択肢(A, B, C)は不要な複雑さを生じるか、フィルタリングアプローチを分散させる。",
        "additional_knowledge": "中央集権的なプロキシソリューションを利用することは、ポリシーの一貫した強制を助け、運用上のオーバーヘッドを低減するために一般に好まれる。",
        "key_terminology": "AWS EC2, Auto Scaling, ルールベースのフィルタリング, ネットワークセキュリティ, AWS Transit Gateway",
        "overall_assessment": "答えDは、マルチアカウント構成におけるアウトバウンドトラフィックフィルタリングの管理に関するAWSベストプラクティスによく支持されている。投票の不一致はルーティングとフィルタリングアプローチについての混乱を示唆している。"
      }
    ],
    "keywords": [
      "AWS EC2",
      "Auto Scaling",
      "Rule-based filtering",
      "Network Security",
      "AWS Transit Gateway"
    ]
  },
  {
    "No": "217",
    "question": "A company uses a load balancer to distribute trafic to Amazon EC2 instances in a single Availability Zone. The company is concerned about\nsecurity and wants a solutions architect to re-architect the solution to meet the following requirements:\n• Inbound requests must be filtered for common vulnerability attacks.\n• Rejected requests must be sent to a third-party auditing application.\n• All resources should be highly available.\nWhich solution meets these requirements?",
    "question_jp": "ある企業は、ロードバランサーを使用して、単一のアベイラビリティーゾーン内のAmazon EC2インスタンスにトラフィックを分散しています。企業はセキュリティを心配しており、ソリューションアーキテクトに以下の要件を満たすようにソリューションを再設計することを望んでいます。\n• インバウンドリクエストは一般的な脆弱性攻撃に対してフィルタリングされなければならない。\n• 拒否されたリクエストはサードパーティの監査アプリケーションに送信される必要がある。\n• すべてのリソースは高可用性である必要がある。\nこの要件を満たすソリューションはどれか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Use Amazon Inspector to monitor trafic to the ALB and EC2 instances. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB. Use an AWS Lambda function to frequently push the Amazon Inspector report to the third- party auditing application.",
        "text_jp": "マルチAZオートスケーリンググループをアプリケーションのAMIを使用して設定します。アプリケーションロ��ドバランサー（ALB）を作成し、ターゲットとして以前に作成されたオートスケーリンググループを選択します。Amazon Inspectorを使用してALBおよびEC2インスタンスへのトラフィックを監視します。WAFにWeb ACLを作成します。AWS WAFを作成し、Web ACLおよびALBを使用します。AWS Lambda関数を使用してAmazon Inspectorレポートをサードパーティの監査アプリケーションに定期的にプッシュします。"
      },
      {
        "key": "B",
        "text": "Configure an Application Load Balancer (ALB) and add the EC2 instances as targets. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB name and enable logging with Amazon CloudWatch Logs. Use an AWS Lambda function to frequently push the logs to the third-party auditing application.",
        "text_jp": "アプリケーションロードバランサー（ALB）を設定し、EC2インスタンスをターゲットとして追加します。WAFにWeb ACLを作成します。AWS WAFを作成し、Web ACLおよびALB名を使用し、Amazon CloudWatch Logsでロギングを有効にします。AWS Lambda関数を使用してログをサードパーティの監査アプリケーションに定期的にプッシュします。"
      },
      {
        "key": "C",
        "text": "Configure an Application Load Balancer (ALB) along with a target group adding the EC2 instances as targets. Create an Amazon Kinesis Data Firehose with the destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber.",
        "text_jp": "アプリケーションロードバランサー（ALB）を設定するとともに、ターゲットグループにEC2インスタンスを追加します。サードパーティの監査アプリケーションの宛先としてAmazon Kinesis Data Firehoseを作成します。WAFにWeb ACLを作成します。AWS WAFを作成し、Web ACLおよびALBを使用し、Kinesis Data Firehoseを宛先として選択してロギングを有効にします。AWS MarketplaceでAWS Managed Rulesにサブスクライブし、WAFをサブスクライバーとして選択します。"
      },
      {
        "key": "D",
        "text": "Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Create an Amazon Kinesis Data Firehose with a destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the WebACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber.",
        "text_jp": "マルチAZオートスケーリンググループをアプリケーションのAMIを使用して設定します。アプリケーションロードバランサー（ALB）を作成し、ターゲットとして以前に作成されたオートスケーリンググループを選択します。サードパーティの監査アプリケーションの宛先としてAmazon Kinesis Data Firehoseを作成します。WAFにWeb ACLを作成します。AWS WAFを作成し、WebACLおよびALBを使用し、Kinesis Data Firehoseを宛先として選択してロギングを有効にします。AWS MarketplaceでAWS Managed Rulesにサブスクライブし、WAFをサブスクライバーとして選択します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (80%) A (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. It effectively filters incoming requests and logs them for auditing purposes.",
        "situation_analysis": "The question outlines a requirement for security against common vulnerabilities, rejection handling, and high availability.",
        "option_analysis": "Option A, C, and D include unnecessary complexity without addressing the core requirements of third-party logging as efficiently as Option B.",
        "additional_knowledge": "Regularly using AWS Managed Rules can help enhance security posture.",
        "key_terminology": "Application Load Balancer, AWS WAF, CloudWatch Logs, AWS Lambda",
        "overall_assessment": "Overall, option B meets the criteria effectively, while other options do not prioritize simplicity and clarity in the context of the question."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。この選択肢は、インバウンドリクエストを効果的にフィルタリングし、監査目的のためにログを記録する。",
        "situation_analysis": "質問は、一般的な脆弱性に対するセキュリティ要件、拒否処理、高可用性の要件を示している。",
        "option_analysis": "選択肢A、C、Dは、選択肢Bのようにサードパーティーへのログ処理を効率的に行うことなく、不必要な複雑さを含んでいる。",
        "additional_knowledge": "AWS Managed Rulesを定期的に使用することでセキュリティの姿勢を強化できる。",
        "key_terminology": "アプリケーションロードバランサー、AWS WAF、CloudWatch Logs、AWS Lambda",
        "overall_assessment": "全体的に見ると、Bが要件を効果的に満たしており、他の選択肢は問題の文脈でのシンプルさと明確さを優先していない。"
      }
    ],
    "keywords": [
      "Application Load Balancer",
      "AWS WAF",
      "CloudWatch Logs",
      "AWS Lambda"
    ]
  },
  {
    "No": "218",
    "question": "A company is running an application in the AWS Cloud. The application consists of microservices that run on a fieet of Amazon EC2 instances in\nmultiple Availability Zones behind an Application Load Balancer. The company recently added a new REST API that was implemented in Amazon\nAPI Gateway. Some of the older microservices that run on EC2 instances need to call this new API.\nThe company does not want the API to be accessible from the public internet and does not want proprietary data to traverse the public internet.\nWhat should a solutions architect do to meet these requirements?",
    "question_jp": "ある企業がAWSクラウド上でアプリケーションを運営しています。このアプリケーションは、アプリケーションロードバランサーの背後にある複数のアベイラビリティゾーンのAmazon EC2インスタンスの群で実行されるマイクロサービスで構成されています。最近、企業はAmazon API Gatewayで実装された新しいREST APIを追加しました。一部の古いEC2インスタンスで実行されるマイクロサービスは、この新しいAPIを呼び出す必要があります。企業はAPIを公共インターネットからアクセス可能にしたくなく、また機密データが公共インターネットを横断することを望んでいません。この要件を満たすために、ソリューションアーキテクトは何をすべきか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Site-to-Site VPN connection between the VPC and the API Gateway. Use API Gateway to generate a unique API Key for each microservice. Configure the API methods to require the key.",
        "text_jp": "VPCとAPI Gatewayの間にAWS Site-to-Site VPN接続を作成します。API Gatewayを使用して各マイクロサービスのためのユニークなAPIキーを生成します。APIメソッドを構成してキーを要求します。"
      },
      {
        "key": "B",
        "text": "Create an interface VPC endpoint for API Gateway, and set an endpoint policy to only allow access to the specific API. Add a resource policy to API Gateway to only allow access from the VPC endpoint. Change the API Gateway endpoint type to private.",
        "text_jp": "API Gateway用のインターフェイスVPCエンドポイントを作成し、特定のAPIへのアクセスのみを許可するエンドポイントポリシーを設定します。API GatewayへのアクセスをVPCエンドポイントからのみ許可するリソースポリシーを追加します。API Gatewayエンドポイントのタイプをプライベートに変更します。"
      },
      {
        "key": "C",
        "text": "Modify the API Gateway to use IAM authentication. Update the IAM policy for the IAM role that is assigned to the EC2 instances to allow access to the API Gateway. Move the API Gateway into a new VPDeploy a transit gateway and connect the VPCs.",
        "text_jp": "API GatewayをIAM認証を使用するように修正します。EC2インスタンスに割り当てられたIAMロールのIAMポリシーを更新してAPI Gatewayへのアクセスを許可します。API Gatewayを新しいVPCに移動しますトランジットゲートウェイを展開してVPCを接続します。"
      },
      {
        "key": "D",
        "text": "Create an accelerator in AWS Global Accelerator, and connect the accelerator to the API Gateway. Update the route table for all VPC subnets with a route to the created Global Accelerator endpoint IP address. Add an API key for each service to use for authentication.",
        "text_jp": "AWS Global Acceleratorでアクセラレーターを作成し、そのアクセラレーターをAPI Gatewayに接続します。すべてのVPCサブネットのルートテーブルを作成したGlobal AcceleratorエンドポイントIPアドレスへのルートで更新します。各サービスが認証に使用するためのAPIキーを追加します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which suggests modifying API Gateway to use IAM authentication.",
        "situation_analysis": "The application runs on EC2 and needs to securely access the new API without exposing it to the internet.",
        "option_analysis": "Option A introduces a Site-to-Site VPN, which while secure, adds complexity and is not necessary here. Option B provides a private API endpoint, which is valid but does not utilize IAM for access control. Option D utilizes a Global Accelerator, which is also unnecessary for this scenario.",
        "additional_knowledge": "The implementation of IAM roles and policies for EC2 instances is a common practice to secure access to AWS services.",
        "key_terminology": "API Gateway, IAM, VPC, EC2, Authentication",
        "overall_assessment": "While community voting shows a strong preference for option B, option C directly addresses the security needs by utilizing AWS IAM, making it the best solution in this context."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCであり、API GatewayをIAM認証を使用するように修正することを提案しています。",
        "situation_analysis": "アプリケーションはEC2上で実行されており、新しいAPIに安全にアクセスする必要がありますが、インターネットに公開したくありません。",
        "option_analysis": "選択肢AはSite-to-Site VPNを導入しますが、安全ではあるものの、複雑さが追加され、このシナリオには必要ありません。選択肢BはプライベートAPIエンドポイントを提供しますが、アクセス制御にIAMを利用していません。選択肢DはGlobal Acceleratorを利用しており、このシナリオでは不必要です。",
        "additional_knowledge": "EC2インスタンスのためにIAMロールとポリシーを実装することは、AWSサービスへのアクセスを保護する一般的な手法です。",
        "key_terminology": "API Gateway, IAM, VPC, EC2, 認証",
        "overall_assessment": "コミュニティ投票では選択肢Bへの強い支持が見られますが、選択肢CはAWS IAMを利用することでセキュリティニーズに直接対応しており、この文脈において最善の解決策です。"
      }
    ],
    "keywords": [
      "API Gateway",
      "IAM",
      "VPC",
      "EC2",
      "Authentication"
    ]
  },
  {
    "No": "219",
    "question": "A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses\nAmazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account.\nOccasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment.\nA solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make\nnoncompliant changes to the security settings for the EC2 instances.\nWhat is the FASTEST way for the solutions architect to meet these requirements?",
    "question_jp": "ある企業は、AWS上に全体のインフラストラクチャを構築しています。この企業は、Amazon EC2インスタンスを使用してeコマースサイトをホストし、静的データを保存するためにAmazon S3を使用しています。企業には、1つのAWSアカウントを通じてクラウド管理および開発を行う3人のエンジニアがいます。時折、1人のエンジニアが別のエンジニアのEC2セキュリティグループ設定を変更し、環境において不遵守の問題を引き起こします。ソリューションアーキテクトは、エンジニアが行った変更を追跡するシステムを設定する必要があります。このシステムは、エンジニアがEC2インスタンスのセキュリティ設定に対して非準拠の変更を行った場合にアラートを送信しなければなりません。ソリューションアーキテクトがこれらの要件を満たすための最も迅速な方法は何でしょうか?",
    "choices": [
      {
        "key": "A",
        "text": "Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the AWS account.",
        "text_jp": "企業のためにAWS Organizationsを設定します。SCP（サービスコントロールポリシー）を適用して、AWSアカウントで行われる非準拠のセキュリティグループ変更を管理および追跡します。"
      },
      {
        "key": "B",
        "text": "Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected.",
        "text_jp": "AWS CloudTrailを有効にして、EC2セキュリティグループの変更をキャプチャします。非準拠のセキュリティ設定が検出されたときにアラートを提供するために、Amazon CloudWatchルールを有効にします。"
      },
      {
        "key": "C",
        "text": "Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment.",
        "text_jp": "AWSアカウントにSCPを有効にして、環境での非準拠のセキュリティグループ変更が行われたときにアラートを提供します。"
      },
      {
        "key": "D",
        "text": "Enable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic.",
        "text_jp": "EC2セキュリティグループにAWS Configを有効にして、非準拠の変更を追跡します。変更をAmazon Simple Notification Service（Amazon SNS）トピックを通じてアラートとして送信します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct Answer: B. Enabling AWS CloudTrail allows tracking of changes to EC2 security groups, and using CloudWatch rules provides alerts for noncompliant changes.",
        "situation_analysis": "The company needs to track changes made by engineers to EC2 security groups and alert on noncompliant modifications. Fast response to such changes is crucial.",
        "option_analysis": "Option B directly captures changes to security groups and sets up an alert mechanism, making it the quickest solution. Option A introduces SCPs, which are less immediate. Option C offers alerts but lacks detailed tracking of changes. Option D is effective but may take longer to implement due to AWS Config setup.",
        "additional_knowledge": "AWS Config can be used for ongoing compliance checks, but the immediate need is for change tracking and alerts.",
        "key_terminology": "AWS CloudTrail, Amazon CloudWatch, EC2 Security Groups, Alerts, Noncompliance",
        "overall_assessment": "The choice of AWS CloudTrail and CloudWatch provides a comprehensive solution for monitoring and alerting rapidly, making it optimal for the given scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答: B。AWS CloudTrailを有効にすることで、EC2セキュリティグループの変更を追跡でき、CloudWatchルールを使用することで非準拠の変更に対してアラートを送信できます。",
        "situation_analysis": "企業は、エンジニアによって行われたEC2セキュリティグループの変更を追跡し、非準拠の修正についてアラートを発する必要があります。このような変更に迅速に対応することが重要です。",
        "option_analysis": "選択肢Bは、セキュリティグループへの変更を直接キャプチャし、アラートメカニズムをセットアップするため、最も迅速な解決策となります。選択肢AはSCPを導入しますが、即時性が劣ります。選択肢Cはアラートを提供しますが、変更の詳細な追跡が欠けています。選択肢Dは効果的ですが、AWS Configの設定に時間がかかる可能性があります。",
        "additional_knowledge": "AWS Configは継続的なコンプライアンスチェックに使用できますが、即時のニーズは変更の追跡とアラートです。",
        "key_terminology": "AWS CloudTrail、Amazon CloudWatch、EC2セキュリティグループ、アラート、非準拠",
        "overall_assessment": "AWS CloudTrailとCloudWatchの選択は、監視と迅速なアラート提供のための包括的なソリューションを提供し、この状況には最適です。"
      }
    ],
    "keywords": [
      "AWS CloudTrail",
      "Amazon CloudWatch",
      "EC2 Security Groups",
      "Alerts",
      "Noncompliance"
    ]
  },
  {
    "No": "220",
    "question": "A company has IoT sensors that monitor trafic patterns throughout a large city. The company wants to read and collect data from the sensors and\nperform aggregations on the data.\nA solutions architect designs a solution in which the IoT devices are streaming to Amazon Kinesis Data Streams. Several applications are reading\nfrom the stream. However, several consumers are experiencing throttling and are periodically encountering a\nReadProvisionedThroughputExceeded error.\nWhich actions should the solutions architect take to resolve this issue? (Choose three.)",
    "question_jp": "ある企業が大都市全体の交通パターンを監視する IoT センサーを導入しています。企業は、これらのセンサーからデータを読み取り、収集し、データの集約を行いたいと考えています。ソリューションアーキテクトは、IoT デバイスが Amazon Kinesis Data Streams にストリーミングするソリューションを設計しました。複数のアプリケーションがストリームから読み取っていますが、いくつかの消費者がスロットリングを経験しており、定期的に ReadProvisionedThroughputExceeded エラーに遭遇しています。ソリューションアーキテクトは、この問題を解決するためにどのアクションを講じるべきでしょうか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Reshard the stream to increase the number of shards in the stream.",
        "text_jp": "ストリームのシャード数を増やすためにストリームを再シャーディングする。"
      },
      {
        "key": "B",
        "text": "Use the Kinesis Producer Library (KPL). Adjust the polling frequency.",
        "text_jp": "Kinesis Producer Library (KPL) を使用する。ポーリング頻度を調整する。"
      },
      {
        "key": "C",
        "text": "Use consumers with the enhanced fan-out feature.",
        "text_jp": "強化ファンアウト機能を持つ消費者を使用する。"
      },
      {
        "key": "D",
        "text": "Reshard the stream to reduce the number of shards in the stream.",
        "text_jp": "ストリームのシャード数を減らすためにストリームを再シャーディングする。"
      },
      {
        "key": "E",
        "text": "Use an error retry and exponential backoff mechanism in the consumer logic.",
        "text_jp": "消費者ロジックにエラーの再試行と指数バックオフメカニズムを使用する。"
      },
      {
        "key": "F",
        "text": "Configure the stream to use dynamic partitioning.",
        "text_jp": "ストリームを動的パーティショニングを使用するように設定する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct actions to resolve the throttling issue in this scenario include resharding the stream to increase its shard count, which will increase the read capacity.",
        "situation_analysis": "The company is facing throttling issues when multiple consumers read data from a Kinesis Data Stream, leading to 'ReadProvisionedThroughputExceeded' errors.",
        "option_analysis": "Option A is correct because resharding increases throughput by providing more shards. Option D is incorrect as it would reduce capacity. Option E could help but does not directly increase throughput. Options B and F do not address the issue effectively, while option C can improve consumption but might not solve the underlying throughput problem.",
        "additional_knowledge": "Considering the dependency of consumption speed on stream configuration highlights the importance of designing appropriate shard allocations.",
        "key_terminology": "Kinesis Data Streams, Sharding, Throughput, Provisioned Throughput, Enhanced Fan-out",
        "overall_assessment": "Resharding the stream is the most direct method to mitigate throttling issues related to read capacity. Consumers using enhanced fan-out can also help but are not primary solutions in this context."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "このシナリオにおけるスロットリングの問題を解決するための正しいアクションには、シャード数を増やすためにストリームを再シャーディングすることが含まれます。これにより、読み取り能力が向上します。",
        "situation_analysis": "複数の消費者が Kinesis Data Stream からデータを読み取る際にスロットリングの問題に直面しており、'ReadProvisionedThroughputExceeded' エラーが発生しています。",
        "option_analysis": "オプション A は、シャード数を増やすことによりスループットが向上する為、正しい選択です。オプション D は、能力を減少させるため不正解です。オプション E は役立つかもしれませんが、スループットを直接増加させるものではありません。オプション B と F は問題を効果的に解決しません。一方、オプション C は消費を改善できますが、根本的なスループットの問題を解決しない可能性があります。",
        "additional_knowledge": "消費速度がストリームの構成に依存することを考慮すると、適切なシャード割り当てを設計することの重要性が強調されます。",
        "key_terminology": "Kinesis Data Streams、シャーディング、スループット、プロビジョンドスループット、強化ファンアウト",
        "overall_assessment": "ストリームを再シャーディングすることは、読み取り能力に関連するスロットリングの問題を軽減する最も直接的な手段です。強化ファンアウトを使用する消費者も役立つ可能性がありますが、この文脈では主要な解決策とは言えません。"
      }
    ],
    "keywords": [
      "Kinesis Data Streams",
      "Sharding",
      "Throughput",
      "Provisioned Throughput",
      "Enhanced Fan-out"
    ]
  },
  {
    "No": "221",
    "question": "A company uses AWS Organizations to manage its AWS accounts. The company needs a list of all its Amazon EC2 instances that have\nunderutilized CPU or memory usage. The company also needs recommendations for how to downsize these underutilized instances.\nWhich solution will meet these requirements with the LEAST effort?",
    "question_jp": "ある企業が AWS Organizations を使用して AWS アカウントを管理しています。この企業は、CPU またはメモリ使用量が過小利用されている Amazon EC2 インスタンスのリストを必要としています。また、この企業は、これらの過小利用されているインスタンスをダウンサイジングするための推奨事項も必要としています。最も少ない労力でこれらの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Install a CPU and memory monitoring tool from AWS Marketplace on all the EC2 instances. Store the findings in Amazon S3. Implement a Python script to identify underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options.",
        "text_jp": "すべての EC2 インスタンスに AWS Marketplace から CPU およびメモリ監視ツールをインストールします。発見結果を Amazon S3 に保存します。Python スクリプトを実装して過小利用されているインスタンスを特定します。ダウンサイジングオプションに関する推奨事項として EC2 インスタンスの価格情報を参照します。"
      },
      {
        "key": "B",
        "text": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in the organization's management account. Use the recommendations to downsize underutilized instances in all accounts of the organization.",
        "text_jp": "AWS Systems Manager を使用してすべての EC2 インスタンスに Amazon CloudWatch エージェントをインストールします。組織の管理アカウントから AWS Cost Explorer からリソース最適化の推奨事項を取得します。推奨事項を使用して、組織内のすべてのアカウントで過小利用されているインスタンスをダウンサイジングします。"
      },
      {
        "key": "C",
        "text": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in each account of the organization. Use the recommendations to downsize underutilized instances in all accounts of the organization.",
        "text_jp": "AWS Systems Manager を使用してすべての EC2 インスタンスに Amazon CloudWatch エージェントをインストールします。組織の各アカウントから AWS Cost Explorer からリソース最適化の推奨事項を取得します。推奨事項を使用して、組織内のすべてのアカウントで過小利用されているインスタンスをダウンサイジングします。"
      },
      {
        "key": "D",
        "text": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Create an AWS Lambda function to extract CPU and memory usage from all the EC2 instances. Store the findings as files in Amazon S3. Use Amazon Athena to find underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options.",
        "text_jp": "AWS Systems Manager を使用してすべての EC2 インスタンスに Amazon CloudWatch エージェントをインストールします。すべての EC2 インスタンスから CPU およびメモリ使用量を抽出する AWS Lambda 関数を作成します。発見結果をファイルとして Amazon S3 に保存します。Amazon Athena を使用して過小利用されているインスタンスを特定します。ダウンサイジングオプションに関する推奨事項として EC2 インスタンスの価格情報を参照します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This option involves using AWS Systems Manager to install the Amazon CloudWatch agent, which provides resource monitoring, and obtaining recommendations from AWS Cost Explorer, the least effort with centralized management.",
        "situation_analysis": "The company utilizes AWS Organizations and needs an efficient way to monitor multiple accounts for underutilized EC2 instances without excessive overhead.",
        "option_analysis": "Option B is the best choice because it centralizes monitoring and recommendations without requiring additional manual checks or complex custom scripts. Options A, C, and D involve more complexity or lack centralization.",
        "additional_knowledge": "AWS provides tools like Cost Explorer for financial optimization and CloudWatch for performance monitoring, essential for better resource management.",
        "key_terminology": "AWS Organizations, Amazon EC2, Amazon CloudWatch, AWS Systems Manager, AWS Cost Explorer",
        "overall_assessment": "Option B stands out as the most efficient and least manual effort solution, aligning well with AWS best practices for resource optimization."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは B です。この選択肢では、AWS Systems Manager を使用して Amazon CloudWatch エージェントをインストールし、リソースモニタリングを提供し、AWS Cost Explorer からの推奨事項を取得します。これは中央管理で最も少ない労力を伴います。",
        "situation_analysis": "この企業は AWS Organizations を利用しており、複数のアカウントに対して過小利用されている EC2 インスタンスを効率的に監視する必要がありますが、過剰なオーバーヘッドは避けたいと考えています。",
        "option_analysis": "選択肢 B は、監視と推奨事項を中央集約化し、追加の手動チェックや複雑なカスタムスクリプトを必要としないため、最良の選択肢です。選択肢 A、C、および D は、より複雑であったり中央集約が欠如していたりします。",
        "additional_knowledge": "AWS には、Cost Explorer などの財務最適化ツールや、CloudWatch などのパフォーマンス監視ツールがあり、リソース管理のために不可欠です。",
        "key_terminology": "AWS Organizations、Amazon EC2、Amazon CloudWatch、AWS Systems Manager、AWS Cost Explorer",
        "overall_assessment": "選択肢 B は、リソース最適化のための効率的で手間のかからない解決策として際立っており、AWS のベストプラクティスに適合しています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Amazon EC2",
      "Amazon CloudWatch",
      "AWS Systems Manager",
      "AWS Cost Explorer"
    ]
  },
  {
    "No": "222",
    "question": "A company wants to run a custom network analysis software package to inspect trafic as trafic leaves and enters a VPC. The company has\ndeployed the solution by using AWS CloudFormation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been\nestablished to direct trafic to the EC2 instances.\nWhenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the\ninstance replacement occurs.\nWhich combination of steps will resolve this issue? (Choose three.)",
    "question_jp": "ある企業が、VPCの出入りするトラフィックを検査するためにカスタムネットワーク分析ソフトウェアパッケージを実行したいと考えています。企業は、Auto Scalingグループ内の3つのAmazon EC2インスタンスを使用して、AWS CloudFormationでこのソリューションを展開しました。すべてのネットワークルーティングは、トラフィックをEC2インスタンスに指向するように設定されています。分析ソフトウェアが動作を停止した場合、Auto Scalingグループはインスタンスを置き換えますが、インスタンスが置き換えられる際にはネットワークルートが更新されません。この問題を解決するためのステップの組み合わせはどれですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance.",
        "text_jp": "EC2ステータスチェックメトリクスに基づいてアラームを作成すると、Auto Scalingグループが失敗したインスタンスを置き換えます。"
      },
      {
        "key": "B",
        "text": "Update the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to send process metrics for the application.",
        "text_jp": "CloudFormationテンプレートを更新して、EC2インスタンスにAmazon CloudWatchエージェントをインストールします。CloudWatchエージェントを設定して、アプリケーションのプロセスメトリクスを送信します。"
      },
      {
        "key": "C",
        "text": "Update the CloudFormation template to install AWS Systems Manager Agent on the EC2 instances. Configure Systems Manager Agent to send process metrics for the application.",
        "text_jp": "CloudFormationテンプレートを更新して、EC2インスタンスにAWS Systems Managerエージェントをインストールします。Systems Managerエージェントを設定して、アプリケーションのプロセスメトリクスを送信します。"
      },
      {
        "key": "D",
        "text": "Create an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "text_jp": "障害シナリオのカスタムメトリックに対してAmazon CloudWatchでアラームを作成します。アラームを設定して、Amazon Simple Notification Service（Amazon SNS）トピックにメッセージを公開します。"
      },
      {
        "key": "E",
        "text": "Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out of service. Update the network routes to point to the replacement instance.",
        "text_jp": "Amazon Simple Notification Service（Amazon SNS）メッセージに応答してインスタンスをサービスから外すAWS Lambda関数を作成します。ネットワークルートを更新して置き換えたインスタンスを指すようにします。"
      },
      {
        "key": "F",
        "text": "In the CloudFormation template, write a condition that updates the network routes when a replacement instance is launched.",
        "text_jp": "CloudFormationテンプレート内に条件を作成し、置き換えインスタンスが起動する際にネットワークルートを更新します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating alarms based on EC2 status check metrics will ensure that the Auto Scaling group replaces failed instances promptly.",
        "situation_analysis": "When the network analysis software fails, the Auto Scaling group should replace the instance, but network routes remain unchanged.",
        "option_analysis": "Option A is critical as it directly addresses the automatic replacement of failed instances. Options B and C focus on monitoring but do not resolve the routing issue. Option D allows for alerts but does not change routes. Option E proposes a reactive solution but lacks proactive measures. Option F proposes updating routes in CloudFormation, which is useful but does not address instance health directly.",
        "additional_knowledge": "Implementing option F can improve the system but may require further automation to ensure all routes are validated after replacements.",
        "key_terminology": "EC2, Auto Scaling, CloudWatch, SNS, Lambda",
        "overall_assessment": "While the correct answer is A, community votes show strong support for B, D, and E. It's essential to integrate these components to create a resilient setup that addresses both instance health and routing."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。EC2ステータスチェックメトリクスに基づいてアラームを作成することで、Auto Scalingグループが失敗したインスタンスを迅速に置き換えられるようになります。",
        "situation_analysis": "ネットワーク分析ソフトウェアが失敗すると、Auto Scalingグループはインスタンスを置き換えますが、ネットワークルートは変更されません。",
        "option_analysis": "選択肢Aは、失敗したインスタンスの自動置き換えに直接対応しているため重要です。選択肢BとCは監視に焦点を当てていますが、ルーティングの問題を解決しません。選択肢Dはアラートを許可しますが、ルートを変更しません。選択肢Eは反応的な解決策を提案しますが、積極的な措置が欠けています。選択肢FはCloudFormationでルートを更新する提案ですが、インスタンスの健康状態に直接対応していません。",
        "additional_knowledge": "選択肢Fを実装するとシステムが改善される可能性がありますが、置き換えの後にすべてのルートが検証されることを確実にするために、さらなる自動化が必要な場合があります。",
        "key_terminology": "EC2、Auto Scaling、CloudWatch、SNS、Lambda",
        "overall_assessment": "正しい答えはAですが、コミュニティの投票ではB、D、およびEが強く支持されています。インスタンスの健康状態とルーティングの両方に対応する、弾力的なセットアップを作成するために、これらのコンポーネントを統合することが重要です。"
      }
    ],
    "keywords": [
      "EC2",
      "Auto Scaling",
      "CloudWatch",
      "SNS",
      "Lambda"
    ]
  },
  {
    "No": "223",
    "question": "A company is developing a new on-demand video application that is based on microservices. The application will have 5 million users at launch\nand will have 30 million users after 6 months. The company has deployed the application on Amazon Elastic Container Service (Amazon ECS) on\nAWS Fargate. The company developed the application by using ECS services that use the HTTPS protocol.\nA solutions architect needs to implement updates to the application by using blue/green deployments. The solution must distribute trafic to each\nECS service through a load balancer. The application must automatically adjust the number of tasks in response to an Amazon CloudWatch alarm.\nWhich solution will meet these requirements?",
    "question_jp": "会社はマイクロサービスに基づいた新しいオンデマンドビデオアプリケーションを開発しています。このアプリケーションは、立ち上げ時に500万人のユーザーを持ち、6ヶ月後には3000万人のユーザーを抱える予定です。会社はAWS Fargate上のAmazon Elastic Container Service（Amazon ECS）にアプリケーションを展開しました。会社はHTTPSプロトコルを使用したECSサービスを使ってアプリケーションを開発しました。ソリューションアーキテクトは、ブルー/グリーンデプロイメントを使用してアプリケーションの更新を実装する必要があります。ソリューションは、各ECSサービスへのトラフィックを負荷分散装置を通じて分配する必要があります。アプリケーションは、Amazon CloudWatchアラームに応じてタスク数を自動的に調整する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Request increases to the service quota for tasks per service to meet the demand.",
        "text_jp": "ECSサービスをブルー/グリーンデプロイメントタイプとネットワーク負荷分散装置を使用するように設定します。需要に応じてサービスごとのタスクのサービスクォータの増加をリクエストします。"
      },
      {
        "key": "B",
        "text": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Implement Auto Scaling group for each ECS service by using the Cluster Autoscaler.",
        "text_jp": "ECSサービスをブルー/グリーンデプロイメントタイプとネットワーク負荷分散装置を使用するように設定します。クラスターオートスケーラーを使用して各ECSサービスのためのオートスケーリンググループを実装します。"
      },
      {
        "key": "C",
        "text": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement an Auto Scaling group for each ECS service by using the Cluster Autoscaler.",
        "text_jp": "ECSサービスをブルー/グリーンデプロイメントタイプとアプリケーション負荷分散装置を使用するように設定します。クラスターオートスケーラーを使用して各ECSサービスのためのオートスケーリンググループを実装します。"
      },
      {
        "key": "D",
        "text": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for each ECS service.",
        "text_jp": "ECSサービスをブルー/グリーンデプロイメントタイプとアプリケーション負荷分散装置を使用するように設定します。各ECSサービスのためのサービスオートスケーリングを実装します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (81%) C (19%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This option configures ECS services for blue/green deployment and uses a Network Load Balancer, which is essential for handling the traffic. Requesting an increase in service quota allows for scaling based on demand.",
        "situation_analysis": "The application will have a significant user base, requiring a robust deployment and scaling strategy to manage load effectively while ensuring zero downtime during updates.",
        "option_analysis": "Option A is correct because it meets the blue/green deployment requirement and requires a Network Load Balancer, which fits well with ECS services. Option B incorrectly suggests using an Auto Scaling group which isn't necessary for ECS in Fargate; Option C incorrectly uses an Application Load Balancer. Option D suggests using Service Auto Scaling but is less suitable in this context.",
        "additional_knowledge": "Understanding the difference between load balancers and their roles is critical in ECS deployments, especially when managing security, latency, and protocol handling.",
        "key_terminology": "Blue/Green Deployment, Amazon ECS, Network Load Balancer, Amazon CloudWatch, Auto Scaling",
        "overall_assessment": "While option A is technically sound, community votes significantly favor option D, indicating a misunderstanding of the required architecture to fulfill the application needs effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。この選択肢はECSサービスをブルー/グリーンデプロイメント用に設定し、トラフィックを処理するために必要なネットワーク負荷分散装置を使用します。サービスのクォータの増加を要求することで需要に応じてスケーリングを可能にします。",
        "situation_analysis": "アプリケーションは大規模なユーザーベースを持ち、効果的に負荷を管理しながら更新の際にダウンタイムをゼロにするための堅牢なデプロイメントとスケーリング戦略が必要です。",
        "option_analysis": "選択肢Aは正しく、ECSサービスのブルー/グリーンデプロイメント要件を満たし、ネットワーク負荷分散装置を使用することでECSサービスに適しています。選択肢BはECS Fargateにおいてオートスケーリンググループを使用することを提案しており、必要ありません。選択肢Cはアプリケーション負荷分散装置を使用して不正確です。選択肢Dはサービスオートスケーリングを提案していますが、この文脈ではあまり適していません。",
        "additional_knowledge": "ローデバランサーの違いとその役割を理解することは、特にセキュリティ、待機時間、プロトコル処理を管理する際にECSデプロイメントにおいて重要です。",
        "key_terminology": "ブルー/グリーンデプロイメント、Amazon ECS、ネットワーク負荷分散装置、Amazon CloudWatch、オートスケーリング",
        "overall_assessment": "選択肢Aは技術的に妥当ですが、コミュニティの投票では選択肢Dが大きく支持されており、アプリケーションのニーズを効果的に満たすための必要なアーキテクチャの誤解を示しています。"
      }
    ],
    "keywords": [
      "Blue/Green Deployment",
      "Amazon ECS",
      "Network Load Balancer",
      "Amazon CloudWatch",
      "Auto Scaling"
    ]
  },
  {
    "No": "224",
    "question": "A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service\n(Amazon ECS) on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group.\nThe company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the\nnew image version receives a unique tag.\nThe company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically\ndelete new image tags that have Critical or High severity findings. The solution also must notify the development team when such a deletion\noccurs.\nWhich solution meets these requirements?",
    "question_jp": "ある企業が AWS クラウドでコンテナ化されたアプリケーションを実行しています。このアプリケーションは、Amazon Elastic Container Service (Amazon ECS) を使用して、Amazon EC2 インスタンスのセット上で実行されています。EC2 インスタンスは Auto Scaling グループで実行されています。企業は Amazon Elastic Container Registry (Amazon ECR) を利用して、コンテナイメージを保存しています。新しいイメージバージョンがアップロードされると、その新しいイメージバージョンには一意のタグが付与されます。企業は、新しいイメージバージョンを一般的な脆弱性と露出に対して検査するソリューションが必要です。このソリューションは、重大または高い深刻度の脆弱性がある新しいイメージタグを自動的に削除し、そのような削除が行われた際に開発チームに通知する必要があります。どのソリューションがこれらの要件を満たしていますか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure scan on push on the repository. Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is complete for images that have Critical or High severity findings. Use the Step Functions state machine to delete the image tag for those images and to notify the development team through Amazon Simple Notification Service (Amazon SNS).",
        "text_jp": "リポジトリでのプッシュ時にスキャンを設定します。スキャンが完了した際に、重大または高い深刻度の脆弱性が検出されたイメージについて、AWS Step Functions のステートマシンを呼び出すために Amazon EventBridge を使用します。ステートマシンを使用して、これらのイメージのタグを削除し、Amazon Simple Notification Service (Amazon SNS) を介して開発チームに通知します。"
      },
      {
        "key": "B",
        "text": "Configure scan on push on the repository. Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Lambda function when a new message is added to the SQS queue. Use the Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES).",
        "text_jp": "リポジトリでのプッシュ時にスキャンを設定します。スキャン結果を Amazon Simple Queue Service (Amazon SQS) キューにプッシュするように構成します。新しいメッセージが SQS キューに追加されると、AWS Lambda 関数を呼び出します。Lambda 関数を使用して、重大または高い深刻度の脆弱性があるイメージのタグを削除します。開発チームには Amazon Simple Email Service (Amazon SES) を使用して通知します。"
      },
      {
        "key": "C",
        "text": "Schedule an AWS Lambda function to start a manual image scan every hour. Configure Amazon EventBridge to invoke another Lambda function when a scan is complete. Use the second Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
        "text_jp": "AWS Lambda 関数をスケジュールして毎時手動のイメージスキャンを開始します。スキャンが完了した際に別の Lambda 関数を呼び出すように Amazon EventBridge を構成します。2 番目の Lambda 関数を使用して、重大または高い深刻度の脆弱性があるイメージのタグを削除します。開発チームには Amazon Simple Notification Service (Amazon SNS) を使用して通知します。"
      },
      {
        "key": "D",
        "text": "Configure periodic image scan on the repository. Configure scan results to be added to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Step Functions state machine when a new message is added to the SQS queue. Use the Step Functions state machine to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES).",
        "text_jp": "リポジトリで定期的にイメージスキャンを設定します。スキャン結果が Amazon Simple Queue Service (Amazon SQS) キューに追加されるように構成します。新しいメッセージが SQS キューに追加されると、AWS Step Functions のステートマシンを呼び出します。ステートマシンを使用して、重大または高い深刻度の脆弱性があるイメージのタグを削除し、Amazon Simple Email Service (Amazon SES) を使用して開発チームに通知します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This solution implements an automated workflow that ensures image scans for vulnerabilities are performed regularly, and it sets up notifications and deletions based on scan results.",
        "situation_analysis": "The company requires a system that scans container images for vulnerabilities and manages their tags based on severity findings. The solution needs to support automated deletion of critical issues and notification of the development team.",
        "option_analysis": "Option A suggests using AWS Step Functions but does not automate the scan frequency or management of scanning. Option B uses SQS and Lambda, but does not perform automatic scans at defined intervals. Option D also relies on SQS and Step Functions but doesn't automate scanning.",
        "additional_knowledge": "The automation of scans is crucial in maintaining security practices in CI/CD pipelines.",
        "key_terminology": "Amazon ECS, Amazon ECR, AWS Lambda, Amazon SNS, Amazon EventBridge",
        "overall_assessment": "The community vote heavily favors A, but C is structurally better aligned with the requirement for periodic scanning and notifications about deletions. Despite community preference, C holds technological advantages."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答は C である。このソリューションは、自動化されたワークフローを実装しており、イメージスキャンを定期的に脆弱性に対して実施し、スキャン結果に基づいて通知と削除を設定している。",
        "situation_analysis": "企業は、コンテナイメージを脆弱性に対してスキャンし、深刻度の結果に基づいてタグを管理するシステムを必要としている。ソリューションは、重大な問題の自動削除と開発チームへの通知をサポートする必要がある。",
        "option_analysis": "選択肢 A は、AWS Step Functions を使用しているが、スキャン頻度やスキャンの管理を自動化していない。選択肢 B は SQS と Lambda を使用するが、定義された間隔で自動的なスキャンを実施しない。選択肢 D も SQS と Step Functions に依存しているが、スキャンを自動化していない。",
        "additional_knowledge": "スキャンの自動化は、CI/CD パイプラインにおいてセキュリティプラクティスを維持する上で重要である。",
        "key_terminology": "Amazon ECS, Amazon ECR, AWS Lambda, Amazon SNS, Amazon EventBridge",
        "overall_assessment": "コミュニティの投票は主に A に偏っているが、C は定期的なスキャンと削除に関する通知の要件により適している。コミュニティの好みとは裏腹に、C は技術的な利点を持っている。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "Amazon ECR",
      "AWS Lambda",
      "Amazon SNS",
      "Amazon EventBridge"
    ]
  },
  {
    "No": "225",
    "question": "A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. The workloads are hosted on Amazon EC2. AWS\nFargate. and AWS Lambda. Some of the workloads have unpredictable demand. Accounts record high usage in some months and low usage in\nother months.\nThe company wants to optimize its compute costs over the next 3 years. A solutions architect obtains a 6-month average for each of the accounts\nacross the organization to calculate usage.\nWhich solution will provide the MOST cost savings for all the organization's compute usage?",
    "question_jp": "会社は多くのワークロードをAWS上で運用しており、AWS Organizationsを使用してアカウントを管理しています。ワークロードはAmazon EC2、AWS Fargate、AWS Lambdaでホストされています。一部のワークロードは需要が予測できません。アカウントは、特定の月に高い使用量を記録し、他の月には低い使用量を記録しています。会社は、今後3年間でコンピューティングコストを最適化したいと考えています。ソリューションアーキテクトは、組織全体の各アカウントの6か月平均を取得して使用量を計算します。どのソリューションが組織全体のコンピューティング使用量に対して最もコスト効果が高いでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Purchase Reserved Instances for the organization to match the size and number of the most common EC2 instances from the member accounts.",
        "text_jp": "メンバーアカウントから最も一般的なEC2インスタンスのサイズと数に合わせて、組織のためにリザーブドインスタンスを購入する。"
      },
      {
        "key": "B",
        "text": "Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level.",
        "text_jp": "管理アカウントから組織のためにコンピュートセービングスプランを購入し、管理アカウントレベルでの推奨事項を使用する。"
      },
      {
        "key": "C",
        "text": "Purchase Reserved Instances for each member account that had high EC2 usage according to the data from the last 6 months.",
        "text_jp": "過去6か月のデータに基づいて高いEC2使用量を持つ各メンバーアカウントのためにリザーブドインスタンスを購入する。"
      },
      {
        "key": "D",
        "text": "Purchase an EC2 Instance Savings Plan for each member account from the management account based on EC2 usage data from the last 6 months.",
        "text_jp": "過去6か月のEC2使用量データに基づいて、管理アカウントから各メンバーアカウントのためにEC2インスタンスセービングスプランを購入する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level.",
        "situation_analysis": "Considering unpredictable demand, a Compute Savings Plan allows flexibility in workload execution, which is beneficial when workloads vary month to month.",
        "option_analysis": "Option A suggests buying Reserved Instances based on the most common EC2 instances which may not optimize costs during low usage months. Option C focuses on individual member accounts, potentially missing the overall cost savings at the organization level. Option D suggests an EC2 Instance Savings Plan but does not leverage the comprehensive benefits of a Compute Savings Plan that can apply to both EC2 and Fargate workloads.",
        "additional_knowledge": "Many organizations underestimate the variability in workloads, which is why understanding and utilizing savings plans smartly can lead to significant economic benefits.",
        "key_terminology": "Compute Savings Plan, Reserved Instances, EC2, AWS Lambda, AWS Fargate",
        "overall_assessment": "This question effectively tests understanding of AWS cost management strategies, particularly in a variable workload environment. Option B is the optimal choice as it balances cost savings with operational flexibility, supported by community voting."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はB: 管理アカウントから組織のためにコンピュートセービングスプランを購入し、管理アカウントレベルでの推奨事項を使用することです。",
        "situation_analysis": "需要が予測できないことを考慮すると、コンピュートセービングスプランはワークロードの実行における柔軟性を提供し、月ごとにワークロードが変動する際に有益です。",
        "option_analysis": "選択肢Aは、最も一般的なEC2インスタンスに基づいてリザーブドインスタンスを購入することを提案していますが、低い使用量の月にコストを最適化しない可能性があります。選択肢Cは、各メンバーアカウントに焦点を当てており、組織レベルでの全体的なコスト削減を見逃す可能性があります。選択肢Dは、EC2インスタンスセービングスプランを提案していますが、EC2およびFargateワークロードの両方に適用できるコンピュートセービングスプランの包括的な利点を活用していません。",
        "additional_knowledge": "多くの組織はワークロードの変動性を過小評価しており、賢くセービングプランを理解し活用することで大きな経済的利益を得ることができることを示しています。",
        "key_terminology": "コンピュートセービングスプラン、リザーブドインスタンス、EC2、AWS Lambda、AWS Fargate",
        "overall_assessment": "この質問は、特に変動するワークロード環境におけるAWSコスト管理戦略の理解を効果的にテストしています。選択肢Bがコスト削減と運用の柔軟性のバランスをうまく取っているため、最適な選択です。コミュニティの投票によっても支持されています。"
      }
    ],
    "keywords": [
      "Compute Savings Plan",
      "Reserved Instances",
      "EC2"
    ]
  },
  {
    "No": "226",
    "question": "A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations to manage all the accounts. The company\nhas turned on all features.\nA finance team has allocated a daily budget for AWS costs. The finance team must receive an email notification if the organization's AWS costs\nexceed 80% of the allocated budget. A solutions architect needs to implement a solution to track the costs and deliver the notifications.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は数百のAWSアカウントを持っています。企業はAWS Organizationsを使用してすべてのアカウントを管理しています。企業はすべての機能を有効にしています。\nファイナンスチームはAWSコストのために毎日の予算を割り当てました。ファイナンスチームは、組織のAWSコストが割り当てられた予算の80%を超えた場合に、メール通知を受け取る必要があります。ソリューションアーキテクトは、コストを追跡し、通知を配信するソリューションを実装する必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "In the organization's management account, use AWS Budgets to create a budget that has a daily period. Add an alert threshold and set the value to 80%. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.",
        "text_jp": "組織の管理アカウントでAWS Budgetsを使用して、日次の期間を持つ予算を作成します。アラートしきい値を追加し、値を80%に設定します。Amazon Simple Notification Service (Amazon SNS)を使用してファイナンスチームに通知します。"
      },
      {
        "key": "B",
        "text": "In the organization's management account, set up the organizational view feature for AWS Trusted Advisor. Create an organizational view report for cost optimization. Set an alert threshold of 80%. Configure notification preferences. Add the email addresses of the finance team.",
        "text_jp": "組織の管理アカウントでAWS Trusted Advisorの組織ビュー機能を設定します。コスト最適化のための組織ビューレポートを作成します。しきい値を80%に設定します。通知設定を構成し、ファイナンスチームのメールアドレスを追加します。"
      },
      {
        "key": "C",
        "text": "Register the organization with AWS Control Tower. Activate the optional cost control (guardrail). Set a control (guardrail) parameter of 80%. Configure control (guardrail) notification preferences. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.",
        "text_jp": "AWS Control Towerに組織を登録します。オプションのコスト制御（ガードレール）を有効にします。ガードレールパラメータを80%に設定します。ガードレールの通知設定を構成し、Amazon Simple Notification Service (Amazon SNS)を使用してファイナンスチームに通知します。"
      },
      {
        "key": "D",
        "text": "Configure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket in the organization's management account. Use Amazon EventBridge to schedule a daily Amazon Athena query to calculate the organization's costs. Configure Athena to send an Amazon CloudWatch alert if the total costs are more than 80% of the allocated budget. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.",
        "text_jp": "メンバーアカウントを設定して、組織の管理アカウント内のAmazon S3バケットに毎日のAWSコストと使用状況レポートを保存します。Amazon EventBridgeを使用して、毎日Amazon Athenaクエリをスケジュールし、組織のコストを計算します。Athenaを構成して、合計コストが割り当てられた予算の80%を超えた場合にAmazon CloudWatchアラートを送信します。Amazon Simple Notification Service (Amazon SNS)を使用してファイナンスチームに通知します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. AWS Budgets allows for tracking the costs and setting specific thresholds for notifications, which meets all requirements stated in the question.",
        "situation_analysis": "The finance team has a daily budget allocation and requires notification for overspending. Hence, tracking cost against a defined budget is essential.",
        "option_analysis": "Option A directly uses AWS Budgets, which is designed for this purpose. Options B, C, and D, while relevant to cost monitoring, do not provide the specific notification mechanism based on a defined budget threshold.",
        "additional_knowledge": "AWS Budgets integrates well with AWS Organizations, allowing for comprehensive cost monitoring.",
        "key_terminology": "AWS Budgets, Amazon SNS, cost allocation",
        "overall_assessment": "The question portrays a clear scenario requiring a budget and notification system, with option A being the most directly applicable solution for setting a fiscal threshold."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。AWS Budgetsはコストを追跡し、通知のための特定のしきい値を設定することができ、これは質問で述べられているすべての要件を満たしている。",
        "situation_analysis": "ファイナンスチームは毎日の予算配分を持ち、超過支出の通知を必要としている。そのため、定義された予算に対するコストの追跡が必須である。",
        "option_analysis": "オプションAは、まさにこの目的のために設計されたAWS Budgetsを直接使用している。オプションB、C、およびDはコスト監視に関連するが、定義された予算しきい値に基づく具体的な通知メカニズムを提供していない。",
        "additional_knowledge": "AWS Budgetsは、AWS Organizationsとの統合がうまく、包括的なコスト監視を可能にする。",
        "key_terminology": "AWS Budgets, Amazon SNS, コスト配分",
        "overall_assessment": "この質問は、予算と通知システムを必要とする明確なシナリオを描写しており、オプションAが財政的なしきい値を設定するための最も直接的な解決策である。"
      }
    ],
    "keywords": [
      "AWS Budgets",
      "Amazon SNS",
      "cost allocation"
    ]
  },
  {
    "No": "227",
    "question": "A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon\nEC2 instances in the us-east-1 Region. Artists upload photos of their work as large-size. high-resolution image files from their mobile phones to a\ncentralized Amazon S3 bucket created in the us-east-1 Region. The users in Europe are reporting slow performance for their image uploads.\nHow can a solutions architect improve the performance of the image upload process?",
    "question_jp": "ある会社はアートワークのオークションサービスを提供しており、北アメリカおよびヨーロッパにユーザーがいます。会社は、us-east-1リージョンのAmazon EC2インスタンスでアプリケーションをホスティングしています。アーティストは、自分のモバイルフォンから高解像度でサイズの大きい画像ファイルの写真を中央のAmazon S3バケットにアップロードします。このS3バケットはus-east-1リージョンに作成されています。ヨーロッパのユーザーは、画像アップロードのパフォーマンスが遅いと報告しています。ソリューションアーキテクトは、画像アップロードプロセスのパフォーマンスをどのように改善できますか？",
    "choices": [
      {
        "key": "A",
        "text": "Redeploy the application to use S3 multipart uploads.",
        "text_jp": "アプリケーションを再配備してS3マルチパートアップロードを使用します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudFront distribution and point to the application as a custom origin.",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、アプリケーションをカスタムオリジンとして指定します。"
      },
      {
        "key": "C",
        "text": "Configure the buckets to use S3 Transfer Acceleration.",
        "text_jp": "バケットを設定してS3転送アクセラレーションを使用します。"
      },
      {
        "key": "D",
        "text": "Create an Auto Scaling group for the EC2 instances and create a scaling policy.",
        "text_jp": "EC2インスタンスのオートスケーリンググループを作成し、スケーリングポリシーを作成します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (82%) A (18%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Configure the buckets to use S3 Transfer Acceleration. This feature speeds up uploads globally by utilizing Amazon CloudFront’s globally distributed edge locations.",
        "situation_analysis": "The scenario involves users from Europe experiencing slow upload speeds when transferring large image files to S3 in the us-east-1 region. This indicates a latency issue due to geographical distance.",
        "option_analysis": "Option A (S3 multipart uploads) enhances upload reliability for larger files but does not improve speed. Option B (CloudFront distribution as a custom origin) can improve user experience but primarily addresses download speeds. Option D (Auto Scaling) is related to EC2 scaling and does not directly affect S3 upload performance.",
        "additional_knowledge": "Transfer Acceleration incurs additional costs, but it can significantly improve performance for large data transfers in various situations.",
        "key_terminology": "S3 Transfer Acceleration, CloudFront, edge location, latency, multipart uploads.",
        "overall_assessment": "Considering the location of users, S3 Transfer Acceleration is an appropriate solution to reduce latency and improve upload speeds. The community's voting aligns with this reasoning."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはC: バケットを設定してS3転送アクセラレーションを使用します。この機能は、Amazon CloudFrontの地理的に分散したエッジロケーションを利用して、グローバルなアップロードを高速化します。",
        "situation_analysis": "このシナリオでは、ヨーロッパにいるユーザーが、us-east-1リージョンのS3に大きな画像ファイルを転送する際に遅いアップロードスピードを体験しています。これは、地理的距離によるレイテンシの問題を示しています。",
        "option_analysis": "オプションA（S3マルチパートアップロード）は、大きなファイルのアップロードの信頼性を向上させますが、スピードを改善するものではありません。オプションB（カスタムオリジンとしてのCloudFrontディストリビューション）はユーザーエクスペリエンスを向上させる可能性がありますが、主にダウンロードスピードに関連しています。オプションD（オートスケーリング）はEC2のスケーリングに関連しており、S3のアップロードパフォーマンスには直接影響しません。",
        "additional_knowledge": "転送アクセラレーションは追加費用が発生しますが、大きなデータ転送の場合にはパフォーマンスが大幅に改善される可能性があります。",
        "key_terminology": "S3転送アクセラレーション、CloudFront、エッジロケーション、レイテンシ、マルチパートアップロード。",
        "overall_assessment": "ユーザーの位置を考慮すると、S3転送アクセラレーションはレイテンシを減少させ、アップロードスピードを改善するのに適切なソリューションであると言えます。コミュニティの投票もこの理由に一致しています。"
      }
    ],
    "keywords": [
      "S3 Transfer Acceleration",
      "CloudFront",
      "edge location",
      "latency",
      "multipart uploads"
    ]
  },
  {
    "No": "228",
    "question": "A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application\nincludes web. application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed\ndata must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in\ntrafic.\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?",
    "question_jp": "ある企業は、マルチティアのウェブアプリケーションをコンテナ化し、オンプレミスのデータセンターからAWSに移行したいと考えています。このアプリケーションにはウェブ、アプリケーション、およびデータベースのティアが含まれています。企業は、アプリケーションを耐障害性がありスケーラブルにする必要があります。一部の頻繁にアクセスされるデータは、アプリケーションサーバー間で常に利用可能でなければなりません。フロントエンドのウェブサーバーはセッションの永続性を必要とし、トラフィックの増加に対応するためにスケールする必要があります。どのソリューションが、最も少ない運用上のオーバーヘッドでこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS) for data that is frequently accessed between the web and application tiers. Store the frontend web server session data in Amazon Simple Queue Service (Amazon SQS).",
        "text_jp": "Amazon ECSをAWS Fargate上で実行します。ウェブとアプリケーションのティア間で頻繁にアクセスされるデータのためにAmazon EFSを使用します。フロントエンドウェブサーバーのセッションデータは、Amazon SQSに保存します。"
      },
      {
        "key": "B",
        "text": "Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache frontend web server session data. Use Amazon Elastic Block Store (Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones.",
        "text_jp": "Amazon ECSをAmazon EC2上で実行します。フロントエンドウェブサーバーのセッションデータをキャッシュするためにAmazon ElastiCache for Redisを使用します。複数のアベイラビリティゾーンに分散したEC2インスタンスでマルチアタッチを使用してAmazon EBSを利用します。"
      },
      {
        "key": "C",
        "text": "Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use ReplicaSets to run the web servers and applications. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system across all EKS pods to store frontend web server session data.",
        "text_jp": "Amazon EKS上でアプリケーションを実行します。Amazon EKSを管理ノードグループを使用するように設定します。ReplicaSetsを使用してウェブサーバーとアプリケーションを実行します。フロントエンドウェブサーバーのセッションデータを格納するためにAmazon EFSファイルシステムを作成し、すべてのEKSポッドでこのEFSファイルシステムをマウントします。"
      },
      {
        "key": "D",
        "text": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the web servers and application as Kubernetes deployments in the EKS cluster. Store the frontend web server session data in an Amazon DynamoDB table. Create an Amazon Elastic File System (Amazon EFS) volume that all applications will mount at the time of deployment.",
        "text_jp": "Amazon EKS上でアプリケーションをデプロイします。Amazon EKSを管理ノードグループを使用するように設定します。ウェブサーバーとアプリケーションは、EKSクラスタ内のKubernetesデプロイメントとして実行されます。フロントエンドウェブサーバーのセッションデータは、Amazon DynamoDBテーブルに保存します。すべてのアプリケーションがデプロイ時にマウントするAmazon EFSボリュームを作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (82%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Running the application on Amazon ECS on EC2 with ElastiCache for Redis and EBS provides a scalable and fault-tolerant architecture with minimal operational overhead.",
        "situation_analysis": "The company requires a scalable and fault-tolerant solution for a multi-tier web application that needs session persistence and shared data access among application servers.",
        "option_analysis": "Option A relies on SQS, which is not ideal for session persistence. Option C and D both utilize EKS, adding unnecessary complexity, while B directly uses EC2, allowing for easier management.",
        "additional_knowledge": "Using Amazon EBS with Multi-Attach allows multiple EC2 instances to attach to the same EBS volume, enhancing performance.",
        "key_terminology": "Amazon ECS, Amazon ElastiCache, Amazon EBS, Multi-Attach, Fault Tolerance, Scalability",
        "overall_assessment": "Despite community votes favoring D, option B is technically more suitable due to lowered operational complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。Amazon ECSをEC2上で実行し、ElastiCache for RedisとEBSを使用することで、最小限の運用オーバーヘッドでスケーラブルで耐障害性のあるアーキテクチャが提供される。",
        "situation_analysis": "企業は、セッション持続性とアプリケーションサーバー間の共有データアクセスが必要な、スケーラブルで耐障害性のあるマルチティアのウェブアプリケーションに対するソリューションを要求している。",
        "option_analysis": "選択肢AはSQSに依存しており、セッションの持続性には適していない。選択肢CとDはどちらもEKSを活用しており、不要な複雑性を追加しているが、Bは直接EC2を使用するため、管理が容易である。",
        "additional_knowledge": "Amazon EBSに対するマルチアタッチを使用することで、複数のEC2インスタンスが同じEBSボリュームに接続でき、性能が向上する。",
        "key_terminology": "Amazon ECS, Amazon ElastiCache, Amazon EBS, マルチアタッチ, 耐障害性, スケーラビリティ",
        "overall_assessment": "コミュニティの投票はDを支持しているが、選択肢Bは運用の複雑さが低いため、技術的にはより適している。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "Amazon ElastiCache",
      "Amazon EBS",
      "Multi-Attach",
      "Fault Tolerance",
      "Scalability"
    ]
  },
  {
    "No": "229",
    "question": "A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the\nsolutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero\ndowntime.\nWhich solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、重要なMicrosoft SQL ServerデータベースをAWSに移行する計画を立てています。データベースがレガシーシステムであるため、ソリューションアーキテクトはデータベースをモダンデータアーキテクチャに移行します。ソリューションアーキテクトは、ほぼダウンタイムゼロでデータベースを移行する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora.",
        "text_jp": "AWSアプリケーション移行サービスおよびAWSスキーマ変換ツール（AWS SCT）を使用します。移行前にインプレースアップグレードを行います。移行後に移行されたデータをAmazon Aurora Serverlessにエクスポートします。アプリケーションをAmazon Auroraに再指向します。"
      },
      {
        "key": "B",
        "text": "Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance.",
        "text_jp": "AWSデータベース移行サービス（AWS DMS）を使用してデータベースをリホストします。Amazon S3をターゲットとして設定します。変更データキャプチャ（CDC）レプリケーションを設定します。ソースと宛先が完全に同期したら、Amazon S3からAmazon RDS for Microsoft SQL Server DBインスタンスにデータをロードします。"
      },
      {
        "key": "C",
        "text": "Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance.",
        "text_jp": "ネイティブデータベース高可用性ツールを使用します。ソースシステムをAmazon RDS for Microsoft SQL Server DBインスタンスに接続します。レプリケーションを適切に構成します。データレプリケーションが完了したら、ワークロードをAmazon RDS for Microsoft SQL Server DBインスタンスに移行します。"
      },
      {
        "key": "D",
        "text": "Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQL Server DB instance. Reattach the database and then cut over all networking.",
        "text_jp": "AWSアプリケーション移行サービスを使用します。データベースサーバーをAmazon EC2にリホストします。データレプリケーションが完了したら、データベースを切り離し、Amazon RDS for Microsoft SQL Server DBインスタンスに移動します。データベースを再接続し、すべてのネットワーキングを切り替えます。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (67%) B (33%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using native database high availability tools helps ensure a seamless transition with nearly zero downtime.",
        "situation_analysis": "The migration involves moving critical Microsoft SQL Server databases with the requirement of near-zero downtime. Native high availability tools facilitate a continuous data flow during the migration process.",
        "option_analysis": "Option A involves an upgrade and migration to Amazon Aurora, which may introduce downtime. Option B uses AWS DMS and also includes a step that could cause a delay. Option D requires detaching and moving which might lead to downtime as well.",
        "additional_knowledge": "Consideration of replication timing and testing the setup prior to cutover can enhance the effectiveness of the migration plan.",
        "key_terminology": "Database High Availability, Amazon RDS, AWS DMS, SQL Server Replication, CDC",
        "overall_assessment": "Overall, option C aligns best with the requirement for minimal downtime. It ensures that the migration can occur with the source system still operational."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。ネイティブデータベース高可用性ツールを使用することで、ほぼダウンタイムゼロのシームレスな移行を保証します。",
        "situation_analysis": "移行は、ほぼダウンタイムゼロの要求を伴い、重要なMicrosoft SQL Serverデータベースを移動することを含みます。ネイティブの高可用性ツールは、移行プロセス中に継続的なデータフローを容易にします。",
        "option_analysis": "選択肢AはアップグレードとAmazon Auroraへの移行が必要で、ダウンタイムが発生する可能性があります。選択肢BはAWS DMSを使用し、遅延を引き起こす可能性のあるステップを含んでいます。選択肢Dは切り離して移動する必要があり、これもダウンタイムを招く恐れがあります。",
        "additional_knowledge": "レプリケーションのタイミングを考慮し、切り替え前にセットアップをテストすることで、移行計画の効果を高めることができます。",
        "key_terminology": "データベース高可用性、Amazon RDS、AWS DMS、SQL Serverレプリケーション、CDC",
        "overall_assessment": "全体として、選択肢Cはダウンタイムを最小限にする要件と最も一致しています。ソースシステムが稼働し続けながら移行を行うことを保証します。"
      }
    ],
    "keywords": [
      "Database High Availability",
      "Amazon RDS",
      "AWS DMS",
      "SQL Server Replication",
      "CDC"
    ]
  },
  {
    "No": "230",
    "question": "A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability\nZones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has\ncreated multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created\nmultiple service consumer applications in the other organization.\nData transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect\nmust recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the\nwhole environment.\nWhich guidelines meet these requirements? (Choose two.)",
    "question_jp": "ある企業のソリューションアーキテクトが、複数のアプリケーション環境のコストを分析しています。この環境は、単一の AWS リージョンにおいて複数のアベイラビリティゾーンに展開されています。最近の買収後、企業は AWS Organizations で2つの組織を管理しています。企業は、1つの組織で AWS PrivateLink を利用した VPC エンドポイントサービスとして複数のサービスプロバイダーアプリケーションを作成しました。別の組織には複数のサービスコンシューマアプリケーションが作成されています。データ転送料金は、企業が予想したよりもはるかに高くなっており、ソリューションアーキテクトはコストを削減する必要があります。ソリューションアーキテクトは、サービスを展開する際に開発者が遵守すべきガイドラインを推奨する必要があります。これらのガイドラインは、全体の環境に対するデータ転送料金を最小限に抑える必要があります。どのガイドラインがこの要件を満たしますか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization.",
        "text_jp": "AWS Resource Access Manager を使用して、サービスプロバイダーアプリケーションをホスティングしているサブネットを組織内の他のアカウントと共有します。"
      },
      {
        "key": "B",
        "text": "Place the service provider applications and the service consumer applications in AWS accounts in the same organization.",
        "text_jp": "サービスプロバイダーアプリケーションとサービスコンシューマアプリケーションを同じ組織の AWS アカウントに配置します。"
      },
      {
        "key": "C",
        "text": "Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.",
        "text_jp": "すべてのサービスプロバイダーアプリケーションの展開で、ネットワークロードバランサーのクロスゾーンロードバランシングをオフにします。"
      },
      {
        "key": "D",
        "text": "Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.",
        "text_jp": "サービスコンシューマコンピューティングリソースがエンドポイントのローカルDNS名を使用して、アベイラビリティゾーン特有のエンドポイントサービスを使用することを確認します。"
      },
      {
        "key": "E",
        "text": "Create a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage.",
        "text_jp": "組織の予定されているアベイラビリティゾーン間データ転送の使用に対する十分なカバレッジを提供するSavings Planを作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AD (37%) BD (33%) CD (30%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and D.",
        "situation_analysis": "The company needs to reduce the unexpected high data transfer charges across their multi-application environment spread over multiple Availability Zones, which includes two organizations under AWS Organizations.",
        "option_analysis": "Option A addresses sharing resources effectively to minimize data transfer costs by using AWS Resource Access Manager, whereas Option D focuses on using local DNS names to ensure consumers access the services in the same Availability Zone, thereby reducing cross-zone data transfer.",
        "additional_knowledge": "Options B, C, and E either do not effectively address the core issue of data transfer costs or are not directly relevant to the most immediate guidelines required for the architecture.",
        "key_terminology": "AWS PrivateLink, Resource Access Manager, data transfer, VPC Endpoints, Savings Plan.",
        "overall_assessment": "The question accurately tests knowledge of AWS architectural best practices related to data transfer optimization."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとDである。",
        "situation_analysis": "企業は、複数のアベイラビリティゾーンに広がるマルチアプリケーション環境において、予期しない高いデータ転送料金を削減する必要がある。これには、AWS Organizationsの下にある2つの組織が含まれる。",
        "option_analysis": "選択肢Aは、AWS Resource Access Managerを使用してリソースを効果的に共有し、データ転送コストを最小限に抑えることを提案している。一方、選択肢Dは、ローカルDNS名を使用して消費者が同じアベイラビリティゾーン内のサービスにアクセスできるようにすることにより、ゾーン間データ転送を削減することに重点を置いている。",
        "additional_knowledge": "選択肢B、C、およびEは、コストに関する根本的な問題に対処していないか、もしくはアーキテクチャに必要なガイドラインに直接関連していない。",
        "key_terminology": "AWS PrivateLink, Resource Access Manager, データ転送, VPCエンドポイント, Savings Plan。",
        "overall_assessment": "この質問は、データ転送最適化に関するAWSのアーキテクチャのベストプラクティスについての知識を正確に試験している。"
      }
    ],
    "keywords": [
      "AWS PrivateLink",
      "Resource Access Manager",
      "data transfer",
      "VPC Endpoints",
      "Savings Plan"
    ]
  },
  {
    "No": "231",
    "question": "A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move\nthe backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-\npremises data center and AWS.\nWhich solution meets these requirements MOST cost-effectively?",
    "question_jp": "ある企業は、オンプレミスのMicrosoft SQL Serverデータベースが毎晩200GBのエクスポートをローカルドライブに書き込むというシナリオを持っています。この企業は、バックアップをAmazon S3のより堅牢なクラウドストレージに移動したいと考えています。企業は、オンプレミスのデータセンターとAWSの間に10 GbpsのAWS Direct Connect接続を設定しました。どの解決策が最もコスト効果的にこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share.",
        "text_jp": "新しいS3バケットを作成します。Direct Connect接続に接続されたVPC内にAWS Storage Gatewayファイルゲートウェイを展開します。新しいSMBファイル共有を作成します。毎晩データベースのエクスポートを新しいSMBファイル共有に書き込みます。"
      },
      {
        "key": "B",
        "text": "Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.",
        "text_jp": "Direct Connect接続に接続されたVPC内にAmazon FSx for Windows File ServerのSingle-AZファイルシステムを作成します。新しいSMBファイル共有を作成します。毎晩データベースエクスポートをAmazon FSxファイルシステムのSMBファイル共有に書き込みます。毎晩のバックアップを有効にします。"
      },
      {
        "key": "C",
        "text": "Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.",
        "text_jp": "Direct Connect接続に接続されたVPC内にAmazon FSx for Windows File ServerのMulti-AZファイルシステムを作成します。新しいSMBファイル共有を作成します。毎晩データベースエクスポートをAmazon FSxファイルシステムのSMBファイル共有に書き込みます。毎晩のバックアップを有効にします。"
      },
      {
        "key": "D",
        "text": "Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket.",
        "text_jp": "新しいS3バケットを作成します。Direct Connect接続に接続されたVPC内にAWS Storage Gatewayボリュームゲートウェイを展開します。新しいSMBファイル共有を作成します。毎晩データベースエクスポートをボリュームゲートウェイ上の新しいSMBファイル共有に書き込み、このデータをS3バケットに自動的にコピーします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. A Storage Gateway file gateway allows seamless integration between on-premises environments and AWS, supporting SMB for file shares.",
        "situation_analysis": "The company needs to securely and cost-effectively move large SQL Server backups to AWS S3, with a reliable connection via Direct Connect.",
        "option_analysis": "Option A is cost-effective and meets the requirements without unnecessary complexity. Option B and C involve Amazon FSx, which could be more expensive than the S3 solution. Option D adds complexity by using a volume gateway when a file gateway suffices.",
        "additional_knowledge": "Using a file gateway, daily exports can be securely transferred to the cloud while providing access to other applications.",
        "key_terminology": "Storage Gateway, AWS Direct Connect, Amazon S3, SMB, cost-effective solution.",
        "overall_assessment": "Considering the high community vote for answer A, it seems to be widely accepted as the most efficient solution for the use case presented."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。Storage Gatewayファイルゲートウェイは、オンプレミス環境とAWSのシームレスな統合を可能にし、ファイル共有のためにSMBをサポートしています。",
        "situation_analysis": "企業は、大規模なSQL ServerバックアップをAWS S3に安全かつコスト効果的に移行する必要があり、Direct Connectによる信頼性のある接続があります。",
        "option_analysis": "選択肢Aはコスト効果が高く、必要のない複雑さなしに要件を満たします。選択肢BとCはAmazon FSxを含み、S3ソリューションよりも高価になる可能性があります。選択肢Dはボリュームゲートウェイを使用することによって複雑さを加えていますが、ファイルゲートウェイで十分です。",
        "additional_knowledge": "ファイルゲートウェイを使用することで、毎日のエクスポートを安全にクラウドに転送でき、他のアプリケーションへのアクセスを提供できます。",
        "key_terminology": "Storage Gateway, AWS Direct Connect, Amazon S3, SMB, コスト効果的な解決策。",
        "overall_assessment": "選択肢Aはコミュニティ投票でも高く支持されており、提示された使用例に対して最も効率的な解決策と見なされているようです。"
      }
    ],
    "keywords": [
      "Storage Gateway",
      "AWS Direct Connect",
      "Amazon S3",
      "SMB",
      "cost-effective solution"
    ]
  },
  {
    "No": "232",
    "question": "A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are\nlocated in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound\ntrafic costs, increase bandwidth throughput, and provide a consistent network experience for end users.\nWhich solution will meet these requirements?",
    "question_jp": "企業は、オンプレミスのデータセンターからAWSへの接続を確立する必要があります。企業は、異なるAWSリージョンにあるすべてのVPCを接続し、VPCネットワーク間の経路伝達機能を持たせる必要があります。また、企業はネットワークのアウトバウンドトラフィックコストを削減し、帯域幅のスループットを増加させ、エンドユーザーに一貫したネットワーク体験を提供する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections that initiate from the central VPC to all other VPCs.",
        "text_jp": "オンプレミスのデータセンターと新しい中央VPCの間にAWS Site-to-Site VPN接続を作成します。中央VPCからすべての他のVPCへのVPCピアリング接続を作成します。"
      },
      {
        "key": "B",
        "text": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region.",
        "text_jp": "オンプレミスのデータセンターとAWSの間にAWS Direct Connect接続を作成します。トランジットVIFをプロビジョニングし、それをDirect Connectゲートウェイに接続します。Direct Connectゲートウェイを各リージョン内のすべての他のVPCに接続します。"
      },
      {
        "key": "C",
        "text": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPUse a transit gateway with dynamic routing. Connect the transit gateway to all other VPCs.",
        "text_jp": "オンプレミスのデータセンターと新しい中央VPCの間にAWS Site-to-Site VPN接続を作成し、動的ルーティングを使用したトランジットゲートウェイを設定します。トランジットゲートウェイをすべての他のVPCに接続します。"
      },
      {
        "key": "D",
        "text": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection between all VPCs in each Region. Create VPC peering connections that initiate from the central VPC to all other VPCs.",
        "text_jp": "オンプレミスのデータセンターとAWSの間にAWS Direct Connect接続を作成します。各リージョン内のすべてのVPC間にAWS Site-to-Site VPN接続を確立します。中央VPCからすべての他のVPCへのVPCピアリング接続を作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This solution provides a direct and reliable connection method that ensures the reduction of outbound traffic costs and increases bandwidth. Utilizing Direct Connect along with a transit gateway facilitates transitive routing between multiple VPCs across different regions.",
        "situation_analysis": "The company requires a robust network architecture that connects its on-premises infrastructure to multiple AWS VPCs across various regions. Key requirements include reducing outbound traffic costs and enhancing bandwidth, along with consistency in network performance for users.",
        "option_analysis": "Option A is not suitable as Site-to-Site VPN connections may not provide the necessary bandwidth and routing capabilities for multiple VPCs. Option C, while using a transit gateway, still does not utilize Direct Connect. Option D complicates the architecture further by introducing VPN connections for each VPC.",
        "additional_knowledge": "Understanding the limitations of Site-to-Site VPNs and the benefits of Direct Connect is crucial for architects designing scalable and efficient cloud networks.",
        "key_terminology": "AWS Direct Connect, transit gateway, VPC peering, Site-to-Site VPN, latency, bandwidth",
        "overall_assessment": "The answer choice B best aligns with AWS architectural principles for connecting multiple VPCs with transitive routing capabilities and reducing costs. It is also backed by community consensus, validated by the 100% voting for option B."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。このソリューションは、アウトバウンドトラフィックコストの削減と帯域幅の増加を確実にする直接的かつ信頼性のある接続方法を提供します。Direct Connectとトランジットゲートウェイを利用することで、異なるリージョン全体の複数のVPC間の経路伝達が可能になります。",
        "situation_analysis": "企業は、オンプレミスインフラストラクチャをさまざまなリージョンにある複数のAWS VPCに接続する堅牢なネットワークアーキテクチャを必要としています。主な要件には、アウトバウンドトラフィックコストの削減や帯域幅の向上、ユーザーに対するネットワーク性能の一貫性があります。",
        "option_analysis": "選択肢Aは、Site-to-Site VPN接続が複数のVPCに必要な帯域幅と経路能力を提供しないため、適切ではありません。選択肢Cは、トランジットゲートウェイを利用していますが、Direct Connectを使用していないため適していません。選択肢Dは、各VPCにVPN接続を導入してアーキテクチャを複雑にしています。",
        "additional_knowledge": "Site-to-Site VPNの限界とDirect Connectの利点を理解することは、スケーラブルで効率的なクラウドネットワークを設計するアーキテクトにとって重要です。",
        "key_terminology": "AWS Direct Connect、トランジットゲートウェイ、VPCピアリング、Site-to-Site VPN、遅延、帯域幅",
        "overall_assessment": "選択肢Bは、トランジットルーティング機能を備えた複数のVPCを接続し、コストを削減するAWSアーキテクチャ原則に最も適しています。また、コミュニティのコンセンサスも支持されており、選択肢Bに対して100%の投票が行われています。"
      }
    ],
    "keywords": [
      "AWS Direct Connect",
      "transit gateway",
      "VPC peering",
      "Site-to-Site VPN",
      "latency",
      "bandwidth"
    ]
  },
  {
    "No": "233",
    "question": "A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a\nseparate member account for development and a separate member account for production. Consolidated billing is linked to the management\naccount. In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in both member\naccounts.\nWhich solution will meet this requirement?",
    "question_jp": "ある企業が、AWS Organizationsの新しい組織に開発および本番のワークロードを移行しています。企業は開発用の別々のメンバーアカウントと、本番用の別々のメンバーアカウントを作成しました。統合請求は管理アカウントにリンクされています。管理アカウントでは、ソリューションアーキテクトが、両方のメンバーアカウントでリソースを停止または終了できるIAMユーザーを作成する必要があります。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to the member accounts.",
        "text_jp": "管理アカウントにIAMユーザーとクロスアカウントロールを作成します。クロスアカウントロールをメンバーアカウントに対する最小権限アクセスで構成します。"
      },
      {
        "key": "B",
        "text": "Create an IAM user in each member account. In the management account, create a cross-account role that has least privilege access. Grant the IAM users access to the cross-account role by using a trust policy.",
        "text_jp": "各メンバーアカウントにIAMユーザーを作成します。管理アカウントで、最小権限アクセスを持つクロスアカウントロールを作成します。IAMユーザーにトラストポリシーを使用してクロスアカウントロールへのアクセスを付与します。"
      },
      {
        "key": "C",
        "text": "Create an IAM user in the management account. In the member accounts, create an IAM group that has least privilege access. Add the IAM user from the management account to each IAM group in the member accounts.",
        "text_jp": "管理アカウントにIAMユーザーを作成します。メンバーアカウントに最小権限アクセスを持つIAMグループを作成します。管理アカウントのIAMユーザーをメンバーアカウントの各IAMグループに追加します。"
      },
      {
        "key": "D",
        "text": "Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy.",
        "text_jp": "管理アカウントにIAMユーザーを作成します。メンバーアカウントに最小権限アクセスを持つクロスアカウントロールを作成します。トラストポリシーを使用してIAMユーザーにロールへのアクセスを付与します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This option ensures that the IAM user in the management account can assume cross-account roles in the member accounts to stop or terminate resources with least privilege.",
        "situation_analysis": "The requirement is to allow an IAM user in the management account to manage resources in member accounts while adhering to the principle of least privilege.",
        "option_analysis": "Option D is correct because it effectively utilizes cross-account roles with trust policies, allowing granular access control. Options A, B, and C either involve unnecessary IAM users in member accounts or do not properly implement cross-account access.",
        "additional_knowledge": "",
        "key_terminology": "IAM, Cross-Account Role, Least Privilege, Trust Policy, AWS Organizations.",
        "overall_assessment": "This question tests knowledge of IAM configurations and best practices for resource management in AWS Organizations. The community supports D fully, indicating strong agreement with its suitability for the scenario presented."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。このオプションは、管理アカウントのIAMユーザーがメンバーアカウント内のクロスアカウントロールを引き受けて、最小権限でリソースを停止または終了できるようにすることを保証する。",
        "situation_analysis": "管理アカウントのIAMユーザーがメンバーアカウントのリソースを管理できるようにする要件があり、最小権限の原則に従う必要がある。",
        "option_analysis": "オプションDは正解で、信頼ポリシーを利用してクロスアカウントロールを効果的に活用し、細粒度のアクセス制御を提供する。オプションA、B、Cは、メンバーアカウント内に不要なIAMユーザーを含むか、適切なクロスアカウントアクセスを実装していない。",
        "additional_knowledge": "",
        "key_terminology": "IAM, クロスアカウントロール, 最小権限, 信頼ポリシー, AWS Organizations。",
        "overall_assessment": "この質問は、AWS Organizationsにおけるリソース管理のためのIAM構成とベストプラクティスの知識を試すものである。コミュニティはDを完全に支持しており、提示されたシナリオに対する適切性に強い合意がある。"
      }
    ],
    "keywords": [
      "IAM",
      "Cross-Account Role",
      "Least Privilege",
      "Trust Policy",
      "AWS Organizations"
    ]
  },
  {
    "No": "234",
    "question": "A company wants to use AWS for disaster recovery for an on-premises application. The company has hundreds of Windows-based servers that run\nthe application. All the servers mount a common share.\nThe company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support native failover and fallback capabilities.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "企業は、オンプレミスアプリケーションの災害復旧にAWSを使用したいと考えています。企業にはアプリケーションを実行する数百のWindowsベースのサーバーがあります。すべてのサーバーは共通の共有をマウントします。\n企業はRTOを15分、RPOを5分に設定しています。このソリューションは、ネイティブのフェイルオーバーおよびフェイルバック機能をサポートする必要があります。\nこれらの要件を最もコスト効果の高い方法で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Storage Gateway File Gateway. Schedule daily Windows server backups. Save the data to Amazon S3. During a disaster, recover the on-premises servers from the backup. During tailback, run the on-premises servers on Amazon EC2 instances.",
        "text_jp": "AWS Storage Gateway File Gatewayを作成し、Windowsサーバーのバックアップを毎日スケジュールします。データをAmazon S3に保存します。災害時には、バックアップからオンプレミスサーバーを復元します。テイルバック中は、オンプレミスサーバーをAmazon EC2インスタンス上で実行します。"
      },
      {
        "key": "B",
        "text": "Create a set of AWS CloudFormation templates to create infrastructure. Replicate all data to Amazon Elastic File System (Amazon EFS) by using AWS DataSync. During a disaster, use AWS CodePipeline to deploy the templates to restore the on-premises servers. Fail back the data by using DataSync.",
        "text_jp": "AWS CloudFormationテンプレートのセットを作成してインフラストラクチャを構築します。AWS DataSyncを使用して、すべてのデータをAmazon Elastic File System（Amazon EFS）に複製します。災害時には、AWS CodePipelineを使用してテンプレートを展開し、オンプレミスサーバーを復元します。データはDataSyncを使用してフェイルバックします。"
      },
      {
        "key": "C",
        "text": "Create an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-active environment on AWS. Replicate data into Amazon S3 by using the s3 sync command. During a disaster, swap DNS endpoints to point to AWS. Fail back the data by using the s3 sync command.",
        "text_jp": "AWS Cloud Development Kit（AWS CDK）パイプラインを作成して、AWS上にマルチサイトのアクティブ-アクティブ環境を立ち上げます。s3 syncコマンドを使用して、データをAmazon S3に複製します。災害時には、DNSエンドポイントをAWSを指すように切り替えます。データはs3 syncコマンドを使用してフェイルバックします。"
      },
      {
        "key": "D",
        "text": "Use AWS Elastic Disaster Recovery to replicate the on-premises servers. Replicate data to an Amazon FSx for Windows File Server file system by using AWS DataSync. Mount the file system to AWS servers. During a disaster, fail over the on-premises servers to AWS. Fail back to new or existing servers by using Elastic Disaster Recovery.",
        "text_jp": "AWS Elastic Disaster Recoveryを使用してオンプレミスサーバーを複製します。AWS DataSyncを使用してデータをAmazon FSx for Windows File Serverファイルシステムに複製します。ファイルシステムをAWSサーバーにマウントします。災害時には、オンプレミスサーバーをAWSにフェイルオーバーします。新規または既存のサーバーにElastic Disaster Recoveryを使用してフェイルバックします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Utilizing AWS CloudFormation templates, DataSync, and CodePipeline ensures the infrastructure can be quickly and effectively restored according to the RTO and RPO requirements.",
        "situation_analysis": "The company requires a disaster recovery solution with strict RTO and RPO, needing a method to handle failover and fallback effectively and ensure data availability.",
        "option_analysis": "Option A involves backups which may not meet the RPO requirements. Option C requires manual DNS switching which can delay failover. Option D offers viable solutions but may be more costly than necessary given the specific RTO/RPO targets.",
        "additional_knowledge": "AWS services are conducive to rapidly deploying and managing disaster recovery solutions.",
        "key_terminology": "AWS CloudFormation, AWS DataSync, Amazon EFS, AWS CodePipeline, disaster recovery, RTO, RPO.",
        "overall_assessment": "The community vote supports option D, but option B is the most cost-effective and meets all requirements as specified in the question. Despite community voting, the technical features of option B align perfectly with the given constraints."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。AWS CloudFormationテンプレート、DataSync、およびCodePipelineを活用することで、インフラストラクチャを迅速かつ効果的に復元でき、RTOおよびRPOの要件を満たします。",
        "situation_analysis": "企業は厳しいRTOおよびRPOを持つ災害復旧ソリューションを必要としており、フェイルオーバーとフェイルバックを効果的に処理し、データの可用性を確保する方法が必要です。",
        "option_analysis": "選択肢Aはバックアップを含むため、RPO要件を満たさない可能性があります。選択肢Cは手動のDNS切り替えが必要で、フェイルオーバーが遅れる可能性があります。選択肢Dは実行可能ですが、特定のRTO / RPOターゲットに対して必要以上にコストがかかるかもしれません。",
        "additional_knowledge": "AWSサービスは、災害復旧ソリューションの迅速な展開と管理を促進します。",
        "key_terminology": "AWS CloudFormation、AWS DataSync、Amazon EFS、AWS CodePipeline、災害復旧、RTO、RPO。",
        "overall_assessment": "コミュニティの投票は選択肢Dを支持していますが、選択肢Bは最もコスト効果が高く、質問で指定されたすべての要件を満たしています。コミュニティ投票に反して、選択肢Bの技術的特徴は与えられた制約に完璧に一致しています。"
      }
    ],
    "keywords": [
      "AWS CloudFormation",
      "AWS DataSync",
      "Amazon EFS",
      "AWS CodePipeline",
      "disaster recovery",
      "RTO",
      "RPO"
    ]
  },
  {
    "No": "235",
    "question": "A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared\nfiles stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when\nthe company increased the cluster size to 1.000 EC2 instances, overall performance was well below expectations.\nWhich collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose\nthree.)",
    "question_jp": "ある企業が、AWSで高性能計算（HPC）クラスターを構築しました。このクラスターは、大量の共有ファイルを生成し、Amazon EFSに保存される密に結合されたワークロードのために設計されています。クラスターは、100台のAmazon EC2インスタンスがあるときにはうまく機能していました。しかし、企業がクラスターのサイズを1,000台のEC2インスタンスに増やしたところ、全体のパフォーマンスが期待を大きく下回る結果となりました。HPCクラスターから最大限のパフォーマンスを引き出すために、ソリューションアーキテクトはどのような設計選択を行うべきですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Ensure the HPC cluster is launched within a single Availability Zone.",
        "text_jp": "HPCクラスターが単一のアベイラビリティゾーン内で起動されることを確認する。"
      },
      {
        "key": "B",
        "text": "Launch the EC2 instances and attach elastic network interfaces in multiples of four.",
        "text_jp": "EC2インスタンスを起動し、4の倍数でエラスティックネットワークインターフェイスをアタッチする。"
      },
      {
        "key": "C",
        "text": "Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.",
        "text_jp": "Elastic Fabric Adapter（EFA）が有効なEC2インスタンスタイプを選択する。"
      },
      {
        "key": "D",
        "text": "Ensure the cluster is launched across multiple Availability Zones.",
        "text_jp": "クラスターが複数のアベイラビリティゾーンにわたって起動されることを確認する。"
      },
      {
        "key": "E",
        "text": "Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array.",
        "text_jp": "Amazon EFSをRAIDアレイ内の複数のAmazon EBSボリュームに置き換える。"
      },
      {
        "key": "F",
        "text": "Replace Amazon EFS with Amazon FSx for Lustre.",
        "text_jp": "Amazon EFSをAmazon FSx for Lustreに置き換える。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ACF (80%) CDF (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Launching the HPC cluster within a single Availability Zone can reduce latency and improve performance by ensuring that all instances can efficiently communicate with each other.",
        "situation_analysis": "The HPC workload is significantly impacted by the network latency between instances. When scaled to 1,000 instances, keeping them in the same Availability Zone minimizes this latency, which is crucial for tightly-coupled workloads.",
        "option_analysis": "Choices B, C, D, E, and F either do not directly address the core issue of latency or do not enhance performance as expected. While EFA (choice C) is beneficial for networking in HPC, it alone does not counter the latency introduced by deploying instances across zones.",
        "additional_knowledge": "When evaluating other options for storage like Amazon EFS or EBS, consider the trade-offs between scalability and performance based on workload requirements.",
        "key_terminology": "Availability Zone, High Performance Computing, Elastic Fabric Adapter, Latency, EC2, EFS, EBS",
        "overall_assessment": "Given the performance drops observed at scale, the recommendation to keep the HPC cluster within a single Availability Zone is sound and aligns well with HPC design best practices. The community support for this choice indicates a strong consensus on this approach."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。HPCクラスターを単一のアベイラビリティゾーン内で起動することで、インスタンス間の通信が効率的になり、レイテンシが低下し、パフォーマンスが向上することが期待される。",
        "situation_analysis": "HPCワークロードは、インスタンス間のネットワークレイテンシに大きく影響される。1,000インスタンスにスケールアップする際、すべてを同じアベイラビリティゾーンに保つことで、このレイテンシを最小限に抑えることが重要である。",
        "option_analysis": "選択肢B、C、D、E、およびFは、基本的なレイテンシの問題に直接対処しないか、期待されたパフォーマンスを向上させない。EFA（選択肢C）はHPCにおけるネットワーク接続に役立つが、全インスタンスをゾーンを超えて展開した場合のレイテンシに対抗するものではない。",
        "additional_knowledge": "Amazon EFSやEBSなどのストレージオプションを評価する際は、ワークロード要件に基づくスケーラビリティとパフォーマンスのトレードオフを考慮する必要がある。",
        "key_terminology": "アベイラビリティゾーン, 高性能計算, Elastic Fabric Adapter, レイテンシ, EC2, EFS, EBS",
        "overall_assessment": "スケールアップで観察されたパフォーマンスの低下を考慮すると、HPCクラスターを単一のアベイラビリティゾーン内に保持するという推奨は妥当であり、HPC設計のベストプラクティスとよく一致する。コミュニティの支持もこのアプローチに対する強い合意を示している。"
      }
    ],
    "keywords": [
      "Availability Zone",
      "High Performance Computing",
      "Elastic Fabric Adapter",
      "Latency",
      "EC2",
      "EFS",
      "EBS"
    ]
  },
  {
    "No": "236",
    "question": "A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire\norganization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique\ntag values.\nWhich solution will meet these requirements?",
    "question_jp": "企業がAWS Organizations構造を設計しています。企業は、組織全体にわたってタグを適用するプロセスを標準化したいと考えています。企業は、新しいリソースを作成する際、特定の値を持つタグを要求します。各OU（組織単位）には、ユニークなタグ値が必要です。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.",
        "text_jp": "必要なタグを持たないリソースの作成を否定するSCP（Service Control Policy）を使用します。企業が各OUに割り当てたタグ値を含むタグポリシーを作成します。タグポリシーをOUに添付します。"
      },
      {
        "key": "B",
        "text": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the organization's management account.",
        "text_jp": "必要なタグを持たないリソースの作成を否定するSCPを使用します。企業が各OUに割り当てたタグ値を含むタグポリシーを作成します。タグポリシーを組織の管理アカウントに添付します。"
      },
      {
        "key": "C",
        "text": "Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.",
        "text_jp": "必要なタグを持つリソースのみ作成を許可するSCPを使用します。企業が各OUに割り当てたタグ値を含むタグポリシーを作成します。タグポリシーをOUに添付します。"
      },
      {
        "key": "D",
        "text": "Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs.",
        "text_jp": "必要なタグを持たないリソースの作成を否定するSCPを使用します。タグのリストを定義します。SCPをOUに添付します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (77%) B (23%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. It allows for the creation of resources only when they have the required tags, ensuring compliance with the company’s tagging standards across various OUs.",
        "situation_analysis": "The company requires standardization of tagging across its organization which includes unique tag values for each OU.",
        "option_analysis": "Option C effectively utilizes SCPs to enforce tagging compliance. Options A, B, and D do not allow resource creation based on required tags and thus fail to meet the specified requirements.",
        "additional_knowledge": "Adhering to tagging standards not only helps in resource management but is also beneficial for cost allocation and security compliance.",
        "key_terminology": "AWS Organizations, Service Control Policies, Tagging Policies",
        "overall_assessment": "Overall, option C aligns best with AWS best practices and the specific requirements stated in the prompt. Community votes may not fully reflect the best technical solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。これは、必要なタグを持つリソースのみを作成することを許可し、さまざまなOU全体で企業のタグ付け基準に従うことを保証します。",
        "situation_analysis": "企業は、組織全体でのタギングの標準化を必要としており、各OUにはユニークなタグ値が含まれています。",
        "option_analysis": "Cの選択肢は、SCPをクレジット用途に使用してタギングの遵守を強制します。選択肢A、B、Dは、必要なタグに基づくリソースの作成を許可しないため、指定された要件を満たしていません。",
        "additional_knowledge": "タグ基準を遵守することは、リソース管理に役立つだけでなく、コスト配分やセキュリティの準拠にも有益です。",
        "key_terminology": "AWS Organizations, サービス制御ポリシー, タグポリシー",
        "overall_assessment": "全体として、選択肢CはAWSのベストプラクティスおよび提示された特定の要件と最も一致しています。コミュニティの投票は、必ずしも最良の技術ソリューションを反映していない場合があります。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "Service Control Policies",
      "Tagging Policies"
    ]
  },
  {
    "No": "237",
    "question": "A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry\nTransport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket.\nRecently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new\ndesign on AWS that is highly available and scalable to prevent a similar occurrence.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業には、10,000を超えるセンサーがあり、これらのセンサーはMessage Queuing Telemetry Transport (MQTT) プロトコルを使用してオンプレミスのApache Kafkaサーバーにデータを送信しています。オンプレミスのKafkaサーバーはデータを変換し、その後結果をAmazon S3バケットにオブジェクトとして保存します。最近、Kafkaサーバーがクラッシュし、復旧中にセンサーデータが失われました。ソリューションアーキテクトは、類似の事象を防ぐために、高可用性かつスケーラブルな新しい設計をAWS上に作成する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a domain name in Amazon Route 53. Create a Route 53 failover policy. Route the sensors to send the data to the domain name.",
        "text_jp": "2つのAmazon EC2インスタンスを起動し、2つのアベイラビリティゾーンにまたがるアクティブ/スタンバイ構成でKafkaサーバーをホストします。Amazon Route 53でドメイン名を作成します。Route 53のフェイルオーバーポリシーを作成します。センサーをドメイン名にデータを送信するようにルーティングします。"
      },
      {
        "key": "B",
        "text": "Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer (NLB) that points to the Amazon MSK broker. Enable NLB health checks. Route the sensors to send the data to the NLB.",
        "text_jp": "オンプレミスのKafkaサーバーをAmazon Managed Streaming for Apache Kafka (Amazon MSK)に移行します。Network Load Balancer (NLB)を作成し、Amazon MSKブローカーを指します。NLBのヘルスチェックを有効にします。センサーをNLBにデータを送信するようにルーティングします。"
      },
      {
        "key": "C",
        "text": "Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core.",
        "text_jp": "AWS IoT Coreをデプロイし、Amazon Kinesis Data Firehoseの配信ストリームに接続します。データ変換を処理するためにAWS Lambda関数を使用します。センサーをAWS IoT Coreにデータを送信するようにルーティングします。"
      },
      {
        "key": "D",
        "text": "Deploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2 instance. Route the sensors to send the data to AWS IoT Core.",
        "text_jp": "AWS IoT Coreをデプロイし、KafkaサーバーをホストするAmazon EC2インスタンスを起動します。AWS IoT Coreを構成してデータをEC2インスタンスに送信します。センサーをAWS IoT Coreにデータを送信するようにルーティングします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (84%) B (16%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using two Amazon EC2 instances in an active/standby configuration across two Availability Zones provides high availability, and creating a failover policy in Route 53 ensures that sensor data can be routed to the available server during an outage.",
        "situation_analysis": "The primary requirement is to maintain data availability and prevent data loss during server outages. The solution needs to be scalable to handle more than 10,000 sensors.",
        "option_analysis": "Option A provides a traditional active/passive setup that can maintain service during failovers. Option B, while it uses Managed Streaming, does not specifically address the sensor direct data streaming architecture as needed. Options C and D, while using AWS services, do not implement the specific Kafka need in a high-availability context.",
        "additional_knowledge": "The increased use of managed services like Amazon MSK is often beneficial for maintenance and scalability, which may suggest additional investigations for future strategic architecture choices.",
        "key_terminology": "Apache Kafka, Route 53, high availability, failover, Amazon EC2",
        "overall_assessment": "Option A is a solid choice for a high availability setup but may require further insights into Amazon MSK for certain Kafka functionalities. Community votes may indicate preference for a managed service approach but may not align with the specific needs stated in the question."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。2つのAmazon EC2インスタンスをアクティブ/スタンバイ構成で2つのアベイラビリティゾーンに配置することにより高可用性を提供し、Route 53でフェイルオーバーポリシーを設けることで、障害時にセンサーデータを利用可能なサーバーにルーティングできる。",
        "situation_analysis": "主な要件は、サーバーの停止中にデータの可用性を維持し、データ損失を防ぐことです。このソリューションは、10,000以上のセンサーを扱うためにスケーラブルでなければならない。",
        "option_analysis": "選択肢Aは、フェイルオーバー時にサービスを維持できる従来のアクティブ/パッシブセットアップを提供する。選択肢Bは、マネージド戦略を使用しているが、センサーがどのようにデータをストリーミングする必要があるかを具体的に考慮していない。選択肢CとDはAWSサービスを利用するが、高可用性の文脈で必要とされるKafkaのニーズを実装できていない。",
        "additional_knowledge": "Amazon MSKのようなマネージドサービスの利用増加は、保守性とスケーラビリティに有益であるため、今後の戦略的アーキテクチャ選択に向けたさらなる調査が必要な場合がある。",
        "key_terminology": "Apache Kafka, Route 53, 高可用性, フェイルオーバー, Amazon EC2",
        "overall_assessment": "選択肢Aは、高可用性の設定において堅実な選択であるが、特定のKafka機能のためにはAmazon MSKへのさらなる洞察が必要となる可能性がある。コミュニティ投票はマネージドサービスアプローチに対する好みを示すかもしれないが、質問に記載された特定のニーズには一致しない可能性がある。"
      }
    ],
    "keywords": [
      "Apache Kafka",
      "Route 53",
      "high availability",
      "failover",
      "Amazon EC2"
    ]
  },
  {
    "No": "238",
    "question": "A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances. Amazon Elastic\nFile System (Amazon EFS) file systems, and Amazon RDS DB instances.\nTo meet regulatory and business requirements, the company must make the following changes for data backups:\n• Backups must be retained based on custom daily, weekly, and monthly requirements.\n• Backups must be replicated to at least one other AWS Region immediately after capture.\n• The backup solution must provide a single source of backup status across the AWS environment.\n• The backup solution must send immediate notifications upon failure of any resource backup.\nWhich combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)",
    "question_jp": "最近、ある企業が新しいアプリケーションワークロードをAWSクラウドでホスティングを開始しました。この企業は、Amazon EC2インスタンス、Amazon Elastic File System (Amazon EFS)ファイルシステム、Amazon RDS DBインスタンスを使用しています。\n規制およびビジネス要件を満たすために、企業はデータバックアップに関して次の変更を行う必要があります。\n• バックアップはカスタム日次、週次、月次要件に基づいて保持する必要がある。\n• バックアップは取得後すぐに少なくとも1つの他のAWSリージョンに複製される必要がある。\n• バックアップソリューションはAWS環境全体にわたるバックアップ状況の単一のソースを提供しなければならない。\n• バックアップソリューションは、リソースバックアップの失敗時に即座に通知を送信する必要がある。\nこれらの要件を最小限の運用負荷で満たすための手順の組み合わせは、どの3つですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Backup plan with a backup rule for each of the retention requirements.",
        "text_jp": "AWS Backupプランを作成し、保持要件ごとにバックアップルールを設定する。"
      },
      {
        "key": "B",
        "text": "Configure an AWS Backup plan to copy backups to another Region.",
        "text_jp": "AWS Backupプランを設定して、バックアップを別のリージョンにコピーする。"
      },
      {
        "key": "C",
        "text": "Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.",
        "text_jp": "AWS Lambda関数を作成してバックアップを別のリージョンに複製し、失敗が発生した場合に通知を送信する。"
      },
      {
        "key": "D",
        "text": "Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED.",
        "text_jp": "バックアッププランにAmazon Simple Notification Service (Amazon SNS)トピックを追加して、完了したジョブがBACKUP_JOB_COMPLETED以外のステータスの場合に通知を送信する。"
      },
      {
        "key": "E",
        "text": "Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements.",
        "text_jp": "保持要件ごとにAmazon Data Lifecycle Manager (Amazon DLM)スナップショットライフサイクルポリシーを作成する。"
      },
      {
        "key": "F",
        "text": "Set up RDS snapshots on each database.",
        "text_jp": "各データベースにRDSスナップショットを設定する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "ABD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is to create an AWS Backup plan with a backup rule for each retention requirement. This allows for managing backups efficiently and ensuring compliance with retention policies.",
        "situation_analysis": "The company is required to have distinct backup retention periods (daily, weekly, and monthly) while ensuring regulatory compliance and the ability to replicate backups across AWS Regions.",
        "option_analysis": "Option A directly addresses the retention requirements and can be configured with minimal overhead, while options B, C, and D are either supplemental or introduce additional management complexity.",
        "additional_knowledge": "Understanding the specifics of AWS services involved in backup management can aid in the effective implementation of these solutions.",
        "key_terminology": "AWS Backup, backup plan, retention policy, replication, notifications.",
        "overall_assessment": "Choosing option A aligns well with AWS best practices for backup management, while the community supports option B for cross-region backups. However, option C would add unnecessary complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は、保持要件ごとにバックアップルールを設定したAWS Backupプランを作成することです。これにより、バックアップを効率的に管理し、保持ポリシーを遵守することが可能になります。",
        "situation_analysis": "企業は、規制遵守とAWSリージョン間でのバックアップの複製を可能にしながら、異なるバックアップ保持期間（日次、週次、月次）を持つ必要があります。",
        "option_analysis": "選択肢Aは直接的に保持要件に応じており、運用負荷を最小限に抑えて設定できる一方、選択肢B、C、Dは補助的であったり、追加の管理複雑さを導入したりします。",
        "additional_knowledge": "バックアップ管理に関与するAWSサービスの具体的な理解は、これらのソリューションの効果的な実装に役立ちます。",
        "key_terminology": "AWS Backup、バックアッププラン、保持ポリシー、複製、通知。",
        "overall_assessment": "選択肢Aは、バックアップ管理におけるAWSのベストプラクティスに適しているため選択されます。一方、選択肢Bはリージョン間バックアップのための選択肢として支持されていますが、選択肢Cは不必要な複雑さを加えることになります。"
      }
    ],
    "keywords": [
      "AWS Backup",
      "backup plan",
      "retention policy",
      "replication",
      "notifications"
    ]
  },
  {
    "No": "239",
    "question": "A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data\nfrom a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the\ndata and provide information back to researchers. The data platform must meet the following requirements:\n• Provide near-real-time analytics of the inbound genomic data\n• Ensure the data is fiexible, parallel, and durable\n• Deliver results of processing to a data warehouse\nWhich strategy should a solutions architect use to meet these requirements?",
    "question_jp": "ある企業は、研究者が多様な人口から大量のデータサンプルを収集するのを支援するために、ゲノム情報を収集する遺伝子報告デバイスを開発しています。このデバイスは、データプラットフォームに毎秒8 KBのゲノムデータをプッシュし、データを処理して分析し、研究者に情報を提供する必要があります。データプラットフォームは以下の要件を満たす必要があります：\n• インバウンドゲノムデータのほぼリアルタイム分析を提供する\n• データがフレキシブルでパラレル、かつ耐久性があること\n• 処理結果をデータウェアハウスに提供する\nソリューションアーキテクトは、これらの要件を満たすためにどの戦略を使用すべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon RDS instance.",
        "text_jp": "Amazon Kinesis Data Firehoseを使用してインバウンドセンサーデータを収集し、Kinesisクライアントでデータを分析し、その結果をAmazon RDSインスタンスに保存する。"
      },
      {
        "key": "B",
        "text": "Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon Redshift cluster using Amazon EMR.",
        "text_jp": "Amazon Kinesis Data Streamsを使用してインバウンドセンサーデータを収集し、Kinesisクライアントでデータを分析し、その結果をAmazon EMRを使用してAmazon Redshiftクラスターに保存する。"
      },
      {
        "key": "C",
        "text": "Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon Redshift cluster.",
        "text_jp": "Amazon S3を使用してインバウンドデバイスデータを収集し、Amazon SQSからKinesisでデータを分析し、その結果をAmazon Redshiftクラスターに保存する。"
      },
      {
        "key": "D",
        "text": "Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR.",
        "text_jp": "Amazon API Gatewayを使用してリクエストをAmazon SQSキューに投入し、AWS Lambda関数でデータを分析し、その結果をAmazon EMRを使用してAmazon Redshiftクラスターに保存する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, as it meets all the requirements specified in the question.",
        "situation_analysis": "The device is pushing genomic data every second, which requires a data ingestion and processing solution that can handle continuous streams of data efficiently.",
        "option_analysis": "Option B utilizes Amazon Kinesis Data Streams, which is designed for real-time data ingestion and processing, making it suitable for the almost real-time analytics requirement. It also allows for parallel processing and can easily scale, fulfilling the flexible and durable data requirement. The data can then be sent to an Amazon Redshift cluster for analytics, which aligns with the need to deliver results to a data warehouse.",
        "additional_knowledge": "Understanding the distinctions between Kinesis Data Firehose and Kinesis Data Streams is important, as Firehose typically is used for batch uploading to data lakes rather than real-time processing.",
        "key_terminology": "Amazon Kinesis, Real-time analytics, Amazon Redshift, Amazon EMR, Data processing.",
        "overall_assessment": "Option B is well-aligned with AWS best practices for data ingestion and analytics, while other options either do not support real-time processing effectively or do not meet the requirements set forth."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBであり、これは質問で指定されたすべての要件を満たしています。",
        "situation_analysis": "デバイスは毎秒ゲノムデータをプッシュしており、これは継続的なデータストリームを効率的に処理できるデータ取り込みと処理のソリューションを必要とします。",
        "option_analysis": "オプションBは、リアルタイムデータの取り込みと処理のために設計されたAmazon Kinesis Data Streamsを利用しており、ほぼリアルタイムの分析要件に適しています。また、並列処理を可能にし、柔軟かつ耐久性のあるデータ要件を満たします。データはその後、分析のためにAmazon Redshiftクラスターに送信され、データウェアハウスに結果を提供する必要があるという要件にも合致しています。",
        "additional_knowledge": "Kinesis Data FirehoseとKinesis Data Streamsの違いを理解することが重要であり、Firehoseは通常、データレイクへのバッチアップロードに使用されるため、リアルタイム処理ではありません。",
        "key_terminology": "Amazon Kinesis、リアルタイム分析、Amazon Redshift、Amazon EMR、データ処理。",
        "overall_assessment": "オプションBは、データ取り込みと分析のためのAWSのベストプラクティスに非常によく合致しており、他のオプションは効果的なリアルタイム処理をサポートしていないか、設定された要件を満たしていません。"
      }
    ],
    "keywords": [
      "Amazon Kinesis",
      "Real-time analytics",
      "Amazon Redshift",
      "Amazon EMR",
      "Data processing"
    ]
  },
  {
    "No": "240",
    "question": "A solutions architect needs to define a reference architecture for a solution for three-tier applications with web. application, and NoSQL data\nlayers. The reference architecture must meet the following requirements:\n• High availability within an AWS Region\n• Able to fail over in 1 minute to another AWS Region for disaster recovery\n• Provide the most eficient solution while minimizing the impact on the user experience\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ソリューションアーキテクトは、WebアプリケーションとNoSQLデータレイヤーを持つ3層アプリケーションのソリューションのリファレンスアーキテクチャを定義する必要があります。リファレンスアーキテクチャは、以下の要件を満たさなければなりません。\n• AWSリージョン内での高可用性\n• 災害復旧のために、1分以内に別のAWSリージョンへフェイルオーバー可能\n• ユーザーエクスペリエンスへの影響を最小限に抑えた最も効率的なソリューションを提供\nこれらの要件を満たすために、どのステップの組み合わせが必要ですか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.",
        "text_jp": "Amazon Route 53のウェイテッドルーティングポリシーを使用して、選択した2つのリージョン間で100/0に設定します。TTLを1時間に設定します。"
      },
      {
        "key": "B",
        "text": "Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.",
        "text_jp": "Amazon Route 53のフェイルオーバールーティングポリシーを使用して、プライマリリージョンから災害復旧リージョンへのフェイルオーバーを行います。TTLを30秒に設定します。"
      },
      {
        "key": "C",
        "text": "Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.",
        "text_jp": "Amazon DynamoDB内でグローバルテーブルを使用して、データに2つの選択したリージョンでアクセスできるようにします。"
      },
      {
        "key": "D",
        "text": "Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.",
        "text_jp": "プライマリリージョンのAmazon DynamoDBテーブルからデータを60分ごとにバックアップし、その後データをAmazon S3に書き込みます。S3のクロスリージョンレプリケーションを使用して、データをプライマリリージョンから災害復旧リージョンにコピーします。災害復旧シナリオでは、スクリプトを使用してデータをDynamoDBにインポートします。"
      },
      {
        "key": "E",
        "text": "Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.",
        "text_jp": "複数のアベイラビリティゾーンにわたるWebおよびアプリケーション層のために、Auto Scalingグループを使用してホットスタンバイモデルを実装します。最小限のサーバーにはゾーンリザーブドインスタンスを使用し、追加リソースにはオンデマンドインスタンスを使用します。"
      },
      {
        "key": "F",
        "text": "Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources.",
        "text_jp": "複数のアベイラビリティゾーンにわたるWebおよびアプリケーション層のために、Auto Scalingグループを使用します。必要なリソースにはスポットインスタンスを使用します。"
      }
    ],
    "answer_key": "F",
    "community_vote_distribution": "BCE (88%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is F, which employs Auto Scaling groups using Spot Instances for web and application layers, providing dynamic resource management and cost efficiency.",
        "situation_analysis": "The architecture must achieve high availability within an AWS Region and facilitate rapid failover to another Region for disaster recovery, all while minimizing user experience disruption. Auto Scaling groups dynamically adjust capacity, maintaining performance during outages or peak demand.",
        "option_analysis": "Option B is semi-viable for rapid failover but lacks the specified efficiency focus. Options A, C, and D do not meet the high availability and disaster recovery requirements. Option E suggests Reserved Instances which may not optimize cost effectively, as Spot Instances offer a more cost-efficient solution as outlined in F.",
        "additional_knowledge": "Spot Instances can save costs significantly, especially under fluctuating workloads; coupled with Auto Scaling, they can provide an agile resource management strategy.",
        "key_terminology": "Auto Scaling, Spot Instances, high availability, disaster recovery, web applications, NoSQL.",
        "overall_assessment": "Overall, the question assesses knowledge of AWS architectural best practices and the ability to implement high availability and disaster recovery efficiently in a cloud environment. The community disproportionately favors option B, which emphasizes rapid failover, though it overlooks the efficiency aspect crucial to the requirement."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はFであり、Webおよびアプリケーション層のためにAuto Scalingグループを使用し、スポットインスタンスを利用することで、動的なリソース管理とコスト効率を提供します。",
        "situation_analysis": "アーキテクチャは、AWSリージョン内での高可用性を実現し、災害復旧のために別のリージョンへ迅速にフェイルオーバーできる必要があります。さらに、ユーザーエクスペリエンスへの影響を最小限にとどめる必要があります。Auto Scalingグループは、容量を動的に調整し、障害やピーク需要時のパフォーマンスを維持します。",
        "option_analysis": "オプションBは確かに迅速なフェイルオーバーを実現する一部として機能しますが、指定された効率性の重視が欠けています。オプションA、C、およびDは、高可用性と災害復旧の要件を満たしていません。オプションEは、ゾーンリザーブドインスタンスを提案しており、コスト最適化が不十分であるため、Fで示されたスポットインスタンスの方が適切です。",
        "additional_knowledge": "スポットインスタンスは、変動するワークロードの下でコストを大幅に削減することができ、Auto Scalingと組み合わせると、アジャイルなリソース管理戦略を提供します。",
        "key_terminology": "Auto Scaling, Spot Instances, 高可用性, 災害復旧, Webアプリケーション, NoSQL。",
        "overall_assessment": "全体として、この質問はAWSアーキテクチャのベストプラクティスに関する知識と、クラウド環境で高可用性および災害復旧を効率的に実装する能力を評価しています。コミュニティはフェイルオーバーの迅速さを強調するBを圧倒的に支持していますが、効率性の要素を無視しています。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "Spot Instances",
      "high availability",
      "disaster recovery",
      "web applications",
      "NoSQL"
    ]
  },
  {
    "No": "241",
    "question": "A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to\nconnect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-\npremises storage. Custom applications analyze this data to detect anomalies.\nThe number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not\nable to scale for peak trafic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the\nscaling challenges.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がスマート車両を製造しています。この企業は、車両データを収集するためのカスタムアプリケーションを使用しています。車両はMQTTプロトコルを使用してアプリケーションに接続します。企業は5分間隔でデータを処理します。企業はその後、車両のテレマティクスデータをオンプレミスストレージにコピーします。カスタムアプリケーションは、このデータを分析して異常を検出します。データを送信する車両の数は継続的に増加しています。新しい車両は、多くのデータを生成します。オンプレミスストレージソリューションはピークトラフィックにスケールすることができず、データ損失を引き起こしています。企業はソリューションをモダナイズし、スケーリングの課題を解決するためにAWSに移行する必要があります。どのソリューションが最小の運用負荷でこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache Kafka application to store the data in Amazon S3. Use a pretrained model in Amazon SageMaker to detect anomalies.",
        "text_jp": "AWS IoT Greengrassを使用して車両データをAmazon Managed Streaming for Apache Kafka (Amazon MSK)に送信します。Apache Kafkaアプリケーションを作成して、データをAmazon S3に保存します。異常を検出するために、Amazon SageMakerの事前トレーニングモデルを使用します。"
      },
      {
        "key": "B",
        "text": "Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies.",
        "text_jp": "AWS IoT Coreを使用して車両データを受信します。データをAmazon Kinesis Data Firehoseデリバリーストリームにルーティングするルールを設定し、データをAmazon S3に保存します。配信ストリームから読み取るAmazon Kinesis Data Analyticsアプリケーションを作成して、異常を検出します。"
      },
      {
        "key": "C",
        "text": "Use AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use the built-in machine learning transforms in AWS Glue to detect anomalies.",
        "text_jp": "AWS IoT FleetWiseを使用して車両データを収集します。データをAmazon Kinesisデータストリームに送信します。Amazon Kinesis Data Firehoseデリバリーストリームを使用して、データをAmazon S3に保存します。AWS Glueの組込機械学習変換を使用して異常を検出します。"
      },
      {
        "key": "D",
        "text": "Use Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use Amazon Lookout for Metrics to detect anomalies.",
        "text_jp": "Amazon MQ for RabbitMQを使用して車両データを収集します。データをAmazon Kinesis Data Firehoseデリバリーストリームに送信し、データをAmazon S3に保存します。Amazon Lookout for Metricsを使用して異常を検出します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (79%) C (21%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct Answer: C. The use of AWS IoT FleetWise effectively addresses the needs for collecting and processing vehicle data in a scalable manner. It minimizes operational overhead while allowing for robust data processing.",
        "situation_analysis": "The company is facing scalability issues with their current on-premises storage solution due to the increasing volume of data from new smart vehicles. AWS offers tools that can provide more scalable architectures without significant additional operational demands.",
        "option_analysis": "Option A involves manually building a Kafka application and doesn't address the operational overhead. Option B leverages Kinesis but may not fully utilize vehicle-specific features. Option D, using RabbitMQ, adds unnecessary complexity for the given use case. Option C is the best option because it utilizes AWS IoT FleetWise for vehicle data specifically, streamlining data collection and processing.",
        "additional_knowledge": "When scaling data solutions for IoT, leveraging managed services like Kinesis is essential for reducing overhead.",
        "key_terminology": "AWS IoT FleetWise, Kinesis, Amazon S3, AWS Glue, anomaly detection",
        "overall_assessment": "This question is critical for assessing understanding of AWS IoT services and their application for specific use cases like vehicle data processing. The community vote indicates an inclination toward Option B, suggesting that while there may be a preference, Option C is better aligned with the requirements presented. AWS IoT FleetWise is a tailored solution for this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解: C。AWS IoT FleetWiseを使用することで、車両データの収集と処理に対するニーズに効果的に対応し、運用オーバーヘッドを最小限に抑えながら、堅牢なデータ処理を実現します。",
        "situation_analysis": "企業は、新しいスマート車両からのデータのボリュームが増加しているため、現在のオンプレミスストレージソリューションのスケーラビリティの問題に直面しています。AWSは、顕著な追加の運用負担なしに、よりスケーラブルなアーキテクチャを提供できるツールを提供しています。",
        "option_analysis": "選択肢Aは、Kafkaアプリケーションを手動で構築する必要があり、運用オーバーヘッドに対処していません。選択肢BはKinesisを活用していますが、車両特有の機能を十分に利用していない可能性があります。選択肢DはRabbitMQを使用しており、与えられたユースケースには不必要な複雑さを追加します。選択肢Cが最良の選択肢であり、AWS IoT FleetWiseを利用して車両データを集中的に収集し、データ処理を効率化します。",
        "additional_knowledge": "IoTのデータソリューションをスケーリングする際は、運用オーバーヘッドを削減するためにKinesisのようなマネージドサービスを利用することが重要です。",
        "key_terminology": "AWS IoT FleetWise, Kinesis, Amazon S3, AWS Glue, 異常検出",
        "overall_assessment": "この質問は、特定のユースケース（車両データ処理）におけるAWS IoTサービスの理解を評価するために重要です。コミュニティの投票は選択肢Bへの傾向を示していますが、選択肢Cが提示された要件とより良く一致しているため正しい選択肢です。AWS IoT FleetWiseは、このシナリオに特化したソリューションです。"
      }
    ],
    "keywords": [
      "AWS IoT FleetWise",
      "Kinesis",
      "Amazon S3",
      "AWS Glue",
      "anomaly detection"
    ]
  },
  {
    "No": "242",
    "question": "During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it\nto an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.\nWhich solution will ensure that the credentials are appropriately secured automatically?",
    "question_jp": "監査中に、セキュリティチームは開発チームがIAMユーザーのシークレットアクセスキーをコード内に記述し、それをAWS CodeCommitリポジトリにコミットしていることを発見しました。セキュリティチームは、このセキュリティの脆弱性のインスタンスを自動的に見つけて修復したいと考えています。どのソリューションが資格情報を適切に自動的に保護することを保証しますか？",
    "choices": [
      {
        "key": "A",
        "text": "Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials",
        "text_jp": "AWS Systems Manager Run Commandを使用して夜間にスクリプトを実行し、開発インスタンス上の資格情報を検索します。見つかった場合は、AWS Secrets Managerを使用して資格情報をローテーションします。"
      },
      {
        "key": "B",
        "text": "Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS.",
        "text_jp": "スケジュールされたAWS Lambda関数を使用して、CodeCommitからアプリケーションコードをダウンロードしスキャンします。資格情報が見つかった場合は、新しい資格情報を生成し、AWS KMSに保存します。"
      },
      {
        "key": "C",
        "text": "Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user.",
        "text_jp": "Amazon Macieを構成してCodeCommitリポジトリ内の資格情報をスキャンします。資格情報が見つかった場合は、AWS Lambda関数をトリガーして資格情報を無効にし、ユーザーに通知します。"
      },
      {
        "key": "D",
        "text": "Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.",
        "text_jp": "CodeCommitトリガーを構成して、新しいコードの提出のスキャンのためにAWS Lambda関数を呼び出します。資格情報が見つかった場合は、AWS IAMでそれらを無効にし、ユーザーに通知します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. It employs AWS Systems Manager Run Command to automate the detection and rotation process for IAM user secrets.",
        "situation_analysis": "The situation involves a security vulnerability where IAM user's secret keys are mistakenly hard-coded in application code, which presents a risk if the code repository is exposed.",
        "option_analysis": "Option A leverages AWS Systems Manager for routine checks, while options B, C, and D have drawbacks such as manual intervention, potential delays, or limited scope.",
        "additional_knowledge": "Integrating automated solutions is key to maintaining security in development workflows, improving resilience against credential leakage.",
        "key_terminology": "AWS Secrets Manager, AWS Systems Manager, IAM, vulnerability management, automation.",
        "overall_assessment": "While the community heavily votes for D, A is technically more holistic as it continually checks and remediates secrets at the instance level."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。これはAWS Systems Manager Run Commandを使用してIAMユーザーのシークレットを自動的に検出し、ローテーションするプロセスを採用しています。",
        "situation_analysis": "この状況では、IAMユーザーのシークレットキーがアプリケーションコードに誤ってハードコーディングされており、コードリポジトリが露出した場合のリスクを示しています。",
        "option_analysis": "オプションAはAWS Systems Managerを利用して定期的なチェックを行いますが、オプションB、C、Dには手動介入、潜在的な遅れ、または限定的な範囲といった欠点があります。",
        "additional_knowledge": "自動化されたソリューションの統合は、開発ワークフローにおけるセキュリティを維持するための鍵であり、資格情報の漏洩に対する耐性を改善します。",
        "key_terminology": "AWS Secrets Manager、AWS Systems Manager、IAM、脆弱性管理、自動化。",
        "overall_assessment": "コミュニティはDを強く支持していますが、Aはインスタンスレベルでのシークレットの継続的なチェックと修復を行うため、技術的にはより包括的です。"
      }
    ],
    "keywords": [
      "AWS Systems Manager",
      "AWS Secrets Manager",
      "IAM",
      "vulnerability management",
      "automation"
    ]
  },
  {
    "No": "243",
    "question": "A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's\ninformation security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the\nminimum permissions necessary to function.\nTo meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.\nWhich combination of steps should the solutions architect take to implement this solution? (Choose two.)",
    "question_jp": "A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's information security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the minimum permissions necessary to function. To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application. Which combination of steps should the solutions architect take to implement this solution? (Choose two.)",
    "choices": [
      {
        "key": "A",
        "text": "Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point.",
        "text_jp": "Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point."
      },
      {
        "key": "B",
        "text": "Create an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint.",
        "text_jp": "Create an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint."
      },
      {
        "key": "C",
        "text": "Create a gateway endpoint for Amazon S3 in each application's VPConfigure the endpoint policy to allow access to an S3 access point. Specify the route table that is used to access the access point.",
        "text_jp": "Create a gateway endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point. Specify the route table that is used to access the access point."
      },
      {
        "key": "D",
        "text": "Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point.",
        "text_jp": "Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point."
      },
      {
        "key": "E",
        "text": "Create a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket.",
        "text_jp": "Create a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket."
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "AC (68%) 14% Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct steps are to create an S3 access point for each application in each AWS account and to ensure the access points are restricted to the application's VPC. This satisfies both the security policy and specific access requirements.",
        "situation_analysis": "The company needs to allow numerous applications across various AWS accounts to access a data lake while adhering to their security policy of not using public access. Minimal permissions must be granted to each application.",
        "option_analysis": "Option D directly addresses the requirements by creating specific access points for each application across different AWS accounts, enforcing VPC restrictions, and updating the bucket policy for security. Options A and C could work, but they do not encompass all applications and account structures as required. Options B and E do not meet the requirement regarding access points.",
        "additional_knowledge": "In scenarios with multiple AWS accounts, resource sharing should also be considered, and AWS Resource Access Manager (RAM) can facilitate sharing VPC endpoints.",
        "key_terminology": "S3 access points, VPC endpoint, security policy, bucket policy, permissions",
        "overall_assessment": "The community vote distribution suggests a preference for Option A, but it lacks the necessary scope to cover applications spread across multiple accounts, which is why Option D is the best choice. There is potential user confusion regarding access point configurations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しいステップは、各AWSアカウントで各アプリケーション用にS3アクセスポイントを作成し、アクセスポイントをアプリケーションのVPCに制限することです。これにより、セキュリティポリシーおよび特定のアクセス要件が満たされます。",
        "situation_analysis": "この会社は、多くのAWSアカウントにまたがる多数のアプリケーションにデータレイクにアクセスさせる必要がありますが、公共のインターネットを使用せず、各アプリケーションには必要最小限の権限を付与しなければなりません。",
        "option_analysis": "選択肢Dは、各アプリケーションに対して異なるAWSアカウントに特定のアクセスポイントを作成し、VPC制限を要求し、バケットポリシーを更新することで、これらの要件に直接対処しています。選択肢AおよびCは機能する可能性がありますが、必要とされるすべてのアプリケーションとアカウント構造を網羅していません。選択肢BおよびEはアクセスポイントに関する要件を満たしていません。",
        "additional_knowledge": "マルチアカウントシナリオでは、リソース共有も考慮すべきであり、AWSリソースアクセスマネージャー（RAM）を使用してVPCエンドポイントを共有することができます。",
        "key_terminology": "S3アクセスポイント、VPCエンドポイント、セキュリティポリシー、バケットポリシー、許可",
        "overall_assessment": "コミュニティの投票分布は選択肢Aへの好みを示唆していますが、マルチアカウントのアプリケーションをカバーするための必要な範囲が欠けているため、選択肢Dが最良の選択です。ユーザーはアクセスポイント設定に関して混乱している可能性があります。"
      }
    ],
    "keywords": [
      "S3 access points",
      "VPC endpoint",
      "security policy",
      "bucket policy",
      "permissions"
    ]
  },
  {
    "No": "244",
    "question": "A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that\nsend application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.\nThe company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring\nsolution that uses Splunk on premises. A solutions architect needs to determine how to send networking trafic to Splunk.\nHow should the solutions architect meet these requirements?",
    "question_jp": "ある企業が、データセンターとAWSとの間にハイブリッドソリューションを開発しました。この企業はAmazon VPCと、アプリケーションログをAmazon CloudWatchに送信するAmazon EC2インスタンスを使用しています。EC2インスタンスは、オンプレミスにホストされている複数の関係データベースからデータを読み取ります。この企業は、どのEC2インスタンスがほぼリアルタイムでデータベースに接続されているかを監視したいと考えています。この企業は、すでにオンプレミスでSplunkを使った監視ソリューションを持っています。ソリューションアーキテクトは、ネットワークトラフィックをSplunkに送信する方法を決定する必要があります。ソリューションアーキテクトは、どのようにしてこれらの要件を満たすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Enable VPC fiows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an Amazon S3 bucket by using the pre-defined export function. Generate ACCESS_KEY and SECRET_KEY AWS credentials. Configure Splunk to pull the logs from the S3 bucket by using those credentials.",
        "text_jp": "VPCフローLogsを有効にし、CloudWatchに送信します。AWS Lambda関数を作成し、CloudWatch Logsを定期的にAmazon S3バケットにエクスポートするために、事前定義されたエクスポート関数を使用します。ACCESS_KEYおよびSECRET_KEYのAWS認証情報を生成します。その認証情報を使用して、SplunkがS3バケットからログを取得するように設定します。"
      },
      {
        "key": "B",
        "text": "Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function with a Kinesis Data Firehose stream processor that extracts individual log events from records sent by CloudWatch Logs subscription filters. Enable VPC fiows logs, and send them to CloudWatch. Create a CloudWatch Logs subscription that sends log events to the Kinesis Data Firehose delivery stream.",
        "text_jp": "Amazon Kinesis Data Firehoseの配信ストリームを作成し、Splunkを宛先として設定します。Kinesis Data Firehoseストリームプロセッサで個々のログイベントを抽出する前処理AWS Lambda関数を設定します。VPCフローLogsを有効にし、CloudWatchに送信します。ログイベントをKinesis Data Firehoseの配信ストリームに送信するCloudWatch Logsのサブスクリプションを作成します。"
      },
      {
        "key": "C",
        "text": "Ask the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs grouped by database name. Export Athena results to another S3 bucket. Invoke an AWS Lambda function to automatically send any new file that is put in the S3 bucket to Splunk.",
        "text_jp": "企業にデータベースへのリクエストをすべてログに記録し、EC2インスタンスのIPアドレスを記録するよう依頼します。CloudWatch LogsをAmazon S3バケットにエクスポートします。Amazon Athenaを使用して、データベース名でグループ化されたログをクエリします。Athenaの結果を別のS3バケットにエクスポートします。AWS Lambda関数を呼び出して、S3バケットに新しいファイルが置かれるたびにSplunkに自動的に送信します。"
      },
      {
        "key": "D",
        "text": "Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1- minute sliding window to collect the events. Create a SQL query that uses the anomaly detection template to monitor any networking trafic anomalies in near-real time. Send the result to an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination.",
        "text_jp": "CloudWatchログをAmazon Kinesisデータストリームに送信し、Amazon Kinesis Data Analytics for SQL Applicationsを使用します。1分間のスライディングウィンドウを設定してイベントを収集します。SQLクエリを作成して、ほぼリアルタイムでネットワークトラフィックの異常を監視するために異常検出テンプレートを使用します。その結果を、Splunkを宛先とするAmazon Kinesis Data Firehoseの配信ストリームに送信します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Enabling VPC flow logs and sending them to CloudWatch allows monitoring network traffic related to EC2 instances and databases.",
        "situation_analysis": "The company needs to monitor near-real-time connection statuses of EC2 instances to the on-premises databases.",
        "option_analysis": "Option A is effective because it uses AWS features to collect and store logs, while other options may complicate the architecture or do not meet real-time monitoring requirements.",
        "additional_knowledge": "Other solutions might introduce latency or not utilize the existing infrastructure efficiently.",
        "key_terminology": "VPC flow logs, AWS Lambda, CloudWatch, Splunk, S3",
        "overall_assessment": "Considering the requirements, option A is the most straightforward and practical solution to meet the needs without unnecessary complexity."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。VPCフローログを有効にしてCloudWatchに送信することで、EC2インスタンスとデータベースに関連するネットワークトラフィックを監視できる。",
        "situation_analysis": "企業は、EC2インスタンスがオンプレミスのデータベースに接続している状況をほぼリアルタイムで監視する必要がある。",
        "option_analysis": "オプションAは、AWSの機能を利用してログを収集し、保存するため効果的で、他のオプションはアーキテクチャを複雑化させたり、リアルタイム監視の要件を満たさない可能性がある。",
        "additional_knowledge": "他のソリューションはレイテンシを導入するか、既存のインフラストラクチャを効率的に利用しない可能性がある。",
        "key_terminology": "VPCフローログ、AWS Lambda、CloudWatch、Splunk、S3",
        "overall_assessment": "要件を考慮すると、オプションAは無駄な複雑さを排除し、ニーズを満たす最も簡潔かつ実用的な解決策である。"
      }
    ],
    "keywords": [
      "VPC flow logs",
      "AWS Lambda",
      "CloudWatch",
      "Splunk",
      "S3"
    ]
  },
  {
    "No": "245",
    "question": "A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the\ndevelopment teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide\nthe information to the company's finance team.\nThe company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States.\nHowever, some resources have been created in other Regions.\nA solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the\naccounts. The solution also must ensure that the company can create resources only in Regions in the United States.\nWhich combination of steps will meet these requirements in the MOST operationally eficient way? (Choose three.)",
    "question_jp": "ある会社には、各5つのAWSアカウントを作成した5つの開発チームがあり、アプリケーションの開発とホスティングを行っています。費用を追跡するために、開発チームは毎月各アカウントにログインし、AWS Billing and Cost Managementコンソールから現在のコストを記録し、その情報を会社の財務チームに提供しています。\n会社には厳格なコンプライアンス要件があり、米国のAWSリージョンでのみリソースが作成されることを保証する必要があります。しかし、一部のリソースは他のリージョンで作成されています。\nソリューションアーキテクトは、財務チームがすべてのアカウントの支出を追跡および統合する能力を持つソリューションを実装する必要があります。このソリューションは、会社が米国のリージョンでのみリソースを作成できるようにする必要があります。\nどの手順の組み合わせが、最も運用効率の良い方法でこれらの要件を満たしますか？（3つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team's S3 bucket.",
        "text_jp": "新しいアカウントを作成して管理アカウントとして使用します。財務チームのためにAmazon S3バケットを作成します。AWSコストと使用状況レポートを使用して月次レポートを作成し、データを財務チームのS3バケットに保存します。"
      },
      {
        "key": "B",
        "text": "Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation.",
        "text_jp": "新しいアカウントを作成して管理アカウントとして使用します。すべての機能が有効になったAWS Organizationsで組織を展開します。既存のすべてのアカウントを組織に招待します。それぞれのアカウントが招待を受け入れることを確認します。"
      },
      {
        "key": "C",
        "text": "Create an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the United States. Apply the SCP to the OU.",
        "text_jp": "すべての開発チームを含むOUを作成します。米国のリージョンでのみリソースを作成することを許可するSCPを作成します。このSCPをOUに適用します。"
      },
      {
        "key": "D",
        "text": "Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU.",
        "text_jp": "すべての開発チームを含むOUを作成します。米国以外のリージョンでのリソース作成を拒否するSCPを作成します。このSCPをOUに適用します。"
      },
      {
        "key": "E",
        "text": "Create an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role. Use AWS Cost Explorer and the Billing and Cost Management console to analyze cost.",
        "text_jp": "管理アカウントにIAMロールを作成します。Billing and Cost Managementコンソールを表示するための権限を含むポリシーを添付します。財務チームのユーザーがそのロールを引き受けることを許可します。AWSコストエクスプローラーとBilling and Cost Managementコンソールを使用してコストを分析します。"
      },
      {
        "key": "F",
        "text": "Create an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role.",
        "text_jp": "各AWSアカウントにIAMロールを作成します。Billing and Cost Managementコンソールを表示するための権限を含むポリシーを添付します。財務チームのユーザーがそのロールを引き受けることを許可します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "BDE (81%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This option provides a consolidated view of expenditures and allows for efficient tracking by creating centralized reports.",
        "situation_analysis": "The company needs to track costs across multiple AWS accounts while restricting resource creation to the US regions only.",
        "option_analysis": "Option A emphasizes a centralized S3 bucket for storing cost reports, which enables easier access for the finance team. Other options fail to consolidate reporting or do not enforce region restrictions as effectively.",
        "additional_knowledge": "Creating an S3 bucket also necessitates managing permissions to ensure that only the finance team can access sensitive billing information.",
        "key_terminology": "AWS Cost and Usage Reports, S3, centralized billing, compliance, AWS Organizations.",
        "overall_assessment": "Option A's approach of using S3 for storage and centralized reporting meets the requirements efficiently but may require careful setup to ensure compliance with regional restrictions."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAである。この選択肢は支出の統合ビューを提供し、中央集約されたレポートを作成することによって効率的な追跡を可能にする。",
        "situation_analysis": "会社は、複数のAWSアカウント全体でコストを追跡する必要があり、リソース作成を米国のリージョンに制限する必要がある。",
        "option_analysis": "選択肢Aはコストレポートを保存するための中央集約化されたS3バケットを強調しており、財務チームが容易にアクセスできるようにしている。他の選択肢は、報告を統合することができないか、区域制限を効果的に強制しない。",
        "additional_knowledge": "S3バケットを作成することは、財務情報に敏感な情報にアクセスできるのは財務チームのみに管理させる必要があることも意味する。",
        "key_terminology": "AWSコストと使用状況レポート、S3、中央集約請求、コンプライアンス、AWS Organizations。",
        "overall_assessment": "選択肢AのS3を使用したストレージと中央集約された報告のアプローチは、要件を効率的に満たすが、区域制限の遵守を保証するためには慎重な設定が必要かもしれない。"
      }
    ],
    "keywords": [
      "AWS Cost and Usage Reports",
      "S3",
      "centralized billing",
      "compliance",
      "AWS Organizations"
    ]
  },
  {
    "No": "246",
    "question": "A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires\nread-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security\nteam.\nHow should a solutions architect meet these requirements?",
    "question_jp": "ある企業が、中央の場所から複数のAWSアカウントを作成し管理する必要があります。セキュリティチームは、自分のAWSアカウントからすべてのアカウントへの読み取り専用アクセスを要求しています。この企業はAWS Organizationsを利用しており、セキュリティチームのためのアカウントを作成しました。\nソリューションアーキテクトは、どのようにしてこれらの要件を満たすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a trust relationship between the IAM policy in each member account and the security account. Ask the security team to use the IAM policy to gain access.",
        "text_jp": "OrganizationAccountAccessRole IAMロールを使用して、各メンバーアカウントに読み取り専用アクセスを持つ新しいIAMポリシーを作成します。各メンバーアカウントのIAMポリシーとセキュリティアカウントの間に信頼関係を確立します。セキュリティチームにIAMポリシーを使用してアクセスさせます。"
      },
      {
        "key": "B",
        "text": "Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.",
        "text_jp": "OrganizationAccountAccessRole IAMロールを使用して、各メンバーアカウントに読み取り専用アクセスを持つ新しいIAMロールを作成します。各メンバーアカウントのIAMロールとセキュリティアカウントの間に信頼関係を確立します。セキュリティチームにIAMロールを使用してアクセスさせます。"
      },
      {
        "key": "C",
        "text": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the management account from the security account. Use the generated temporary credentials to gain access.",
        "text_jp": "セキュリティチームに、AWS Security Token Service（AWS STS）を使用して、セキュリティアカウントから管理アカウントのOrganizationAccountAccessRole IAMロールに対してAssumeRole APIを呼び出すように依頼します。生成された一時的な認証情報を使用してアクセスします。"
      },
      {
        "key": "D",
        "text": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the member account from the security account. Use the generated temporary credentials to gain access.",
        "text_jp": "セキュリティチームに、AWS Security Token Service（AWS STS）を使用して、セキュリティアカウントからメンバーアカウントのOrganizationAccountAccessRole IAMロールに対してAssumeRole APIを呼び出すように依頼します。生成された一時的な認証情報を使用してアクセスします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. This option creates a new IAM role with read-only access in each member account and establishes a trust relationship with the security account, allowing read-only access by the security team.",
        "situation_analysis": "The company needs centralized control over multiple AWS accounts and requires read-only access for the security team to monitor all accounts without the ability to make changes.",
        "option_analysis": "Option A suggests creating an IAM policy, but the requirement is to create an IAM role for cross-account access. Options C and D do not correctly secure access through a role established in each member account with read-only permissions.",
        "additional_knowledge": "Establishing trust relationships between IAM roles and accounts is crucial for secure and effective access management.",
        "key_terminology": "AWS Organizations, IAM Roles, OrganizationAccountAccessRole, Security Token Service (STS), Cross-account access",
        "overall_assessment": "The question effectively tests knowledge of AWS Identity and Access Management in conjunction with AWS Organizations. The answer the community supports aligns with best practices for account management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBである。この選択肢は、各メンバーアカウントに読み取り専用アクセスを持つ新しいIAMロールを作成し、セキュリティアカウントとの信頼関係を確立することで、セキュリティチームによる読み取り専用アクセスを可能にする。",
        "situation_analysis": "この企業は中央集権的な管理を必要としており、セキュリティチームがすべてのアカウントを監視できるように、変更を加えることなく読み取り専用アクセスを必要としている。",
        "option_analysis": "選択肢AはIAMポリシーを作成することを提案するが、要件は各メンバーアカウントに対するクロスアカウントアクセスのためのIAMロールを作成することである。選択肢CとDは、各メンバーアカウントに確立されたロールを介して読み取り専用権限を用いたアクセスを正しく保護していない。",
        "additional_knowledge": "IAMロールとアカウント間の信頼関係を確立することは、安全で効果的なアクセス管理にとって重要である。",
        "key_terminology": "AWS Organizations、IAMロール、OrganizationAccountAccessRole、Security Token Service (STS)、クロスアカウントアクセス",
        "overall_assessment": "この問題は、AWS Identity and Access ManagementとAWS Organizationsを連携させた知識を効果的にテストしている。コミュニティが支持する答えは、アカウント管理のベストプラクティスに一致している。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "IAM Roles",
      "OrganizationAccountAccessRole",
      "Security Token Service",
      "Cross-account access"
    ]
  },
  {
    "No": "247",
    "question": "A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private\nsubnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the\ninternet from the private subnets.\nA solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route trafic to the internet through an\negress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.\nWhich set of additional steps should the solutions architect take to meet these requirements?",
    "question_jp": "大企業は、数百のAWSアカウントに展開されたVPCでワークロードを実行しています。各VPCは、複数のアベイラビリティーゾーンにまたがるパブリックサブネットとプライベートサブネットで構成されています。NATゲートウェイはパブリックサブネットに展開されており、プライベートサブネットからインターネットへのアウトバウンド接続を許可しています。ソリューションアーキテクトはハブアンドスポークデザインに取り組んでおり、スポークVPC内のすべてのプライベートサブネットは、出口VPCを介してインターネットにトラフィックをルーティングする必要があります。ソリューションアーキテクトはすでに中央AWSアカウントに出口VPC内にNATゲートウェイを展開しています。これらの要件を満たすために、ソリューションアーキテクトが取るべき追加のステップは何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.",
        "text_jp": "出口VPCとスポークVPC間にピアリング接続を作成します。インターネットアクセスを許可するために必要なルーティングを構成します。"
      },
      {
        "key": "B",
        "text": "Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet.",
        "text_jp": "トランジットゲートウェイを作成し、既存のAWSアカウントと共有します。既存のVPCをトランジットゲートウェイにアタッチします。インターネットアクセスを許可するために必要なルーティングを構成します。"
      },
      {
        "key": "C",
        "text": "Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet.",
        "text_jp": "すべてのアカウントにトランジットゲートウェイを作成します。NATゲートウェイをトランジットゲートウェイにアタッチします。インターネットアクセスを許可するために必要なルーティングを構成します。"
      },
      {
        "key": "D",
        "text": "Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.",
        "text_jp": "出口VPCとスポークVPC間にAWS PrivateLink接続を作成します。インターネットアクセスを許可するために必要なルーティングを構成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating peering connections between the egress VPC and the spoke VPCs allows the private subnets to route their internet traffic through the NAT gateway efficiently.",
        "situation_analysis": "The requirement is to route traffic from private subnets in spoke VPCs to the internet via a NAT gateway in an egress VPC. The existing network architecture involves multiple AWS accounts and VPCs.",
        "option_analysis": "Option A provides a direct method for routing traffic efficiently through peering connections. Option B introduces complexity with a transit gateway that may not be necessary for this scenario. Option C proposes additional transit gateways in every account, which is redundant. Option D relies on AWS PrivateLink, which is generally used for private connections and does not facilitate internet access directly.",
        "additional_knowledge": "Using NAT gateways is essential in scenarios where resources in private subnets need outbound internet access.",
        "key_terminology": "VPC Peering, NAT Gateway, AWS Architecture",
        "overall_assessment": "Option A is the most straightforward solution, aligning with AWS best practices for interconnecting VPCs and managing outbound internet traffic."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAです。出口VPCとスポークVPC間にピアリング接続を作成することで、プライベートサブネットがインターネットトラフィックをNATゲートウェイを通じて効率よくルーティングできます。",
        "situation_analysis": "要件は、スポークVPC内のプライベートサブネットから、出口VPC内のNATゲートウェイを介してインターネットにトラフィックをルーティングすることです。既存のネットワークアーキテクチャは、複数のAWSアカウントとVPCを含んでいます。",
        "option_analysis": "選択肢Aは、効率的にトラフィックをルーティングするための直接的な方法を提供します。選択肢Bは、トランジットゲートウェイを使用して複雑さを導入しますが、このシナリオには不要かもしれません。選択肢Cは、すべてのアカウントに余分なトランジットゲートウェイを提案しており、冗長です。選択肢DはAWS PrivateLinkに依存しており、プライベート接続に一般的に使用され、インターネットアクセスを直接提供しません。",
        "additional_knowledge": "NATゲートウェイの使用は、プライベートサブネット内のリソースが外向けにインターネットアクセスを必要とするシナリオでは不可欠です。",
        "key_terminology": "VPCピアリング、NATゲートウェイ、AWSアーキテクチャ",
        "overall_assessment": "選択肢Aは、AWSのベストプラクティスに沿った最も便利で効率的な解決策です。VPCを相互接続し、出力インターネットトラフィックを管理するのに役立ちます。"
      }
    ],
    "keywords": [
      "VPC Peering",
      "NAT Gateway",
      "AWS Architecture"
    ]
  },
  {
    "No": "248",
    "question": "An education company is running a web application used by college students around the world. The application runs in an Amazon Elastic\nContainer Service (Amazon ECS) cluster in an Auto Scaling group behind an Application Load Balancer (ALB). A system administrator detects a\nweekly spike in the number of failed login attempts, which overwhelm the application's authentication service. All the failed login attempts\noriginate from about 500 different IP addresses that change each week. A solutions architect must prevent the failed login attempts from\noverwhelming the authentication service.\nWhich solution meets these requirements with the MOST operational eficiency?",
    "question_jp": "教育会社が、世界中の大学生に使用されるウェブアプリケーションを運営しています。このアプリケーションは、アプリケーションロードバランサー（ALB）の背後にあるオートスケーリンググループの中のAmazon Elastic Container Service（Amazon ECS）クラスターで実行されています。システム管理者は、失敗したログイン試行の数が毎週急増することを検出しました。これにより、アプリケーションの認証サービスが圧倒されています。すべての失敗したログイン試行は、毎週変わる約500の異なるIPアドレスから発生しています。ソリューションアーキテクトは、失敗したログイン試行が認証サービスを圧倒するのを防がなければなりません。最も運用効率の高い要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses.",
        "text_jp": "AWS Firewall Managerを使用してセキュリティグループとセキュリティグループポリシーを作成し、IPアドレスからのアクセスを拒否します。"
      },
      {
        "key": "B",
        "text": "Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB.",
        "text_jp": "AWS WAFウェブACLを作成し、レートベースのルールを設定し、ルールアクションをブロックにします。ウェブACLをALBに接続します。"
      },
      {
        "key": "C",
        "text": "Use AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges.",
        "text_jp": "AWS Firewall Managerを使用してセキュリティグループとセキュリティグループポリシーを作成し、特定のCIDR範囲へのアクセスのみを許可します。"
      },
      {
        "key": "D",
        "text": "Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB.",
        "text_jp": "AWS WAFウェブACLを作成し、IPセットマッチルールを設定し、ルールアクションをブロックにします。ウェブACLをALBに接続します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Using a rate-based rule in AWS WAF allows for dynamic blocking of IPs that exceed defined request thresholds, providing operational efficiency in mitigating login attack attempts.",
        "situation_analysis": "The application experiences weekly spikes in failed logins, which creates a significant burden on the authentication service.",
        "option_analysis": "Option B allows for automated blocking of bad actors, while options A, C, and D do not address the dynamic nature of the IPs sufficiently or do not provide efficient mitigation strategies.",
        "additional_knowledge": "Understanding how AWS WAF can be integrated with ALB adds a layer of security while allowing for effective traffic management.",
        "key_terminology": "AWS WAF, rate-based rules, Application Load Balancer, IP addresses, security group.",
        "overall_assessment": "Given the high volume of failed login attempts from changing IP addresses, B is the best choice for keeping the authentication service operationally efficient while mitigating risk."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです。AWS WAFにおけるレートベースのルールを使用することで、設定されたリクエストの閾値を超えるIPを動的にブロックすることが可能になり、効果的にログイン攻撃を軽減しながら運用効率を向上させることができます。",
        "situation_analysis": "アプリケーションは毎週失敗したログインの急増に直面しており、これは認証サービスに大きな負担をかけています。",
        "option_analysis": "オプションBは悪意のあるアクターの自動ブロックを可能にしますが、オプションA、C、DはIPの動的な性質に十分に対処していないか、効果的な軽減戦略を提供していません。",
        "additional_knowledge": "AWS WAFがALBとどのように統合できるかを理解すると、効果的なトラフィック管理をしながらセキュリティを強化できます。",
        "key_terminology": "AWS WAF, レートベースのルール, アプリケーションロードバランサー, IPアドレス, セキュリティグループ。",
        "overall_assessment": "変化するIPアドレスからの高頻度の失敗したログイン試行を考慮すると、Bが認証サービスを運用効率よく維持しつつ、リスクを軽減するための最適な選択肢となります。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "rate-based rules",
      "Application Load Balancer",
      "IP addresses",
      "security group"
    ]
  },
  {
    "No": "249",
    "question": "A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public\nSFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for\noutbound trafic. Changes to the SFTP endpoint IP addresses are not permitted.\nThe company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.\nWhich solution meets these requirements?",
    "question_jp": "ある企業は、毎日複数のファイルを取り込むオンプレミスのソフトウェア・アズ・ア・サービス（SaaS）ソリューションを運用しています。この企業は、顧客がファイル転送を円滑に行えるように、複数の公開SFTPエンドポイントを提供しています。顧客は、ファイアウォールの許可リストにSFTPエンドポイントのIPアドレスを追加します。SFTPエンドポイントのIPアドレスの変更は許可されていません。この企業は、SaaSソリューションをAWSに移行し、ファイル転送サービスの運用負担を軽減したいと考えています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWS Transfer to store the files in Amazon S3.",
        "text_jp": "顧客所有のIPアドレスのブロックを企業のAWSアカウントに登録します。アドレスプールからElastic IPアドレスを作成し、それらをAWS Transfer for SFTPエンドポイントに割り当てます。AWS Transferを使用してファイルをAmazon S3に保存します。"
      },
      {
        "key": "B",
        "text": "Add a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign them to an Application Load Balancer (ALB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the ALStore the files in attached Amazon Elastic Block Store (Amazon EBS) volumes.",
        "text_jp": "顧客所有のIPアドレスのブロックを含むサブネットをVPCに追加します。アドレスプールからElastic IPアドレスを作成し、それらをアプリケーションロードバランサー（ALB）に割り当て、EC2インスタンスをAuto Scalingグループの背後で実行し、FTPサービスをホスティングします。ファイルは接続されたAmazon Elastic Block Store（Amazon EBS）ボリュームに保存します。"
      },
      {
        "key": "C",
        "text": "Register the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load Balancer (NLB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the NLB. Store the files in Amazon S3.",
        "text_jp": "顧客所有のIPアドレスのブロックをAmazon Route 53に登録します。Route 53で、Network Load Balancer（NLB）を指すエイリアスレコードを作成します。EC2インスタンスをAuto Scalingグループの背後で実行し、FTPサービスをホスティングします。ファイルはAmazon S3に保存します。"
      },
      {
        "key": "D",
        "text": "Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and assign them to an Amazon S3 VPC endpoint. Enable SFTP support on the S3 bucket.",
        "text_jp": "顧客所有のIPアドレスのブロックを企業のAWSアカウントに登録します。アドレスプールからElastic IPアドレスを作成し、それらをAmazon S3のVPCエンドポイントに割り当てます。S3バケットでSFTPサポートを有効にします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution allows the company to use a VPC endpoint for S3 with SFTP support, adhering to the requirement of maintaining the same IP addresses for SFTP endpoints.",
        "situation_analysis": "The company's requirement is to migrate the SaaS solution to AWS without changing the IP addresses that customers use to access the SFTP endpoints.",
        "option_analysis": "Option D directly meets the requirement of not changing IP addresses while enabling SFTP access to S3. Option A uses AWS Transfer for SFTP but does not specify the VPC endpoint. Option B and C suggest using other AWS infrastructure which may lead to IP address changes or additional overhead.",
        "additional_knowledge": "AWS documentation details how to set up VPC endpoints for various services and the importance of keeping static IP addresses when required.",
        "key_terminology": "Amazon S3, VPC endpoint, SFTP support, AWS Transfer for SFTP, Elastic IP",
        "overall_assessment": "The community vote supporting A indicates some disagreement on operational efficiency compared to D, which is valid according to requirements."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。このソリューションは、SFTPサポートを有効にしたS3用のVPCエンドポイントを使用できるため、SFTPエンドポイントのIPアドレスを変更せずに顧客の要件に適応できる。",
        "situation_analysis": "企業の要件は、顧客がSFTPエンドポイントにアクセスする際に使用するIPアドレスを変更せずに、SaaSソリューションをAWSに移行することである。",
        "option_analysis": "選択肢Dは、IPアドレスを変更せずにSFTPアクセスをS3に対して有効にする要件に直接対応している。選択肢AはAWS Transfer for SFTPを使用するが、VPCエンドポイントについては明記していない。選択肢BおよびCは、他のAWSインフラを使用することを示唆しており、IPアドレスの変更や追加の運用負担を招く可能性がある。",
        "additional_knowledge": "AWSドキュメントでは、様々なサービスのためのVPCエンドポイントを設定する方法と、必要に応じて静的IPアドレスを保持する重要性について詳述されている。",
        "key_terminology": "Amazon S3, VPCエンドポイント, SFTPサポート, AWS Transfer for SFTP, Elastic IP",
        "overall_assessment": "コミュニティ票でAが支持されていることは、Dに対する運用効率に関するいくつかの意見の相違を示しているが、要件に従った正当なものである。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "VPC endpoint",
      "SFTP support",
      "AWS Transfer for SFTP",
      "Elastic IP"
    ]
  },
  {
    "No": "250",
    "question": "A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high-\nthroughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the\napplication to be fault tolerant.\nWhich solution will meet these requirements?",
    "question_jp": "会社は、1つのAWSリージョンで5つのAmazon EC2インスタンスで実行する必要がある新しいアプリケーションを持っています。このアプリケーションは、高スループットで低レイテンシのネットワーク接続を、アプリケーションが実行されるすべてのEC2インスタンス間で必要としています。アプリケーションに耐障害性の要件はありません。この要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.",
        "text_jp": "5つの新しいEC2インスタンスをクラスター配置グループに起動します。EC2インスタンスタイプが強化されたネットワーキングをサポートすることを確認してください。"
      },
      {
        "key": "B",
        "text": "Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each EC2 instance.",
        "text_jp": "同じアベイラビリティーゾーンにオートスケーリンググループ内に5つの新しいEC2インスタンスを起動します。各EC2インスタンスに追加のElasticネットワークインターフェースを接続します。"
      },
      {
        "key": "C",
        "text": "Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking.",
        "text_jp": "パーティション配置グループに5つの新しいEC2インスタンスを起動します。EC2インスタンスタイプが強化されたネットワーキングをサポートすることを確認してください。"
      },
      {
        "key": "D",
        "text": "Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance.",
        "text_jp": "スプレッド配置グループに5つの新しいEC2インスタンスを起動します。各EC2インスタンスに追加のElasticネットワークインターフェースを接続します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is 'A'. Launching instances in a cluster placement group allows for the highest networking throughput and lowest latency between those instances, provided they support enhanced networking.",
        "situation_analysis": "The application requires high-throughput and low-latency connections, which are paramount in scenarios like data processing or real-time analytics.",
        "option_analysis": "Option A enables cluster placement offering the right environment for high performance, while options B, C, and D do not ensure the same level of network performance due to their nature of scalability and distribution.",
        "additional_knowledge": "Instances must be of the type that supports enhanced networking such as those using Elastic Network Adapter (ENA) or Intel 82599 Virtual Function.",
        "key_terminology": "Amazon EC2, Cluster Placement Group, Enhanced Networking, Low Latency, High Throughput",
        "overall_assessment": "Given the requirement for high throughput and low latency, Option A is supremely aligned with AWS best practices for such applications."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは「A」です。クラスター配置グループでインスタンスを起動することにより、それらのインスタンス間で最高のネットワークスループットと最低のレイテンシを実現できます。ただし、強化されたネットワーキングをサポートしている必要があります。",
        "situation_analysis": "アプリケーションは、高スループットと低レイテンシの接続を必要としており、これはデータ処理やリアルタイム分析などのシナリオで非常に重要です。",
        "option_analysis": "選択肢Aは、パフォーマンスのために必要なクラスター配置を可能にしますが、選択肢B、C、Dはスケーラビリティおよび配分の性質によって、同じレベルのネットワークパフォーマンスを保証しません。",
        "additional_knowledge": "インスタンスは、Elastic Network Adapter (ENA) や Intel 82599 Virtual Function を使用するなど、強化されたネットワーキングをサポートするタイプでなければなりません。",
        "key_terminology": "Amazon EC2, クラスター配置グループ, 強化されたネットワーキング, 低レイテンシ, 高スループット",
        "overall_assessment": "高スループットと低レイテンシの要件を考慮すると、選択肢AはこのようなアプリケーションにAWSのベストプラクティスに非常に合致しています。"
      }
    ],
    "keywords": [
      "Amazon EC2",
      "Cluster Placement Group",
      "Enhanced Networking",
      "Low Latency",
      "High Throughput"
    ]
  },
  {
    "No": "251",
    "question": "A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon\nAPI Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.\nAfter initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The\ncompany believes this trafic is originating from a botnet and wants to secure its API while minimizing cost.\nWhich approach should the company take to secure its API?",
    "question_jp": "ある企業が、アメリカに拠点を置く6つのパートナーと情報を共有するためのREST APIを作成しています。この企業は、Amazon API Gatewayのリージョナルエンドポイントを作成しました。6つのパートナーは、日々の売上数字を投稿するために、APIに1日に1回アクセスします。初期展開後、企業は世界中の500の異なるIPアドレスから発信される1,000リクエスト/秒を観察します。この企業は、このトラフィックがボットネットから発生していると考え、コストを最小限に抑えつつAPIを保護したいと考えています。企業はAPIをどのように保護すべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Configure CloudFront with an origin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only the OAI can run the POST method.",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、APIをオリジンとして設定します。クライアントが1日に5件以上のリクエストを送信するのをブロックするルールを持つAWS WAFウェブACLを作成します。ウェブACLをCloudFrontディストリビューションに関連付けます。CloudFrontをオリジンアクセスアイデンティティ（OAI）で構成し、それをディストリビューションに関連付けます。API Gatewayを設定して、OAIのみがPOSTメソッドを実行できるようにします。"
      },
      {
        "key": "B",
        "text": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Add a custom header to the CloudFront distribution populated with an API key. Configure the API to require an API key on the POST method.",
        "text_jp": "Amazon CloudFrontディストリビューションを作成し、APIをオリジンとして設定します。クライアントが1日に5件以上のリクエストを送信するのをブロックするルールを持つAWS WAFウェブACLを作成します。ウェブACLをCloudFrontディストリビューションに関連付けます。APIキーでポピュレートされたカスタムヘッダーをCloudFrontディストリビューションに追加します。APIがPOSTメソッドでAPIキーを要求するように構成します。"
      },
      {
        "key": "C",
        "text": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a resource policy with a request limit and associate it with the API. Configure the API to require an API key on the POST method.",
        "text_jp": "AWS WAFウェブACLを作成し、6つのパートナーが使用するIPアドレスへのアクセスを許可するルールを持たせます。ウェブACLをAPIに関連付けます。リクエスト制限を持つリソースポリシーを作成し、APIに関連付けます。APIがPOSTメソッドでAPIキーを要求するように構成します。"
      },
      {
        "key": "D",
        "text": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan.",
        "text_jp": "AWS WAFウェブACLを作成し、6つのパートナーが使用するIPアドレスへのアクセスを許可するルールを持たせます。ウェブACLをAPIに関連付けます。リクエスト制限を持つ使用プランを作成し、APIに関連付けます。APIキーを作成し、それを使用プランに追加します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. This option effectively restricts access to the API based on the known IP addresses of the trusted partners and implements a resource policy to control request limits.",
        "situation_analysis": "The company needs to secure its API against an unusual rise in traffic, which is believed to be coming from a botnet. It needs to ensure that only trusted partners can access the API while also controlling the request limits.",
        "option_analysis": "Option A suggests using an OAI with CloudFront, but does not address limiting requests based on trusted partner IPs directly. Option B introduces an API key but fails to restrict access by IP. Option D is similar to C with a usage plan but might add unnecessary complexity when the WAF can directly manage IP access.",
        "additional_knowledge": "Implementation of additional security measures and monitoring can further enhance the API's resilience against undesired traffic.",
        "key_terminology": "AWS WAF, IP address filtering, resource policy, request limits, API Gateway",
        "overall_assessment": "Option C is well-aligned with best practices for API security, allowing only authorized IP access. While the community vote leaned towards option D, which introduces additional complexity, option C remains the preferred solution for its simplicity and effectiveness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい選択肢はCである。この選択肢は、信頼できるパートナーの既知のIPアドレスに基づいてAPIへのアクセスを効果的に制限し、リクエストの制限を管理するリソースポリシーを実装している。",
        "situation_analysis": "企業は、ボットネットからのものであると考えられるトラフィックの異常上昇からAPIを保護する必要がある。信頼できるパートナーのみがAPIにアクセスできるようにする一方、リクエストの制限を管理する必要がある。",
        "option_analysis": "選択肢AはCloudFrontとOAIを使用することを示唆しているが、信頼できるパートナーのIPに基づくリクエストの制限に直接対処していない。選択肢BはAPIキーを導入するが、IPによるアクセス制限を行わない。選択肢DはCに似ているが、使用プランの追加は後者の方が簡潔で効果的であるのに対し、不必要な複雑さを加えるかもしれない。",
        "additional_knowledge": "追加のセキュリティ対策と監視の実施は、APIの不要なトラフィックに対する耐性をさらに強化することができる。",
        "key_terminology": "AWS WAF、IPアドレスフィルタリング、リソースポリシー、リクエスト制限、API Gateway",
        "overall_assessment": "選択肢CはAPIセキュリティのベストプラクティスに適合しており、認証済みのIPのアクセスを許可する。コミュニティの投票は選択肢Dに傾いているが、Dは追加の複雑性をもたらし、選択肢Cがそのシンプルさと効果のために好まれるソリューションである。"
      }
    ],
    "keywords": [
      "AWS WAF",
      "IP address filtering",
      "resource policy",
      "request limits",
      "API Gateway"
    ]
  },
  {
    "No": "252",
    "question": "A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor\nall data activity on all the databases.\nWhich solution will achieve this goal?",
    "question_jp": "会社は、単一のAWSリージョン内でアプリケーション用にAmazon Aurora PostgreSQL DBクラスターを使用しています。会社のデータベースチームは、すべてのデータベースにおけるすべてのデータアクティビティを監視する必要があります。この目標を達成するためのソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch Service cluster for further analysis.",
        "text_jp": "AWSデータベース移行サービス（AWS DMS）の変更データキャプチャ（CDC）タスクを設定します。ソースとしてAurora DBクラスターを指定します。ターゲットとしてAmazon Kinesis Data Firehoseを指定します。Kinesis Data Firehoseを使用してデータをAmazon OpenSearch Serviceクラスターにアップロードし、さらに分析します。"
      },
      {
        "key": "B",
        "text": "Start a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program the Lambda function to decrypt the messages from EventBridge and to publish all database activity to Amazon S3 for further analysis.",
        "text_jp": "Aurora DBクラスターでデータベースアクティビティストリームを開始し、アクティビティストリームをAmazon EventBridgeにキャプチャします。EventBridgeのターゲットとしてAWS Lambda関数を定義します。Lambda関数をプログラムしてEventBridgeからのメッセージを復号化し、すべてのデータベースアクティビティをAmazon S3に公開します。"
      },
      {
        "key": "C",
        "text": "Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis.",
        "text_jp": "Aurora DBクラスターでデータベースアクティビティストリームを開始し、アクティビティストリームをAmazon Kinesisデータストリームにプッシュします。Amazon Kinesis Data Firehoseを設定してKinesisデータストリームを消費し、データをAmazon S3に配信します。"
      },
      {
        "key": "D",
        "text": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. Run queries on the Amazon Redshift data to determine database activities on the Aurora database.",
        "text_jp": "AWSデータベース移行サービス（AWS DMS）の変更データキャプチャ（CDC）タスクを設定します。ソースとしてAurora DBクラスターを指定します。ターゲットとしてAmazon Kinesis Data Firehoseを指定します。Kinesis Data Firehoseを使用してデータをAmazon Redshiftクラスターにアップロードします。Amazon Redshiftデータ上でクエリを実行して、Auroraデータベースのデータベースアクティビティを特定します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This option effectively uses AWS DMS to track changes from the Aurora database and sends them to Amazon Redshift for analysis.",
        "situation_analysis": "The company needs to monitor all database activities across a single Aurora PostgreSQL cluster, necessitating a solution that captures changes and enables analysis.",
        "option_analysis": "Option A uses OpenSearch Service, which is not the most efficient way for structured data analysis. Option B captures activity through EventBridge but does not effectively consolidate data for analytical queries since it's sent to S3 without subsequent processing. Option C sends the data to S3 but lacks Redshift's querying capabilities. Option D leverages AWS DMS to ensure all changes are tracked and analyzed in a structured way using Redshift.",
        "additional_knowledge": "",
        "key_terminology": "Amazon Aurora, AWS DMS, Amazon Redshift, data capture, change data capture",
        "overall_assessment": "Although community votes are in favor of option C (100%), option D is more robust because it provides a structured way to analyze the changes made to the Aurora databases."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。この選択肢は、AWS DMSを使用してAuroraデータベースの変更を追跡し、データをAmazon Redshiftに送信して分析する。",
        "situation_analysis": "会社は単一のAurora PostgreSQLクラスター全体でデータベースアクティビティを監視する必要があり、変更をキャプチャし分析を可能にするソリューションが求められている。",
        "option_analysis": "選択肢AはOpenSearch Serviceを使用するが、構造化データの分析には最も効率的ではない。選択肢BはEventBridgeを介してアクティビティをキャプチャするが、データはS3に送信され、その後の処理がないため、分析に効果的にデータを統合していない。選択肢CはデータをS3に送信するが、Redshiftのクエリ機能が不足している。選択肢Dは、AWS DMSを活用してすべての変更が追跡され、Redshiftを使用した構造化分析が可能である。",
        "additional_knowledge": "",
        "key_terminology": "Amazon Aurora、AWS DMS、Amazon Redshift、データキャプチャ、変更データキャプチャ",
        "overall_assessment": "コミュニティの投票は選択肢C（100％）に支持が集まっているが、選択肢DはAuroraデータベースの変更を分析できる構造化された方法を提供するため、より堅牢である。"
      }
    ],
    "keywords": [
      "Amazon Aurora",
      "AWS DMS",
      "Amazon Redshift",
      "data capture",
      "change data capture"
    ]
  },
  {
    "No": "253",
    "question": "An entertainment company recently launched a new game. To ensure a good experience for players during the launch period, the company\ndeployed a static quantity of 12 r6g.16xlarge (memory optimized) Amazon EC2 instances behind a Network Load Balancer. The company's\noperations team used the Amazon CloudWatch agent and a custom metric to include memory utilization in its monitoring strategy.\nAnalysis of the CloudWatch metrics from the launch period showed consumption at about one quarter of the CPU and memory that the company\nexpected. Initial demand for the game has subsided and has become more variable. The company decides to use an Auto Scaling group that\nmonitors the CPU and memory consumption to dynamically scale the instance fieet. A solutions architect needs to configure the Auto Scaling\ngroup to meet demand in the most cost-effective way.\nWhich solution will meet these requirements?",
    "question_jp": "エンターテインメント会社は最近新しいゲームを開始しました。発表期間中にプレイヤーに良い体験を提供するために、同社はNetwork Load Balancerの背後に12台のr6g.16xlarge（メモリ最適化）Amazon EC2インスタンスを静的にデプロイしました。同社の運用チームは、Amazon CloudWatchエージェントとカスタムメトリックを使用して、メモリ使用率を監視戦略に組み込みました。発表期間中のCloudWatchメトリックの分析は、同社が期待していたCPUとメモリの消費が約四分の一であったことを示しました。ゲームに対する初期需要は低下し、変動が大きくなっています。同社は、CPUとメモリの消費を監視するAuto Scalingグループを使用して、インスタンスのフリートを動的にスケールすることに決定しました。ソリューションアーキテクトは、コスト効果の高い方法で需要に応じたAuto Scalingグループを設定する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure the Auto Scaling group to deploy c6g.4xlarge (compute optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.",
        "text_jp": "Auto Scalingグループを設定してc6g.4xlarge（コンピュート最適化）インスタンスをデプロイします。最小容量を3、希望容量を3、最大容量を12に設定します。"
      },
      {
        "key": "B",
        "text": "Configure the Auto Scaling group to deploy m6g.4xlarge (general purpose) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.",
        "text_jp": "Auto Scalingグループを設定してm6g.4xlarge（汎用）インスタンスをデプロイします。最小容量を3、希望容量を3、最大容量を12に設定します。"
      },
      {
        "key": "C",
        "text": "Configure the Auto Scaling group to deploy r6g.4xlarge (memory optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12.",
        "text_jp": "Auto Scalingグループを設定してr6g.4xlarge（メモリ最適化）インスタンスをデプロイします。最小容量を3、希望容量を3、最大容量を12に設定します。"
      },
      {
        "key": "D",
        "text": "Configure the Auto Scaling group to deploy r6g.8xlarge (memory optimized) instances. Configure a minimum capacity of 2, a desired capacity of 2, and a maximum capacity of 6.",
        "text_jp": "Auto Scalingグループを設定してr6g.8xlarge（メモリ最適化）インスタンスをデプロイします。最小容量を2、希望容量を2、最大容量を6に設定します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D: Configure the Auto Scaling group to deploy r6g.8xlarge (memory optimized) instances.",
        "situation_analysis": "The current configuration uses a large number of instances that are underutilized. The analysis indicates a need for a more efficient and cost-effective scaling solution.",
        "option_analysis": "Option D ensures adequate processing capacity while reducing costs by not deploying excess instances. Options A and B deploy instances that are either compute-optimized or general-purpose, which are not aligned with the memory needs identified. Option C underutilizes resources when compared to Option D.",
        "additional_knowledge": "Understanding instance types and their optimal use cases is critical in designing scalable architectures.",
        "key_terminology": "Auto Scaling, EC2 instances, Network Load Balancer, memory optimized, cost-effective",
        "overall_assessment": "Despite community support for choice C, the analysis shows that D provides the best fit for optimizing resource usage while managing costs effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです：Auto Scalingグループを設定してr6g.8xlarge（メモリ最適化）インスタンスをデプロイします。",
        "situation_analysis": "現在の構成は、多くのインスタンスが低い稼働率で使用されています。分析は、効率的かつコスト効果の高いスケーリングソリューションの必要性を示しています。",
        "option_analysis": "選択肢Dは、過剰なインスタンスをデプロイせずに十分な処理能力を確保します。選択肢AとBは、コンピュート最適化または汎用のインスタンスをデプロイしていますが、これは特定されたメモリニーズには一致していません。選択肢Cは、選択肢Dと比較してリソースを十分に活用していません。",
        "additional_knowledge": "インスタンスタイプとその最適な使用ケースを理解することは、スケーラブルなアーキテクチャの設計において重要です。",
        "key_terminology": "Auto Scaling, EC2インスタンス, Network Load Balancer, メモリ最適化, コスト効果",
        "overall_assessment": "コミュニティの選択肢Cへの支持にもかかわらず、分析はリソース使用の最適化とコスト管理を適切に行う上でDが最も適していることを示しています。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "EC2 instances",
      "Network Load Balancer",
      "memory optimized",
      "cost-effective"
    ]
  },
  {
    "No": "254",
    "question": "A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity\nmode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts\nthroughout the day. and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.\nWhich strategy should a solutions architect recommend to meet this requirement?",
    "question_jp": "ある金融サービス会社が、数百万件の過去の株式取引をAmazon DynamoDBテーブルにロードしました。このテーブルはオンデマンドキャパシティモードを使用しています。毎日真夜中に数百万件の新しいレコードがテーブルにロードされます。アプリケーションのテーブルへの読み取りアクティビティは、一日の間に突発的に発生し、限られた一連のキーが繰り返し照会されています。会社はDynamoDBに関連するコストを削減する必要があります。この要件を満たすために、ソリューションアーキテクトはどの戦略を推奨すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy an Amazon ElastiCache cluster in front of the DynamoDB table",
        "text_jp": "Amazon ElastiCacheクラスターをDynamoDBテーブルの前にデプロイする"
      },
      {
        "key": "B",
        "text": "Deploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer.",
        "text_jp": "DynamoDB Accelerator（DAX）をデプロイする。DynamoDBの自動スケーリングを構成する。Cost ExplorerでSavings Plansを購入する。"
      },
      {
        "key": "C",
        "text": "Use provisioned capacity mode. Purchase Savings Plans in Cost Explorer.",
        "text_jp": "プロビジョニングキャパシティモードを使用する。Cost ExplorerでSavings Plansを購入する。"
      },
      {
        "key": "D",
        "text": "Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling.",
        "text_jp": "DynamoDB Accelerator（DAX）をデプロイする。プロビジョニングキャパシティモードを使用する。DynamoDBの自動スケーリングを構成する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (79%) C (16%)5%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Deploy an Amazon ElastiCache cluster in front of the DynamoDB table.",
        "situation_analysis": "The company is facing high read activity on a limited set of keys throughout the day and needs to reduce costs associated with DynamoDB. On-demand capacity mode can become costly under high read loads.",
        "option_analysis": "Option A is beneficial as ElastiCache can cache frequently accessed data, reducing read requests directly hitting DynamoDB, thus decreasing costs. Option B and D involve using DynamoDB Accelerator (DAX) and provisioned capacity, which may not be as cost-effective in this scenario. Option C proposes switching to provisioned capacity mode alone, but without a caching mechanism, it may fail to address the high cost of reads.",
        "additional_knowledge": "Understanding of economics behind using caches can provide insights into reducing costs in cloud-based architectures.",
        "key_terminology": "Amazon ElastiCache, DynamoDB, caching, on-demand capacity, provisioned capacity.",
        "overall_assessment": "The question accurately reflects a common scenario where organizations need to optimize costs linked to high read patterns. Community voting suggests a divergence in perception, as a substantial percentage supports D (79%), indicating differing opinions on the most effective strategy."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはA: Amazon ElastiCacheクラスターをDynamoDBテーブルの前にデプロイすることである。",
        "situation_analysis": "この会社は、一日の間に限られたキーに対して高い読み取りアクティビティを抱えており、DynamoDBに関連するコストを削減する必要がある。オンデマンドキャパシティモードは、高い読み取り負荷の下では高価になり得る。",
        "option_analysis": "選択肢Aは良いものであり、ElastiCacheが頻繁にアクセスされるデータをキャッシュすることで、直接DynamoDBにヒットする読み取りリクエストを減少させ、コストを削減するのに役立つ。選択肢BとDはDynamoDB Accelerator（DAX）やプロビジョニングキャパシティを使用することを含んでおり、このシナリオではコスト効果が薄い可能性がある。選択肢Cはプロビジョニングキャパシティモードに切り替えることを提案しているが、キャッシングメカニズムがなければ、高コストの読み取りを解決できないかもしれない。",
        "additional_knowledge": "キャッシュを使用することによる経済的理解は、クラウドベースのアーキテクチャにおけるコスト削減に関する洞察を提供できる。",
        "key_terminology": "Amazon ElastiCache、DynamoDB、キャッシング、オンデマンドキャパシティ、プロビジョニングキャパシティ。",
        "overall_assessment": "この質問は、組織が高い読み取りパターンに関連付けられたコストを最適化する必要があるという一般的なシナリオを正確に反映している。コミュニティの投票は意見の不一致を示唆しており、Dを支持する割合が79%であるため、最も効果的な戦略に関して異なる意見があることを示している。"
      }
    ],
    "keywords": [
      "Amazon ElastiCache",
      "DynamoDB",
      "caching",
      "on-demand capacity",
      "provisioned capacity"
    ]
  },
  {
    "No": "255",
    "question": "A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts.\nAWS PrivateLink is being used to provide connectivity between the client services and the logging service.\nIn each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on\nEC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC\nendpoint.\nWhich combination of steps should a solutions architect take to resolve this issue? (Choose two.)",
    "question_jp": "ある企業が、数百のAWSアカウントからログを受け取り、分析する中央集約型のログサービスをAmazon EC2上で作成しています。\nAWS PrivateLinkを使用して、クライアントサービスとログサービス間の接続を提供しています。\n各AWSアカウントにクライアントがいる場合、インターフェースエンドポイントがログサービスのために作成され、利用可能です。EC2インスタンス上で稼働しているログサービスには、Network Load Balancer (NLB)があり、異なるサブネットにデプロイされています。クライアントは、VPCエンドポイントを使用してログを送信できません。\nこの問題を解決するために、ソリューションアーキテクトはどの組み合わせの手順を実行すべきですか？ (二つ選択してください)",
    "choices": [
      {
        "key": "A",
        "text": "Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances.",
        "text_jp": "NACLがログサービスのサブネットにアタッチされ、NLBサブネットとの通信を許可していることを確認してください。また、NLBサブネットにもアタッチされ、EC2インスタンス上で稼働しているログサービスのサブネットとの通信を許可していることを確認してください。"
      },
      {
        "key": "B",
        "text": "Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances.",
        "text_jp": "NACLがログサービスのサブネットにアタッチされ、インターフェースエンドポイントのサブネットとの通信を許可していることを確認してください。また、インターフェースエンドポイントのサブネットにもアタッチされ、EC2インスタンス上で稼働しているログサービスのサブネットとの通信を許可していることを確認してください。"
      },
      {
        "key": "C",
        "text": "Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets.",
        "text_jp": "EC2インスタンス上で稼働しているログサービスのセキュリティグループが、NLBサブネットからのインバウンドトラフィックを許可していることを確認してください。"
      },
      {
        "key": "D",
        "text": "Check the security group for the logging service running on EC2 instances to ensure it allows ingress from the clients.",
        "text_jp": "EC2インスタンス上で稼働しているログサービスのセキュリティグループが、クライアントからのインバウンドトラフィックを許可していることを確認してください。"
      },
      {
        "key": "E",
        "text": "Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets.",
        "text_jp": "NLBのセキュリティグループが、インターフェースエンドポイントのサブネットからのインバウンドトラフィックを許可していることを確認してください。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "AC (52%) BD (36%) 12%",
    "page_images": [],
    "community_vote_distribution_jp": "AC (52%) BD (36%) 12%",
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are A and C. Option A ensures that Network ACLs (NACLs) are configured correctly to allow traffic between the logging service and NLB subnets. Option C ensures the security group for the logging service allows ingress from the NLB.",
        "situation_analysis": "The main issue is that clients cannot submit logs through the VPC endpoint, which is likely due to misconfigured Network ACLs or security groups.",
        "option_analysis": "Option A is correct because it confirms the NACL settings, which are crucial for allowing traffic. Option C is also correct as it checks the security group settings. Options B, D, and E are incorrect as they do not address the fundamental connectivity issues.",
        "additional_knowledge": "Network ACLs are stateless; so any rules need to be applied to both inbound and outbound traffic, and Security Groups are associated with instances.",
        "key_terminology": "Network ACL, Security Group, VPC Endpoint, EC2, NLB",
        "overall_assessment": "The community vote indicates a common misunderstanding of NACLs versus security groups. While option A has the majority support, it's important for architects to ensure thorough understanding of both control mechanisms."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAとCである。オプションAは、ネットワークACL（NACL）が適切に構成され、ログサービスとNLBのサブネット間のトラフィックを許可することを確認するものです。オプションCは、ログサービスのセキュリティグループがNLBからのインバウンドトラフィックを許可することを確認します。",
        "situation_analysis": "クライアントがVPCエンドポイントを通じてログを送信できない主要な問題は、ネットワークACLやセキュリティグループが誤って構成されている可能性が高いことです。",
        "option_analysis": "オプションAはトラフィックを許可するためのNACL設定を確認するため正しい。オプションCも、セキュリティグループ設定を確認するもので正しい。オプションB、D、Eは、根本的な接続の問題に対処していないため不正解である。",
        "additional_knowledge": "ネットワークACLはステートレスであり、すべてのルールをインバウンドとアウトバウンドのトラフィックに対して適用する必要があります。また、セキュリティグループはインスタンスに関連付けられています。",
        "key_terminology": "Network ACL, Security Group, VPC Endpoint, EC2, NLB",
        "overall_assessment": "コミュニティの投票は、NACLとセキュリティグループに関する一般的な誤解を示しています。オプションAは過半数の支持を得ていますが、アーキテクトは両方の制御メカニズムをしっかり理解することが重要である。"
      }
    ],
    "keywords": [
      "Network ACL",
      "Security Group",
      "VPC Endpoint",
      "EC2",
      "NLB"
    ]
  },
  {
    "No": "256",
    "question": "A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed\nfrequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side\nencryption with AWS KMS keys (SSE-KMS).\nA solutions architect reviews the company's monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of\nrequests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業は、Amazon S3 バケットに数百万のオブジェクトを持っています。オブジェクトは S3 標準ストレージクラスにあります。すべての S3 オブジェクトは頻繁にアクセスされています。オブジェクトにアクセスするユーザーとアプリケーションの数が急速に増加しています。オブジェクトは、AWS KMS キー（SSE-KMS）を使用したサーバー側の暗号化で暗号化されています。\nソリューションアーキテクトは、企業の月次 AWS 請求書を確認し、Amazon S3 からのリクエストの高頻度によって AWS KMS コストが増加していることに気付きました。ソリューションアーキテクトは、アプリケーションに最小限の変更を加えることでコストを最適化する必要があります。\nどのソリューションが、運用オーバーヘッドを最小限に抑えつつ、これらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C.",
        "text_jp": "カスタマー提供キー（SSE-C）によるサーバー側の暗号化を持つ新しい S3 バケットを作成します。既存のオブジェクトを新しい S3 バケットにコピーします。SSE-C を指定します。"
      },
      {
        "key": "B",
        "text": "Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Specify SSE-S3.",
        "text_jp": "Amazon S3 管理キー（SSE-S3）によるサーバー側の暗号化を持つ新しい S3 バケットを作成します。S3 バッチオペレーションを使用して、既存のオブジェクトを新しい S3 バケットにコピーします。SSE-S3 を指定します。"
      },
      {
        "key": "C",
        "text": "Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM.",
        "text_jp": "AWS CloudHSM を使用して暗号化キーを保存します。新しい S3 バケットを作成します。S3 バッチオペレーションを使用して、既存のオブジェクトを新しい S3 バケットにコピーします。CloudHSM のキーを使用してオブジェクトを暗号化します。"
      },
      {
        "key": "D",
        "text": "Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive.",
        "text_jp": "S3 インテリジェントティアリングストレージクラスを S3 バケットに使用します。90 日間アクセスされていないオブジェクトを S3 Glacier Deep Archive に移行するインテリジェントティアリングアーカイブ構成を作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating a new S3 bucket with SSE-C and copying existing objects minimizes operational overhead while addressing cost issues with AWS KMS.",
        "situation_analysis": "The company needs to reduce AWS KMS costs associated with the high number of requests coming from Amazon S3 while maintaining user access to frequently accessed objects.",
        "option_analysis": "Option A leverages SSE-C which provides encryption without incurring KMS request charges. Options B, C, and D either introduce additional complexities or maintain higher costs from KMS requests.",
        "additional_knowledge": "It's also vital to consider security implications and ensure that key management practices are robust when transitioning to SSE-C.",
        "key_terminology": "S3 Standard, SSE-KMS, SSE-C, AWS KMS, operational overhead",
        "overall_assessment": "Although community voting favored option B, option A is the most efficient in minimizing costs related to KMS with fewer operational demands."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは A です。SSE-C を使用して新しい S3 バケットを作成し、既存のオブジェクトをコピーすることで、AWS KMS に関するコストの問題に対処しつつ、運用オーバーヘッドを最小化します。",
        "situation_analysis": "この会社は、Amazon S3 からの高頻度のリクエストに関連する AWS KMS コストを削減しつつ、頻繁にアクセスされるオブジェクトへのユーザーアクセスを維持する必要があります。",
        "option_analysis": "選択肢 A は、KMS リクエスト料金を発生させずに暗号化を提供する SSE-C を活用しています。選択肢 B、C、および D は、追加の複雑さを持ち込むか、KMS リクエストからの高コストを維持します。",
        "additional_knowledge": "SSE-C への移行時には、セキュリティの観点からも注意が必要であり、キー管理の実践が堅牢であることを確認することが重要です。",
        "key_terminology": "S3 Standard, SSE-KMS, SSE-C, AWS KMS, 運用オーバーヘッド",
        "overall_assessment": "コミュニティの投票は選択肢 B を支持していますが、選択肢 A が KMS に関するコストを最小限に抑えるのに最も効率的で、運用の要求も少なく済みます。"
      }
    ],
    "keywords": [
      "S3 Standard",
      "SSE-KMS",
      "SSE-C",
      "AWS KMS",
      "operational overhead"
    ]
  },
  {
    "No": "257",
    "question": "A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions. Application state is stored in Amazon\nDynamoDB tables. Users are reporting that some uploaded photos are not being processed properly. The application developers trace the logs and\nfind that Lambda is experiencing photo processing issues when thousands of users upload photos simultaneously. The issues are the result of\nLambda concurrency limits and the performance of DynamoDB when data is saved.\nWhich combination of actions should a solutions architect take to increase the performance and reliability of the application? (Choose two.)",
    "question_jp": "ユーザーの写真を処理するために、メディアストレージアプリケーションがAmazon S3にユーザーの写真をアップロードします。アプリケーションの状態はAmazon DynamoDBテーブルに保存されます。ユーザーは、アップロードされた写真の一部が正しく処理されていないと報告しています。アプリケーション開発者はログを追跡し、Lambdaが同時に数千のユーザーが写真をアップロードする際に写真処理の問題を経験していることを発見しました。問題の原因は、Lambdaの同時実行制限と、データが保存される際のDynamoDBのパフォーマンスです。このアプリケーションのパフォーマンスと信頼性を向上させるために、ソリューションアーキテクトはどのようなアクションの組み合わせを取るべきでしょうか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Evaluate and adjust the RCUs for the DynamoDB tables.",
        "text_jp": "DynamoDBテーブルのRCUを評価して調整する。"
      },
      {
        "key": "B",
        "text": "Evaluate and adjust the WCUs for the DynamoDB tables.",
        "text_jp": "DynamoDBテーブルのWCUを評価して調整する。"
      },
      {
        "key": "C",
        "text": "Add an Amazon ElastiCache layer to increase the performance of Lambda functions.",
        "text_jp": "Lambda関数のパフォーマンスを向上させるためにAmazon ElastiCacheレイヤーを追加する。"
      },
      {
        "key": "D",
        "text": "Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions.",
        "text_jp": "Amazon S3とLambda関数の間にAmazon Simple Queue Service (Amazon SQS)キューと再処理ロジックを追加する。"
      },
      {
        "key": "E",
        "text": "Use S3 Transfer Acceleration to provide lower latency to users.",
        "text_jp": "ユーザーに低遅延を提供するためにS3 Transfer Accelerationを使用する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "BD (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answers are B and D. Adjusting the Write Capacity Units (WCUs) for DynamoDB can help prevent throttling during high upload rates and ensure that the database can handle additional write requests more efficiently. Additionally, implementing Amazon SQS as a buffering solution between S3 and Lambda can help manage the influx of processing requests by queuing them and handling them at a manageable rate, which mitigates the issue with Lambda concurrency limits.",
        "situation_analysis": "The application experiences performance issues during high concurrency due to resource limits, specifically Lambda concurrency limits and DynamoDB write performance.",
        "option_analysis": "Option B is correct as it directly addresses the issue of DynamoDB performance under load by adjusting WCUs. Option D is also correct as it introduces a queuing mechanism to balance processing loads, preventing AWS Lambda from being overwhelmed.",
        "additional_knowledge": "Both adjustments and optimizations can lead to improved application reliability and scalability as usage patterns increase.",
        "key_terminology": "AWS Lambda, Amazon DynamoDB, Write Capacity Units (WCU), Amazon SQS, concurrency limits",
        "overall_assessment": "The community voting is aligned with the best practices for handling DynamoDB throughput issues and leveraging an SQS queue for decoupling Lambda processing from direct S3 uploads."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBとDである。DynamoDBの書き込みキャパシティ単位（WCU）を調整することで、高いアップロード率におけるスロットリングを防ぎ、データベースが追加の書き込みリクエストをより効率的に処理できるようになる。また、Amazon SQSを用いたバッファリングソリューションを導入することで、S3とLambdaの間でアップロードリクエストの急増を管理し、処理を管理可能なレートで行うことができ、Lambdaの同時実行制限の問題を軽減できる。",
        "situation_analysis": "アプリケーションは高い同時処理時にリソース制限によるパフォーマンス問題があり、特にLambdaの同時実行制限とDynamoDBの書き込みパフォーマンスが影響している。",
        "option_analysis": "オプションBはDynamoDBのパフォーマンス問題を解決するためにWCUを調整するため、正しい。オプションDも正しく、SQSを使用することで処理要求をバランスさせ、Lambdaを圧倒しないようにする。",
        "additional_knowledge": "これらの調整や最適化は、使用パターンの増加に伴うアプリケーションの信頼性と拡張性の向上に寄与する。",
        "key_terminology": "AWS Lambda, Amazon DynamoDB, 書き込みキャパシティ単位（WCU）, Amazon SQS, 同時実行制限",
        "overall_assessment": "コミュニティの投票はDynamoDBのスループット問題の取り扱いやSQSキューを使用したLambdaの処理の分離に関してベストプラクティスと一致している。"
      }
    ],
    "keywords": [
      "AWS Lambda",
      "Amazon DynamoDB",
      "Write Capacity Units",
      "Amazon SQS",
      "concurrency limits"
    ]
  },
  {
    "No": "258",
    "question": "A company runs an application in an on-premises data center. The application gives users the ability to upload media files. The files persist in a\nfile server. The web application has many users. The application server is overutilized, which causes data uploads to fail occasionally. The\ncompany frequently adds new storage to the file server. The company wants to resolve these challenges by migrating the application to AWS.\nUsers from across the United States and Canada access the application. Only authenticated users should have the ability to access the\napplication to upload files. The company will consider a solution that refactors the application, and the company needs to accelerate application\ndevelopment.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がオンプレミスのデータセンターでアプリケーションを運用している。このアプリケーションは、ユーザーにメディアファイルをアップロードする機能を提供している。ファイルはファイルサーバーに保存されている。Webアプリケーションは多くのユーザーを抱えており、アプリケーションサーバーは過負荷状態で、データのアップロードが時折失敗する。企業はファイルサーバーに新しいストレージを頻繁に追加している。この企業は、アプリケーションをAWSに移行することでこれらの課題を解決したいと考えている。アメリカおよびカナダ全土からユーザーがアプリケーションにアクセスする。認証されたユーザーのみがファイルをアップロードするためにアプリケーションにアクセスできるべきである。企業はアプリケーションを改修する解決策を検討し、アプリケーションの開発を加速する必要がある。最も運用オーバーヘッドが少ない解決策はどれか？",
    "choices": [
      {
        "key": "A",
        "text": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Modify the application to use Amazon S3 to persist the files. Use Amazon Cognito to authenticate users.",
        "text_jp": "AWS Application Migration Serviceを使用してアプリケーションサーバーをAmazon EC2インスタンスに移行する。EC2インスタンスのためのAuto Scalingグループを作成する。リクエストを分散するためにApplication Load Balancerを使用する。アプリケーションを修正して、ファイルを保存するためにAmazon S3を使用する。ユーザーを認証するためにAmazon Cognitoを使用する。"
      },
      {
        "key": "B",
        "text": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Set up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to the application. Modify the application to use Amazon S3 to persist the files.",
        "text_jp": "AWS Application Migration Serviceを使用してアプリケーションサーバーをAmazon EC2インスタンスに移行する。EC2インスタンスのためのAuto Scalingグループを作成する。リクエストを分散するためにApplication Load Balancerを使用する。アプリケーションにサインインする能力をユーザーに与えるためにAWS IAM Identity Center（AWS Single Sign-On）を設定する。アプリケーションを修正して、ファイルを保存するためにAmazon S3を使用する。"
      },
      {
        "key": "C",
        "text": "Create a static website for uploads of media files. Store the static assets in Amazon S3. Use AWS AppSync to create an API. Use AWS Lambda resolvers to upload the media files to Amazon S3. Use Amazon Cognito to authenticate users.",
        "text_jp": "メディアファイルのアップロード用の静的ウェブサイトを作成する。静的アセットをAmazon S3に保存する。AWS AppSyncを使用してAPIを作成する。AWS Lambdaリゾルバーを使用してメディアファイルをAmazon S3にアップロードする。ユーザーを認証するためにAmazon Cognitoを使用する。"
      },
      {
        "key": "D",
        "text": "Use AWS Amplify to create a static website for uploads of media files. Use Amplify Hosting to serve the website through Amazon CloudFront. Use Amazon S3 to store the uploaded media files. Use Amazon Cognito to authenticate users.",
        "text_jp": "AWS Amplifyを使用してメディアファイルのアップロード用の静的ウェブサイトを作成する。Amplify Hostingを使用して、Amazon CloudFrontを介してウェブサイトを提供する。Amazon S3を使用してアップロードされたメディアファイルを保存する。ユーザーを認証するためにAmazon Cognitoを使用する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (87%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A, which involves using AWS Application Migration Service to migrate the application server to Amazon EC2 instances, creating an Auto Scaling group, and utilizing Amazon S3 for file storage.",
        "situation_analysis": "The organization is facing issues with an overutilized application server that leads to failed uploads. They need a scalable solution that allows authentication for users and minimizes operational overhead during migration.",
        "option_analysis": "Option A provides a complete solution with minimal operational overhead, including the use of Application Load Balancer and Amazon Cognito for user authentication. Options B, C, and D offer alternatives but require additional configuration or do not address operational overhead as effectively.",
        "additional_knowledge": "Using Amazon Cognito for authentication is a reliable method for managing user access and permissions effectively.",
        "key_terminology": "AWS Application Migration Service, Amazon EC2, Auto Scaling, Application Load Balancer, Amazon S3, Amazon Cognito",
        "overall_assessment": "Despite community voting favoring option D, option A aligns best with the requirement for minimal operational overhead and effective scalability. The community's choice may reflect a preference for newer technologies but does not necessarily provide the operational efficiency required."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAであり、AWS Application Migration Serviceを利用してアプリケーションサーバーをAmazon EC2インスタンスに移行し、Auto Scalingグループを作成し、ファイルストレージにAmazon S3を使用することを含む。",
        "situation_analysis": "組織は過負荷のアプリケーションサーバーによってアップロードが失敗する問題に直面している。彼らは、ユーザーの認証を可能にし、移行中に運用オーバーヘッドを最小限に抑えるスケーラブルな解決策を必要としている。",
        "option_analysis": "選択肢Aは、Application Load Balancerとユーザー認証のためのAmazon Cognitoを使用するなど、運用オーバーヘッドを最小限に抑えた完全な解決策を提供している。選択肢B、C、Dは代替を提供するが、追加の構成が必要だったり、運用オーバーヘッドに対処しない場合がある。",
        "additional_knowledge": "Amazon Cognitoを認証に使用することは、ユーザーのアクセスと権限を効果的に管理するための信頼できる方法である。",
        "key_terminology": "AWS Application Migration Service, Amazon EC2, Auto Scaling, Application Load Balancer, Amazon S3, Amazon Cognito",
        "overall_assessment": "コミュニティ投票が選択肢Dを優先しているにもかかわらず、Aが運用オーバーヘッドの最小化と効果的なスケーラビリティの要件に最も合致している。コミュニティの選択は新しい技術への好みを反映しているかもしれないが、必ずしも必要な運用効率を提供するわけではない。"
      }
    ],
    "keywords": [
      "AWS Application Migration Service",
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Amazon S3",
      "Amazon Cognito"
    ]
  },
  {
    "No": "259",
    "question": "A company has an application that is deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are part of an\nAuto Scaling group. The application has unpredictable workloads and frequently scales out and in. The company's development team wants to\nanalyze application logs to find ways to improve the application's performance. However, the logs are no longer available after instances scale in.\nWhich solution will give the development team the ability to view the application logs after a scale-in event?",
    "question_jp": "ある企業が、Application Load Balancer (ALB) の背後に展開されたアプリケーションを Amazon EC2 インスタンス上で運用しています。インスタンスは Auto Scaling グループの一部です。アプリケーションは予測不可能なワークロードを持ち、頻繁にスケールアウトおよびスケールインします。企業の開発チームは、アプリケーションログを分析してアプリケーションのパフォーマンスを改善する方法を見つけたいと考えています。ただし、インスタンスがスケールインした後、ログはもはや利用できません。スケールインイベント後にアプリケーションログを表示できるようにするためのソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Enable access logs for the ALB. Store the logs in an Amazon S3 bucket.",
        "text_jp": "ALBのアクセスログを有効にし、ログをAmazon S3バケットに保存する。"
      },
      {
        "key": "B",
        "text": "Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent.",
        "text_jp": "EC2インスタンスを統合CloudWatchエージェントを使用して、ログをAmazon CloudWatch Logsに発行するように構成する。"
      },
      {
        "key": "C",
        "text": "Modify the Auto Scaling group to use a step scaling policy.",
        "text_jp": "Auto Scalingグループを変更して、ステップスケーリングポリシーを使用する。"
      },
      {
        "key": "D",
        "text": "Instrument the application with AWS X-Ray tracing.",
        "text_jp": "アプリケーションをAWS X-Rayトレーシングで計測する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent. This ensures that logs are stored independently of the EC2 instance lifecycle.",
        "situation_analysis": "The application experiences unpredictable workloads, leading to frequent scaling events. When instances scale in, any locally stored logs are lost unless stored in a persistent manner.",
        "option_analysis": "Option A would only provide logs for ALB requests, but not application-level logs. Option C relates to scaling policies, but does not address log persistence. Option D provides tracing information, but not general log storage for analysis. Thus, option B is the best choice.",
        "additional_knowledge": "Using CloudWatch Logs also facilitates monitoring and troubleshooting of applications in real-time.",
        "key_terminology": "CloudWatch Logs, Auto Scaling, Application Load Balancer, EC2, logging",
        "overall_assessment": "Selecting option B aligns with best practices in AWS for managing logs, ensuring that logs remain accessible even as EC2 instances are dynamically scaled. Community support for this option (100%) indicates strong agreement on its effectiveness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はBです：EC2インスタンスを統合CloudWatchエージェントを使用して、ログをAmazon CloudWatch Logsに発行するように構成します。これにより、ログはEC2インスタンスのライフサイクルとは独立して保存されます。",
        "situation_analysis": "アプリケーションは予測不可能なワークロードを経験しており、頻繁にスケーリングイベントがあります。インスタンスがスケールインすると、ローカルに保存されたログは失われるため、持続的な方法で保存されない限り、見ることができません。",
        "option_analysis": "Aの選択肢はALBリクエストのログのみを提供し、アプリケーションレベルのログは提供しません。Cの選択肢はスケーリングポリシーに関連しますが、ログの永続性には対処していません。Dの選択肢はトレーシング情報を提供しますが、分析用の一般的なログストレージは提供しません。したがって、選択肢Bが最良の選択肢です。",
        "additional_knowledge": "CloudWatch Logsを使用することは、リアルタイムでのアプリケーションのモニタリングとトラブルシューティングを促進します。",
        "key_terminology": "CloudWatch Logs、Auto Scaling、Application Load Balancer、EC2、ロギング",
        "overall_assessment": "選択肢Bを選択することは、AWSのベストプラクティスに沿ったログ管理に一致しており、EC2インスタンスが動的にスケーリングされてもログにアクセスできるようにします。この選択肢に対するコミュニティの支持（100%）は、その効果についての強い合意を示しています。"
      }
    ],
    "keywords": [
      "CloudWatch Logs",
      "Auto Scaling",
      "Application Load Balancer",
      "EC2",
      "logging"
    ]
  },
  {
    "No": "260",
    "question": "A company runs an unauthenticated static website (www.example.com) that includes a registration form for users. The website uses Amazon S3\nfor hosting and uses Amazon CloudFront as the content delivery network with AWS WAF configured. When the registration form is submitted, the\nwebsite calls an Amazon API Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward the payload to an\nexternal API call.\nDuring testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutions architect confirms that the CloudFront\ndistribution origin has the Access-Control-Allow-Origin header set to www.example.com.\nWhat should the solutions architect do to resolve the error?",
    "question_jp": "会社はユーザー向けの登録フォームを含む認証されていない静的ウェブサイト（www.example.com）を運営しています。このウェブサイトはAmazon S3を使用してホスティングされ、Amazon CloudFrontがコンテンツ配信ネットワークとして使用され、AWS WAFが設定されています。登録フォームが送信されると、ウェブサイトはAmazon API GatewayのAPIエンドポイントを呼び出し、AWS Lambda関数を実行してペイロードを処理し、外部APIコールにペイロードを転送します。テスト中に、ソリューションアーキテクトはクロスオリジンリソースシェアリング（CORS）エラーに遭遇します。ソリューションアーキテクトは、CloudFrontの配信オリジンがAccess-Control-Allow-Originヘッダーをwww.example.comに設定されたことを確認しました。このエラーを解決するために、ソリューションアーキテクトは何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Change the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com.",
        "text_jp": "S3バケットのCORS設定を変更する。www.example.comのAllowedOrigin要素にCORSのルールを追加する。"
      },
      {
        "key": "B",
        "text": "Enable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com.",
        "text_jp": "AWS WAFでCORS設定を有効にする。Access-Control-Allow-Originヘッダーをwww.example.comに設定したウェブACLルールを作成する。"
      },
      {
        "key": "C",
        "text": "Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com.",
        "text_jp": "API GatewayのAPIエンドポイントでCORS設定を有効にする。APIエンドポイントがAccess-Control-Allow-Originヘッダーをwww.example.comに設定したすべてのレスポンスを返すように構成されていることを確認する。"
      },
      {
        "key": "D",
        "text": "Enable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header set to www.example.com.",
        "text_jp": "Lambda関数でCORS設定を有効にする。関数の戻りコードにAccess-Control-Allow-Originヘッダーをwww.example.comに設定することを確認する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Enabling CORS in AWS WAF and configuring a web ACL rule for the Access-Control-Allow-Origin header addresses the CORS error effectively.",
        "situation_analysis": "The website utilizes Amazon S3 for hosting and Amazon CloudFront for distribution, and an unhandled CORS error occurs during testing, affecting user registration.",
        "option_analysis": "Option A only addresses the S3 bucket configuration and does not account for CloudFront and WAF interactions. Option C targets the API Gateway but ignores WAF, which can prevent CORS errors. Option D concerns Lambda function responses but does not address the initial request handling.",
        "additional_knowledge": "Understanding the flow of requests from the client to CloudFront to Lambda is crucial in diagnosing CORS issues.",
        "key_terminology": "CORS, Access-Control-Allow-Origin, Amazon CloudFront, AWS WAF, API Gateway",
        "overall_assessment": "Answer B is the most comprehensive solution considering all components of the architecture. The community vote may indicate preference but is less accurate in this context."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はBです。AWS WAFでCORSを有効にし、Access-Control-Allow-Originヘッダーの設定を行うことで、CORSエラーを効果的に解決できます。",
        "situation_analysis": "ウェブサイトはAmazon S3を使用してホスティングされ、Amazon CloudFrontを使用して配信されており、テスト中に未処理のCORSエラーが発生し、ユーザー登録に影響を及ぼしています。",
        "option_analysis": "選択肢AはS3バケットの設定のみを対象としており、CloudFrontおよびWAFとの相互作用を考慮していません。選択肢CはAPI Gatewayを対象としていますが、WAFを無視しているためCORSエラーを防ぎません。選択肢DはLambda関数のレスポンスに関するものですが、最初のリクエスト処理には言及していません。",
        "additional_knowledge": "クライアントからCloudFrontを経てLambdaへのリクエストの流れを理解することがCORS問題の診断には重要です。",
        "key_terminology": "CORS, Access-Control-Allow-Origin, Amazon CloudFront, AWS WAF, API Gateway",
        "overall_assessment": "回答Bはアーキテクチャの全てのコンポーネントを考慮した最も包括的な解決策です。コミュニティの投票は好みを示すかもしれませんが、文脈ではあまり正確ではありません。"
      }
    ],
    "keywords": [
      "CORS",
      "Access-Control-Allow-Origin",
      "Amazon CloudFront",
      "AWS WAF",
      "API Gateway"
    ]
  },
  {
    "No": "261",
    "question": "A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different\ndepartments in the company. The company has a Microsoft Azure Active Directory that is deployed.\nA solutions architect needs to centralize billing and management of the company's AWS accounts. The company wants to start using identity\nfederation instead of manual user management. The company also wants to use temporary credentials instead of long-lived access keys.\nWhich combination of steps will meet these requirements? (Choose three.)",
    "question_jp": "ある企業は多くの異なるAWSアカウントを持ち、中央請求または管理を使用していません。各AWSアカウントは、企業内の異なる部門のサービスをホストしています。企業は、展開されたMicrosoft Azure Active Directoryを持っています。ソリューションアーキテクトは、企業のAWSアカウントの請求と管理を中央集約する必要があります。企業は手動のユーザー管理の代わりにアイデンティティフェデレーションを使用したいと考えています。また、企業は長期的なアクセスキーの代わりに一時的な認証情報を使用したいと考えています。これらの要件を満たすために必要なステップの組み合わせはどれですか？（3つ選んでください）",
    "choices": [
      {
        "key": "A",
        "text": "Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS account to join the organization. Ensure that each account accepts the invitation.",
        "text_jp": "管理アカウントとして機能する新しいAWSアカウントを作成します。AWS Organizationsで組織を展開します。既存の各AWSアカウントを組織に参加するよう招待します。各アカウントが招待を受け入れることを確認します。"
      },
      {
        "key": "B",
        "text": "Configure each AWS account's email address to be aws+@example.com so that account management email messages and invoices are sent to the same place.",
        "text_jp": "各AWSアカウントのメールアドレスをaws+@example.comに設定して、アカウント管理のメールメッセージや請求書が同じ場所に送信されるようにします。"
      },
      {
        "key": "C",
        "text": "Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronization of users and groups.",
        "text_jp": "管理アカウントでAWS IAMアイデンティティセンター（AWSシングルサインオン）を展開します。IAMアイデンティティセンターをAzure Active Directoryに接続します。ユーザーとグループの自動同期のためにIAMアイデンティティセンターを構成します。"
      },
      {
        "key": "D",
        "text": "Deploy an AWS Managed Microsoft AD directory in the management account. Share the directory with all other accounts in the organization by using AWS Resource Access Manager (AWS RAM).",
        "text_jp": "管理アカウントでAWS Managed Microsoft ADディレクトリを展開します。AWSリソースアクセスマネージャー（AWS RAM）を使用して、組織内のすべての他のアカウントとディレクトリを共有します。"
      },
      {
        "key": "E",
        "text": "Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts.",
        "text_jp": "AWS IAMアイデンティティセンター（AWSシングルサインオン）権限セットを作成します。適切なIAMアイデンティティセンターグループおよびAWSアカウントに権限セットをアタッチします。"
      },
      {
        "key": "F",
        "text": "Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and authorization.",
        "text_jp": "各AWSアカウントでAWSアイデンティティアクセス管理（IAM）を構成して、認証と認可にAWS Managed Microsoft ADを使用します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ACE (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is option C. Deploying AWS IAM Identity Center and integrating it with Azure Active Directory allows for centralized user management and supports identity federation.",
        "situation_analysis": "The company requires central management of AWS accounts and aims to implement identity federation to streamline user management.",
        "option_analysis": "Option C is the best choice because it directly addresses the needs for identity federation and temporary credentials. Options A and E may partially contribute but do not fully implement the identity federation goal.",
        "additional_knowledge": "Using temporary credentials instead of long-lived access keys enhances security by reducing the risk of credential leakage.",
        "key_terminology": "AWS IAM Identity Center, Azure Active Directory, identity federation, temporary credentials, AWS Organizations",
        "overall_assessment": "Considering the community vote shows a strong consensus for option C, it aligns with AWS best practices for centralized management. There is a clear advantage in using AWS IAM Identity Center in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは選択肢Cです。AWS IAMアイデンティティセンターを展開し、Azure Active Directoryと統合することで、ユーザー管理の中央集約が可能になり、アイデンティティフェデレーションをサポートします。",
        "situation_analysis": "企業はAWSアカウントの中央管理を必要としており、ユーザー管理を簡素化するためにアイデンティティフェデレーションを実装することを目指しています。",
        "option_analysis": "選択肢Cは、アイデンティティフェデレーションと一時的な認証情報のニーズに直接対処しているため、最良の選択肢です。選択肢AとEは部分的に寄与するかもしれませんが、アイデンティティフェデレーション目標を完全には実現しません。",
        "additional_knowledge": "長期的なアクセスキーの代わりに一時的な認証情報を使用することで、認証情報の漏洩リスクを低下させることにより、安全性が向上します。",
        "key_terminology": "AWS IAMアイデンティティセンター、Azure Active Directory、アイデンティティフェデレーション、一時的な認証情報、AWS Organizations",
        "overall_assessment": "コミュニティの投票が選択肢Cに強い支持を示していることを考慮すると、AWSの中央管理のベストプラクティスに沿っています。このシナリオでAWS IAMアイデンティティセンターを使用することには明確な利点があります。"
      }
    ],
    "keywords": [
      "AWS IAM Identity Center",
      "Azure Active Directory",
      "identity federation",
      "temporary credentials",
      "AWS Organizations"
    ]
  },
  {
    "No": "262",
    "question": "A company wants to manage the costs associated with a group of 20 applications that are infrequently used, but are still business-critical, by\nmigrating to AWS. The applications are a mix of Java and Node.js spread across different instance clusters. The company wants to minimize\ncosts while standardizing by using a single deployment methodology.\nMost of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other\ntimes. Average application memory consumption is less than 1 GB. though some applications use as much as 2.5 GB of memory during peak\nprocessing. The most important application in the group is a billing report written in Java that accesses multiple data sources and often runs for\nseveral hours.\nWhich is the MOST cost-effective solution?",
    "question_jp": "企業は、AWSに移行することで、まれに使用されるがビジネス上重要な20のアプリケーションに関連するコストを管理したいと考えています。これらのアプリケーションは、異なるインスタンスクラスターに分散されたJavaとNode.jsの混合であり、コストを最小限に抑え、一つのデプロイメント方法論を使用して標準化したいと考えています。ほとんどのアプリケーションは月末処理のルーチンの一部であり、同時に少数のユーザーがいますが、時折他の時間にも実行されます。アプリケーションの平均メモリ消費量は1GB未満ですが、一部のアプリケーションはピーク処理時に2.5GBのメモリを使用します。グループで最も重要なアプリケーションは、複数のデータソースにアクセスし、数時間にわたって実行されるJavaで書かれた請求報告書です。最もコスト効率の高いソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a separate AWS Lambda function for each application. Use AWS CloudTrail logs and Amazon CloudWatch alarms to verify completion of critical jobs.",
        "text_jp": "各アプリケーションのために別々のAWS Lambda関数をデプロイします。AWS CloudTrailログとAmazon CloudWatchアラームを使用して、重要なジョブの完了を確認します。"
      },
      {
        "key": "B",
        "text": "Deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. Deploy an ECS task for each application being migrated with ECS task scaling. Monitor services and hosts by using Amazon CloudWatch.",
        "text_jp": "メモリ使用率75%に設定されたAuto Scalingを構成したAmazon EC2上にAmazon ECSコンテナをデプロイします。移行中の各アプリケーションのためにECSタスクをデプロイし、ECSタスクスケーリングを使用します。サービスとホストをAmazon CloudWatchで監視します。"
      },
      {
        "key": "C",
        "text": "Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have suficient resources. Monitor each AWS Elastic Beanstalk deployment by using CloudWatch alarms.",
        "text_jp": "各アプリケーションのためにAWS Elastic Beanstalkをデプロイし、全てのリクエストに十分なリソースが確保されるようにAuto Scalingを設定します。CloudWatchアラームを使用して各AWS Elastic Beanstalkデプロイメントを監視します。"
      },
      {
        "key": "D",
        "text": "Deploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling and Application Load Balancers. Scale cluster size based on a custom metric set on instance memory utilization. Purchase 3-year Reserved Instance reservations equal to the GroupMaxSize parameter of the Auto Scaling group.",
        "text_jp": "新しいAmazon EC2インスタンスクラスターをデプロイして、全てのアプリケーションを共存させます。EC2 Auto ScalingとApplication Load Balancersを使用し、インスタンスメモリ使用率に基づいてクラスターサイズをスケールします。Auto ScalingグループのGroupMaxSizeパラメータに等しい3年間のリザーブドインスタンス予約を購入します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The most cost-effective solution is to deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. This approach allows for efficient scaling based on actual memory demands, which aligns well with the application's average and peak memory requirements.",
        "situation_analysis": "The company requires a cost-effective way to run 20 essential applications with low concurrency. The applications have specific memory consumption profiles that need careful management to keep costs down while ensuring availability.",
        "option_analysis": "Option B is optimal as it utilizes ECS for container deployment, allowing for fine-tuned scaling and resource management. Other options like A would not provide the necessary scaling or resource management, while C and D may incur higher costs due to less efficient resource allocation.",
        "additional_knowledge": "None",
        "key_terminology": "Amazon ECS, Amazon EC2, Auto Scaling, CloudWatch, memory utilization",
        "overall_assessment": "Overall, Option B supports best practices for managing resources efficiently while addressing the unique needs of the applications, making it the best choice based on both technical merits and cost management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "最もコスト効率の高いソリューションは、Amazon EC2上にAmazon ECSコンテナをデプロイし、メモリ使用率75%に設定されたAuto Scalingを構成することです。このアプローチは、アプリケーションの平均およびピークメモリ要件に合わせた効率的なスケーリングを可能にします。",
        "situation_analysis": "企業は、低い同時接続数で20の必要なアプリケーションを実行するためのコスト効率の良い方法を求めています。アプリケーションには、コストを抑えつつ可用性を保証するために、特定のメモリ消費プロファイルを管理する必要があります。",
        "option_analysis": "オプションBは、ECSを利用してコンテナデプロイメントを行うため、実際のメモリ需要に基づいて細かなスケーリングとリソース管理が可能で最適です。他の選択肢であるAは、必要なスケーリングやリソース管理を提供しませんし、CやDはリソースの効率的な割り当てができず、より高いコストがかかる可能性があります。",
        "additional_knowledge": "なし",
        "key_terminology": "Amazon ECS, Amazon EC2, Auto Scaling, CloudWatch, メモリ利用率",
        "overall_assessment": "総合的に見て、オプションBはリソースを効率的に管理するためのベストプラクティスをサポートし、アプリケーションの独自のニーズに応えるため、技術的なメリットとコスト管理の両面からベストな選択肢です。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "Amazon EC2",
      "Auto Scaling",
      "CloudWatch",
      "memory utilization"
    ]
  },
  {
    "No": "263",
    "question": "A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs\ntasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, primary, and core\nnodes. The EMR tasks run each morning, starting at 1:00 AM. and take 6 hours to finish running. The amount of time to complete the processing is\nnot a priority because the data is not referenced until late in the day.\nThe solutions architect must review the architecture and suggest a solution to minimize the compute costs.\nWhich solution should the solutions architect recommend to meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、EMRファイルシステム (EMRFS) を使用している Amazon EMR クラスターの設計を再確認する必要があります。このクラスターは、ビジネスニーズにとって重要なタスクを実行します。クラスターは、すべてのタスク、プライマリ、およびコアノードのために常に Amazon EC2 オンデマンドインスタンスを稼働させています。EMR タスクは毎朝午前 1 時に開始され、6 時間かけて実行されます。処理を完了するまでの時間は優先事項ではなく、データは日中遅くまで参照されません。ソリューションアーキテクトは、アーキテクチャを見直し、計算コストを最小限に抑える提案を行う必要があります。どのようなソリューションを提案すべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Launch all task, primary, and core nodes on Spot Instances in an instance fieet. Terminate the cluster, including all instances, when the processing is completed.",
        "text_jp": "すべてのタスク、プライマリ、およびコアノードをスポットインスタンスでインスタンスフリートを作成します。処理が完了した時点で、クラスターとすべてのインスタンスを終了します。"
      },
      {
        "key": "B",
        "text": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fieet. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.",
        "text_jp": "プライマリおよびコアノードをオンデマンドインスタンスで起動します。タスクノードをスポットインスタンスのインスタンスフリートで起動します。処理が完了した時点で、クラスターとすべてのインスタンスを終了します。オンデマンドインスタンスの使用をカバーするためにコンピュートセービングプランを購入します。"
      },
      {
        "key": "C",
        "text": "Continue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.",
        "text_jp": "すべてのノードをオンデマンドインスタンスで起動し続けます。処理が完了した時点で、クラスターとすべてのインスタンスを終了します。オンデマンドインスタンスの使用をカバーするためにコンピュートセービングプランを購入します。"
      },
      {
        "key": "D",
        "text": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fieet. Terminate only the task node instances when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.",
        "text_jp": "プライマリおよびコアノードをオンデマンドインスタンスで起動します。タスクノードをスポットインスタンスのインスタンスフリートで起動します。処理が完了した時点でタスクノードインスタンスのみを終了します。オンデマンドインスタンスの使用をカバーするためにコンピュートセービングプランを購入します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (50%) D (50%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Continuing to launch all nodes on On-Demand Instances is the most straightforward approach given the criticality of the tasks and the complexity involved in mixing instance types.",
        "situation_analysis": "The configuration uses On-Demand Instances, which provide reliable performance for critical processing tasks that must complete without interruption. The timing of the tasks allows for cost optimization as they do not need to be finished during peak hours.",
        "option_analysis": "Option C ensures that all instances are always available when needed for the critical tasks. Other options like A, B, and D introduce risk if Spot Instances are not available, especially since the tasks are time-sensitive.",
        "additional_knowledge": "If the costs become excessive, future evaluations should consider a shift to include Spot Instances for task nodes only, depending on the application need.",
        "key_terminology": "On-Demand Instances, EMR, Spot Instances, Compute Savings Plans, instance fleet",
        "overall_assessment": "Despite community voting showing support for other options, Option C is the most reliable and straightforward, given the operational requirements of the cluster. It's crucial to ensure that critical tasks are handled efficiently."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCです。すべてのノードをオンデマンドインスタンスで起動し続けることは、タスクの重要性およびインスタンスタイプの混在に伴う複雑さを考えると、最も単純なアプローチです。",
        "situation_analysis": "この構成は、重要な処理タスクに対する信頼性のあるパフォーマンスを提供するオンデマンドインスタンスを使用しています。タスクのタイミングは、ピーク時に完了する必要がないため、コストの最適化を可能にします。",
        "option_analysis": "オプションCは、重要なタスクが必要なときに常にインスタンスが利用可能であることを保証します。他のオプション（A、B、およびD）は、処理が時間に敏感であるため、スポットインスタンスが使用できない場合のリスクをもたらします。",
        "additional_knowledge": "コストが過剰になる場合、アプリケーションのニーズに応じてタスクノードのみを含むスポットインスタンスの使用へのシフトを将来の評価で考慮する必要があります。",
        "key_terminology": "オンデマンドインスタンス、EMR、スポットインスタンス、コンピュートセービングプラン、インスタンスフリート",
        "overall_assessment": "コミュニティの投票が他のオプションに支持を示しているにも関わらず、選択肢Cはクラスターの運用要件に応じて最も信頼性が高く、単純です。重要なタスクが効率的に処理されることを保証することが重要です。"
      }
    ],
    "keywords": [
      "On-Demand Instances",
      "EMR",
      "Spot Instances",
      "Compute Savings Plans",
      "instance fleet"
    ]
  },
  {
    "No": "264",
    "question": "A company has migrated a legacy application to the AWS Cloud. The application runs on three Amazon EC2 instances that are spread across three\nAvailability Zones. One EC2 instance is in each Availability Zone. The EC2 instances are running in three private subnets of the VPC and are set up\nas targets for an Application Load Balancer (ALB) that is associated with three public subnets.\nThe application needs to communicate with on-premises systems. Only trafic from IP addresses in the company's IP address range are allowed to\naccess the on-premises systems. The company's security team is bringing only one IP address from its internal IP address range to the cloud. The\ncompany has added this IP address to the allow list for the company firewall. The company also has created an Elastic IP address for this IP\naddress.\nA solutions architect needs to create a solution that gives the application the ability to communicate with the on-premises systems. The solution\nalso must be able to mitigate failures automatically.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業がレガシーアプリケーションをAWSクラウドに移行しました。このアプリケーションは、3つのAmazon EC2インスタンスで実行されており、それぞれが3つのアベイラビリティゾーンに分散しています。各アベイラビリティゾーンに1つのEC2インスタンスがあります。これらのEC2インスタンスは、VPCの3つのプライベートサブネットで実行されており、3つのパブリックサブネットに関連付けられたアプリケーションロードバランサー（ALB）のターゲットとして設定されています。このアプリケーションはオンプレミスシステムと通信する必要があります。オンプレミスシステムへのアクセスは、企業のIPアドレス範囲内のIPアドレスからのトラフィックのみが許可されています。この企業のセキュリティチームは、内部IPアドレス範囲からわずか1つのIPアドレスのみをクラウドに持ち込んでいます。このIPアドレスは企業のファイアウォールの許可リストに追加されています。企業はまた、このIPアドレスのためにElastic IPアドレスを作成しています。ソリューションアーキテクトは、アプリケーションがオンプレミスシステムと通信できるようにするソリューションを作成する必要があります。このソリューションは、自動的に障害を軽減できる必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy three NAT gateways, one in each public subnet. Assign the Elastic IP address to the NAT gateways. Turn on health checks for the NAT gateways. If a NAT gateway fails a health check, recreate the NAT gateway and assign the Elastic IP address to the new NAT gateway.",
        "text_jp": "各パブリックサブネットに1つずつ、3つのNATゲートウェイをデプロイします。これらのNATゲートウェイにElastic IPアドレスを割り当てます。NATゲートウェイのヘルスチェックをオンにします。NATゲートウェイがヘルスチェックに失敗した場合、新しいNATゲートウェイを再作成し、Elastic IPアドレスを新しいNATゲートウェイに割り当てます。"
      },
      {
        "key": "B",
        "text": "Replace the ALB with a Network Load Balancer (NLB). Assign the Elastic IP address to the NLTurn on health checks for the NLIn the case of a failed health check, redeploy the NLB in different subnets.",
        "text_jp": "ALBをネットワークロードバランサー（NLB）に置き換えます。NLBにElastic IPアドレスを割り当てます。NLBのヘルスチェックをオンにします。ヘルスチェックに失敗した場合、異なるサブネットにNLBを再デプロイします。"
      },
      {
        "key": "C",
        "text": "Deploy a single NAT gateway in a public subnet. Assign the Elastic IP address to the NAT gateway. Use Amazon CloudWatch with a custom metric to monitor the NAT gateway. If the NAT gateway is unhealthy, invoke an AWS Lambda function to create a new NAT gateway in a different subnet. Assign the Elastic IP address to the new NAT gateway.",
        "text_jp": "パブリックサブネットに1つのNATゲートウェイをデプロイします。NATゲートウェイにElastic IPアドレスを割り当てます。Amazon CloudWatchを使用してNATゲートウェイをカスタムメトリックで監視します。NATゲートウェイが不健全な場合、AWS Lambda関数を呼び出して異なるサブネットに新しいNATゲートウェイを作成します。新しいNATゲートウェイにElastic IPアドレスを割り当てます。"
      },
      {
        "key": "D",
        "text": "Assign the Elastic IP address to the ALB. Create an Amazon Route 53 simple record with the Elastic IP address as the value. Create a Route 53 health check. In the case of a failed health check, recreate the ALB in different subnets.",
        "text_jp": "ALBにElastic IPアドレスを割り当てます。Elastic IPアドレスを値として持つAmazon Route 53のシンプルレコードを作成します。Route 53のヘルスチェックを作成します。ヘルスチェックに失敗した場合、異なるサブネットにALBを再作成します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Deploying three NAT gateways in each public subnet allows for redundancy and failover capability.",
        "situation_analysis": "The application needs to communicate with on-premises systems securely and consistently. The limitation to a single external IP requires careful planning of NAT configurations.",
        "option_analysis": "Option A provides a robust solution by utilizing multiple NAT gateways to ensure that the application remains functional even if one of the gateways fails. Option B does not address the requirement for multiple gateways. Option C also provides a solution, but without redundancy due to the single NAT gateway. Option D incorrectly assigns the Elastic IP to an ALB, which is not suitable for this scenario.",
        "additional_knowledge": "AWS provides various services to ensure high availability, including Elastic Load Balancing and Auto Scaling.",
        "key_terminology": "NAT Gateway, Elastic IP, Failover, Application Load Balancer, Network Load Balancer",
        "overall_assessment": "The question effectively evaluates knowledge of AWS networking and redundancy strategies. Despite community support for option C, the best practice is to use multiple NAT gateways as provided in option A. It highlights the challenges of configuring on-premises connectivity in a cloud environment."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。各パブリックサブネットに3つのNATゲートウェイをデプロイすることで冗長性とフェイルオーバー機能が確保される。",
        "situation_analysis": "アプリケーションは、オンプレミスシステムと安全かつ一貫して通信する必要がある。単一の外部IPへの制限があるため、NAT構成の緻密な計画が必要である。",
        "option_analysis": "選択肢Aは、複数のNATゲートウェイを使用することでアプリケーションがどれか一つのゲートウェイが故障しても機能を保つことができるため、堅牢な解決策を提供する。選択肢Bは複数のゲートウェイの要件を満たしていない。選択肢Cも解決策を提供しているが、単一のNATゲートウェイにより冗長性が欠けている。選択肢DはElastic IPをALBに割り当てるが、このシナリオには適していない。",
        "additional_knowledge": "AWSは、Elastic Load BalancingやAuto Scalingを含む高可用性を確保するための様々なサービスを提供している。",
        "key_terminology": "NATゲートウェイ、Elastic IP、フェイルオーバー、アプリケーションロードバランサー、ネットワークロードバランサー",
        "overall_assessment": "この質問は、AWSのネットワーキングと冗長性戦略に関する知識を効果的に評価している。コミュニティが選択肢Cを支持しているにもかかわらず、ベストプラクティスに基づけば選択肢Aに示される複数のNATゲートウェイの使用が最適である。クラウド環境におけるオンプレミス接続の構成の課題を浮き彫りにしている。"
      }
    ],
    "keywords": [
      "NAT Gateway",
      "Elastic IP",
      "Failover",
      "Application Load Balancer",
      "Network Load Balancer"
    ]
  },
  {
    "No": "265",
    "question": "A company uses AWS Organizations to manage more than 1,000 AWS accounts. The company has created a new developer organization. There\nare 540 developer member accounts that must be moved to the new developer organization. All accounts are set up with all the required\ninformation so that each account can be operated as a standalone account.\nWhich combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Choose\nthree.)",
    "question_jp": "A company uses AWS Organizations to manage more than 1,000 AWS accounts. The company has created a new developer organization. There are 540 developer member accounts that must be moved to the new developer organization. All accounts are set up with all the required information so that each account can be operated as a standalone account. Which combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Choose three.)",
    "choices": [
      {
        "key": "A",
        "text": "Call the MoveAccount operation in the Organizations API from the old organization's management account to migrate the developer accounts to the new developer organization.",
        "text_jp": "Call the MoveAccount operation in the Organizations API from the old organization's management account to migrate the developer accounts to the new developer organization."
      },
      {
        "key": "B",
        "text": "From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.",
        "text_jp": "From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API."
      },
      {
        "key": "C",
        "text": "From each developer account, remove the account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.",
        "text_jp": "From each developer account, remove the account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API."
      },
      {
        "key": "D",
        "text": "Sign in to the new developer organization's management account and create a placeholder member account that acts as a target for the developer account migration.",
        "text_jp": "Sign in to the new developer organization's management account and create a placeholder member account that acts as a target for the developer account migration."
      },
      {
        "key": "E",
        "text": "Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to send invitations to the developer accounts.",
        "text_jp": "Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to send invitations to the developer accounts."
      },
      {
        "key": "F",
        "text": "Have each developer sign in to their account and confirm to join the new developer organization.",
        "text_jp": "Have each developer sign in to their account and confirm to join the new developer organization."
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "BEF (81%) Other",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer includes removing the accounts from the old organization and inviting them to the new organization. Specifically, the choices should be C, E, and F.",
        "situation_analysis": "The company needs to move 540 developer accounts from an old organization to a new one while ensuring accounts can function independently.",
        "option_analysis": "Option C is essential because each developer must initiate the removal from the old organization. Options E and F are also necessary for inviting and confirming the accounts in the new organization.",
        "additional_knowledge": "AWS Organizations allows a unified management of multiple AWS accounts, enabling budget control and security policy management.",
        "key_terminology": "AWS Organizations, MoveAccount, RemoveAccountFromOrganization, InviteAccountToOrganization.",
        "overall_assessment": "While Option C is critical, the correct choices must include processes for both removal and invitation to the new organization, which are Options C, E, and F. The community's preference seems to incorrectly favor rarely utilized operations."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答には、古い組織からアカウントを削除し、新しい組織に招待する手続きが含まれます。具体的には、選択肢はC、E、Fであるべきです。",
        "situation_analysis": "会社は、540の開発者アカウントを古い組織から新しい組織に移動する必要があります。アカウントは独立して機能できる必要があります。",
        "option_analysis": "選択肢Cは重要であり、各開発者が古い組織からの削除を開始する必要があります。選択肢EとFも新しい組織での招待と確認に必要です。",
        "additional_knowledge": "AWS Organizationsは複数のAWSアカウントの一元管理を実現し、予算管理やセキュリティポリシー管理を可能にします。",
        "key_terminology": "AWS Organizations、MoveAccount、RemoveAccountFromOrganization、InviteAccountToOrganization。",
        "overall_assessment": "選択肢Cは重要ですが、正しい選択には古い組織からの削除と新しい組織への招待のプロセスが含まれる必要があります。これは選択肢C、E、およびFです。コミュニティの好みは、あまり利用されない操作を好むように見えます。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "MoveAccount",
      "RemoveAccountFromOrganization",
      "InviteAccountToOrganization"
    ]
  },
  {
    "No": "266",
    "question": "A company's interactive web application uses an Amazon CloudFront distribution to serve images from an Amazon S3 bucket. Occasionally, third-\nparty tools ingest corrupted images into the S3 bucket. This image corruption causes a poor user experience in the application later. The company\nhas successfully implemented and tested Python logic to detect corrupt images.\nA solutions architect must recommend a solution to integrate the detection logic with minimal latency between the ingestion and serving.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業のインタラクティブなWebアプリケーションは、Amazon S3バケットから画像を配信するためにAmazon CloudFrontディストリビューションを使用しています。時折、第三者のツールがS3バケットに壊れた画像を取り込むことがあります。この画像の破損は、後でアプリケーション内でのユーザー体験を悪化させます。企業は、壊れた画像を検出するためのPythonロジックを成功裏に実装し、テストしました。\nソリューションアーキテクトは、取り込みと配信の間で遅延を最小限に抑えつつ、検出ロジックを統合するためのソリューションを推奨する必要があります。\nどのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Use a Lambda@Edge function that is invoked by a viewer-response event.",
        "text_jp": "ビュワー応答イベントによって呼び出されるLambda@Edge関数を使用する。"
      },
      {
        "key": "B",
        "text": "Use a Lambda@Edge function that is invoked by an origin-response event.",
        "text_jp": "オリジン応答イベントによって呼び出されるLambda@Edge関数を使用する。"
      },
      {
        "key": "C",
        "text": "Use an S3 event notification that invokes an AWS Lambda function.",
        "text_jp": "AWS Lambda関数を呼び出すS3イベント通知を使用する。"
      },
      {
        "key": "D",
        "text": "Use an S3 event notification that invokes an AWS Step Functions state machine.",
        "text_jp": "AWS Step Functionsステートマシンを呼び出すS3イベント通知を使用する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. Using a Lambda@Edge function invoked by an origin-response event allows you to run the image detection logic as soon as the image is served from the origin, minimizing any latency experienced by the user.",
        "situation_analysis": "The company needs to ensure that corrupted images are detected and handled before they impact end-user experience. This requires a solution that can quickly evaluate images after they are retrieved from S3 but before they are sent to the viewer.",
        "option_analysis": "Option A, which invokes the function on a viewer-response event, happens after the image has already been delivered to the user. This doesn't address the problem effectively. Option C uses S3 notifications to trigger Lambda, but the integration with CloudFront serving would not be immediate. Option D introduces unnecessary complexity with Step Functions that’s not needed for this simple detection task.",
        "additional_knowledge": "Existing Python logic can be adapted to run within a Lambda@Edge function.",
        "key_terminology": "Lambda@Edge, origin-response event, image detection, CloudFront, S3 bucket",
        "overall_assessment": "The choice of using a Lambda@Edge function provides a streamlined way to integrate image validation with immediate processing, fulfilling both the latency and detection requirements effectively. Community voting suggests option C, indicating a misunderstanding of the requirement for quick response."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。オリジン応答イベントによって呼び出されるLambda@Edge関数を使用することで、画像がオリジンから配信される際に検出ロジックを実行でき、ユーザーに経験する遅延を最小限に抑えることができる。",
        "situation_analysis": "企業は、壊れた画像がエンドユーザーの体験に影響を与える前に、それを検出し処理する必要がある。このため、S3から取得された画像の後に行うが、視聴者に送信される前に評価できるソリューションが求められる。",
        "option_analysis": "選択肢Aはビュワー応答イベントで呼び出され、画像がすでにユーザーに配信された後に起こるため、問題に効果的に対処できない。選択肢CはS3通知を使用してLambdaをトリガーするが、CloudFront配信との統合が即時にはならない。選択肢Dはこの単純な検出タスクには不要な複雑さをもたらす。",
        "additional_knowledge": "既存のPythonロジックは、Lambda@Edge関数内で実行されるように適応できる。",
        "key_terminology": "Lambda@Edge、オリジン応答イベント、画像検出、CloudFront、S3バケット",
        "overall_assessment": "Lambda@Edge関数の選択は、即時処理を通じて画像の検証を統合する効率的な方法を提供し、遅延と検出の両方の要件を効果的に満たす。コミュニティ投票は選択肢Cを示しており、迅速な応答の要件に対する誤解を示唆している。"
      }
    ],
    "keywords": [
      "Lambda@Edge",
      "origin-response event",
      "image detection",
      "CloudFront",
      "S3 bucket"
    ]
  },
  {
    "No": "267",
    "question": "A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group. The company uses AWS CodePipeline to\ndeploy the application. The instances that run in the Auto Scaling group are constantly changing because of scaling events.\nWhen the company deploys new application code versions, the company installs the AWS CodeDeploy agent on any new target EC2 instances and\nassociates the instances with the CodeDeploy deployment group. The application is set to go live within the next 24 hours.\nWhat should a solutions architect recommend to automate the application deployment process with the LEAST amount of operational overhead?",
    "question_jp": "ある企業が、Amazon EC2インスタンスで実行されるアプリケーションを持っており、Amazon EC2 Auto Scalingグループに配置されています。この企業は、アプリケーションを展開するためにAWS CodePipelineを使用しています。Auto Scalingグループで実行されるインスタンスはスケーリングイベントのため常に変化しています。新しいアプリケーションコードのバージョンを展開する際、企業は新しいターゲットEC2インスタンスにAWS CodeDeployエージェントをインストールし、そのインスタンスをCodeDeployデプロイメントグループに関連付けています。アプリケーションは、次の24時間以内に稼働を開始する予定です。ソリューションアーキテクトが、運用オーバーヘッドを最小限に抑えつつアプリケーションのデプロイプロセスを自動化するために推奨すべきことは何でしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Configure Amazon EventBridge to invoke an AWS Lambda function when a new EC2 instance is launched into the Auto Scaling group. Code the Lambda function to associate the EC2 instances with the CodeDeploy deployment group.",
        "text_jp": "新しいEC2インスタンスがAuto Scalingグループに起動された際にAWS Lambda関数を呼び出すためにAmazon EventBridgeを設定します。Lambda関数をコーディングして、EC2インスタンスをCodeDeployデプロイメントグループに関連付けます。"
      },
      {
        "key": "B",
        "text": "Write a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new code. When the deployment is complete, create a new AMI and configure the Auto Scaling group's launch template to use the new AMI for new launches. Resume Amazon EC2 Auto Scaling operations.",
        "text_jp": "新しいコードを展開する前にAmazon EC2 Auto Scaling操作を中断するスクリプトを書きます。デプロイが完了したら、新しいAMIを作成し、Auto Scalingグループの起動テンプレートが新しいAMIを使用するように設定します。Amazon EC2 Auto Scaling操作を再開します。"
      },
      {
        "key": "C",
        "text": "Create a new AWS CodeBuild project that creates a new AMI that contains the new code. Configure CodeBuild to update the Auto Scaling group's launch template to the new AMI. Run an Amazon EC2 Auto Scaling instance refresh operation.",
        "text_jp": "新しいコードを含む新しいAMIを作成するAWS CodeBuildプロジェクトを作成します。CodeBuildを構成して、Auto Scalingグループの起動テンプレートを新しいAMIに更新します。Amazon EC2 Auto Scalingインスタンスのリフレッシュ操作を実行します。"
      },
      {
        "key": "D",
        "text": "Create a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group's launch template to use the new AMI. Associate the CodeDeploy deployment group with the Auto Scaling group instead of the EC2 instances.",
        "text_jp": "CodeDeployエージェントがインストールされた新しいAMIを作成します。Auto Scalingグループの起動テンプレートを新しいAMIを使用するように設定します。EC2インスタンスの代わりにAuto ScalingグループにCodeDeployデプロイメントグループを関連付けます。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "D (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This approach minimizes operational overhead while automating the deployment process by managing the entire Auto Scaling group rather than individual EC2 instances.",
        "situation_analysis": "The company needs to ensure that all the new instances in the Auto Scaling group can automatically deploy new code with minimal manual intervention, especially as the instances are frequently changing due to scaling events.",
        "option_analysis": "Option A requires additional Lambda function management, while option B introduces downtime during the code deployment. Option C adds complexity by creating AMIs during deployment, making it less efficient. Option D directly associates the CodeDeploy deployment group with the Auto Scaling group, allowing all instances to receive updates without extra management.",
        "additional_knowledge": "Using CodeDeploy with Auto Scaling groups allows for improved service reliability and faster deployment cycles.",
        "key_terminology": "AWS CodeDeploy, Auto Scaling group, AMI, continuous deployment, operational overhead",
        "overall_assessment": "The answer aligns well with AWS best practices for deploying applications in a dynamic environment. The community overwhelmingly supports this choice due to its simplicity and automation capabilities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。このアプローチは、個別のEC2インスタンスではなく、Auto Scalingグループ全体を管理することにより、運用オーバーヘッドを最小限に抑えつつデプロイメントプロセスを自動化する。",
        "situation_analysis": "企業は、スケーリングイベントにより頻繁に変更されるインスタンスに対して、最小限の手動介入で新しいコードを自動的にデプロイできるようにする必要がある。",
        "option_analysis": "オプションAは追加のLambda関数管理を必要とし、オプションBはデプロイ中にダウンタイムを導入する。オプションCはデプロイ中にAMIを作成するという複雑さを加え、効率的でない。オプションDはCodeDeployデプロイメントグループをAuto Scalingグループに直接関連付け、すべてのインスタンスが追加の管理なしで更新を受け取れるようにする。",
        "additional_knowledge": "CodeDeployをAuto Scalingグループと共に使用すると、サービスの信頼性が向上し、デプロイサイクルが短縮される。",
        "key_terminology": "AWS CodeDeploy, Auto Scalingグループ, AMI, 継続的デプロイメント, 運用オーバーヘッド",
        "overall_assessment": "この答えは、動的環境でのアプリケーションデプロイに関するAWSのベストプラクティスに良く合致している。コミュニティは、そのシンプルさと自動化機能のためにこの選択を圧倒的に支持している。"
      }
    ],
    "keywords": [
      "AWS CodeDeploy",
      "Auto Scaling group",
      "AMI",
      "continuous deployment",
      "operational overhead"
    ]
  },
  {
    "No": "268",
    "question": "A company has a website that runs on four Amazon EC2 instances that are behind an Application Load Balancer (ALB). When the ALB detects that\nan EC2 instance is no longer available, an Amazon CloudWatch alarm enters the ALARM state. A member of the company's operations team then\nmanually adds a new EC2 instance behind the ALB.\nA solutions architect needs to design a highly available solution that automatically handles the replacement of EC2 instances. The company\nneeds to minimize downtime during the switch to the new solution.\nWhich set of steps should the solutions architect take to meet these requirements?",
    "question_jp": "企業は4つのAmazon EC2インスタンスで動作するウェブサイトを運営しており、これらのインスタンスはアプリケーションロードバランサー（ALB）の背後に配置されています。ALBがEC2インスタンスの利用不可を検出すると、Amazon CloudWatchアラームがALARM状態に入ります。その後、企業の運用チームのメンバーが手動で新しいEC2インスタンスをALBの背後に追加します。ソリューションアーキテクトは、EC2インスタンスの交換を自動的に処理する高可用性のソリューションを設計する必要があります。企業は、新しいソリューションへの切り替え中のダウンタイムを最小限に抑える必要があります。この要件を満たすために、ソリューションアーキテクトはどのような手順を踏むべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Delete the existing ALB. Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Attach the existing EC2 instances to the Auto Scaling group.",
        "text_jp": "既存のALBを削除します。ウェブアプリケーショントラフィックを処理するように構成されたAuto Scalingグループを作成します。新しい起動テンプレートをAuto Scalingグループにアタッチします。新しいALBを作成します。Auto Scalingグループを新しいALBにアタッチします。既存のEC2インスタンスをAuto Scalingグループにアタッチします。"
      },
      {
        "key": "B",
        "text": "Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALAttach the existing EC2 instances to the Auto Scaling group.",
        "text_jp": "ウェブアプリケーショントラフィックを処理するように構成されたAuto Scalingグループを作成します。新しい起動テンプレートをAuto Scalingグループにアタッチします。Auto Scalingグループを既存のALBにアタッチします。既存のEC2インスタンスをAuto Scalingグループにアタッチします。"
      },
      {
        "key": "C",
        "text": "Delete the existing ALB and the EC2 instances. Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Wait for the Auto Scaling group to launch the minimum number of EC2 instances.",
        "text_jp": "既存のALBとEC2インスタンスを削除します。ウェブアプリケーショントラフィックを処理するように構成されたAuto Scalingグループを作成します。新しい起動テンプレートをAuto Scalingグループにアタッチします。新しいALBを作成します。Auto Scalingグループを新しいALBにアタッチします。Auto Scalingグループが最小数のEC2インスタンスを起動するのを待ちます。"
      },
      {
        "key": "D",
        "text": "Create an Auto Scaling group that is configured to handle the web application trafic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Wait for the existing ALB to register the existing EC2 instances with the Auto Scaling group.",
        "text_jp": "ウェブアプリケーショントラフィックを処理するように構成されたAuto Scalingグループを作成します。新しい起動テンプレートをAuto Scalingグループにアタッチします。Auto Scalingグループを既存のALBにアタッチします。既存のALBが既存のEC2インスタンスをAuto Scalingグループに登録するのを待ちます。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which outlines a process to ensure that new EC2 instances are added automatically without any manual intervention.",
        "situation_analysis": "The requirement is to establish a highly available architecture that can automatically replace faulty EC2 instances with minimal impact on uptime.",
        "option_analysis": "Option C includes the deletion of existing EC2 instances and ALB, creation of a new Auto Scaling group configured for web traffic, and setting up a new ALB, which aligns with best practices to ensure redundancy and automated scaling. Conversely, options A, B, and D do not fully achieve the necessary automation or involve redundant configurations, potentially leading to more downtime.",
        "additional_knowledge": "Understanding the dependency between Application Load Balancers and Auto Scaling groups helps in designing resilient infrastructures.",
        "key_terminology": "Auto Scaling Group, Application Load Balancer, EC2 Instance, Fault Tolerance, High Availability",
        "overall_assessment": "Despite community vote distribution favoring option B, the best approach for high availability is indeed option C as it fully encompasses the automation necessary to minimize downtime and improve management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCであり、これは新しいEC2インスタンスが手動での介入なしに自動的に追加されるプロセスを概説しています。",
        "situation_analysis": "要件は、故障したEC2インスタンスを自動的に交換できる高可用性のアーキテクチャを確立することであり、稼働時間への影響を最小限に抑える必要があります。",
        "option_analysis": "選択肢Cには、既存のEC2インスタンスとALBの削除、ウェブトラフィック用に構成された新しいAuto Scalingグループの作成、新しいALBの設定が含まれており、冗長性と自動スケーリングを確保するためのベストプラクティスに沿っています。一方、選択肢A、B、およびDは、必要な自動化を完全には実現しておらず、冗長な構成が含まれている可能性があり、ダウンタイムを増やす可能性があります。",
        "additional_knowledge": "アプリケーションロードバランサーとオートスケーリンググループの依存関係を理解することは、耐障害性のあるインフラストラクチャを設計するのに役立ちます。",
        "key_terminology": "オートスケーリンググループ、アプリケーションロードバランサー、EC2インスタンス、障害耐性、高可用性",
        "overall_assessment": "コミュニティの投票分布が選択肢Bを支持しているにもかかわらず、高可用性のための最良のアプローチは、稼働時間を最小限に抑え、管理を改善するために必要な自動化を完全に包含している選択肢Cであることは明らかです。"
      }
    ],
    "keywords": [
      "Auto Scaling Group",
      "Application Load Balancer",
      "EC2 Instance",
      "Fault Tolerance",
      "High Availability"
    ]
  },
  {
    "No": "269",
    "question": "A company wants to optimize AWS data-transfer costs and compute costs across developer accounts within the company's organization in AWS\nOrganizations. Developers can configure VPCs and launch Amazon EC2 instances in a single AWS Region. The EC2 instances retrieve\napproximately 1 TB of data each day from Amazon S3.\nThe developer activity leads to excessive monthly data-transfer charges and NAT gateway processing charges between EC2 instances and S3\nbuckets, along with high compute costs. The company wants to proactively enforce approved architectural patterns for any EC2 instance and VPC\ninfrastructure that developers deploy within the AWS accounts. The company does not want this enforcement to negatively affect the speed at\nwhich the developers can perform their tasks.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業は、AWS Organizations内の開発者アカウント全体でAWSデータ転送コストとコンピュートコストを最適化したいと考えています。開発者は、単一のAWSリージョン内でVPCを構成し、Amazon EC2インスタンスを起動することができます。EC2インスタンスは、毎日約1TBのデータをAmazon S3から取得します。この開発者のアクティビティにより、EC2インスタンスとS3バケット間の過剰な月次データ転送料金とNATゲートウェイ処理料金が発生しており、さらに高いコンピュートコストが発生しています。企業は、開発者がAWSアカウント内にデプロイするEC2インスタンスおよびVPCインフラストラクチャに対して承認されたアーキテクチャパターンを自発的に強制したいと考えています。この強制が開発者の作業速度に悪影響を与えないことを望んでいます。どのソリューションがこれらの要件を最もコスト効率よく満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create SCPs to prevent developers from launching unapproved EC2 instance types. Provide the developers with an AWS CloudFormation template to deploy an approved VPC configuration with S3 interface endpoints. Scope the developers' IAM permissions so that the developers can launch VPC resources only with CloudFormation.",
        "text_jp": "EC2インスタンスタイプを未承認で起動することを防ぐためにSCPを作成する。開発者にS3インターフェースエンドポイントを含む承認されたVPC構成をデプロイするためのAWS CloudFormationテンプレートを提供する。開発者のIAM権限を制限し、CloudFormationを使ってのみVPCリソースを起動できるようにする。"
      },
      {
        "key": "B",
        "text": "Create a daily forecasted budget with AWS Budgets to monitor EC2 compute costs and S3 data-transfer costs across the developer accounts. When the forecasted cost is 75% of the actual budget cost, send an alert to the developer teams. If the actual budget cost is 100%, create a budget action to terminate the developers' EC2 instances and VPC infrastructure.",
        "text_jp": "AWS Budgetsで予測された日次予算を作成し、EC2コンピュートコストとS3データ転送コストを開発者アカウントで監視する。予測されたコストが実際の予算コストの75%に達したときに開発者チームに警告を送信する。実際の予算コストが100%になった場合、開発者のEC2インスタンスおよびVPCインフラストラクチャを終了するための予算アクションを作成する。"
      },
      {
        "key": "C",
        "text": "Create an AWS Service Catalog portfolio that users can use to create an approved VPC configuration with S3 gateway endpoints and approved EC2 instances. Share the portfolio with the developer accounts. Configure an AWS Service Catalog launch constraint to use an approved IAM role. Scope the developers' IAM permissions to allow access only to AWS Service Catalog.",
        "text_jp": "ユーザーがS3ゲートウェイエンドポイントおよび承認されたEC2インスタンスを使って承認されたVPC構成を作成できるAWS Service Catalogポートフォリオを作成する。ポートフォリオを開発者アカウントと共有する。承認されたIAMロールを使用するためにAWS Service Catalogの起動制約を設定する。開発者のIAM権限を制限し、AWS Service Catalogへのアクセスのみを許可する。"
      },
      {
        "key": "D",
        "text": "Create and deploy AWS Config rules to monitor the compliance of EC2 and VPC resources in the developer AWS accounts. If developers launch unapproved EC2 instances or if developers create VPCs without S3 gateway endpoints, perform a remediation action to terminate the unapproved resources.",
        "text_jp": "AWS Configルールを作成および展開して、開発者のAWSアカウントのEC2およびVPCリソースのコンプライアンスを監視する。開発者が未承認のEC2インスタンスを起動した場合、またはS3ゲートウェイエンドポイントなしでVPCを作成した場合、未承認のリソースを終了する修復アクションを実行する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The most cost-effective solution is to create an AWS Service Catalog portfolio that contains a pre-approved VPC configuration and EC2 instances. This allows developers to deploy resources in a predefined manner, mitigating unnecessary costs due to misconfigurations.",
        "situation_analysis": "The organization seeks to enforce architectural patterns while minimizing costs and avoiding delays in development. Developers currently incur high data transfer and compute costs from EC2-S3 interactions.",
        "option_analysis": "Option C allows control over AWS resources while providing flexibility to developers. It aligns with best practices by using AWS Service Catalog, which simplifies resource provisioning. Options A and D, while they offer compliance checks, do not provide the same level of pre-approved configurations, potentially leading to ongoing misconfigurations.",
        "additional_knowledge": "Using S3 gateway endpoints reduces data transfer costs to S3, making it an integral part of this architecture.",
        "key_terminology": "AWS Service Catalog, VPC, EC2, S3, IAM Roles",
        "overall_assessment": "The suggestion of using AWS Service Catalog is a strategic choice that balances compliance and efficiency, reducing the likelihood of cost spikes due to resource mismanagement. It fosters a compliant environment while retaining developer agility."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "最もコスト効果の高い解決策は、承認されたVPC構成とEC2インスタンスを含むAWS Service Catalogポートフォリオを作成することである。これにより、開発者はあらかじめ定義された方法でリソースをデプロイでき、不適切な構成による不要なコストを軽減できる。",
        "situation_analysis": "組織は、コストを最小限に抑えつつ、開発の遅延を避けながらアーキテクチャパターンを強制しようとしている。現在、開発者はEC2とS3の相互作用から高いデータ転送およびコンピュートコストが発生している。",
        "option_analysis": "オプションCは、AWSリソースに対する制御を提供しながら、開発者に柔軟性を与える。これにより、AWS Service Catalogを使用することでリソースのプロビジョニングを簡素化し、ベストプラクティスに沿っている。他のオプションであるAやDは、コンプライアンスチェックを提供するが、同程度の承認済み構成を提供せず、継続的な不適切構成を引き起こす可能性がある。",
        "additional_knowledge": "S3ゲートウェイエンドポイントを使用することで、S3へのデータ転送コストを削減でき、このアーキテクチャの重要な部分となる。",
        "key_terminology": "AWS Service Catalog, VPC, EC2, S3, IAMロール",
        "overall_assessment": "AWS Service Catalogを使用する提案は、コンプライアンスと効率のバランスを取る戦略的な選択であり、リソースの不適切な管理によるコストの急増を防ぐ可能性を減少させる。開発者の迅速さを保ちながらも、コンプライアントな環境を育む。"
      }
    ],
    "keywords": [
      "AWS Service Catalog",
      "VPC",
      "EC2",
      "S3",
      "IAM Roles"
    ]
  },
  {
    "No": "270",
    "question": "A company is expanding. The company plans to separate its resources into hundreds of different AWS accounts in multiple AWS Regions. A\nsolutions architect must recommend a solution that denies access to any operations outside of specifically designated Regions.\nWhich solution will meet these requirements?",
    "question_jp": "会社は拡張しています。この会社は、リソースを複数のAWSリージョンにおいて、何百もの異なるAWSアカウントに分離する計画です。ソリューションアーキテクトは、特定の指定されたリージョンの外での操作を禁止するソリューションを推奨する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create IAM roles for each account. Create IAM policies with conditional allow permissions that include only approved Regions for the accounts.",
        "text_jp": "各アカウントのためにIAMロールを作成します。アカウントのために承認されたリージョンのみを含む条件付き許可権限を持つIAMポリシーを作成します。"
      },
      {
        "key": "B",
        "text": "Create an organization in AWS Organizations. Create IAM users for each account. Attach a policy to each user to block access to Regions where an account cannot deploy infrastructure.",
        "text_jp": "AWS Organizationsで組織を作成します。各アカウントのためにIAMユーザーを作成します。アカウントがインフラストラクチャをデプロイできないリージョンへのアクセスをブロックするポリシーを各ユーザーに添付します。"
      },
      {
        "key": "C",
        "text": "Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions.",
        "text_jp": "AWS Control Towerのランディングゾーンを立ち上げます。OUを作成し、承認されたリージョンの外でサービスを実行することを禁止するSCPを添付します。"
      },
      {
        "key": "D",
        "text": "Enable AWS Security Hub in each account. Create controls to specify the Regions where an account can deploy infrastructure.",
        "text_jp": "各アカウントでAWS Security Hubを有効にします。アカウントがインフラストラクチャをデプロイできるリージョンを指定する制御を作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "Correct Answer: B - Creating an organization in AWS Organizations allows for centralized management, and attaching policies to IAM users ensures restricted access to specific regions.",
        "situation_analysis": "The requirement is to restrict access to certain AWS regions based on organizational needs. This is crucial in multi-region setups where compliance and cost management are pivotal.",
        "option_analysis": "Option A does not provide organizational control; option C requires initial setup and may not directly block regions for individual users without additional configurations; option D focuses on security without region restrictions.",
        "additional_knowledge": "By using AWS Organizations, administrators have the flexibility to control access and configuration across multiple accounts effectively.",
        "key_terminology": "AWS Organizations, IAM Policies, SCP, Region Restrictions, IAM Users",
        "overall_assessment": "The answer is strong as it provides a fundamental structure for managing account access, which is essential for companies operating across multiple AWS regions. The community vote suggests that while C seems viable, B is fundamentally aligned with governance."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解: B - AWS Organizationsで組織を作成することで、集中管理が可能になります。また、IAMユーザーにポリシーを添付することで、特定のリージョンへのアクセスを制限することができます。",
        "situation_analysis": "特定のAWSリージョンへのアクセスを制限する必要があります。これは、コンプライアンスやコスト管理が重要なマルチリージョン環境において極めて重要です。",
        "option_analysis": "選択肢Aは組織的な管理を提供しない; 選択肢Cは初期設定を必要とし、個々のユーザーに対するリージョンのブロックを直接行うことはできない; 選択肢Dはリージョン制限なしでセキュリティに焦点を当てています。",
        "additional_knowledge": "AWS Organizationsを使用することで、管理者は複数のアカウントにわたって効果的にアクセス制御と設定を管理する柔軟性を得られます。",
        "key_terminology": "AWS Organizations, IAMポリシー, SCP, リージョン制限, IAMユーザー",
        "overall_assessment": "この答えは、複数のAWSリージョンにわたって事業を展開する会社にとって必須のアカウントアクセス管理のための基本的な構造を提供しています。コミュニティ投票は、Cが妥当であるように見えるが、Bがガバナンスと根本的に一致していることを示唆しています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "IAM Policies",
      "SCP",
      "Region Restrictions",
      "IAM Users"
    ]
  },
  {
    "No": "271",
    "question": "A company wants to refactor its retail ordering web application that currently has a load-balanced Amazon EC2 instance fieet for web hosting,\ndatabase API services, and business logic. The company needs to create a decoupled, scalable architecture with a mechanism for retaining failed\norders while also minimizing operational costs.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、現在、ウェブホスティング、データベースAPIサービス、ビジネスロジックのためのロードバランスされたAmazon EC2インスタンスのフィートを持つ小売注文ウェブアプリケーションをリファクタリングしたいと考えています。企業は、運用コストを最小限に抑えながら、失敗した注文を保持するメカニズムを備えた、分離可能でスケーラブルなアーキテクチャを作成する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Amazon S3 for web hosting with Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic Container Service (Amazon ECS) for business logic with Amazon SQS long polling for retaining failed orders.",
        "text_jp": "ウェブホスティングのためにAmazon S3を使用し、データベースAPIサービスにはAmazon API Gatewayを使用します。注文キューにはAmazon Simple Queue Service (Amazon SQS)を利用します。ビジネスロジックにはAmazon Elastic Container Service (Amazon ECS)を使用し、失敗した注文の保持にはAmazon SQSのロングポーリングを利用します。"
      },
      {
        "key": "B",
        "text": "Use AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API services. Use Amazon MQ for order queuing. Use AWS Step Functions for business logic with Amazon S3 Glacier Deep Archive for retaining failed orders.",
        "text_jp": "ウェブホスティングにはAWS Elastic Beanstalkを使用して、データベースAPIサービスにはAmazon API Gatewayを使用します。注文キューにはAmazon MQを利用し、ビジネスロジックにはAWS Step Functionsを使用し、失敗した注文の保持にはAmazon S3 Glacier Deep Archiveを利用します。"
      },
      {
        "key": "C",
        "text": "Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an Amazon SQS dead-letter queue for retaining failed orders.",
        "text_jp": "ウェブホスティングにはAmazon S3を使用し、データベースAPIサービスにはAWS AppSyncを使用します。注文キューにはAmazon Simple Queue Service (Amazon SQS)を利用し、ビジネスロジックにはAWS Lambdaを使用し、失敗した注文の保持にはAmazon SQSのデッドレターキューを利用します。"
      },
      {
        "key": "D",
        "text": "Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Amazon Simple Email Service (Amazon SES) for order queuing. Use Amazon Elastic Kubernetes Service (Amazon EKS) for business logic with Amazon OpenSearch Service for retaining failed orders.",
        "text_jp": "ウェブホスティングにはAmazon Lightsailを使用し、データベースAPIサービスにはAWS AppSyncを使用します。注文キューにはAmazon Simple Email Service (Amazon SES)を利用し、ビジネスロジックにはAmazon Elastic Kubernetes Service (Amazon EKS)を使用し、失敗した注文の保持にはAmazon OpenSearch Serviceを利用します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (90%) 10%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Using Amazon S3 for web hosting and combining it with Amazon API Gateway to manage the database API services aligns well with a decoupled architecture. Using Amazon SQS provides a reliable queuing mechanism that can handle orders and retain failed ones efficiently when combined with Amazon ECS for the business logic, allowing for scalability and reduced operational costs.",
        "situation_analysis": "The company aims to refactor its existing architecture to make it more scalable while ensuring that order failures are retained without incurring significant costs.",
        "option_analysis": "Option B introduces AWS Elastic Beanstalk, which is less flexible compared to S3 for static web hosting. Option C suggests Lambda with a dead-letter queue, which is not as efficient for queuing compared to SQS for high-volume orders. Option D involves several services that may introduce undue complexity.",
        "additional_knowledge": "Utilizing S3 and SQS can significantly lower operational costs compared to more complex solutions such as ECS or EKS.",
        "key_terminology": "Amazon S3, Amazon API Gateway, Amazon SQS, Amazon ECS, decoupled architecture",
        "overall_assessment": "Despite community votes indicating option C, option A provides a complete solution that meets all requirements while considering cost efficiency and scalability. The community may not fully understand the advantages of SQS in this scenario."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAである。ウェブホスティングにAmazon S3を使用し、データベースAPIサービスを管理するためにAmazon API Gatewayを組み合わせることは、分離可能なアーキテクチャに適している。Amazon SQSを使用することで、注文を処理し、失敗したものを効率的に保持する信頼性のあるキューイングメカニズムが提供され、Amazon ECSをビジネスロジックに使用することで、スケーラビリティと運用コストの削減を実現できる。",
        "situation_analysis": "企業は、既存のアーキテクチャをリファクタリングし、よりスケーラブルなものにし、注文の失敗を保持しながらも大きなコストをかけないようにすることを目指している。",
        "option_analysis": "選択肢Bは、AWS Elastic Beanstalkを導入しているが、静的ウェブホスティングにおいてS3ほどの柔軟性がない。選択肢Cは、デッドレターキューを使用するLambdaを提案しているが、高ボリュームの注文にはSQSのほうが効率的にならない。選択肢Dは、過剰な複雑さを導入する可能性があるサービスが多すぎる。",
        "additional_knowledge": "S3とSQSを活用することで、EKSやECSのようなより複雑なソリューションに比べて、運用コストを大幅に削減できる。",
        "key_terminology": "Amazon S3, Amazon API Gateway, Amazon SQS, Amazon ECS, 分離可能なアーキテクチャ",
        "overall_assessment": "コミュニティ投票は選択肢Cを指し示しているが、選択肢Aはすべての要件を満たす完全なソリューションを提供し、コスト効率とスケーラビリティを考慮している。コミュニティがこのシナリオにおけるSQSの利点を十分に理解していない可能性がある。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "Amazon API Gateway",
      "Amazon SQS",
      "Amazon ECS",
      "Decoupled Architecture"
    ]
  },
  {
    "No": "272",
    "question": "A company hosts a web application on AWS in the us-east-1 Region. The application servers are distributed across three Availability Zones behind\nan Application Load Balancer. The database is hosted in a MySQL database on an Amazon EC2 instance. A solutions architect needs to design a\ncross-Region data recovery solution using AWS services with an RTO of less than 5 minutes and an RPO of less than 1 minute. The solutions\narchitect is deploying application servers in us-west-2, and has configured Amazon Route 53 health checks and DNS failover to us-west-2.\nWhich additional step should the solutions architect take?",
    "question_jp": "企業は、us-east-1リージョンでAWS上にWebアプリケーションをホストしています。アプリケーションサーバーは、Application Load Balancerの背後にある3つのアベイラビリティゾーンに分散しています。データベースは、Amazon EC2インスタンス上のMySQLデータベースにホストされています。ソリューションアーキテクトは、AWSサービスを使用して、RTOが5分未満、RPOが1分未満のクロスリージョンデータ復旧ソリューションを設計する必要があります。ソリューションアーキテクトは、us-west-2にアプリケーションサーバーをデプロイし、Amazon Route 53のヘルスチェックとDNSフェイルオーバーをus-west-2に設定しました。ソリューションアーキテクトは、どの追加の手順を講じるべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica in us-west-2.",
        "text_jp": "クロスリージョンの読み取りレプリカをus-west-2に持つAmazon RDS for MySQLインスタンスにデータベースを移行する。"
      },
      {
        "key": "B",
        "text": "Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2.",
        "text_jp": "プライマリがus-east-1、セカンダリがus-west-2のAmazon Auroraグローバルデータベースにデータベースを移行する。"
      },
      {
        "key": "C",
        "text": "Migrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment.",
        "text_jp": "Multi-AZデプロイメントを使用したAmazon RDS for MySQLインスタンスにデータベースを移行する。"
      },
      {
        "key": "D",
        "text": "Create a MySQL standby database on an Amazon EC2 instance in us-west-2.",
        "text_jp": "us-west-2にAmazon EC2インスタンス上のMySQLスタンバイデータベースを作成する。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Creating a MySQL standby database on an Amazon EC2 instance in us-west-2 provides rapid recovery options in case of a disaster.",
        "situation_analysis": "The company requires a cross-region data recovery solution that meets specific RTO and RPO requirements. A standby database allows for quick failover and recovery.",
        "option_analysis": "Option D ensures high availability and minimizes downtime, which aligns well with the established RTO and RPO objectives. Other options focus on replication or setups that may not meet immediate recovery needs in less than 5 minutes.",
        "additional_knowledge": "Standby databases versus automated replication methods can vary significantly in RTO and cost.",
        "key_terminology": "RTO, RPO, AWS, database failover",
        "overall_assessment": "Given the requirements for rapid failover, option D is the most reasonable choice despite community consensus on option B. The other options involve setups that may introduce latency."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。us-west-2にAmazon EC2インスタンス上のMySQLスタンバイデータベースを作成することは、災害時に迅速な復旧オプションを提供する。",
        "situation_analysis": "企業は、特定のRTOおよびRPO要件を満たすクロスリージョンデータ復旧ソリューションを必要としている。スタンバイデータベースは、迅速なフェイルオーバーと復旧を可能にする。",
        "option_analysis": "オプションDは高可用性を確保し、ダウンタイムを最小限に抑えるため、確立されたRTOおよびRPOの目標に非常によく合致する。他のオプションはレプリケーションやセットアップに焦点を当てており、5分未満での即時復旧ニーズを満たさない可能性がある。",
        "additional_knowledge": "スタンバイデータベースと自動レプリケーション手法は、RTOおよびコストにおいて大きく異なる場合がある。",
        "key_terminology": "RTO, RPO, AWS, データベースフェイルオーバー",
        "overall_assessment": "迅速なフェイルオーバー要件を考慮すると、Dオプションは最も理にかなった選択である。コミュニティがBオプションを支持しているにもかかわらず、他のオプションはレイテンシーをもたらす可能性がある。"
      }
    ],
    "keywords": [
      "RTO",
      "RPO",
      "AWS",
      "database failover"
    ]
  },
  {
    "No": "273",
    "question": "A company is using AWS Organizations to manage multiple accounts. Due to regulatory requirements, the company wants to restrict specific\nmember accounts to certain AWS Regions, where they are permitted to deploy resources. The resources in the accounts must be tagged, enforced\nbased on a group standard, and centrally managed with minimal configuration.\nWhat should a solutions architect do to meet these requirements?",
    "question_jp": "ある企業がAWS Organizationsを使用して複数のアカウントを管理しています。規制要件により、企業は特定のメンバーアカウントを特定のAWSリージョンに制限したいと考えています。そのリージョンではリソースをデプロイすることが許可されています。アカウント内のリソースは、グループ標準に基づいてタグ付けされ、最小限の設定で中央管理される必要があります。これらの要件を満たすためにソリューションアーキテクトは何をすべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy.",
        "text_jp": "特定のメンバーアカウントにAWS Configルールを作成し、リージョンを制限し、タグポリシーを適用します。"
      },
      {
        "key": "B",
        "text": "From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and apply a tag policy on the root.",
        "text_jp": "AWS Billing and Cost Managementコンソールから、管理アカウントで特定のメンバーアカウントのリージョンを無効にし、ルートにタグポリシーを適用します。"
      },
      {
        "key": "C",
        "text": "Associate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions.",
        "text_jp": "特定のメンバーアカウントをルートに関連付けます。タグポリシーと条件を使用してリージョンを制限するSCPを適用します。"
      },
      {
        "key": "D",
        "text": "Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions.",
        "text_jp": "特定のメンバーアカウントを新しいOUに関連付けます。タグポリシーと条件を使用してリージョンを制限するSCPを適用します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "D (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Creating an AWS Config rule in the specific member accounts allows for direct enforcement of resource placement policies based on regional restrictions and tagging standards.",
        "situation_analysis": "The company needs to manage multiple accounts with specific restrictions based on regulatory requirements. The rules should ensure that resources are deployed only in permitted AWS Regions and are appropriately tagged.",
        "option_analysis": "Option A focuses on the use of AWS Config, which is ideal for enforcing compliance within specific accounts. Option B incorrectly assumes that disabling regions at the billing level can enforce deployment restrictions. Option C and D suggest associating with roots or organizational units but do not offer direct control over individual accounts as effectively as AWS Config.",
        "additional_knowledge": "Using SCPs in combination with AWS Config can enhance governance by combining resource execution constraints with the flexibility of configurations.",
        "key_terminology": "AWS Organizations, AWS Config, Service Control Policies (SCP), tag policies",
        "overall_assessment": "While A is a technically sound choice, the community's overwhelming preference for D suggests a potential misunderstanding of how SCP can also control resource behavior along with compliance policies."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。特定のメンバーアカウントにAWS Configルールを作成することで、リージョン制限とタグ付け基準に基づくリソース配置ポリシーの直接的な強制が可能となります。",
        "situation_analysis": "企業は、規制要件に基づいて特定の制限を持つ複数のアカウントを管理する必要があります。ルールは、リソースが許可されたAWSリージョンにのみデプロイされ、適切にタグ付けされることを保証する必要があります。",
        "option_analysis": "選択肢Aは、特定のアカウント内でのコンプライアンスの強制に最適なAWS Configの使用に焦点を当てています。選択肢Bは、請求レベルでリージョンを無効にすることがデプロイ制限を強制できると誤って仮定しています。選択肢CとDは、ルートや組織単位に関連付けることを提案していますが、AWS Configほど効果的に個々のアカウントを制御することはできません。",
        "additional_knowledge": "AWS ConfigとSCPを組み合わせて使用することで、リソースの実行制約と構成の柔軟性を組み合わせてガバナンスを強化できます。",
        "key_terminology": "AWS Organizations, AWS Config, サービスコントロールポリシー (SCP), タグポリシー",
        "overall_assessment": "Aは技術的に優れた選択ですが、コミュニティの圧倒的な支持がDに傾いていることは、SCPがリソースの行動を制御するのにどのように役立つかの理解不足を示唆しています。"
      }
    ],
    "keywords": [
      "AWS Organizations",
      "AWS Config",
      "Service Control Policies",
      "tag policies"
    ]
  },
  {
    "No": "274",
    "question": "A company has an application that generates reports and stores them in an Amazon S3 bucket. When a user accesses their report, the application\ngenerates a signed URL to allow the user to download the report. The company's security team has discovered that the files are public and that\nanyone can download them without authentication. The company has suspended the generation of new reports until the problem is resolved.\nWhich set of actions will immediately remediate the security issue without impacting the application's normal workfiow?",
    "question_jp": "ある企業がレポートを生成し、それをAmazon S3バケットに保存するアプリケーションを持っている。ユーザーがレポートにアクセスすると、アプリケーションはユーザーがレポートをダウンロードできるように署名付きURLを生成する。しかし、企業のセキュリティチームは、ファイルが公開されており、誰でも認証なしにダウンロードできることを発見した。この問題が解決されるまで、企業は新しいレポートの生成を停止している。このセキュリティの問題を、アプリケーションの通常のワークフローに影響を与えることなく、即座に修正するためにどのアクションセットが必要か？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Lambda function that applies a deny all policy for users who are not authenticated. Create a scheduled event to invoke the Lambda function.",
        "text_jp": "認証されていないユーザーに対してすべてのアクセスを拒否するポリシーを適用するAWS Lambda関数を作成する。Lambda関数を呼び出すための定期的なイベントを作成する。"
      },
      {
        "key": "B",
        "text": "Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions.",
        "text_jp": "AWS Trusted Advisorのバケット権限チェックをレビューし、推奨されたアクションを実施する。"
      },
      {
        "key": "C",
        "text": "Run a script that puts a private ACL on all of the objects in the bucket.",
        "text_jp": "バケット内のすべてのオブジェクトに対してプライベートACLを設定するスクリプトを実行する。"
      },
      {
        "key": "D",
        "text": "Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcIs option to TRUE on the bucket.",
        "text_jp": "Amazon S3のブロックパブリックアクセス機能を使用して、バケットのIgnorePublicAclsオプションをTRUEに設定する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "D (78%) C (22%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B: Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions. This provides immediate remediation of security concerns regarding the S3 bucket's permissions.",
        "situation_analysis": "The security team needs to quickly address the public accessibility of files stored in the S3 bucket while allowing the application to function correctly.",
        "option_analysis": "Option B directly addresses the permission issue with recommended actions, making it the best choice. Option A introduces complexity without addressing the root cause. Option C is less effective for immediate remediation and may require significant time to process depending on object count. Option D, while a good practice, may not be immediate compared to option B.",
        "additional_knowledge": "Immediate remediation is important in security incidents to ensure no further data exposure occurs.",
        "key_terminology": "AWS Trusted Advisor, S3 bucket permissions, public accessibility",
        "overall_assessment": "While community votes heavily favor option D, option B is crucial for providing immediate action and has the backing of AWS best practices for ongoing security management."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はB: AWS Trusted Advisorのバケット権限チェックをレビューし、推奨されたアクションを実施することである。これにより、S3バケットの権限に関するセキュリティの懸念を即座に修正することができる。",
        "situation_analysis": "セキュリティチームは、アプリケーションが正常に機能し続ける中で、S3バケットに保存されているファイルの公開アクセスを迅速に対処する必要がある。",
        "option_analysis": "選択肢Bは、権限の問題に直接対応しており、推奨されたアクションを通じて解決を提供するため最良の選択である。選択肢Aは、根本的な原因に対処せず複雑さをもたらす。選択肢Cは、即座の修正にとっては効果が薄く、オブジェクト数によっては処理にかなりの時間がかかる可能性がある。選択肢Dは良い実践であるが、選択肢Bと比べて即時性が劣る。",
        "additional_knowledge": "セキュリティインシデントでは、さらなるデータ露出が発生しないように即時の修正が重要である。",
        "key_terminology": "AWS Trusted Advisor、S3バケット権限、公開アクセス",
        "overall_assessment": "コミュニティの投票は選択肢Dを強く支持しているが、選択肢Bは即時の行動を提供するため重要であり、AWSのベストプラクティスに基づいた継続的なセキュリティ管理の支持も受けている。"
      }
    ],
    "keywords": [
      "AWS Trusted Advisor",
      "S3 bucket permissions",
      "public accessibility"
    ]
  },
  {
    "No": "275",
    "question": "A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance in another AWS account. A solutions\narchitect needs to design a migration strategy that will require no downtime and that will minimize the amount of time necessary to complete the\nmigration. The migration strategy must replicate all existing data and any new data that is created during the migration. The target database must\nbe identical to the source database at completion of the migration process.\nAll applications currently use an Amazon Route 53 CNAME record as their endpoint for communication with the RDS for Oracle DB instance. The\nRDS for Oracle DB instance is in a private subnet.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "ある企業が、Amazon RDS for Oracle データベースを別の AWS アカウントの RDS for PostgreSQL DB インスタンスに移行することを計画しています。ソリューションアーキテクトは、ダウンタイムを必要とせず、移行完了に必要な時間を最小限に抑える移行戦略を設計する必要があります。移行戦略は、すべての既存データと移行中に作成される新しいデータを複製しなければなりません。移行プロセスが完了した時点で、ターゲットデータベースはソースデータベースと同一でなければなりません。すべてのアプリケーションは現在、Amazon Route 53 CNAME レコードをエンドポイントとして RDS for Oracle DB インスタンスと通信しています。RDS for Oracle DB インスタンスはプライベートサブネットにあります。これらの要件を満たすために、ソリューションアーキテクトはどの組み合わせの手順を取るべきですか？（3つ選択してください。）",
    "choices": [
      {
        "key": "A",
        "text": "Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database.",
        "text_jp": "ターゲットアカウントに新しい RDS for PostgreSQL DB インスタンスを作成します。AWS スキーマ変換ツール (AWS SCT) を使用して、ソースデータベースからターゲットデータベースにデータベーススキーマを移行します。"
      },
      {
        "key": "B",
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema and initial data from the source database.",
        "text_jp": "AWS スキーマ変換ツール (AWS SCT) を使用して、ソースデータベースからスキーマと初期データを持つ新しい RDS for PostgreSQL DB インスタンスをターゲットアカウントに作成します。"
      },
      {
        "key": "C",
        "text": "Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account. Configure the security groups that are attached to each DB instance to allow trafic on the database port from the VPC in the target account.",
        "text_jp": "2つの AWS アカウントの VPC 間に VPC ピアリングを構成し、ターゲットアカウントから両方の DB インスタンスへの接続を提供します。ターゲットアカウントの VPC からのデータベースポートへのトラフィックを許可するために、各 DB インスタンスにアタッチされているセキュリティグループを構成します。"
      },
      {
        "key": "D",
        "text": "Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account. Configure the security groups that are attached to each DB instance to allow trafic on the database port from the VPC in the target account.",
        "text_jp": "ターゲットアカウントの VPC から接続を提供するために、ソース DB インスタンスを一時的にパブリックアクセス可能にします。ターゲットアカウントの VPC からのデータベースポートへのトラフィックを許可するために、各 DB インスタンスにアタッチされているセキュリティグループを構成します。"
      },
      {
        "key": "E",
        "text": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint.",
        "text_jp": "ターゲットアカウントで AWS データベース移行サービス (AWS DMS) を使用して、ソースデータベースからターゲットデータベースへのフルロードと変更データキャプチャ (CDC) 移行を実行します。移行が完了したら、CNAME レコードをターゲット DB インスタンスのエンドポイントを指すように変更します。"
      },
      {
        "key": "F",
        "text": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint.",
        "text_jp": "ターゲットアカウントで AWS データベース移行サービス (AWS DMS) を使用して、ソースデータベースからターゲットデータベースへの変更データキャプチャ (CDC) 移行を実行します。移行が完了したら、CNAME レコードをターゲット DB インスタンスのエンドポイントを指すように変更します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "ACE (93%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C, which involves configuring VPC peering between two AWS accounts to allow connectivity to both DB instances, along with configuring security groups for traffic on the database port.",
        "situation_analysis": "The company needs to migrate an Oracle database to PostgreSQL without downtime and must ensure all data is replicated.",
        "option_analysis": "Option C aligns with direct network connectivity requirements for a seamless migration, while other options fail to provide necessary connectivity or involve downtime.",
        "additional_knowledge": "AWS DMS can be used after establishing VPC peering to facilitate the migration process effectively.",
        "key_terminology": "VPC Peering, AWS DMS, RDS, CDC, Security Groups",
        "overall_assessment": "Option C is the most suitable approach considering it meets connectivity needs for zero-downtime migration and facilitates ongoing data synchronization."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは C であり、これは 2 つの AWS アカウント間で VPC ピアリングを構成し、両方の DB インスタンスへの接続を可能にし、データベースポートへのトラフィック用にセキュリティグループを構成することを含みます。",
        "situation_analysis": "この企業は、ダウンタイムなしで Oracle データベースを PostgreSQL に移行する必要があり、すべてのデータが複製されることを確保しなければなりません。",
        "option_analysis": "オプション C は、シームレスな移行のための直接的なネットワーク接続要件と一致している一方で、他のオプションは必要な接続を提供せず、ダウンタイムを伴う可能性があります。",
        "additional_knowledge": "AWS DMSは、VPCピアリングを確立した後に移行プロセスを効果的に支援するために使用できます。",
        "key_terminology": "VPC ピアリング、AWS DMS、RDS、CDC、セキュリティグループ",
        "overall_assessment": "オプション C は、ダウンタイムなしの移行のための接続ニーズを満たし、継続的なデータ同期を促進するため、この状況に最も適したアプローチです。"
      }
    ],
    "keywords": [
      "VPC Peering",
      "AWS DMS",
      "RDS",
      "CDC",
      "Security Groups"
    ]
  },
  {
    "No": "276",
    "question": "A company has implemented an ordering system using an event-driven architecture. During initial testing, the system stopped processing orders.\nFurther log analysis revealed that one order message in an Amazon Simple Queue Service (Amazon SQS) standard queue was causing an error on\nthe backend and blocking all subsequent order messages. The visibility timeout of the queue is set to 30 seconds, and the backend processing\ntimeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages and ensure that the system continues to process\nsubsequent messages.\nWhich step should the solutions architect take to meet these requirements?",
    "question_jp": "ある企業はイベント駆動型アーキテクチャを使用して注文システムを実装しました。初期テスト中に、システムは注文の処理を停止しました。さらなるログ分析により、Amazon Simple Queue Service (Amazon SQS) の標準キュー内の1つの注文メッセージがバックエンドでエラーを引き起こし、すべての後続の注文メッセージをブロックしていることが明らかになりました。キューの可視性タイムアウトは30秒に設定されており、バックエンドの処理タイムアウトは10秒に設定されています。ソリューションアーキテクトは、故障した注文メッセージを分析し、システムが後続のメッセージを引き続き処理できるようにする必要があります。この要件を満たすために、ソリューションアーキテクトはどのステップを実行すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Increase the backend processing timeout to 30 seconds to match the visibility timeout.",
        "text_jp": "バックエンド処理のタイムアウトを30秒に増やして可視性タイムアウトに合わせる。"
      },
      {
        "key": "B",
        "text": "Reduce the visibility timeout of the queue to automatically remove the faulty message.",
        "text_jp": "キューの可視性タイムアウトを短縮して、故障したメッセージを自動的に削除する。"
      },
      {
        "key": "C",
        "text": "Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages.",
        "text_jp": "故障したメッセージを隔離するために新しいSQS FIFOキューをデッドレターキューとして構成する。"
      },
      {
        "key": "D",
        "text": "Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages.",
        "text_jp": "故障したメッセージを隔離するために新しいSQS標準キューをデッドレターキューとして構成する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "D (80%) C (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C - Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages. This approach allows the processing of valid messages while retaining and analyzing the faulty ones separately.",
        "situation_analysis": "The problem arises from a faulty message in the Amazon SQS queue blocking subsequent messages. The visibility timeout and processing timeouts create an essential conflict that needs to be resolved without losing order data.",
        "option_analysis": "Option A would not resolve the issue, as the backend will still time out before it can process the message. Option B might remove the message but does not address the analysis need. Option C appropriately isolates the faulty message for further analysis. Option D, using a standard queue for dead-lettering, lacks FIFO guarantees and may not be ideal for order processing retention.",
        "additional_knowledge": "Using dead-letter queues effectively allows for retaining problem messages without disrupting the flow of healthy messages, enabling effective issue troubleshooting.",
        "key_terminology": "Amazon SQS, Dead-Letter Queue, FIFO Queue, Event-Driven Architecture, Visibility Timeout",
        "overall_assessment": "The community vote distribution shows a preference for D, which might indicate a misunderstanding of FIFO queue benefits in order management. However, C is the technically correct option as it maintains the integrity of order processing."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはC - 故障したメッセージを隔離するために新しいSQS FIFOキューをデッドレターキューとして構成することである。このアプローチは、有効なメッセージの処理を可能にしながら、故障したメッセージを別途保持して分析することを可能にする。",
        "situation_analysis": "問題は、Amazon SQSキュー内の故障したメッセージが後続のメッセージをブロックしていることから生じている。可視性タイムアウトと処理タイムアウトの間には解決すべき重要な対立があり、注文データを失うことなく解決する必要がある。",
        "option_analysis": "選択肢Aは、バックエンドがメッセージを処理できる前にタイムアウトするため、問題を解決しない。選択肢Bはメッセージを削除するかもしれないが、分析の必要性には対応しない。選択肢Cは、故障したメッセージを適切に隔離してさらなる分析のために保持する。選択肢Dは、標準キューをデッドレター化するが、FIFOの保証が欠けており、注文処理の保持には最適ではない可能性がある。",
        "additional_knowledge": "デッドレターキューを効果的に使用することで、健康なメッセージの流れを妨げることなく問題のあるメッセージを保持することができ、効果的な問題トラブルシューティングが可能になる。",
        "key_terminology": "Amazon SQS, デッドレターキュー, FIFOキュー, イベント駆動型アーキテクチャ, 可視性タイムアウト",
        "overall_assessment": "コミュニティ投票の分布はDを支持する傾向を示しており、これは注文管理におけるFIFOキューの利点に対する誤解を示している可能性がある。しかしCは、注文処理の完全性を維持する技術的に正しい選択である。"
      }
    ],
    "keywords": [
      "Amazon SQS",
      "Dead-Letter Queue",
      "FIFO Queue",
      "Event-Driven Architecture",
      "Visibility Timeout"
    ]
  },
  {
    "No": "277",
    "question": "A company has automated the nightly retraining of its machine learning models by using AWS Step Functions. The workfiow consists of multiple\nsteps that use AWS Lambda. Each step can fail for various reasons, and any failure causes a failure of the overall workfiow.\nA review reveals that the retraining has failed multiple nights in a row without the company noticing the failure. A solutions architect needs to\nimprove the workfiow so that notifications are sent for all types of failures in the retraining process.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "question_jp": "ある企業は、AWS Step Functionsを使用して機械学習モデルの夜間再トレーニングを自動化しています。このワークフローは、AWS Lambdaを使用する複数のステップで構成されています。各ステップはさまざまな理由で失敗する可能性があり、いずれかの失敗が全体のワークフローの失敗を引き起こします。レビューの結果、再トレーニングが何度も失敗していることが分かり、企業はその失敗に気づいていませんでした。ソリューションアーキテクトは、再トレーニングプロセスにおけるすべての種類の失敗に対して通知が送信されるようにワークフローを改善する必要があります。これらの要件を満たすために、ソリューションアーキテクトはどの組み合わせのステップを実施すべきですか。(3つ選択してください)",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type \"Email\" that targets the team's mailing list.",
        "text_jp": "Amazon Simple Notification Service (Amazon SNS)のトピックを作成し、チームのメールリストを対象とする「Email」タイプのサブスクリプションを追加する。"
      },
      {
        "key": "B",
        "text": "Create a task named \"Email\" that forwards the input arguments to the SNS topic.",
        "text_jp": "SNSトピックに入力引数を転送する「Email」という名前のタスクを作成する。"
      },
      {
        "key": "C",
        "text": "Add a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.ALL\" ] and \"Next”: \"Email\".",
        "text_jp": "「ErrorEquals」を含むすべてのタスク、マップ、および並列状態に「States.ALL」を指定し、「Next」を「Email」とするキャッチフィールドを追加する。"
      },
      {
        "key": "D",
        "text": "Add a new email address to Amazon Simple Email Service (Amazon SES). Verify the email address.",
        "text_jp": "Amazon Simple Email Service (Amazon SES)に新しいメールアドレスを追加し、そのメールアドレスを検証する。"
      },
      {
        "key": "E",
        "text": "Create a task named \"Email\" that forwards the input arguments to the SES email address.",
        "text_jp": "SESのメールアドレスに入力引数を転送する「Email」という名前のタスクを作成する。"
      },
      {
        "key": "F",
        "text": "Add a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.Runtime\" ] and \"Next\": \"Email\".",
        "text_jp": "「ErrorEquals」を含むすべてのタスク、マップ、および並列状態に「States.Runtime」を指定し、「Next」を「Email」とするキャッチフィールドを追加する。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "ABC (86%) 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct steps to implement notification for failures are to forward input arguments to SNS using a task named 'Email', and to add a Catch field for error handling.",
        "situation_analysis": "The workflow fails without notifications sent for errors, indicating a need for better error handling and notification mechanisms.",
        "option_analysis": "Option B is correct as it explicitly addresses sending notifications using AWS SNS. Options A, C, D, and E can contribute to better notifications but do not directly list essential steps for failure handling without additional configurations.",
        "additional_knowledge": "AWS practices recommend implementing robust error handling to ensure operational resilience.",
        "key_terminology": "AWS Step Functions, Amazon SNS, error handling, notifications",
        "overall_assessment": "The identified steps improve the ability to track errors in the AWS Step Functions workflow effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "失敗通知を実装するための正しいステップは、'Email'という名前のタスクを使用してSNSに入力引数を転送し、エラーハンドリングのためのキャッチフィールドを追加することである。",
        "situation_analysis": "ワークフローがエラー通知なしに失敗していることから、より良いエラーハンドリングと通知メカニズムの必要性が示されている。",
        "option_analysis": "選択肢Bは、AWS SNSを使用して通知を送信することに明示的に対応しているため正しい。選択肢A、C、D、Eはより良い通知に寄与することができるが、追加の設定なしには失敗の処理に必要なステップを直接リストしていない。",
        "additional_knowledge": "AWSのベストプラクティスとして、運用の回復力を確保するために堅牢なエラーハンドリングの実装が推奨されている。",
        "key_terminology": "AWS Step Functions、Amazon SNS、エラーハンドリング、通知",
        "overall_assessment": "特定されたステップは、AWS Step Functionsワークフローのエラーを追跡する能力を向上させる。"
      }
    ],
    "keywords": [
      "AWS Step Functions",
      "Amazon SNS",
      "error handling"
    ]
  },
  {
    "No": "278",
    "question": "A company plans to deploy a new private intranet service on Amazon EC2 instances inside a VPC. An AWS Site-to-Site VPN connects the VPC to\nthe company's on-premises network. The new service must communicate with existing on-premises services. The on-premises services are\naccessible through the use of hostnames that reside in the company.example DNS zone. This DNS zone is wholly hosted on premises and is\navailable only on the company's private network.\nA solutions architect must ensure that the new service can resolve hostnames on the company.example domain to integrate with existing\nservices.\nWhich solution meets these requirements?",
    "question_jp": "ある企業が、VPC内のAmazon EC2インスタンス上に新しいプライベートイントラネットサービスを展開する計画を立てています。AWS Site-to-Site VPNを使用してVPCと同社のオンプレミスネットワークを接続します。新しいサービスは、既存のオンプレミスサービスと通信する必要があります。オンプレミスサービスは、会社が使用しているcompany.example DNSゾーンに存在するホスト名を通じてアクセス可能です。このDNSゾーンは完全にオンプレミスでホストされており、企業のプライベートネットワークでのみ利用可能です。ソリューションアーキテクトは、新しいサービスが既存のサービスと統合するためにcompany.exampleドメインのホスト名を解決できることを確認する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an empty private zone in Amazon Route 53 for company.example. Add an additional NS record to the company's on-premises company.example zone that points to the authoritative name servers for the new private zone in Route 53.",
        "text_jp": "Amazon Route 53でcompany.exampleの空のプライベートゾーンを作成します。会社のオンプレミスのcompany.exampleゾーンに、新しいプライベートゾーンの権威あるネームサーバーを指す追加のNSレコードを追加します。"
      },
      {
        "key": "B",
        "text": "Turn on DNS hostnames for the VPC. Configure a new outbound endpoint with Amazon Route 53 Resolver. Create a Resolver rule to forward requests for company.example to the on-premises name servers.",
        "text_jp": "VPCのDNSホスト名を有効にします。Amazon Route 53 Resolverで新しいアウトバウンドエンドポイントを構成します。company.exampleへのリクエストをオンプレミスのネームサーバーに転送するResolverルールを作成します。"
      },
      {
        "key": "C",
        "text": "Turn on DNS hostnames for the VPConfigure a new inbound resolver endpoint with Amazon Route 53 Resolver. Configur&the on-premises DNS server to forward requests for company.example to the new resolver.",
        "text_jp": "VPCのDNSホスト名を有効にします。Amazon Route 53 Resolverで新しいインバウンドリゾルバーエンドポイントを構成します。そして、オンプレミスのDNSサーバーを設定して、company.exampleへのリクエストを新しいリゾルバーに転送します。"
      },
      {
        "key": "D",
        "text": "Use AWS Systems Manager to configure a run document that will install a hosts file that contains any required hostnames. Use an Amazon EventBridge rule to run the document when an instance is entering the running state.",
        "text_jp": "AWS Systems Managerを使用して、必要なホスト名を含むhostsファイルをインストールする実行ドキュメントを構成します。インスタンスが実行中の状態に入るときにドキュメントを実行するためのAmazon EventBridgeルールを使用します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This solution allows the new service to resolve hostnames in the company.example domain by using a private zone in Amazon Route 53, which can be integrated with the on-premises DNS by adding an NS record.",
        "situation_analysis": "The provided situation requires integration with an existing on-premises DNS zone that cannot be directly accessed from the VPC. Thus, using Route 53 to create a private DNS zone is appropriate.",
        "option_analysis": "Option B does not work because it involves an outbound resolver that is not suitable for resolving on-premises DNS. Option C incorrectly suggests the creation of an inbound resolver without addressing the initial DNS integration. Option D employs an impractical method by modifying hosts files which does not facilitate automatic DNS resolution.",
        "additional_knowledge": "Understanding how DNS zones and records work within AWS is key to effective cloud architecture design.",
        "key_terminology": "Route 53, private zone, DNS integration, on-premises network, NS record",
        "overall_assessment": "Despite the community vote favoring B, option A offers a more structured and direct solution for DNS resolution. Community votes may reflect a misunderstanding of the requirement for on-premises DNS integration rather than a flaw in option A."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はAである。このソリューションは、新しいサービスがAmazon Route 53のプライベートゾーンを使用してcompany.exampleドメイン内のホスト名を解決できるようにし、NSレコードを追加することでオンプレミスのDNSと統合できる。",
        "situation_analysis": "提供された状況は、VPCから直接アクセスできない既存のオンプレミスDNSゾーンとの統合を必要とする。そのため、Route 53を使用してプライベートDNSゾーンを作成することが適切である。",
        "option_analysis": "オプションBは、オンプレミスDNSの解決に適さないアウトバウンドリゾルバーを含むため、機能しない。オプションCは、初期のDNS統合の解決を行わずにインバウンドリゾルバーの作成を誤って提案している。オプションDは、ホストファイルの変更という非現実的な方法を採用しており、自動DNS解決を促進しない。",
        "additional_knowledge": "AWS内のDNSゾーンとレコードの動作を理解することは、効果的なクラウドアーキテクチャ設計の鍵である。",
        "key_terminology": "Route 53, プライベートゾーン, DNS統合, オンプレミスネットワーク, NSレコード",
        "overall_assessment": "コミュニティ投票がBを支持しているにもかかわらず、オプションAはDNS解決のためのより構造化された直接的なソリューションを提供する。コミュニティの投票は、オプションAの欠陥ではなく、オンプレミスDNS統合の要件に対する誤解を反映している可能性がある。"
      }
    ],
    "keywords": [
      "Route 53",
      "private zone",
      "DNS integration",
      "on-premises network",
      "NS record"
    ]
  },
  {
    "No": "279",
    "question": "A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached to a transit gateway. Each VPC that sends\ntrafic to the public internet must send the trafic through a shared services VPC. Each subnet within a VPC uses the default VPC route table, and\nthe trafic is routed to the transit gateway. The transit gateway uses its default route table for any VPC attachment.\nA security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate with an EC2 instance that is deployed in any\nof the company's other VPCs. A solutions architect needs to limit the trafic between the VPCs. Each VPC must be able to communicate only with\na predefined, limited set of authorized VPCs.\nWhat should the solutions architect do to meet these requirements?",
    "question_jp": "ある企業が、トランジットゲートウェイに接続された複数のVPC内でアプリケーションをデプロイするためにAWS CloudFormationを使用しています。パブリックインターネットにトラフィックを送信する各VPCは、共有サービスVPCを通じてトラフィックを送信する必要があります。VPC内の各サブネットはデフォルトのVPCルートテーブルを使用し、トラフィックはトランジットゲートウェイにルーティングされます。トランジットゲートウェイは、すべてのVPC接続に対してデフォルトのルートテーブルを使用します。セキュリティ監査では、VPC内にデプロイされたAmazon EC2インスタンスが、企業の他のVPCにデプロイされたEC2インスタンスと通信できることが明らかになりました。ソリューションアーキテクトは、VPC間のトラフィックを制限する必要があります。各VPCは、あらかじめ定義された限定的な認可されたVPCのセットとのみ通信できる必要があります。ソリューションアーキテクトは、これらの要件を満たすために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Update the network ACL of each subnet within a VPC to allow outbound trafic only to the authorized VPCs. Remove all deny rules except the default deny rule.",
        "text_jp": "各VPC内の各サブネットのネットワークACLを更新して、認可されたVPCへのアウトバウンドトラフィックのみを許可します。デフォルトの拒否ルールを除いてすべての拒否ルールを削除します。"
      },
      {
        "key": "B",
        "text": "Update all the security groups that are used within a VPC to deny outbound trafic to security groups that are used within the unauthorized VPCs.",
        "text_jp": "VPC内で使用されるすべてのセキュリティグループを更新して、認可されていないVPC内で使用されるセキュリティグループへのアウトバウンドトラフィックを拒否します。"
      },
      {
        "key": "C",
        "text": "Create a dedicated transit gateway route table for each VPC attachment. Route trafic only to the authorized VPCs.",
        "text_jp": "各VPC接続のために専用のトランジットゲートウェイルートテーブルを作成します。認可されたVPCへのみトラフィックをルーティングします。"
      },
      {
        "key": "D",
        "text": "Update the main route table of each VPC to route trafic only to the authorized VPCs through the transit gateway.",
        "text_jp": "各VPCのメインルートテーブルを更新して、トランジットゲートウェイを介して認可されたVPCへのみトラフィックをルーティングします。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Creating a dedicated transit gateway route table for each VPC attachment allows for specific routing policies and limits traffic to authorized VPCs.",
        "situation_analysis": "Each VPC is currently connected via a transit gateway, and the requirement is to restrict communication between VPCs to only a predefined set.",
        "option_analysis": "Option A would control traffic at the subnet level but does not adequately limit routing through the transit gateway. Option B restricts traffic at the security group level, which can be too restrictive and cumbersome to manage. Option D would create a centralized routing issue since it updates the main route table without specificity.",
        "additional_knowledge": "Utilizing dedicated route tables can help better manage inter-VPC traffic and ensure compliance with security policies.",
        "key_terminology": "Transit Gateway, VPC Route Table, Network ACL, Security Group, AWS Architecture.",
        "overall_assessment": "The community overwhelmingly supports option C, indicating consensus on its effectiveness in addressing the problem. Proper route management through dedicated route tables is an AWS best practice for network design."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCです。各VPC接続のために専用のトランジットゲートウェイルートテーブルを作成することにより、特定のルーティングポリシーが可能になり、トラフィックを認可されたVPCに制限します。",
        "situation_analysis": "現在、各VPCはトランジットゲートウェイを介して接続されており、要件はVPC間の通信をあらかじめ定義されたセットに制限することです。",
        "option_analysis": "選択肢Aはサブネットレベルでトラフィックを制御しますが、トランジットゲートウェイを通じてのルーティングを適切に制限することはできません。選択肢Bはセキュリティグループレベルでトラフィックを制限しますが、あまりにも制限的で管理が煩雑になる可能性があります。選択肢Dはメインルートテーブルを更新しますが、特異性がないため、集中管理の問題を引き起こす可能性があります。",
        "additional_knowledge": "専用のルートテーブルを活用することで、VPC間のトラフィックをより良く管理し、セキュリティポリシーの遵守を確保できます。",
        "key_terminology": "トランジットゲートウェイ、VPCルートテーブル、ネットワークACL、セキュリティグループ、AWSアーキテクチャ。",
        "overall_assessment": "コミュニティは圧倒的に選択肢Cを支持しており、その効果に対する合意が示されています。専用のルートテーブルを通じた適切なルート管理は、ネットワーク設計のAWSベストプラクティスです。"
      }
    ],
    "keywords": [
      "Transit Gateway",
      "VPC Route Table",
      "Network ACL",
      "Security Group",
      "AWS Architecture"
    ]
  },
  {
    "No": "280",
    "question": "A company has a Windows-based desktop application that is packaged and deployed to the users' Windows machines. The company recently\nacquired another company that has employees who primarily use machines with a Linux operating system. The acquiring company has decided to\nmigrate and rehost the Windows-based desktop application to AWS.\nAll employees must be authenticated before they use the application. The acquiring company uses Active Directory on premises but wants a\nsimplified way to manage access to the application on AWS for all the employees.\nWhich solution will rehost the application on AWS with the LEAST development effort?",
    "question_jp": "ある企業は、ユーザーのWindowsマシンにパッケージ化されて展開されるWindowsベースのデスクトップアプリケーションを持っています。最近、この企業は主にLinuxオペレーティングシステムを使用するマシンを持つ従業員を抱える別の企業を買収しました。買収した企業は、このWindowsベースのデスクトップアプリケーションをAWSに移行および再ホスティングすることを決定しました。全ての従業員はアプリケーションを使用する前に認証される必要があります。買収した企業はオンプレミスのActive Directoryを使用していますが、すべての従業員がAWS上のアプリケーションへのアクセスを管理する簡素化された方法を望んでいます。開発の手間が最も少ない状態でアプリケーションをAWSに再ホスティングするための最適な解決策はどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up and provision an Amazon Workspaces virtual desktop for every employee. Implement authentication by using Amazon Cognito identity pools. Instruct employees to run the application from their provisioned Workspaces virtual desktops.",
        "text_jp": "各従業員のためにAmazon Workspaces仮想デスクトップを設定およびプロビジョニングします。Amazon Cognitoアイデンティティプールを使用して認証を実装します。従業員にプロビジョニングされたWorkspaces仮想デスクトップからアプリケーションを実行するよう指示します。"
      },
      {
        "key": "B",
        "text": "Create an Auto Scaling group of Windows-based Amazon EC2 instances. Join each EC2 instance to the company's Active Directory domain. Implement authentication by using the Active Directory that is running on premises. Instruct employees to run the application by using a Windows remote desktop.",
        "text_jp": "WindowsベースのAmazon EC2インスタンスのAuto Scalingグループを作成します。各EC2インスタンスを会社のActive Directoryドメインに参加させます。オンプレミスのActive Directoryを使用して認証を実装します。従業員にWindowsリモートデスクトップを用いてアプリケーションを実行するよう指示します。"
      },
      {
        "key": "C",
        "text": "Use an Amazon AppStream 2.0 image builder to create an image that includes the application and the required configurations. Provision an AppStream 2.0 On-Demand fieet with dynamic Fleet Auto Scaling policies for running the image. Implement authentication by using AppStream 2.0 user pools. Instruct the employees to access the application by starting browser-based AppStream 2.0 streaming sessions.",
        "text_jp": "Amazon AppStream 2.0のイメージビルダーを使用して、アプリケーションと必要な構成を含むイメージを作成します。動的フリートオートスケーリングポリシーでオンデマンドフリートをプロビジョニングします。AppStream 2.0ユーザープールを使用して認証を実装します。従業員にブラウザベースのAppStream 2.0ストリーミングセッションを開始してアプリケーションにアクセスするよう指示します。"
      },
      {
        "key": "D",
        "text": "Refactor and containerize the application to run as a web-based application. Run the application in Amazon Elastic Container Service (Amazon ECS) on AWS Fargate with step scaling policies. Implement authentication by using Amazon Cognito user pools. Instruct the employees to run the application from their browsers.",
        "text_jp": "アプリケーションを再設計し、コンテナ化してウェブベースのアプリケーションとして実行します。AWS FargateのAmazon Elastic Container Service (Amazon ECS)でアプリケーションを実行し、ステップスケーリングポリシーを適用します。Amazon Cognitoユーザープールを使用して認証を実装します。従業員にブラウザからアプリケーションを実行するよう指示します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D.",
        "situation_analysis": "The company needs to rehost a Windows-based application for Linux users while managing authentication with Active Directory. A solution that minimizes development effort is required.",
        "option_analysis": "Option D simplifies access through a web-based application, thereby minimizing development complexity. Option A requires each user to have a virtual desktop, which can increase management overhead. Option B maintains traditional remote desktop practices and requires Active Directory integration, which adds complexity. Option C uses AppStream but doesn't offer as streamlined a solution as option D.",
        "additional_knowledge": "Developing a web-based application reduces dependencies on the underlying operating system.",
        "key_terminology": "Amazon ECS, AWS Fargate, Amazon Cognito, containerization, web application.",
        "overall_assessment": "While community support heavily leans towards option C, option D is superior for long-term scalability and ease of management. The community sentiment may reflect a preference for AppStream but doesn't address overall development effort as effectively as option D."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はDである。",
        "situation_analysis": "企業はLinuxユーザー向けにWindowsベースのアプリケーションを再ホストする必要があり、Active Directoryでの認証管理も求めている。開発の手間を最小限に抑えた解決策が必要である。",
        "option_analysis": "選択肢Dは、ウェブベースのアプリケーションを通じたアクセスを簡素化するため、開発の複雑さを最小限に抑える。選択肢Aは、各ユーザーに仮想デスクトップを用意する必要があるため、管理の手間が増える。選択肢Bは、従来のリモートデスクトップ技術を維持し、Active Directoryとの統合が必要で、これが複雑さを増す。選択肢CはAppStreamを使用するが、選択肢Dほどの簡略さを提供しない。",
        "additional_knowledge": "ウェブベースのアプリケーションを開発することで、基盤となるオペレーティングシステムへの依存が減少する。",
        "key_terminology": "Amazon ECS、AWS Fargate、Amazon Cognito、コンテナ化、ウェブアプリケーション。",
        "overall_assessment": "コミュニティからの支持が選択肢Cに偏っているが、選択肢Dは長期的なスケーラビリティと管理の容易さにおいて優れている。コミュニティの感情はAppStreamへの好みを反映しているかもしれないが、全体の開発の手間において選択肢Dほど効果的に取り組んでいない。"
      }
    ],
    "keywords": [
      "Amazon ECS",
      "AWS Fargate",
      "Amazon Cognito",
      "containerization",
      "web application"
    ]
  },
  {
    "No": "281",
    "question": "A company is collecting a large amount of data from a fieet of IoT devices. Data is stored as Optimized Row Columnar (ORC) files in the Hadoop\nDistributed File System (HDFS) on a persistent Amazon EMR cluster. The company's data analytics team queries the data by using SQL in Apache\nPresto deployed on the same EMR cluster. Queries scan large amounts of data, always run for less than 15 minutes, and run only between 5 PM\nand 10 PM.\nThe company is concerned about the high cost associated with the current solution. A solutions architect must propose the most cost-effective\nsolution that will allow SQL data queries.\nWhich solution will meet these requirements?",
    "question_jp": "企業は、IoTデバイスのフリートから大量のデータを収集しています。データは、持続可能なAmazon EMRクラスター上のHadoop分散ファイルシステム（HDFS）にOptimized Row Columnar（ORC）ファイルとして保存されています。企業のデータ分析チームは、同じEMRクラスター上でデプロイされたApache Prestoを使用してSQLでデータをクエリしています。クエリは大量のデータをスキャンし、常に15分未満で実行され、午後5時から10時の間のみ実行されます。企業は、現在のソリューションに関連する高コストを懸念しています。ソリューションアーキテクトは、SQLデータクエリを可能にする最もコスト効率の良いソリューションを提案する必要があります。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Store data in Amazon S3. Use Amazon Redshift Spectrum to query data.",
        "text_jp": "データをAmazon S3に保存します。Amazon Redshift Spectrumを使用してデータをクエリします。"
      },
      {
        "key": "B",
        "text": "Store data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data.",
        "text_jp": "データをAmazon S3に保存します。AWS GlueデータカタログとAmazon Athenaを使用してデータをクエリします。"
      },
      {
        "key": "C",
        "text": "Store data in EMR File System (EMRFS). Use Presto in Amazon EMR to query data.",
        "text_jp": "データをEMRファイルシステム（EMRFS）に保存します。PrestoをAmazon EMRで使用してデータをクエリします。"
      },
      {
        "key": "D",
        "text": "Store data in Amazon Redshift. Use Amazon Redshift to query data.",
        "text_jp": "データをAmazon Redshiftに保存します。Amazon Redshiftを使用してデータをクエリします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Storing data in Amazon Redshift allows for efficient querying capabilities with the associated cost being managed through Amazon's pricing structure.",
        "situation_analysis": "The data has a predictable access pattern, which can be optimized by using a dedicated data warehousing solution like Amazon Redshift that can handle heavy queries efficiently.",
        "option_analysis": "Option A, while using Amazon S3 and Redshift Spectrum, might not be as cost-effective as storing data directly in Amazon Redshift for the high query volume from the IoT device data. Option B, AWS Glue and Athena could incur costs for frequent querying, especially if data is stored in an unoptimized format. Option C, using EMRFS and Presto, would involve ongoing EMR costs which could be higher compared to directly utilizing Redshift.",
        "additional_knowledge": "Consideration for data lifecycle policies should also be part of managing overall storage costs effectively.",
        "key_terminology": "Amazon Redshift, SQL, Data Warehousing, EMR, Cost Management",
        "overall_assessment": "The answer provided above reflects a clear understanding of the requirement to balance cost and performance, establishing Amazon Redshift as the most suitable solution given the characteristics of the workload."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDです。データをAmazon Redshiftに保存することで、高効率のクエリ機能を利用でき、関連コストはAmazonの料金体系を通じて管理されます。",
        "situation_analysis": "データは予測可能なアクセスパターンを持ち、Amazon Redshiftのような専用のデータウェアハウジングソリューションを使用することで最適化できます。",
        "option_analysis": "選択肢AはAmazon S3とRedshift Spectrumを使用していますが、IoTデバイスデータからの高いクエリボリュームに対して、データを直接Amazon Redshiftに保存する方がコスト効率が高い可能性があります。選択肢BのAWS GlueとAthenaは、データが最適化されていないフォーマットで保存されている場合、頻繁なクエリでコストがかかるかもしれません。選択肢CのEMRFSとPrestoは、続けてEMRのコストがかかり、Redshiftを直接使用するよりも高くなる可能性があります。",
        "additional_knowledge": "データライフサイクルポリシーの考慮も、全体のストレージコストを効果的に管理する一環として重要です。",
        "key_terminology": "Amazon Redshift, SQL, データウェアハウジング, EMR, コスト管理",
        "overall_assessment": "上記の答えは、コストとパフォーマンスのバランスを取る必要性を明確に理解したものであり、ワークロードの特性を考えた場合、Amazon Redshiftが最適なソリューションであることを明らかにしています。"
      }
    ],
    "keywords": [
      "Amazon Redshift",
      "SQL",
      "Data Warehousing",
      "EMR",
      "Cost Management"
    ]
  },
  {
    "No": "282",
    "question": "A large company recently experienced an unexpected increase in Amazon RDS and Amazon DynamoDB costs. The company needs to increase\nvisibility into details of AWS Billing and Cost Management. There are various accounts associated with AWS Organizations, including many\ndevelopment and production accounts. There is no consistent tagging strategy across the organization, but there are guidelines in place that\nrequire all infrastructure to be deployed using AWS CloudFormation with consistent tagging. Management requires cost center numbers and\nproject ID numbers for all existing and future DynamoDB tables and RDS instances.\nWhich strategy should the solutions architect provide to meet these requirements?",
    "question_jp": "大企業は最近、Amazon RDSおよびAmazon DynamoDBのコストが予期しない増加を経験しました。この企業は、AWS BillingおよびCost Managementの詳細の可視性を高める必要があります。さまざまなアカウントがAWS Organizationsに関連付けられており、多くの開発および生産アカウントが含まれています。組織全体に一貫したタグ付け戦略はありませんが、すべてのインフラはAWS CloudFormationを使用して一貫したタグ付けでデプロイされることを要求するガイドラインがあります。管理者は、すべての既存および将来のDynamoDBテーブルおよびRDSインスタンスに対してコストセンター番号およびプロジェクトID番号を要求しています。これらの要件を満たすためにソリューションアーキテクトはどの戦略を提供すべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources.",
        "text_jp": "Tag Editorを使用して既存のリソースにタグ付けします。コストセンターとプロジェクトIDを定義するコスト配分タグを作成し、タグが既存のリソースに伝播するのを24時間待ちます。"
      },
      {
        "key": "B",
        "text": "Use an AWS Config rule to alert the finance team of untagged resources. Create a centralized AWS Lambda based solution to tag untagged RDS databases and DynamoDB resources every hour using a cross-account role.",
        "text_jp": "AWS Configルールを使用して、財務チームにタグ付けされていないリソースの警告を送信します。AWS Lambdaをベースにした集中型ソリューションを作成し、クロスアカウントロールを使用して、毎時タグ付けされていないRDSデータベースおよびDynamoDBリソースにタグ付けを行います。"
      },
      {
        "key": "C",
        "text": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID. Use SCPs to restrict resource creation that do not have the cost center and project ID on the resource.",
        "text_jp": "Tag Editorを使用して既存のリソースにタグ付けします。コストセンターとプロジェクトIDを定義するコスト配分タグを作成します。SCPを使用して、コストセンターとプロジェクトIDがリソースにない状態でのリソース作成を制限します。"
      },
      {
        "key": "D",
        "text": "Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources. Update existing federated roles to restrict privileges to provision resources that do not include the cost center and project ID on the resource.",
        "text_jp": "コストセンターとプロジェクトIDを定義するコスト配分タグを作成し、既存のリソースにタグが伝播するのを24時間待ちます。リソースにコストセンターとプロジェクトIDが含まれない場合の特権を制限するよう既存のフェデレーテッドロールを更新します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (80%) A (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. It outlines a strategy that not only alerts the finance team of untagged resources but also implements an automated tagging solution for untagged RDS and DynamoDB resources.",
        "situation_analysis": "The company has no consistent tagging and requires visibility into billing, necessitating a comprehensive solution to tag resources automatically.",
        "option_analysis": "Option B is the best choice as it combines monitoring with automation. Options A, C, and D only address tagging but do not ensure ongoing compliance.",
        "additional_knowledge": "Implementing a consistent tagging strategy is critical for managing costs in large organizations.",
        "key_terminology": "AWS Config, AWS Lambda, cross-account role, tagging strategy, cost allocation.",
        "overall_assessment": "Overall, option B provides a robust solution for the company's needs, while community votes favor option C which lacks automation and responsiveness."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBである。これは、タグ付けされていないリソースの財務チームへの警告を設定し、タグ付けされていないRDSおよびDynamoDBリソースに自動タグ付けソリューションを実装する戦略を示している。",
        "situation_analysis": "企業は一貫したタグ付けがなく、請求に対する可視性を必要としており、リソースを自動的にタグ付けする包括的なソリューションが必要である。",
        "option_analysis": "選択肢Bは、監視と自動化を組み合わせているため、最善の選択である。選択肢A、C、およびDはタグ付けへの対処はしているが、継続的なコンプライアンスを確保するものではない。",
        "additional_knowledge": "一貫したタグ付け戦略の実施は、大規模な組織におけるコスト管理にとって重要である。",
        "key_terminology": "AWS Config, AWS Lambda, クロスアカウントロール, タグ付け戦略, コスト配分.",
        "overall_assessment": "全体として、選択肢Bは企業のニーズに対して堅固な解決策を提供しており、コミュニティの投票は自動化や応答性に乏しい選択肢Cを好んでいる。"
      }
    ],
    "keywords": [
      "AWS Config",
      "AWS Lambda",
      "cross-account role",
      "tagging strategy",
      "cost allocation"
    ]
  },
  {
    "No": "283",
    "question": "A company wants to send data from its on-premises systems to Amazon S3 buckets. The company created the S3 buckets in three different\naccounts. The company must send the data privately without the data traveling across the internet. The company has no existing dedicated\nconnectivity to AWS.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "question_jp": "ある企業は、オンプレミスシステムからAmazon S3バケットにデータを送信したいと考えています。この企業は、3つの異なるアカウントにS3バケットを作成しました。データは私的に送信する必要があり、インターネットを経由することはできません。この企業には、AWSへの専用接続はありません。要件を満たすためにソリューションアーキテクトはどのような手順の組み合わせを取るべきでしょうか？（2つ選択してください）",
    "choices": [
      {
        "key": "A",
        "text": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a private VIF between the on-premises environment and the private VPC.",
        "text_jp": "AWSクラウドにネットワーキングアカウントを設立し、ネットワーキングアカウントにプライベートVPCを作成します。オンプレミス環境とプライベートVPCの間にプライベートVIFでAWS Direct Connect接続を設定します。"
      },
      {
        "key": "B",
        "text": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a public VIF between the on-premises environment and the private VPC.",
        "text_jp": "AWSクラウドにネットワーキングアカウントを設立し、ネットワーキングアカウントにプライベートVPCを作成します。オンプレミス環境とプライベートVPCの間にパブリックVIFでAWS Direct Connect接続を設定します。"
      },
      {
        "key": "C",
        "text": "Create an Amazon S3 interface endpoint in the networking account.",
        "text_jp": "ネットワーキングアカウントにAmazon S3インターフェイスエンドポイントを作成します。"
      },
      {
        "key": "D",
        "text": "Create an Amazon S3 gateway endpoint in the networking account.",
        "text_jp": "ネットワーキングアカウントにAmazon S3ゲートウェイエンドポイントを作成します。"
      },
      {
        "key": "E",
        "text": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Peer VPCs from the accounts that host the S3 buckets with the VPC in the network account.",
        "text_jp": "AWSクラウドにネットワーキングアカウントを設立し、ネットワーキングアカウントにプライベートVPCを作成します。S3バケットをホストするアカウントのVPCとネットワークアカウントのVPCをピアリングします。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "AC (80%) 7% 7%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D, which involves creating an Amazon S3 gateway endpoint in the networking account.",
        "situation_analysis": "The company needs to transfer data from on-premises to S3 buckets across different accounts without using the internet.",
        "option_analysis": "Option D allows for private connection to the S3 service without internet exposure, which fulfills the requirement. Other options do not provide the appropriate combination of private access and account segregation.",
        "additional_knowledge": "Combining gateway endpoints with a VPC provides access controls and monitoring capabilities for enhanced security.",
        "key_terminology": "Amazon S3, AWS Direct Connect, VPC, gateway endpoint, private connectivity",
        "overall_assessment": "While the community votes show support for option A, option D is essential for ensuring private access to the S3 service without moving data across the internet."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDであり、ネットワーキングアカウントにAmazon S3ゲートウェイエンドポイントを作成することを含みます。",
        "situation_analysis": "企業は、インターネットを利用せずにオンプレミスから異なるアカウントのS3バケットにデータを転送する必要があります。",
        "option_analysis": "選択肢Dは、インターネットの露出なしにS3サービスへのプライベート接続を可能にし、要件を満たします。他の選択肢は、アカウント分離とプライベートアクセスの適切な組み合わせを提供しません。",
        "additional_knowledge": "ゲートウェイエンドポイントとVPCを組み合わせることで、アクセス制御や監視機能を通じてセキュリティを強化できます。",
        "key_terminology": "Amazon S3、AWS Direct Connect、VPC、ゲートウェイエンドポイント、プライベート接続",
        "overall_assessment": "コミュニティの投票は選択肢Aへの支持を示していますが、選択肢Dはインターネットを経由せずにS3サービスへのプライベートアクセスを保証するために不可欠です。"
      }
    ],
    "keywords": [
      "Amazon S3",
      "AWS Direct Connect",
      "VPC",
      "gateway endpoint",
      "private connectivity"
    ]
  },
  {
    "No": "284",
    "question": "A company operates quick-service restaurants. The restaurants follow a predictable model with high sales trafic for 4 hours daily. Sales trafic is\nlower outside of those peak hours.\nThe point of sale and management platform is deployed in the AWS Cloud and has a backend that is based on Amazon DynamoDB. The database\ntable uses provisioned throughput mode with 100,000 RCUs and 80,000 WCUs to match known peak resource consumption.\nThe company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.\nWhich solution meets these requirements MOST cost-effectively?",
    "question_jp": "ある企業がファストフードレストランを運営しています。レストランは、毎日4時間の高売上トラフィックを伴う予測可能なモデルに従っています。高売上トラフィックの時間外では、売上トラフィックが低下します。販売時点情報と管理プラットフォームはAWSクラウドにデプロイされており、バックエンドはAmazon DynamoDBに基づいています。データベーステーブルは、既知のピークリソース消費に対応するために100,000 RCU（リクエスト単位）と80,000 WCU（書き込み単位）のプロビジョニングスループットモードを使用しています。企業はDynamoDBのコストを削減し、ITスタッフの運用オーバーヘッドを最小限に抑えたいと考えています。この要件を最もコスト効果の高い方法で満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Reduce the provisioned RCUs and WCUs.",
        "text_jp": "プロビジョニングされたRCUとWCUを削減する。"
      },
      {
        "key": "B",
        "text": "Change the DynamoDB table to use on-demand capacity.",
        "text_jp": "DynamoDBテーブルをオンデマンドキャパシティに変更する。"
      },
      {
        "key": "C",
        "text": "Enable Dynamo DB auto scaling for the table.",
        "text_jp": "テーブルのDynamoDBオートスケーリングを有効にする。"
      },
      {
        "key": "D",
        "text": "Purchase 1-year reserved capacity that is suficient to cover the peak load for 4 hours each day.",
        "text_jp": "ピーク負荷を4時間カバーするのに十分な1年予約キャパシティを購入する。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (80%) D (20%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A: Reduce the provisioned RCUs and WCUs. This solution directly addresses the cost reduction desired by the company without increasing operational overhead.",
        "situation_analysis": "The company's operations have a clearly defined peak time of 4 hours per day, which means the DyanmoDB resource use can be adjusted accordingly.",
        "option_analysis": "Option A allows for immediate cost savings by reducing unused capacity. Option B might lead to higher costs if usage spikes beyond expected levels. Option C could help manage traffic but may incur additional costs when scaling up resources. Option D would require upfront costs without addressing the need for agility.",
        "additional_knowledge": "Regularly reviewing and adjusting provisioned capacity is essential for optimizing costs in AWS.",
        "key_terminology": "DynamoDB, RCU, WCU, provisioned throughput, cost management",
        "overall_assessment": "While option C is popular among community votes, the company's context clearly favors option A due to its straightforward cost-saving nature without extra complexities."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はA: プロビジョニングされたRCUとWCUを削減するです。このソリューションは、企業が望むコスト削減を直接的に実現し、運用オーバーヘッドを増加させることなく対処します。",
        "situation_analysis": "この企業の業務には、明確に定義されたピーク時間があり、その時間は毎日4時間です。したがって、DynamoDBのリソース使用は、それに応じて調整することができます。",
        "option_analysis": "選択肢Aは、未使用のキャパシティを削減することで即時のコスト節約を可能にします。選択肢Bは、使用が予想を超えた場合に高コストにつながる可能性があります。選択肢Cはトラフィックの管理に役立つ可能性がありますが、リソースを拡張する際に追加コストがかかるかもしれません。選択肢Dは、敏捷性の必要性に対処せずに、前払いのコストを必要とします。",
        "additional_knowledge": "プロビジョニングされたキャパシティを定期的に見直し、調整することは、AWSでのコスト最適化において必須です。",
        "key_terminology": "DynamoDB, RCU, WCU, プロビジョニングスループット, コスト管理",
        "overall_assessment": "選択肢Cがコミュニティ投票の中で人気がありますが、企業の文脈は明らかにAを支持しています。その理由は、追加の複雑さを伴わないコスト節約の明確な特性です。"
      }
    ],
    "keywords": [
      "DynamoDB",
      "RCU",
      "WCU",
      "provisioned throughput",
      "cost management"
    ]
  },
  {
    "No": "285",
    "question": "A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application currently\ndoes not use API keys to authorize requests. The API model is as follows:\nGET /posts/{postId}: to get post details\nGET /users/{userId}: to get user details\nGET /comments/{commentId}: to get comments details\nThe company has noticed users are actively discussing topics in the comments section, and the company wants to increase user engagement by\nmaking the comments appear in real time.\nWhich design should be used to reduce comment latency and improve user experience?",
    "question_jp": "会社はAmazon API Gateway、Amazon DynamoDB、およびAWS Lambdaを使用してAWS上にブログ投稿アプリケーションをホストしています。アプリケーションは現在、リクエストの認証にAPIキーを使用していません。APIモデルは次のとおりです：GET /posts/{postId}: 投稿の詳細を取得するため、GET /users/{userId}: ユーザーの詳細を取得するため、GET /comments/{commentId}: コメントの詳細を取得するため。会社は、ユーザーがコメントセクションで積極的にトピックについて議論していることに気づき、コメントをリアルタイムで表示することでユーザーのエンゲージメントを高めたいと考えています。コメントのレイテンシーを減らし、ユーザーエクスペリエンスを向上させるために、どの設計を使用するべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Use edge-optimized API with Amazon CloudFront to cache API responses.",
        "text_jp": "Amazon CloudFrontを使用してAPIレスポンスをキャッシュするエッジ最適化APIを使用する。"
      },
      {
        "key": "B",
        "text": "Modify the blog application code to request GET/comments/{commentId} every 10 seconds.",
        "text_jp": "ブログアプリケーションコードを変更して、10秒ごとにGET/comments/{commentId}をリクエストする。"
      },
      {
        "key": "C",
        "text": "Use AWS AppSync and leverage WebSockets to deliver comments.",
        "text_jp": "AWS AppSyncを使用し、WebSocketを活用してコメントを配信する。"
      },
      {
        "key": "D",
        "text": "Change the concurrency limit of the Lambda functions to lower the API response time.",
        "text_jp": "APIレスポンス時間を短縮するためにLambda関数の同時実行制限を変更する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "正解はCであり、AWS AppSyncを使用してWebSocketを活用することが推奨される。このアプローチはリアルタイム性を提供し、ユーザーがコメントを即座に受け取ることを可能にする。",
        "situation_analysis": "ユーザーのエンゲージメントを高めるためには、リアルタイムのコメント配信が重要である。現在の構成は効率的ではない。",
        "option_analysis": "選択肢Aはキャッシュを使用するが、リアルタイム性を提供できない。選択肢Bは多くのリクエストを生成し、効率が悪い。選択肢Dは改善される可能性があるが、根本的な問題は解決しない。",
        "additional_knowledge": "他のアプローチとしては、AWS IoTを利用する方法も考えられる。",
        "key_terminology": "AWS AppSync, WebSocket, Real-time data, Data synchronization, API Gateway",
        "overall_assessment": "質問はAWSのリアルタイムデータ配信の理解を測るもので、Cの選択肢がベストであることを支持するコミュニティの投票が全てCに集中していることからも明らかである。"
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はCであり、AWS AppSyncを使用してWebSocketを活用することが推奨される。このアプローチはリアルタイム性を提供し、ユーザーがコメントを即座に受け取ることを可能にする。",
        "situation_analysis": "ユーザーのエンゲージメントを高めるためには、リアルタイムのコメント配信が重要である。現在の構成は効率的ではない。",
        "option_analysis": "選択肢Aはキャッシュを使用するが、リアルタイム性を提供できない。選択肢Bは多くのリクエストを生成し、効率が悪い。選択肢Dは改善される可能性があるが、根本的な問題は解決しない。",
        "additional_knowledge": "他のアプローチとしては、AWS IoTを利用する方法も考えられる。",
        "key_terminology": "AWS AppSync, WebSocket, Real-time data, Data synchronization, API Gateway",
        "overall_assessment": "質問はAWSのリアルタイムデータ配信の理解を測るもので、Cの選択肢がベストであることを支持するコミュニティの投票が全てCに集中していることからも明らかである。"
      }
    ],
    "keywords": [
      "AWS AppSync",
      "WebSocket",
      "Real-time data",
      "Data synchronization",
      "API Gateway"
    ]
  },
  {
    "No": "286",
    "question": "A company manages hundreds of AWS accounts centrally in an organization in AWS Organizations. The company recently started to allow product\nteams to create and manage their own S3 access points in their accounts. The S3 access points can be accessed only within VPCs, not on the\ninternet.\nWhat is the MOST operationally eficient way to enforce this requirement?",
    "question_jp": "ある企業は、AWS Organizations内で数百のAWSアカウントを中央管理しています。最近、企業は製品チームに自分たちのアカウントでS3アクセスのポイントを作成し管理することを許可し始めました。S3アクセスのポイントは、インターネットではなくVPC内でのみアクセス可能です。\nこの要件を満たすための最も運用効率の良い方法は何ですか？",
    "choices": [
      {
        "key": "A",
        "text": "Set the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
        "text_jp": "S3アクセスのポイントリソースポリシーを設定し、s3:CreateAccessPointアクションをVPCとして評価されない場合は拒否します。"
      },
      {
        "key": "B",
        "text": "Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
        "text_jp": "組織のルートレベルにSCPを作成し、s3:CreateAccessPointアクションをVPCとして評価されない場合は拒否します。"
      },
      {
        "key": "C",
        "text": "Use AWS CloudFormation StackSets to create a new IAM policy in each AWS account that allows the s3:CreateAccessPoint action only if the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
        "text_jp": "AWS CloudFormation StackSetsを使用して、各AWSアカウントに新しいIAMポリシーを作成し、s3:CreateAccessPointアクションをs3:AccessPointNetworkOrigin条件キーがVPCとして評価される場合にのみ許可します。"
      },
      {
        "key": "D",
        "text": "Set the S3 bucket policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
        "text_jp": "S3バケットポリシーを設定し、s3:CreateAccessPointアクションをVPCとして評価されない場合は拒否します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (92%) 8%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Setting the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC is the most efficient method.",
        "situation_analysis": "The requirement is to ensure that S3 access points can only be created if they are networked to a VPC, which is a core aspect of the company's security requirements.",
        "option_analysis": "Option A directly addresses the requirement by modifying the resource policy associated with the S3 access point. Option B, while valid, may not be the most operationally efficient approach, as it applies at the organizational level rather than the resource level. Option C introduces unnecessary complexity with CloudFormation, and Option D is limited to bucket-level permissions affecting all access points.",
        "additional_knowledge": "Resource-based policies are critical in providing fine-grained access control in AWS.",
        "key_terminology": "Resource Policy, VPC, Access Points, AWS Organizations, Security Requirements",
        "overall_assessment": "Option A is aligned with AWS best practices as it enforces permissions at the required scope and does so without introducing complexity. The community vote distribution suggests that many do not see the operational benefits of the other options."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。S3アクセスのポイントリソースポリシーを設定して、s3:CreateAccessPointアクションをs3:AccessPointNetworkOrigin条件キーがVPCとして評価されない場合は拒否することが最も効率的な方法である。",
        "situation_analysis": "要件は、S3アクセスポイントを作成する際に、VPCに接続されている場合のみに制限することが求められており、これは企業のセキュリティ要件の重要な側面である。",
        "option_analysis": "選択肢Aは、S3アクセスのポイントに関連するリソースポリシーを修正することで、要求に直接対応している。選択肢Bは有効であるが、組織のレベルで適用されるため、運用効率的なアプローチであるとは言えない。選択肢CはCloudFormationの使用により不必要な複雑さを持ち込む。選択肢Dは、バケットレベルの権限にのみ制限され、すべてのアクセスポイントに影響を与える。",
        "additional_knowledge": "リソースベースのポリシーは、AWSにおける詳細なアクセス制御を提供する上で重要である。",
        "key_terminology": "リソースポリシー, VPC, アクセスポイント, AWS Organizations, セキュリティ要件",
        "overall_assessment": "選択肢Aは、必要なスコープで権限を強制し、複雑さを持ち込むことなく、AWSのベストプラクティスに従っている。コミュニティの投票分布は、他のオプションの運用上の利点を多くの人が見ていないことを示唆している。"
      }
    ],
    "keywords": [
      "Resource Policy",
      "VPC",
      "Access Points",
      "AWS Organizations",
      "Security Requirements"
    ]
  },
  {
    "No": "287",
    "question": "A solutions architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The\nsolutions architect creates an environment that is identical to the existing application environment and deploys the application to the new\nenvironment.\nWhat should be done next to complete the update?",
    "question_jp": "ソリューションアーキテクトは、AWS Elastic Beanstalk内のアプリケーション環境をブルー/グリーンデプロイメント手法を用いて更新する必要があります。ソリューションアーキテクトは、既存のアプリケーション環境と同一の環境を作成し、新しい環境にアプリケーションをデプロイします。次に、更新を完了するために何をすべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Redirect to the new environment using Amazon Route 53.",
        "text_jp": "新しい環境にリダイレクトするためにAmazon Route 53を使用します。"
      },
      {
        "key": "B",
        "text": "Select the Swap Environment URLs option.",
        "text_jp": "Swap Environment URLsオプションを選択します。"
      },
      {
        "key": "C",
        "text": "Replace the Auto Scaling launch configuration.",
        "text_jp": "Auto Scalingの起動設定を置き換えます。"
      },
      {
        "key": "D",
        "text": "Update the DNS records to point to the green environment.",
        "text_jp": "DNSレコードを更新してグリーン環境にポイントします。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Redirecting to the new environment using Amazon Route 53 completes the blue/green deployment process by ensuring users are directed to the updated application without downtime.",
        "situation_analysis": "In a blue/green deployment, creating a new environment that mirrors the existing one is essential. The next step involves directing traffic to the new environment.",
        "option_analysis": "Option A is correct as it utilizes Route 53 for redirection. Option B is incorrect because selecting the Swap Environment URLs directly would also work, but is not the best practice for all scenarios. Options C and D are incorrect since replacing the launch configuration and merely updating DNS records does not complete the deployment process effectively.",
        "additional_knowledge": "Understanding when to swap URLs versus redirecting traffic is crucial in maintaining application uptime and user experience.",
        "key_terminology": "Blue/Green Deployment, Amazon Route 53, AWS Elastic Beanstalk, DNS Redirection, Auto Scaling",
        "overall_assessment": "The question is well-structured and tests the understanding of blue/green deployment strategies in AWS. Despite community support for option B, option A is the more applicable action within the blue/green methodology."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。新しい環境にリダイレクトするためにAmazon Route 53を使用することは、ダウンタイムなしで更新されたアプリケーションにユーザーを方向付けることで、ブルー/グリーンデプロイメントプロセスを完了します。",
        "situation_analysis": "ブルー/グリーンデプロイメントでは、既存の環境を反映した新しい環境を作成することが重要です。次のステップは、トラフィックを新しい環境に向けることです。",
        "option_analysis": "選択肢Aは、ルート53を利用してリダイレクトを行うため正解です。選択肢Bは、Swap Environment URLsを選択することも機能しますが、すべてのシナリオに対して最善の実践ではありません。選択肢CとDは、起動設定を置き換えたりDNSレコードを更新するだけでは、デプロイメントプロセスを効果的に完了しないため不正解です。",
        "additional_knowledge": "URLを交換するタイミングとトラフィックをリダイレクトするタイミングを理解することは、アプリケーションの稼働時間とユーザーエクスペリエンスを維持する上で重要です。",
        "key_terminology": "ブルー/グリーンデプロイメント、Amazon Route 53、AWS Elastic Beanstalk、DNSリダイレクション、Auto Scaling",
        "overall_assessment": "この質問はよく構成されており、AWSにおけるブルー/グリーンデプロイメント戦略の理解をテストしています。コミュニティが選択肢Bを支持しているにもかかわらず、選択肢Aはブルー/グリーン手法内でより適切なアクションです。"
      }
    ],
    "keywords": [
      "Blue/Green Deployment",
      "Amazon Route 53",
      "AWS Elastic Beanstalk",
      "DNS Redirection",
      "Auto Scaling"
    ]
  },
  {
    "No": "288",
    "question": "A company is building an image service on the web that will allow users to upload and search random photos. At peak usage, up to 10,000 users\nworldwide will upload their images. The will then overlay text on the uploaded images, which will then be published on the company website.\nWhich design should a solutions architect implement?",
    "question_jp": "会社は、ユーザーがランダムな写真をアップロードして検索できるウェブ上の画像サービスを構築しています。ピーク時には、世界中で最大10,000人のユーザーが画像をアップロードします。その後、アップロードされた画像にテキストをオーバーレイし、それを会社のウェブサイトに公開します。ソリューションアーキテクトはどのようなデザインを実装するべきでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Store the uploaded images in Amazon Elastic File System (Amazon EFS). Send application log information about each image to Amazon CloudWatch Logs. Create a fieet of Amazon EC2 instances that use CloudWatch Logs to determine which images need to be processed. Place processed images in another directory in Amazon EFS. Enable Amazon CloudFront and configure the origin to be the one of the EC2 instances in the fieet.",
        "text_jp": "アップロードされた画像をAmazon Elastic File System (Amazon EFS)に保存します。各画像に関するアプリケーションログ情報をAmazon CloudWatch Logsに送信します。CloudWatch Logsを使用してどの画像を処理する必要があるかを判断するために、EC2インスタンスのフリートを作成します。処理された画像をAmazon EFSの別のディレクトリに置きます。Amazon CloudFrontを有効にし、オリジンをフリートのEC2インスタンスの1つに設定します。"
      },
      {
        "key": "B",
        "text": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to Amazon Simple Notification Service (Amazon SNS). Create a fieet of Amazon EC2 instances behind an Application Load Balancer (ALB) to pull messages from Amazon SNS to process the images and place them in Amazon Elastic File System (Amazon EFS). Use Amazon CloudWatch metrics for the SNS message volume to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the ALB in front of the EC2 instances.",
        "text_jp": "アップロードされた画像をAmazon S3バケットに保存し、S3バケットイベント通知を設定してAmazon Simple Notification Service (Amazon SNS)にメッセージを送信します。Application Load Balancer (ALB)の背後にあるEC2インスタンスのフリートを作成し、Amazon SNSからメッセージを取得して画像を処理し、Amazon Elastic File System (Amazon EFS)に置きます。SNSメッセージのボリュームに対してAmazon CloudWatchメトリクスを使用してEC2インスタンスをスケールアウトします。Amazon CloudFrontを有効にし、オリジンをEC2インスタンスの前にあるALBに設定します。"
      },
      {
        "key": "C",
        "text": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to the Amazon Simple Queue Service (Amazon SQS) queue. Create a fieet of Amazon EC2 instances to pull messages from the SQS queue to process the images and place them in another S3 bucket. Use Amazon CloudWatch metrics for queue depth to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the S3 bucket that contains the processed images.",
        "text_jp": "アップロードされた画像をAmazon S3バケットに保存し、S3バケットイベント通知を設定してAmazon Simple Queue Service (Amazon SQS)キューにメッセージを送信します。EC2インスタンスのフリートを作成してSQSキューからメッセージを取得し画像を処理して別のS3バケットに置きます。キューの深さに対してAmazon CloudWatchメトリクスを使用してEC2インスタンスをスケールアウトします。Amazon CloudFrontを有効にし、処理された画像を含むS3バケットをオリジンとして設定します。"
      },
      {
        "key": "D",
        "text": "Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume mounted to a fieet of Amazon EC2 Spot instances. Create an Amazon DynamoDB table that contains information about each uploaded image and whether it has been processed. Use an Amazon EventBridge rule to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to reference an Elastic Load Balancer in front of the fieet of EC2 instances.",
        "text_jp": "アップロードされた画像を共有Amazon Elastic Block Store (Amazon EBS)ボリュームに保存し、EC2スポットインスタンスのフリートにマウントします。アップロードされた各画像に関する情報と、処理済みであるかどうかの情報を含むAmazon DynamoDBテーブルを作成します。EC2インスタンスをスケールアウトするためにAmazon EventBridgeルールを使用します。Amazon CloudFrontを有効にし、オリジンとしてEC2インスタンスのフリートの前にあるElastic Load Balancerを参照します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "C (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. Storing images in a shared Amazon EBS volume allows for high-speed access and ensures that only available EC2 Spot instances are using them. This design also leverages DynamoDB for tracking image processing status and EventBridge for scaling, which is optimal for a variable user load.",
        "situation_analysis": "The scenario involves high image upload frequency, needing to handle scaling efficiently while ensuring reliable processing.",
        "option_analysis": "Options A, B, and C utilize EFS or S3 but do not effectively manage scaling based on Spot instance availability like D does. Moreover, they introduce unnecessary complexity in processing and notifications.",
        "additional_knowledge": "It's essential to understand the cost and performance trade-offs associated with EBS and Spot Instances as opposed to other storage solutions.",
        "key_terminology": "Amazon EBS, Amazon DynamoDB, EventBridge, EC2 Spot Instances, CloudFront",
        "overall_assessment": "While community votes heavily favored option C, the best practices for handling variable workloads and costs are better aligned with D, making it the appropriate choice despite community sentiment."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。共有のAmazon EBSボリュームに画像を保存することで、高速なアクセスが可能になり、利用可能なEC2スポットインスタンスのみがこれを使用することが保証される。この設計は、画像処理状況を追跡するためにDynamoDBを活用し、スケーリングにはEventBridgeを使用するため、変動するユーザーロードに最適である。",
        "situation_analysis": "このシナリオは、高頻度の画像アップロードが関与しており、効率的にスケーリングしながら、信頼性のある処理を保証する必要がある。",
        "option_analysis": "選択肢A、B、CはEFSまたはS3を利用するが、スポットインスタンスの可用性に基づいてスケーリングを効果的に管理することができない。それに対し、Dはこれをうまく管理している。さらに、他のオプションは処理や通知に不必要な複雑さを導入している。",
        "additional_knowledge": "EBSとスポットインスタンスに関するコストとパフォーマンスのトレードオフを理解することが重要である。",
        "key_terminology": "Amazon EBS、Amazon DynamoDB、EventBridge、EC2スポットインスタンス、CloudFront",
        "overall_assessment": "コミュニティ投票はCを強く支持しているが、変動するワークロードおよびコストを管理するためのベストプラクティスはDにより適合しているため、正式な選択としてはDが適切である。"
      }
    ],
    "keywords": [
      "Amazon EBS",
      "Amazon DynamoDB",
      "EventBridge",
      "EC2 Spot Instances",
      "CloudFront"
    ]
  },
  {
    "No": "289",
    "question": "A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data\navailable to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not\ntolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of\ncustomers need to see updates from the other group in real time.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業が、us-east-1 リージョンに Amazon RDS for MySQL DB インスタンスにデータベースを展開しました。この企業は、データをヨーロッパの顧客に提供する必要があります。ヨーロッパの顧客は、アメリカ合衆国（US）の顧客と同じデータにアクセスする必要があり、高いアプリケーショのレイテンシや古いデータは許容できません。ヨーロッパの顧客とアメリカの顧客の両方がデータベースに書き込む必要があります。両グループの顧客は、リアルタイムで他のグループからの更新を確認する必要があります。どのソリューションがこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure the application to use the Aurora database and resume writes. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.",
        "text_jp": "Amazon Aurora MySQL レプリカを RDS for MySQL DB インスタンスの作成します。RDS DB インスタンスへのアプリケーション書き込みを一時停止します。Aurora レプリカをスタンドアロンの DB クラスターに昇格させます。アプリケーションを Aurora データベースを使用するように再構成し、書き込みを再開します。DB クラスターに eu-west-1 をセカンダリリージョンとして追加します。DB クラスターで書き込みフォワーディングを有効にします。eu-west-1 でアプリケーションを展開します。アプリケーションを eu-west-1 の Aurora MySQL エンドポイントを使用するように構成します。"
      },
      {
        "key": "B",
        "text": "Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the primary DB instance. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.",
        "text_jp": "RDS for MySQL DB インスタンスのために eu-west-1 にクロスリージョンレプリカを追加します。レプリカを構成して、書き込みクエリをプライマリ DB インスタンスに逆レプリケートします。eu-west-1 にアプリケーションを展開します。アプリケーションを eu-west-1 の RDS for MySQL エンドポイントを使用するように構成します。"
      },
      {
        "key": "C",
        "text": "Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1 from the snapshot. Configure MySQL logical replication from us-east-1 to eu-west-1. Enable write forwarding on the DB cluster. Deploy the application in eu-wes&1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.",
        "text_jp": "RDS for MySQL DB インスタンスから最も最近のスナップショットを eu-west-1 にコピーします。スナップショットから eu-west-1 に新しい RDS for MySQL DB インスタンスを作成します。us-east-1 から eu-west-1 への MySQL 論理レプリケーションを構成します。DB クラスターで書き込みフォワーディングを有効にします。eu-west-1 でアプリケーションを展開します。アプリケーションを eu-west-1 の RDS for MySQL エンドポイントを使用するように構成します。"
      },
      {
        "key": "D",
        "text": "Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.",
        "text_jp": "RDS for MySQL DB インスタンスを Amazon Aurora MySQL DB クラスターに変換します。eu-west-1 を DB クラスターのセカンダリリージョンとして追加します。DB クラスターで書き込みフォワーディングを有効にします。eu-west-1 にアプリケーションを展開します。アプリケーションを eu-west-1 の Aurora MySQL エンドポイントを使用するように構成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (58%) D (42%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, which involves adding a cross-region replica in eu-west-1 that replicates write queries back to the primary DB instance.",
        "situation_analysis": "The company needs to ensure low latency and real-time data synchronization between customers in the US and Europe, while allowing both regions to write to the database.",
        "option_analysis": "Option A suggests creating an Aurora replica and pausing writes, which may lead to delays. Option C suggests using snapshots and logical replication, which also introduces latency. Option D changes the RDS to Aurora but does not address cross-region writes efficiently.",
        "additional_knowledge": "The practical implementation of cross-region replicas and their settings is essential for businesses operating globally.",
        "key_terminology": "Amazon RDS, MySQL, cross-region replication, write forwarding, real-time data synchronization.",
        "overall_assessment": "The community vote favored options A and D; however, option B is the most suitable for real-time synchronization and low latency. It is crucial to prioritize solutions that can handle write operations across regions effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解は B であり、eu-west-1 にクロスリージョンレプリカを追加し、書き込みクエリをプライマリ DB インスタンスに逆レプリケートします。",
        "situation_analysis": "企業は、米国とヨーロッパの顧客間で低レイテンシとリアルタイムデータの同期を確保する必要があり、両地域でデータベースへの書き込みを許可します。",
        "option_analysis": "選択肢 A は、Aurora レプリカを作成し書き込みを一時停止することを提案していますが、これにより遅延が発生する可能性があります。選択肢 C はスナップショットと論理レプリケーションを使用することを提案していますが、これも遅延を引き起こします。選択肢 D は RDS を Aurora に変換しますが、効率的なクロスリージョン書き込みに対応していません。",
        "additional_knowledge": "クロスリージョンレプリカの実装と設定は、グローバルに事業を展開する企業にとって不可欠です。",
        "key_terminology": "Amazon RDS, MySQL, クロスリージョンレプリケーション, 書き込みフォワーディング, リアルタイムデータ同期。",
        "overall_assessment": "コミュニティの投票では A および D が支持されていましたが、B はリアルタイムの同期と低レイテンシを提供する最も適切な選択肢です。地域を超えた書き込み操作を効果的に処理できるソリューションを優先することが重要です。"
      }
    ],
    "keywords": [
      "Amazon RDS",
      "MySQL",
      "cross-region replication",
      "write forwarding",
      "real-time data synchronization"
    ]
  },
  {
    "No": "290",
    "question": "A company is serving files to its customers through an SFTP server that is accessible over the internet. The SFTP server is running on a single\nAmazon EC2 instance with an Elastic IP address attached. Customers connect to the SFTP server through its Elastic IP address and use SSH for\nauthentication. The EC2 instance also has an attached security group that allows access from all customer IP addresses.\nA solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the\ndisruption to customers who access files. The solution must not change the way customers connect.\nWhich solution will meet these requirements?",
    "question_jp": "ある企業は、インターネットを通じて顧客にファイルを提供するために、SFTPサーバーを運用しています。このSFTPサーバーは、Elastic IPアドレスが割り当てられた単一のAmazon EC2インスタンス上で動作しています。顧客はElastic IPアドレスを介してSFTPサーバーに接続し、SSHを使用して認証を行います。また、EC2インスタンスには、すべての顧客IPアドレスからのアクセスを許可するセキュリティグループがアタッチされています。ソリューションアーキテクトは、可用性を向上させ、インフラ管理の複雑さを最小化し、ファイルにアクセスする顧客への影響を最小限に抑えるソリューションを実装する必要があります。このソリューションは顧客が接続する方法を変更してはなりません。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a publicly accessible endpoint. Associate the SFTP Elastic IP address with the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.",
        "text_jp": "Elastic IPアドレスをEC2インスタンスからデタッチします。SFTPファイルホスティング用のAmazon S3バケットを作成します。AWS Transfer Familyサーバーを作成します。Transfer Familyサーバーを公開アクセス可能なエンドポイントで構成します。SFTP Elastic IPアドレスを新しいエンドポイントに関連付けます。Transfer FamilyサーバーをS3バケットにポイントします。SFTPサーバーからS3バケットにすべてのファイルを同期します。"
      },
      {
        "key": "B",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a VPC-hosted, internet-facing endpoint. Associate the SFTP Elastic IP address with the new endpoint. Attach the security group with customer IP addresses to the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.",
        "text_jp": "Elastic IPアドレスをEC2インスタンスからデタッチします。SFTPファイルホスティング用のAmazon S3バケットを作成します。AWS Transfer Familyサーバーを作成します。Transfer FamilyサーバーをVPCホストのインターネット向けエンドポイントで構成します。SFTP Elastic IPアドレスを新しいエンドポイントに関連付けます。顧客IPアドレスを含むセキュリティグループを新しいエンドポイントにアタッチします。Transfer FamilyサーバーをS3バケットにポイントします。SFTPサーバーからS3バケットにすべてのファイルを同期します。"
      },
      {
        "key": "C",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create a new Amazon Elastic File System (Amazon EFS) file system to be used for SFTP file hosting. Create an AWS Fargate task definition to run an SFTP server. Specify the EFS file system as a mount in the task definition. Create a Fargate service by using the task definition, and place a Network Load Balancer (NLB) in front of the service. When configuring the service, attach the security group with customer IP addresses to the tasks that run the SFTP server. Associate the Elastic IP address with the NLB. Sync all files from the SFTP server to the S3 bucket.",
        "text_jp": "Elastic IPアドレスをEC2インスタンスからデタッチします。SFTPファイルホスティング用の新しいAmazon Elastic File System (Amazon EFS)ファイルシステムを作成します。AWS Fargateタスク定義を作成してSFTPサーバーを実行します。タスク定義にEFSファイルシステムをマウントとして指定します。タスク定義を使用してFargateサービスを作成し、サービスの前にネットワークロードバランサー (NLB) を配置します。サービスを構成する際、SFTPサーバーを実行するタスクに顧客IPアドレスを含むセキュリティグループをアタッチします。Elastic IPアドレスをNLBに関連付けます。SFTPサーバーからS3バケットにすべてのファイルを同期します。"
      },
      {
        "key": "D",
        "text": "Disassociate the Elastic IP address from the EC2 instance. Create a multi-attach Amazon Elastic Block Store (Amazon EBS) volume to be used for SFTP file hosting. Create a Network Load Balancer (NLB) with the Elastic IP address attached. Create an Auto Scaling group with EC2 instances that run an SFTP server. Define in the Auto Scaling group that instances that are launched should attach the new multi-attach EBS volume. Configure the Auto Scaling group to automatically add instances behind the NLB. Configure the Auto Scaling group to use the security group that allows customer IP addresses for the EC2 instances that the Auto Scaling group launches. Sync all files from the SFTP server to the new multi-attach EBS volume.",
        "text_jp": "Elastic IPアドレスをEC2インスタンスからデタッチします。SFTPファイルホスティング用のマルチアタッチAmazon Elastic Block Store (Amazon EBS)ボリュームを作成します。Elastic IPアドレスが接続されたネットワークロードバランサー (NLB) を作成します。SFTPサーバーを実行するEC2インスタンスで構成されたAuto Scalingグループを作成します。起動されるインスタンスが新しいマルチアタッチEBSボリュームにアタッチするようにAuto Scalingグループで定義します。Auto Scalingグループを構成して、NLBの背後にインスタンスを自動的に追加します。Auto Scalingグループが起動したEC2インスタンスに対して顧客IPアドレスを許可するセキュリティグループを使用するように構成します。すべてのファイルを新しいマルチアタッチEBSボリュームに同期します。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "B (88%) 13%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: This solution implements a new architecture using AWS Fargate and Amazon EFS, providing better availability and reducing management complexity.",
        "situation_analysis": "The current setup lacks redundancy as it relies on a single EC2 instance. High availability is critical for the SFTP service, especially to avoid disruptions.",
        "option_analysis": "Choice C utilizes AWS Fargate to run the SFTP server, allowing for automatic scaling, while using Amazon EFS provides a resilient file storage solution. Other options either increase complexity or do not meet availability requirements adequately.",
        "additional_knowledge": "It is crucial to understand AWS service integrations to ensure optimal solutions.",
        "key_terminology": "AWS Fargate, Amazon EFS, Network Load Balancer",
        "overall_assessment": "Despite community belief favoring option B, option C presents a more robust architecture for availability and ease of management. Community votes may reflect a misunderstanding of the architectural capabilities of AWS."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい回答はCである。このソリューションは、AWS FargateとAmazon EFSを使用して新しいアーキテクチャを実装し、可用性を向上させ、管理の複雑さを軽減するものである。",
        "situation_analysis": "現在のセットアップは、単一のEC2インスタンスに依存しているため、冗長性が欠けている。高可用性はSFTPサービスにとって重要であり、特に中断を回避するために必要である。",
        "option_analysis": "選択肢Cは、SFTPサーバーを実行するためにAWS Fargateを利用し、自動スケーリングを可能にしている。また、Amazon EFSを使用することで、耐障害性のあるファイルストレージソリューションを提供している。他の選択肢は、複雑さを増すか、可用性要件を十分に満たしていない。",
        "additional_knowledge": "最適なソリューションを確保するために、AWSサービスの統合を理解することが重要である。",
        "key_terminology": "AWS Fargate, Amazon EFS, Network Load Balancer",
        "overall_assessment": "コミュニティが選択肢Bを支持しているにもかかわらず、選択肢Cは可用性と管理の容易さのためにより堅牢なアーキテクチャを提供している。コミュニティの投票は、AWSのアーキテクチャ能力の誤解を反映している可能性がある。"
      }
    ],
    "keywords": [
      "AWS Fargate",
      "Amazon EFS",
      "Network Load Balancer"
    ]
  },
  {
    "No": "291",
    "question": "A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics takes 4\nhours to complete. The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run\nfails.\nThe current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations. These EC2 instances run full time to ingest and\nstore the streaming data in attached Amazon Elastic Block Store (Amazon EBS) volumes. A scheduled script launches EC2 On-Demand Instances\neach night to perform the nightly processing. The instances access the stored data from NFS shares on the ingestion servers. The script\nterminates the instances when the processing is complete.\nThe Reserved Instance reservations are expiring. The company needs to determine whether to purchase new reservations or implement a new\ndesign.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業がストリーミング市場データを取り込み、処理する。データレートは一定である。集約統計を計算する夜間プロセスは、完了するまでに4時間かかる。統計分析はビジネスにとって重要ではなく、特定の実行が失敗した場合、データポイントは次の反復で処理される。現在のアーキテクチャは、1年間の予約があるAmazon EC2リザーブドインスタンスのプールを使用している。これらのEC2インスタンスはフルタイムで稼働し、ストリーミングデータを取り込み、接続されたAmazon Elastic Block Store（Amazon EBS）ボリュームに保存する。スケジュールされたスクリプトが毎晩EC2オンデマンドインスタンスを起動し、夜間処理を実行する。インスタンスは、取り込みサーバー上のNFS共有から保存されたデータにアクセスする。スクリプトは、処理が完了するとインスタンスを終了する。リザーブドインスタンスの予約が期限切れになる。この企業は、新しい予約を購入するべきか、それとも新しい設計を実装するべきかを決定する必要がある。最もコスト効果の高い要件を満たす解決策はどれか？",
    "choices": [
      {
        "key": "A",
        "text": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a scheduled script to launch a fieet of EC2 On-Demand Instances each night to perform the batch processing of the S3 data. Configure the script to terminate the instances when the processing is complete.",
        "text_jp": "取り込みプロセスを更新して、Amazon Kinesis Data Firehoseを使用し、データをAmazon S3に保存する。毎晩、S3データのバッチ処理を行うためにEC2オンデマンドインスタンスのフリートを起動するスケジュールされたスクリプトを使用する。処理が完了するとインスタンスを終了するようにスクリプトを設定する。"
      },
      {
        "key": "B",
        "text": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price.",
        "text_jp": "取り込みプロセスを更新して、Amazon Kinesis Data Firehoseを使用し、データをAmazon S3に保存する。AWSバッチを使用して、最大スポット価格がオンデマンド価格の50％であるスポットインスタンスで夜間処理を実行する。"
      },
      {
        "key": "C",
        "text": "Update the ingestion process to use a fieet of EC2 Reserved Instances with 3-year reservations behind a Network LoadBalancer. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price.",
        "text_jp": "取り込みプロセスを更新して、ネットワークロードバランサの背後に3年間の予約があるEC2リザーブドインスタンスのフリートを使用する。AWSバッチを使用して、最大スポット価格がオンデマンド価格の50％であるスポットインスタンスで夜間処理を実行する。"
      },
      {
        "key": "D",
        "text": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use Amazon EventBridge to schedule an AWS Lambda function to run nightly to query Amazon Redshift to generate the daily statistics.",
        "text_jp": "取り込みプロセスを更新して、Amazon Kinesis Data Firehoseを使用し、データをAmazon Redshiftに保存する。Amazon EventBridgeを使用して、夜間にAWS Lambda関数を実行してAmazon Redshiftをクエリし、日次統計を生成するようにスケジュールする。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "B (91%) 9%",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. This option effectively leverages AWS services to create a streamlined data ingestion and processing workflow that is also cost-effective.",
        "situation_analysis": "The company has a constant data ingestion rate and a non-critical nightly processing requirement. A cost-effective solution is needed due to expiring Reserved Instances.",
        "option_analysis": "Option A utilizes Amazon Kinesis Data Firehose to save data into Amazon S3, enabling easy access for processing. This is a cost-effective approach compared to maintaining reserved instances. Options B and C unnecessarily complicate the architecture, while D involves additional costs and complexity by introducing Redshift.",
        "additional_knowledge": "COGS (Cost Of Goods Sold) considerations indicate that moving to a more serverless architecture with on-demand services could significantly reduce operational costs while maintaining the needed processing capabilities.",
        "key_terminology": "Amazon Kinesis Data Firehose, Amazon S3, EC2 On-Demand, AWS Batch, Spot Instances",
        "overall_assessment": "Option A has been identified by community votes as both cost-effective and suitable given the requirements. Other options may offer higher complexity or costs but do not meet the criteria as effectively as A."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAである。この選択肢は、AWSサービスを活用して、コスト効果のあるデータ取り込みと処理のワークフローを実現するものである。",
        "situation_analysis": "企業は、一定のデータ取り込みレートと非クリティカルな夜間処理の要件を持っている。リザーブドインスタンスの期限切れに伴い、コスト効果の高いソリューションが求められている。",
        "option_analysis": "選択肢Aは、Amazon Kinesis Data Firehoseを使用してデータをAmazon S3に保存し、処理のための容易なアクセスを提供する。これはリザーブドインスタンスを維持するよりもコスト効果の高いアプローチである。選択肢BとCは、不要にアーキテクチャを複雑にしており、選択肢Dは追加のコストと複雑さを伴うため、Redshiftを導入している。",
        "additional_knowledge": "COGS（売上原価）の考慮事項として、よりサーバーレスなアーキテクチャに移行し、オンデマンドサービスを利用することで、運用コストを大幅に削減でき、必要な処理能力を維持できる可能性がある。",
        "key_terminology": "Amazon Kinesis Data Firehose、Amazon S3、EC2オンデマンド、AWSバッチ、スポットインスタンス",
        "overall_assessment": "選択肢Aは、コスト効果が高く、要件を考慮した場合に適切であるとコミュニティの投票で確認されている。他の選択肢は、より高い複雑さやコストを示す可能性があるが、Aほど効果的に基準を満たさない可能性がある。"
      }
    ],
    "keywords": [
      "Amazon Kinesis Data Firehose",
      "Amazon S3",
      "EC2 On-Demand",
      "AWS Batch",
      "Spot Instances"
    ]
  },
  {
    "No": "292",
    "question": "A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a Linux VM. Uploaded files are made available to\ndownstream applications through an NFS share.\nAs part of the migration to AWS, a solutions architect must implement high availability. The solution must provide external vendors with a set of\nstatic public IP addresses that the vendors can allow. The company has set up an AWS Direct Connect connection between its on-premises data\ncenter and its VPC.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "question_jp": "ある企業がオンプレミスのSFTPサイトをAWSに移行する必要があります。そのSFTPサイトは現在、LinuxのVM上で稼働しています。アップロードされたファイルは、NFS共有を介してダウンストリームアプリケーションに提供されます。AWSへの移行の一環として、ソリューションアーキテクトは高可用性を実現しなければなりません。このソリューションは、外部ベンダーに対して、ベンダーが許可できる固定のパブリックIPアドレスのセットを提供する必要があります。会社は、オンプレミスのデータセンターとVPC間にAWS Direct Connect接続を設定しました。これらの要件を満たすためのソリューションは、最も運用のオーバーヘッドが少ないものはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "Create an AWS Transfer Family server. Configure an internet-facing VPC endpoint for the Transfer Family server. Specify an Elastic IP address for each subnet. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.",
        "text_jp": "AWS Transfer Familyサーバーを作成します。Transfer Familyサーバーのためにインターネット向けのVPCエンドポイントを設定します。各サブネットに対してElastic IPアドレスを指定します。Transfer Familyサーバーを構成し、複数のアベイラビリティゾーンに展開されたAmazon Elastic File System (Amazon EFS)にファイルを配置します。既存のNFS共有にアクセスするダウンストリームアプリケーションの構成を変更し、EFSエンドポイントをマウントします。"
      },
      {
        "key": "B",
        "text": "Create an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.",
        "text_jp": "AWS Transfer Familyサーバーを作成します。Transfer Familyサーバーのためにパブリックにアクセス可能なエンドポイントを設定します。Transfer Familyサーバーを構成し、複数のアベイラビリティゾーンに展開されたAmazon Elastic File System (Amazon EFS)にファイルを配置します。既存のNFS共有にアクセスするダウンストリームアプリケーションの構成を変更し、EFSエンドポイントをマウントします。"
      },
      {
        "key": "C",
        "text": "Use AWS Application Migration Service to migrate the existing Linux VM to an Amazon EC2 instance. Assign an Elastic IP address to the EC2 instance. Mount an Amazon Elastic File System (Amazon EFS) file system to the EC2 instance. Configure the SFTP server to place files in the EFS file system. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.",
        "text_jp": "AWS Application Migration Serviceを使用して既存のLinux VMをAmazon EC2インスタンスに移行します。EC2インスタンスにElastic IPアドレスを割り当てます。Amazon Elastic File System (Amazon EFS)をEC2インスタンスにマウントします。SFTPサーバーを構成し、EFSファイルシステムにファイルを配置します。既存のNFS共有にアクセスするダウンストリームアプリケーションの構成を変更し、EFSエンドポイントをマウントします。"
      },
      {
        "key": "D",
        "text": "Use AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon FSx for Lustre file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the FSx for Lustre endpoint instead.",
        "text_jp": "AWS Application Migration Serviceを使用して既存のLinux VMをAWS Transfer Familyサーバーに移行します。Transfer Familyサーバーのためにパブリックにアクセス可能なエンドポイントを設定します。Transfer Familyサーバーを構成し、複数のアベイラビリティゾーンに展開されたAmazon FSx for Lustreファイルシステムにファイルを配置します。既存のNFS共有にアクセスするダウンストリームアプリケーションの構成を変更し、FSx for Lustreエンドポイントをマウントします。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B. The implementation of a publicly accessible AWS Transfer Family server that connects to an EFS provides the required functionality with minimal operational overhead.",
        "situation_analysis": "The situation requires a seamless transition of an SFTP service to AWS while ensuring high availability and public accessibility of the service for external vendors.",
        "option_analysis": "Option A suggests creating an internet-facing VPC endpoint which entails additional management and management of Elastic IPs, increasing operational overhead. Option C introduces an EC2 instance which requires additional management overhead as compared to using managed services like AWS Transfer Family. Option D suggests using FSx for Lustre which is not necessary as EFS is sufficient for this use case.",
        "additional_knowledge": "AWS best practices emphasize using managed services where possible to reduce operational complexity.",
        "key_terminology": "AWS Transfer Family, Amazon EFS, AWS Direct Connect, High Availability, Public IP",
        "overall_assessment": "The solution in option B aligns best with the requirements of minimal operational overhead while meeting the high availability, ease of access, and connection to the existing NFS share."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはBです。パブリックにアクセス可能なAWS Transfer Familyサーバーを構築し、EFSに接続することにより、必要な機能を最小限の運用オーバーヘッドで提供します。",
        "situation_analysis": "この状況では、外部ベンダーに対するサービスの高可用性と公開アクセスを確保しながら、SFTPサービスをAWSにシームレスに移行する必要があります。",
        "option_analysis": "選択肢Aはインターネット向けのVPCエンドポイントを作成することを提案していますが、この設定には追加の管理が必要で、Elastic IPの管理も増え、運用オーバーヘッドが増加します。選択肢CはEC2インスタンスを導入するため、管理のオーバーヘッドが増えますが、AWS Transfer Familyのようなマネージドサービスを利用する方がより効率的です。選択肢DはFSx for Lustreを使用することを示唆していますが、このユースケースではEFSで十分です。",
        "additional_knowledge": "AWSのベストプラクティスでは、運用の複雑さを軽減するために可能な限りマネージドサービスを利用することが強調されています。",
        "key_terminology": "AWS Transfer Family、Amazon EFS、AWS Direct Connect、高可用性、パブリックIP",
        "overall_assessment": "選択肢Bは、最小限の運用オーバーヘッドの要件を最もよく満たしており、高可用性、アクセスの容易さ、既存のNFS共有との接続を実現しています。"
      }
    ],
    "keywords": [
      "AWS Transfer Family",
      "Amazon EFS",
      "AWS Direct Connect",
      "High Availability",
      "Public IP"
    ]
  },
  {
    "No": "293",
    "question": "A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two\nAvailability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and\nconnectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as\nfollows:\nVPC CIDR: 10.0.0.0/23 -\nAZ1 subnet CIDR: 10.0.0.0/24 -\nAZ2 subnet CIDR: 10.0.1.0/24 -\nSince deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional\nIPv4 address space and without service downtime. Which solution will meet these requirements?",
    "question_jp": "ソリューションアーキテクトは、Amazon EC2インスタンスがAuto Scalingグループに配置されている運用ワークロードを持っています。VPCアーキテクチャは2つのアベイラビリティーゾーン（AZ）にまたがっており、それぞれにサブネットがあります。VPCはオンプレミス環境に接続されており、接続を中断することはできません。Auto Scalingグループの最大サイズは、サービス中のインスタンスが20です。VPCのIPv4アドレスは次のとおりです：VPC CIDR: 10.0.0.0/23 - AZ1サブネットCIDR: 10.0.0.0/24 - AZ2サブネットCIDR: 10.0.1.0/24 - 配置以来、新しいAZがリージョンで利用可能になりました。ソリューションアーキテクトは、IPv4アドレススペースを追加せず、サービスのダウンタイムなしで新しいAZを採用したいと考えています。どのソリューションがこれらの要件を満たしますか？",
    "choices": [
      {
        "key": "A",
        "text": "Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.",
        "text_jp": "Auto ScalingグループをAZ2サブネットのみに使用するように更新します。AZ1サブネットを削除して再作成し、以前のアドレス空間の半分を使用します。Auto Scalingグループも新しいAZ1サブネットを使用するように調整します。インスタンスが正常になると、Auto ScalingグループをAZ1サブネットのみに使用するように調整します。現在のAZ2サブネットを削除します。元のAZ1サブネットのアドレス空間の後半を使用して新しいAZ2サブネットを作成します。元のAZ2サブネットのアドレス空間の半分を使用して新しいAZ3サブネットを作成し、その後Auto Scalingグループをすべての3つの新しいサブネットをターゲットに更新します。"
      },
      {
        "key": "B",
        "text": "Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets.",
        "text_jp": "AZ1サブネットのEC2インスタンスを終了します。AZ1サブネットを削除して再作成し、そのアドレス空間の半分を使用します。この新しいサブネットを使用するようにAuto Scalingグループを更新します。2つ目のAZについてもこれを繰り返します。AZ3に新しいサブネットを定義し、その後Auto Scalingグループをすべての3つの新しいサブネットをターゲットに更新します。"
      },
      {
        "key": "C",
        "text": "Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC.",
        "text_jp": "同じIPv4アドレススペースで新しいVPCを作成し、各AZに1つのサブネットを定義します。既存のAuto Scalingグループを新しいVPCの新しいサブネットをターゲットにするように更新します。"
      },
      {
        "key": "D",
        "text": "Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have half the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.",
        "text_jp": "Auto ScalingグループをAZ2サブネットのみに使用するように更新します。AZ1サブネットのアドレス空間を半分に更新します。Auto Scalingグループを再びAZ1サブネットを使用するように調整します。インスタンスが正常になると、Auto ScalingグループをAZ1サブネットのみに使用するように調整します。現在のAZ2サブネットを更新し、元のAZ1サブネットのアドレス空間の後半を割り当てます。元のAZ2サブネットのアドレス空間の半分を使用して新しいAZ3サブネットを作成し、その後Auto Scalingグループをすべての3つの新しいサブネットをターゲットに更新します。"
      }
    ],
    "answer_key": "D",
    "community_vote_distribution": "A (82%) D (18%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is D. This solution allows for the expansion to a new AZ without additional IPv4 addressing and avoids downtime.",
        "situation_analysis": "The requirement is to add a new Availability Zone without affecting the operational workload and to maintain the connectivity to an on-premises environment.",
        "option_analysis": "Option D adjusts the existing AZ1 and AZ2 subnets properly to create space for AZ3 while ensuring that the Auto Scaling Group has healthy instances during the transition. Other options like A and B could cause downtime with instance termination.",
        "additional_knowledge": "In scenarios where additional AZs are introduced, it's essential to modify existing infrastructure such that it remains resilient.",
        "key_terminology": "Auto Scaling, Availability Zone (AZ), EC2 instances, VPC, subnet",
        "overall_assessment": "The design in option D aligns with AWS best practices by minimizing the risk of service interruption and effectively managing IP space."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはDである。このソリューションにより、追加のIPv4アドレススペースなしで新しいAZに拡張でき、ダウンタイムを避けることができる。",
        "situation_analysis": "新しいアベイラビリティーゾーンを追加しながら、運用ワークロードに影響を与えず、オンプレミス環境との接続を維持することが求められている。",
        "option_analysis": "選択肢Dは、AZ1とAZ2のサブネットを適切に調整してAZ3のためのスペースを作り、移行中にAuto Scalingグループに正常なインスタンスが存在することを保証している。他の選択肢であるAやBは、インスタンスの終了を伴うためダウンタイムを引き起こす可能性がある。",
        "additional_knowledge": "追加のAZが導入されるシナリオでは、既存のインフラストラクチャを調整し、堅牢さを維持することが不可欠である。",
        "key_terminology": "Auto Scaling、アベイラビリティーゾーン（AZ）、EC2インスタンス、VPC、サブネット",
        "overall_assessment": "選択肢Dの設計は、サービス中断のリスクを最小限に抑え、IPスペースを効果的に管理するというAWSのベストプラクティスに一致している。"
      }
    ],
    "keywords": [
      "Auto Scaling",
      "Availability Zone",
      "EC2 instances",
      "VPC",
      "subnet"
    ]
  },
  {
    "No": "294",
    "question": "A company uses an organization in AWS Organizations to manage the company's AWS accounts. The company uses AWS CloudFormation to\ndeploy all infrastructure. A finance team wants to build a chargeback model. The finance team asked each business unit to tag resources by using\na predefined list of project values.\nWhen the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based on project, the team noticed noncompliant\nproject values. The company wants to enforce the use of project tags for new resources.\nWhich solution will meet these requirements with the LEAST effort?",
    "question_jp": "会社はAWS Organizationsを使用して会社のAWSアカウントを管理している。会社はAWS CloudFormationを使用してすべてのインフラストラクチャをデプロイしている。財務チームはチャージバックモデルを構築したいと考えている。財務チームは各事業部に対して、プロジェクトの値の事前定義されたリストを使用してリソースにタグを付けるよう要請した。財務チームがAWS Cost ExplorerのAWSコストと使用状況レポートを使用し、プロジェクトに基づいてフィルタリングしたところ、準拠していないプロジェクト値を発見した。会社は新しいリソースに対してプロジェクトタグの使用を強制したいと考えている。最も労力をかけずにこの要件を満たす解決策はどれか？",
    "choices": [
      {
        "key": "A",
        "text": "Create a tag policy that contains the allowed project tag values in the organization's management account. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.",
        "text_jp": "許可されるプロジェクトタグの値を含むタグポリシーを組織の管理アカウントで作成する。プロジェクトタグが追加されない限り、cloudformation:CreateStack API操作を拒否するSCPを作成する。このSCPを各OUにアタッチする。"
      },
      {
        "key": "B",
        "text": "Create a tag policy that contains the allowed project tag values in each OU. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.",
        "text_jp": "許可されるプロジェクトタグの値を各OUに含むタグポリシーを作成する。プロジェクトタグが追加されない限り、cloudformation:CreateStack API操作を拒否するSCPを作成する。このSCPを各OUにアタッチする。"
      },
      {
        "key": "C",
        "text": "Create a tag policy that contains the allowed project tag values in the AWS management account. Create an IAM policy that denies the cloudformation:CreateStack API operation unless a project tag is added. Assign the policy to each user.",
        "text_jp": "許可されるプロジェクトタグの値を含むタグポリシーをAWS管理アカウントで作成する。プロジェクトタグが追加されない限り、cloudformation:CreateStack API操作を拒否するIAMポリシーを作成する。このポリシーを各ユーザーに割り当てる。"
      },
      {
        "key": "D",
        "text": "Use AWS Service Catalog to manage the CloudFormation stacks as products. Use a TagOptions library to control project tag values. Share the portfolio with all OUs that are in the organization.",
        "text_jp": "AWS Service Catalogを使用してCloudFormationスタックを製品として管理する。プロジェクトタグの値を制御するためにTagOptionsライブラリを使用する。ポートフォリオを組織内のすべてのOUと共有する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C: Create a tag policy that contains the allowed project tag values in the AWS management account and an IAM policy that denies the cloudformation:CreateStack API operation unless a project tag is added. This approach allows for consistent enforcement across all users.",
        "situation_analysis": "The company needs to enforce the use of project tags to ensure compliance and facilitate chargeback. The goal is to implement a solution that minimizes effort but effectively controls resource tagging.",
        "option_analysis": "Option A creates a tag policy in the management account but applies the SCP to OUs, which may not enforce compliance at the user level. Option B applies the policy to each OU but still relies on SCPs, which may not be as straightforward. Option D utilizes AWS Service Catalog but introduces additional complexity without necessarily enforcing tagging compliance as directly.",
        "additional_knowledge": "Using AWS Organizations, companies can manage permissions and enforce policies across multiple AWS accounts efficiently.",
        "key_terminology": "Tag Policy, IAM Policy, AWS Organizations, CloudFormation, SCP",
        "overall_assessment": "Answer C is the most straightforward and effective method for ensuring compliance with project tagging requirements. The alignment of IAM policies with tag policies ensures comprehensive control."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである：AWS管理アカウントに許可されるプロジェクトタグの値を含むタグポリシーを作成し、プロジェクトタグが追加されない限りcloudformation:CreateStack API操作を拒否するIAMポリシーを作成する。このアプローチは、すべてのユーザーにわたる一貫した強制を可能にする。",
        "situation_analysis": "会社は、準拠を保証し、チャージバックを促進するためにプロジェクトタグの使用を強制する必要がある。目標は、労力を最小限に抑えつつ、リソースのタグ付けを効果的に制御するソリューションを実装することである。",
        "option_analysis": "オプションAは、管理アカウントでタグポリシーを作成するが、OUにSCPを適用するため、ユーザーレベルでの準拠の強制が疑問視される。オプションBは、各OUにポリシーを適用するが、依然としてSCPに依存しており、やや直感的でなくなる。オプションDはAWS Service Catalogを利用するが、タグ付けの準拠を直接強制するものではなく、余分な複雑さをもたらす。",
        "additional_knowledge": "AWS Organizationsを使用することで、企業は複数のAWSアカウントにわたって権限を管理し、ポリシーを効率的に強制することができる。",
        "key_terminology": "タグポリシー, IAMポリシー, AWS Organizations, CloudFormation, SCP",
        "overall_assessment": "回答Cは、プロジェクトタグの要件に対する準拠を保証するための最も簡潔で効果的な方法である。IAMポリシーとタグポリシーの整合性は、包括的な制御を確保する。"
      }
    ],
    "keywords": [
      "Tag Policy",
      "IAM Policy",
      "AWS Organizations",
      "CloudFormation",
      "SCP"
    ]
  },
  {
    "No": "295",
    "question": "An application is deployed on Amazon EC2 instances that run in an Auto Scaling group. The Auto Scaling group configuration uses only one type\nof instance.\nCPU and memory utilization metrics show that the instances are underutilized. A solutions architect needs to implement a solution to permanently\nreduce the EC2 cost and increase the utilization.\nWhich solution will meet these requirements with the LEAST number of configuration changes in the future?",
    "question_jp": "アプリケーションは、オートスケーリンググループで実行される Amazon EC2 インスタンスにデプロイされています。オートスケーリンググループの構成では、一種類のインスタンスのみが使用されています。 CPU およびメモリの使用率メトリクスは、インスタンスが低利用であることを示しています。ソリューションアーキテクトは、EC2 コストを恒久的に削減し、利用率を向上させるためのソリューションを実装する必要があります。将来的に最小限の設定変更でこの要件を満たすソリューションはどれですか？",
    "choices": [
      {
        "key": "A",
        "text": "List instance types that have properties that are similar to the properties that the current instances have. Modify the Auto Scaling group's launch template configuration to use multiple instance types from the list.",
        "text_jp": "現在のインスタンスの特性に類似した特性を持つインスタンスタイプのリストを作成します。そのリストから複数のインスタンスタイプを使用するようにオートスケーリンググループの起動テンプレート構成を変更します。"
      },
      {
        "key": "B",
        "text": "Use the information about the application's CPU and memory utilization to select an instance type that matches the requirements. Modify the Auto Scaling group's configuration by adding the new instance type. Remove the current instance type from the configuration.",
        "text_jp": "アプリケーションの CPU とメモリの使用率に関する情報を使用して、要件に適合するインスタンスタイプを選択します。新しいインスタンスタイプを追加することでオートスケーリンググループの構成を変更します。現在のインスタンスタイプは構成から削除します。"
      },
      {
        "key": "C",
        "text": "Use the information about the application's CPU and memory utilization to specify CPU and memory requirements in a new revision of the Auto Scaling group's launch template. Remove the current instance type from the configuration.",
        "text_jp": "アプリケーションの CPU とメモリの使用率に関する情報を使用して、オートスケーリンググループの起動テンプレートの新しいリビジョンに CPU とメモリの要件を指定します。現在のインスタンスタイプは構成から削除します。"
      },
      {
        "key": "D",
        "text": "Create a script that selects the appropriate instance types from the AWS Price List Bulk API. Use the selected instance types to create a new revision of the Auto Scaling group's launch template.",
        "text_jp": "AWS Price List Bulk API から適切なインスタンスタイプを選択するスクリプトを作成します。選択されたインスタンスタイプを使用して、オートスケーリンググループの起動テンプレートの新しいリビジョンを作成します。"
      }
    ],
    "answer_key": "B",
    "community_vote_distribution": "C (70%) B (30%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is B, as it directly targets the goal of matching EC2 instance types with the application's usage patterns.",
        "situation_analysis": "The application is underutilizing the EC2 resources, indicating a need for a more appropriate instance type that aligns with its CPU and memory requirements.",
        "option_analysis": "Option A complicates future configurations by adding multiple instance types without ensuring they meet actual utilization needs. Option C proposes adjusting requirements but may not provide the necessary instance type. Option D introduces unnecessary complexity with scripting. Option B simplifies the process by selecting a suitable instance type based on actual utilization.",
        "additional_knowledge": "Monitoring tools and AWS's Well-Architected Framework should be considered for better resource management.",
        "key_terminology": "Auto Scaling Group, EC2 Instance Types, Cost Optimization, Resource Utilization, CPU and Memory Metrics.",
        "overall_assessment": "The question requires an understanding of EC2 instance selection based on performance metrics. While community voting leans towards option C, option B remains the most straightforward solution."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えは B であり、アプリケーションの使用パターンに合わせて EC2 インスタンスタイプを選択することを直接的に目指している。",
        "situation_analysis": "アプリケーションが EC2 リソースを低利用しているため、CPU とメモリの要件に適したインスタンスタイプが必要であることを示している。",
        "option_analysis": "選択肢 A は複数のインスタンスタイプの追加により将来的な設定が複雑になる可能性がある。一方、選択肢 C は要件を調整することを提案しているが、必要なインスタンスタイプを提供できないかもしれない。選択肢 D はスクリプトを使用することで不要な複雑さをもたらす。選択肢 B は、実際の利用状況に基づいて適切なインスタンスタイプを選択することで、プロセスを簡素化している。",
        "additional_knowledge": "より良いリソース管理のために、モニタリングツールや AWS の Well-Architected Framework を考慮すべきである。",
        "key_terminology": "オートスケーリンググループ, EC2 インスタンスタイプ, コスト最適化, リソース利用, CPU とメモリのメトリクス。",
        "overall_assessment": "問題は、パフォーマンスメトリクスに基づく EC2 インスタンスの選定に関する理解を必要とする。コミュニティ投票は選択肢 C に傾いているが、解決策としては選択肢 B が最も明確である。"
      }
    ],
    "keywords": [
      "Auto Scaling Group",
      "EC2 Instance Types",
      "Cost Optimization",
      "Resource Utilization",
      "CPU and Memory Metrics"
    ]
  },
  {
    "No": "296",
    "question": "A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway The\napplication data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning\nby using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.\nA solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.\nWhich solution will meet these requirements MOST cost-effectively?",
    "question_jp": "ある企業が、Amazon Elastic Container Service（Amazon ECS）およびAmazon API Gatewayを使用してコンテナ化されたアプリケーションを実装しています。アプリケーションデータは、Amazon AuroraデータベースおよびAmazon DynamoDBデータベースに保存されています。この企業は、AWS CloudFormationを使用してインフラストラクチャのプロビジョニングを自動化しています。また、AWS CodePipelineを使用してアプリケーションデプロイメントを自動化しています。ソリューションアーキテクトは、2時間のRPO（復旧ポイント目標）および4時間のRTO（復旧時間目標）を満たす災害復旧（DR）戦略を実装する必要があります。どのソリューションが最もコスト効果的にこれらの要件を満たすでしょうか？",
    "choices": [
      {
        "key": "A",
        "text": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront with origin failover to route trafic to the secondary Region during a DR scenario.",
        "text_jp": "AuroraグローバルデータベースとDynamoDBグローバルテーブルを設定して、データベースを二次AWSリージョンに複製します。プライマリリージョンおよび二次リージョンにおいて、Regionalエンドポイントを持つAPI Gateway APIを構成します。DRシナリオの際に二次リージョンへのトラフィックをルーティングするために、Amazon CloudFrontをオリジンフェイルオーバーで実装します。"
      },
      {
        "key": "B",
        "text": "Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB Streams, EventBridge. and Lambda to replicate the DynamoDB databases to the secondary Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch trafic from the primary Region to the secondary Region.",
        "text_jp": "AWS Database Migration Service（AWS DMS）、Amazon EventBridge、およびAWS Lambdaを使用してAuroraデータベースを二次AWSリージョンに複製します。DynamoDBデータベースを二次リージョンに複製するために、DynamoDB Streams、EventBridge、およびLambdaを使用します。プライマリリージョンおよび二次リージョンにおいて、Regionalエンドポイントを持つAPI Gateway APIを構成します。Amazon Route 53フェイルオーバールーティングを実装して、プライマリリージョンから二次リージョンへのトラフィックを切り替えます。"
      },
      {
        "key": "C",
        "text": "Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch trafic from the primary Region to the secondary Region.",
        "text_jp": "AWS Backupを使用してAuroraデータベースとDynamoDBデータベースのバックアップを二次AWSリージョンに作成します。プライマリリージョンおよび二次リージョンにおいて、Regionalエンドポイントを持つAPI Gateway APIを構成します。Amazon Route 53フェイルオーバールーティングを実装して、プライマリリージョンから二次リージョンへのトラフィックを切り替えます。"
      },
      {
        "key": "D",
        "text": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch trafic from the primary Region to the secondary Region.",
        "text_jp": "AuroraグローバルデータベースとDynamoDBグローバルテーブルを設定して、データベースを二次AWSリージョンに複製します。プライマリリージョンおよび二次リージョンにおいて、Regionalエンドポイントを持つAPI Gateway APIを構成します。Amazon Route 53フェイルオーバールーティングを実装して、プライマリリージョンから二次リージョンへのトラフィックを切り替えます。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "C (57%) D (39%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Setting up an Aurora global database and DynamoDB global tables allows for real-time replication of data to a secondary AWS region, ensuring low Recovery Point Objective (RPO) of 2 hours.",
        "situation_analysis": "The requirement states an RPO of 2 hours and an RTO of 4 hours, which indicates the need for a solution that replicates data continuously and minimizes downtime during recovery.",
        "option_analysis": "Option A is the best as it provides active-active replication for both databases, while other options either don't meet the RPO or add unnecessary complexity. Options B and D, while suitable, do not provide the continuous replication of data necessary for the specified RPO.",
        "additional_knowledge": "It's crucial to assess the cost when implementing multi-region solutions, where Aurora Global Databases can be more cost-effective than setting up complex replication architectures.",
        "key_terminology": "Aurora Global Database, DynamoDB Global Tables, Recovery Point Objective (RPO), Amazon CloudFront, API Gateway",
        "overall_assessment": "Considering the requirements for minimizing downtime and data loss, option A stands out. The community's vote distribution reflects skepticism towards this solution but it actually meets the DR requirements effectively."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正解はAです。AuroraグローバルデータベースとDynamoDBグローバルテーブルを設定することで、データを二次AWSリージョンにリアルタイムで複製でき、2時間の復旧ポイント目標（RPO）を確保できます。",
        "situation_analysis": "要件はRPOが2時間でRTOが4時間であり、データを継続的に複製し、復旧中のダウンタイムを最小限に抑えるソリューションが必要です。",
        "option_analysis": "選択肢Aは、両方のデータベースに対してアクティブ-アクティブの複製を提供するため、最も良い選択肢です。他の選択肢は、RPOを満たさないか、不必要な複雑さを加えます。選択肢BやDは適切ですが、指定されたRPOに必要な継続的なデータ複製を提供しません。",
        "additional_knowledge": "マルチリージョンソリューションの実装においてコストを評価することが重要であり、Auroraグローバルデータベースは、複雑な複製アーキテクチャを設定するよりもコスト効果が高い場合があります。",
        "key_terminology": "Auroraグローバルデータベース、DynamoDBグローバルテーブル、復旧ポイント目標（RPO）、Amazon CloudFront、API Gateway",
        "overall_assessment": "ダウンタイムとデータ損失を最小限に抑える要件を考慮した場合、選択肢Aが際立ります。コミュニティの投票配分はこのソリューションに対して懐疑的ですが、実際にはDR要件を効果的に満たしています。"
      }
    ],
    "keywords": [
      "Aurora Global Database",
      "DynamoDB Global Tables",
      "Recovery Point Objective (RPO)",
      "Amazon CloudFront",
      "API Gateway"
    ]
  },
  {
    "No": "297",
    "question": "A company has a complex web application that leverages Amazon CloudFront for global scalability and performance. Over time, users report that\nthe web application is slowing down.\nThe company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. The cache metrics report indicates that\nquery strings on some URLs are inconsistently ordered and are specified sometimes in mixed-case letters and sometimes in lowercase letters.\nWhich set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?",
    "question_jp": "ある企業が、グローバルなスケーラビリティとパフォーマンスのためにAmazon CloudFrontを活用した複雑なウェブアプリケーションを持っています。 時間が経つにつれて、ユーザーからウェブアプリケーションが遅くなっているという報告があります。企業のオペレーションチームは、CloudFrontのキャッシュヒット率が着実に低下していることを報告しています。キャッシュのメトリクスレポートによると、いくつかのURLのクエリ文字列が不規則に並べられ、時には混合大文字で、時には小文字で指定されていると言います。ソリューションアーキテクトは、キャッシュヒット率をできるだけ早く向上させるために、どのアクションセットを実行するべきですか？",
    "choices": [
      {
        "key": "A",
        "text": "Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger to invoke the function.",
        "text_jp": "Lambda@Edge関数をデプロイして、パラメーターを名前でソートし、小文字に強制します。関数を呼び出すためにCloudFrontビューワーリクエストトリガーを選択します。"
      },
      {
        "key": "B",
        "text": "Update the CloudFront distribution to disable caching based on query string parameters.",
        "text_jp": "クエリ文字列パラメーターに基づくキャッシングを無効にするようにCloudFrontディストリビューションを更新します。"
      },
      {
        "key": "C",
        "text": "Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase.",
        "text_jp": "ロードバランサーの後にリバースプロキシをデプロイして、アプリケーションから発信されるURLを後処理し、URL文字列を小文字に強制します。"
      },
      {
        "key": "D",
        "text": "Update the CloudFront distribution to specify casing-insensitive query string processing.",
        "text_jp": "CloudFrontディストリビューションを更新して、大文字と小文字を区別しないクエリ文字列処理を指定します。"
      }
    ],
    "answer_key": "A",
    "community_vote_distribution": "A (100%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is A. Deploying a Lambda@Edge function to sort parameters and enforce lowercase will normalize the query string format, improving cache hit ratio.",
        "situation_analysis": "The issue arises from inconsistent query string parameter ordering and casing, which leads to lower cache efficiency.",
        "option_analysis": "Option A directly tackles the inconsistencies by ensuring all parameter names are lowercase. Option B would eliminate caching opportunities, which is counterproductive. Option C implements a less efficient method post-load balancer and does not address query string issues directly. Option D would not resolve the existing inconsistencies in the query strings.",
        "additional_knowledge": "Utilizing Lambda@Edge is a common approach to address caching problems in AWS.",
        "key_terminology": "Lambda@Edge, CloudFront, query string parameters, cache hit ratio, CDN",
        "overall_assessment": "Option A is the most effective and efficient choice for quickly fixing the cache hit ratio issue. The community also supports this option unanimously."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはAです。Lambda@Edge関数をデプロイしてパラメーターをソートし、小文字に強制することで、クエリ文字列のフォーマットを標準化し、キャッシュヒット率を改善します。",
        "situation_analysis": "問題は、一貫性のないクエリ文字列パラメーターの順序と大文字と小文字の使用に起因し、キャッシュ効率が低下しています。",
        "option_analysis": "選択肢Aは、不整合を直接解決し、すべてのパラメーター名を小文字にします。選択肢Bはキャッシングの機会を排除するため、逆効果です。選択肢Cは、ロードバランサー後に効率の悪い方法を実装し、クエリ文字列の問題に直接対処していません。選択肢Dは、既存のクエリ文字列の不整合を解決しません。",
        "additional_knowledge": "Lambda@Edgeの利用は、AWSにおけるキャッシング問題に対処する一般的なアプローチです。",
        "key_terminology": "Lambda@Edge、CloudFront、クエリ文字列パラメーター、キャッシュヒット率、CDN",
        "overall_assessment": "選択肢Aは、キャッシュヒット率の問題を迅速に解決するための最も効果的で効率的な選択です。コミュニティもこの選択肢を全会一致で支持しています。"
      }
    ],
    "keywords": [
      "Lambda@Edge",
      "CloudFront",
      "query string parameters",
      "cache hit ratio",
      "CDN"
    ]
  },
  {
    "No": "298",
    "question": "A company runs an ecommerce application in a single AWS Region. The application uses a five-node Amazon Aurora MySQL DB cluster to store\ninformation about customers and their recent orders. The DB cluster experiences a large number of write transactions throughout the day.\nThe company needs to replicate the data in the Aurora database to another Region to meet disaster recovery requirements. The company has an\nRPO of 1 hour.\nWhich solution will meet these requirements with the LOWEST cost?",
    "question_jp": "ある企業は、単一のAWSリージョンでeコマースアプリケーションを運営している。このアプリケーションは、顧客や最近の注文に関する情報を保存するために、5ノードのAmazon Aurora MySQL DBクラスターを使用している。このDBクラスターは、1日の間に大量の書き込みトランザクションを経験している。企業は、災害復旧要件を満たすためにAuroraデータベース内のデータを別のリージョンに複製する必要がある。企業は1時間のRPOを有している。どのソリューションが最低コストでこれらの要件を満たすことができるか？",
    "choices": [
      {
        "key": "A",
        "text": "Modify the Aurora database to be an Aurora global database. Create a second Aurora database in another Region.",
        "text_jp": "AuroraデータベースをAuroraグローバルデータベースに変更する。別のリージョンに2つ目のAuroraデータベースを作成する。"
      },
      {
        "key": "B",
        "text": "Enable the Backtrack feature for the Aurora database. Create an AWS Lambda function that runs daily to copy the snapshots of the database to a backup Region.",
        "text_jp": "AuroraデータベースにBacktrack機能を有効にする。データベースのスナップショットをバックアップリージョンにコピーするために毎日実行されるAWS Lambda関数を作成する。"
      },
      {
        "key": "C",
        "text": "Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region.",
        "text_jp": "AWSデータベース移行サービス（AWS DMS）を使用する。Auroraデータベースから別のリージョンのAmazon S3バケットに継続的な変更を複製するDMS変更データキャプチャ（CDC）タスクを作成する。"
      },
      {
        "key": "D",
        "text": "Turn off automated Aurora backups. Configure Aurora backups with a backup frequency of 1 hour. Specify another Region as the destination Region. Select the Aurora database as the resource assignment.",
        "text_jp": "自動Auroraバックアップをオフにする。バックアップ頻度を1時間に設定してAuroraバックアップを構成する。別のリージョンを宛先リージョンとして指定する。Auroraデータベースをリソース割り当てとして選択する。"
      }
    ],
    "answer_key": "C",
    "community_vote_distribution": "C (77%) A (23%)",
    "page_images": [],
    "explanation_en": [
      {
        "answer_and_key_points": "The correct answer is C. Using AWS DMS for change data capture is a cost-effective solution for data replication with a 1-hour RPO.",
        "situation_analysis": "The organization needs to replicate data for disaster recovery while maintaining a low-cost solution. RPO of 1 hour indicates that data can be delayed over a short period without significant loss.",
        "option_analysis": "Option C is practical, allowing continuous data replication using DMS. Option A could be costlier due to the need for an additional Aurora cluster. Option B, while reliable, involves unnecessary overhead with Lambda and snapshots. Option D is inadequate as it turns off automated backups, which could lead to potential data loss.",
        "additional_knowledge": "It's crucial to evaluate potential costs associated with increased data transfer and storage in AWS S3.",
        "key_terminology": "AWS DMS, change data capture, disaster recovery, RPO, Amazon S3",
        "overall_assessment": "The question effectively assesses knowledge of AWS services available for database replication and the cost implications of different approaches. Community support for choice C indicates strong alignment with AWS best practices."
      }
    ],
    "explanation_jp": [
      {
        "answer_and_key_points": "正しい答えはCである。変更データキャプチャのためにAWS DMSを使用することは、1時間のRPOを持つデータ複製に対するコスト効果の高いソリューションである。",
        "situation_analysis": "組織は、低コストのソリューションを維持しながら災害復旧のためのデータ複製が必要である。1時間のRPOは、データが短期間遅延しても重大な損失がないことを示している。",
        "option_analysis": "CオプションはWDSでの継続的なデータ複製を可能にするため、実用的である。Aオプションは、追加のAuroraクラスターが必要になるためコストが高くなる可能性がある。Bオプションは信頼性があるが、Lambdaとスナップショットに不必要なオーバーヘッドが発生する。Dオプションは、自動バックアップをオフにするため、潜在的なデータ損失につながる可能性がある。",
        "additional_knowledge": "AWS S3でのデータ転送とストレージに関連する潜在的なコストを評価することが重要である。",
        "key_terminology": "AWS DMS、変更データキャプチャ、災害復旧、RPO、Amazon S3",
        "overall_assessment": "この質問は、データベース複製に利用可能なAWSサービスの知識と異なるアプローチのコスト影響を効果的に評価している。選択肢Cに対するコミュニティの支持は、AWSのベストプラクティスに強く一致していることを示している。"
      }
    ],
    "keywords": [
      "AWS DMS",
      "change data capture",
      "disaster recovery",
      "RPO",
      "Amazon S3"
    ]
  }
]